@inproceedings{10.1145/3623504.3623570,
author = {van Rozen, Riemer},
title = {Game Engine Wizardry for Programming Mischief},
year = {2023},
isbn = {9798400703997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623504.3623570},
doi = {10.1145/3623504.3623570},
abstract = {Programming grants individuals the capability to design, create, and bring ideas to life. To improve their skills, programmers require powerful languages and programming environments for understanding the impact of gradual code changes. We investigate how modern game engine technology can be leveraged for creating visual input and feedback mechanisms that drive exploratory and live programming.  
In this paper, we report experiences on creating a visual programming environment for Machinations, a domain-specific language for game design. We share initial findings on how to automate the development of graph- and tree- based editors in Godot, an open source game engine. Our results show that today’s game engine technology provides a solid foundation for future programming language research.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Programming Abstractions and Interactive Notations, Tools, and Environments},
pages = {36–43},
numpages = {8},
keywords = {programming environments, live programming, language workbenches, game engines},
location = {Cascais, Portugal},
series = {PAINT 2023}
}

@inproceedings{10.1145/3643658.3643918,
author = {Pinto Gomez, Carlos and Petrillo, Fabio},
title = {Improving Bug Reproduction through Game Engine State Analysis},
year = {2024},
isbn = {9798400705618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643658.3643918},
doi = {10.1145/3643658.3643918},
abstract = {In video game development, bug detection and reproduction are crucial for productivity. Automating these tasks has been a longstanding goal in the industry to reduce quality management costs. While Artificial Intelligence (AI) offers avenues for automation, its integration into the game development cycle is often cost-prohibitive.This paper presents a novel detection technique that leverages data readily available from the game engine and a time-saving tool for bug reproduction. Our results demonstrate a significant improvement in the bug detection and reproduction workflow, reducing the time spent by testers and developers by one-third compared to manual methods without compromising on manual testers' detection accuracy.The primary aim of this paper is to introduce the technique and validate its compatibility with game engine workflows. Utilizing the Unity engine, the developed tool specifically targets camera clipping bugs. Thus, the proposed technique, while effective, is somewhat limited in scope. For broader applications, source code adaptations are necessary to integrate with other game engines and detect various bug types.},
booktitle = {Proceedings of the ACM/IEEE 8th International Workshop on Games and Software Engineering},
pages = {28–35},
numpages = {8},
keywords = {video games, bugs, quality assurance, testing},
location = {Lisbon, Portugal},
series = {GAS '24}
}

@inproceedings{10.1145/3590837.3590936,
author = {Chugh, Dr. Aarti and Jain, Dr. Charu and Kumar, Dr. Anil},
title = {Design of Artificial Intelligence Enabled Game Engine},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590936},
doi = {10.1145/3590837.3590936},
abstract = {Game engines are primarily designed for the development of video games but can also be used as rendering engines for 2D and 3D graphics. They include all relevant libraries and supported programs. Developers can use them to construct games for game consoles and other types of computer softwares. They also aid in the porting of games to multiple platforms. Developers look for platforms which are easily available and provide facilities to create impressive, interesting and animated games. But these software are difficult to run on everyday machines, especially laptops, limiting their use to professionals with dedicated systems. This paper discusses design of new game engine with all the standard features of a rendering engine. The artificial intelligence (AI) aspect of the engine requires rendering of visual data at a lower resolution to improve performance and then using trained Deep Networks to upscale the image to the native resolution of the user's monitor. The AI will ensure that upscaling will be adding detail rather than averaging the nearby pixels.CCS CONCEPTS • Software and its engineering • Software creation and management • Designing software • Software implementation planning • Software design techniques},
booktitle = {Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
articleno = {99},
numpages = {5},
keywords = {Artificial Intelligence, Deep Learning, Game Design, Game Engine, Image Rendering},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@inproceedings{10.1145/3631085.3631225,
author = {De Oliveira, Saulo Soares and Souza, Carlos Henrique R. and Silva, Jefferson Carvalho and Carvalho, S\'{e}rgio T.},
title = {Towards Scalable Cloud Gaming Systems: Decoupling Physics from the Game Engine},
year = {2024},
isbn = {9798400716270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631085.3631225},
doi = {10.1145/3631085.3631225},
abstract = {By leveraging cloud computing resources, cloud gaming, also known as Games-as-a-Service (GaaS), has emerged as a new computer game delivery paradigm that promises gaming anywhere, anytime, on any device. In this regard, game engine architectures are one of the main objects of study when adapting digital games to cloud gaming. However, most of the proposed cloud gaming architectures cannot take full advantage of cloud computing resources to offer system scalability due to a monolith design. In this paper, we evaluate the impacts of a distributed architecture in terms of system performance by employing the service-oriented architecture paradigm to offload the game physics calculations as a decoupled system. As a differential, our work contributes to the discussion by implementing a proof-of-concept to compare the game engine performance between the monolith and distributed approaches by utilizing a modern public cloud provider, which as far as we know, has not yet been done in the literature. As a result, the distributed approach had a better performance in computation-intensive physics calculations scenarios. In this case, the communication overhead is outweighed by performance gains, as the physics engine was deployed on a compute-optimized VM. This further indicates the benefits of decoupling game engine systems, not only better game performance but also taking a step further for a scalable cloud gaming system, which can scale better both vertically and horizontally.},
booktitle = {Proceedings of the 22nd Brazilian Symposium on Games and Digital Entertainment},
pages = {151–160},
numpages = {10},
keywords = {Cloud computing, Cloud gaming, Distributed architecture, Games-as-a-Service, Scalability},
location = {Rio Grande (RS), Brazil},
series = {SBGames '23}
}

@inproceedings{10.1145/3524458.3547126,
author = {Mazzuca, Laura and Garbugli, Andrea and Sabbioni, Andrea and Bujari, Armir and Corradi, Antonio},
title = {Towards a Resource-aware Middleware Support for Distributed Game Engine Design},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524458.3547126},
doi = {10.1145/3524458.3547126},
abstract = {Recently, we are witnessing an increasing interest in a paradigm shift in the way video games are designed and implemented. Starting from an era where a single developer was in charge of the whole creative process, we have moved now toward extremely large groups with a multi-layered organisation. Moreover, today’s game engines suffer from a number of architectural constraints, and will not likely be able to meet the flexibility and scalability required by game developers of the next generation. This increasing complexity, the tremendous growth of projects size, and the rapidly evolving AR/VR trend, call for the adoption of agile development and cost-effective management approaches leveraging on distributed computing environments and primitives. In this work, we present the concept of a distributed game engine architecture, which relies on a resource-aware middleware solution providing run-time quality support to components spanning edge-cloud environments.},
booktitle = {Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
pages = {409–413},
numpages = {5},
keywords = {Edge Cloud Computing, Gaming as a Service, Microservices},
location = {Limassol, Cyprus},
series = {GoodIT '22}
}

@inproceedings{10.1145/3641236.3665165,
author = {Johnson, Justin},
title = {Recipes for Creating Digital Game Prototypes},
year = {2024},
isbn = {9798400705182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641236.3665165},
doi = {10.1145/3641236.3665165},
abstract = {This lab introduces aspiring or current game designers with limited programming knowledge, to concepts that can be used to create digital game prototypes. Four basic concepts are presented as ingredients (vectors, time, collisions, and lines and rays), each with explanations and examples demonstrated in Unreal and Unity. Then examples are provided on how the ingredients can be combined into recipes to create common gameplay mechanics for prototyping. This is not an exhaustive list of everything someone needs to develop digital game prototypes, however, it serves as a good starting point for beginners who may want to start designing and prototyping games with limited knowledge of programming in game engines. This lab assumes a basic understanding of Unreal, Unity, or a comparable game engine, but programming experience is not required. Upon completion of this lab, participants should have several recipes they can use, build on, and modify for creating simple game play mechanics for prototyping. The example projects used in the lab will be available for download.},
booktitle = {ACM SIGGRAPH 2024 Labs},
articleno = {12},
numpages = {2},
keywords = {game design, game engine, prototyping},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@article{10.1145/3588440,
author = {Siekkinen, Matti and K\"{a}m\"{a}r\"{a}inen, Teemu},
title = {Neural Network Assisted Depth Map Packing for Compression Using Standard Hardware Video Codecs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3588440},
doi = {10.1145/3588440},
abstract = {Depth maps are needed by various graphics rendering and processing operations. Depth map streaming is often necessary when such operations are performed in a distributed system and it requires in most cases fast performing compression, which is why video codecs are often used. Hardware implementations of standard video codecs enable relatively high resolution and frame rate combinations, even on resource constrained devices, but unfortunately those implementations do not currently support RGB+depth extensions. However, they can be used for depth compression by first packing the depth maps into RGB or YUV frames. We investigate depth map compression using a combination of depth map packing followed by encoding with a standard video codec. We show that the precision at which depth maps are packed has a large and nontrivial impact on the resulting error caused by the combination of the packing scheme and lossy compression when the bitrate is constrained. Consequently, we propose a variable precision packing scheme assisted by a neural network model that predicts the optimal precision for each depth map given a bitrate constraint. We demonstrate that the model yields near optimal predictions and that it can be integrated into a game engine with very low overhead using modern hardware.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jun},
articleno = {174},
numpages = {20},
keywords = {Depth map, video encoding, neural network, game engine}
}

@inproceedings{10.1145/3610548.3618230,
author = {Shan, Mengyi and Curless, Brian and Kemelmacher-Shlizerman, Ira and Seitz, Steve},
title = {Animating Street View},
year = {2023},
isbn = {9798400703157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610548.3618230},
doi = {10.1145/3610548.3618230},
abstract = {We present a system that automatically brings street view imagery to life by populating it with naturally behaving, animated pedestrians and vehicles. Our approach is to remove existing people and vehicles from the input image, insert moving objects with proper scale, angle, motion and appearance, plan paths and traffic behavior, as well as render the scene with plausible occlusion and shadowing effects. The system achieves these by reconstructing the still image street scene, simulating crowd behavior, and rendering with consistent lighting, visibility, occlusions, and shadows. We demonstrate results on a diverse range of street scenes including regular still images and panoramas.},
booktitle = {SIGGRAPH Asia 2023 Conference Papers},
articleno = {64},
numpages = {12},
keywords = {Single image animation, crowd simulation, game engine, rendering, scene reconstruction},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@inproceedings{10.1145/3573382.3616035,
author = {Mulvany, Gerard T.},
title = {Designing for Digital Hybridisation of Theatre},
year = {2023},
isbn = {9798400700293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573382.3616035},
doi = {10.1145/3573382.3616035},
abstract = {This doctoral consortium submission presents a research program investigating the transition to digital hybridisation in theatre, specifically focusing on passive and active modalities facilitated by game-engine technologies. By adopting a "research through design" methodology, three theatre productions are being hybridised in collaboration with Malthouse Theatre to explore the potential of hybridisation in theatre and its impact on the evolving landscape of digital experiences in the performing arts. Each production employs a pragmatist approach and Research through Design methodology, supplemented by participant studies, to comprehensively explore hybridisation design practices and evaluate their effectiveness. Although participant study results are pending, existing publications provide initial insights into the projects. The ultimate goal of this research is to contribute to the understanding of design processes and practices in digital hybridisation, thus advancing the field of contemporary theatre and digital experiences.},
booktitle = {Companion Proceedings of the Annual Symposium on Computer-Human Interaction in Play},
pages = {330–332},
numpages = {3},
keywords = {archival, creative methodologies, game-engine, hybridisation, metaverse, preservation, theatre},
location = {Stratford, ON, Canada},
series = {CHI PLAY Companion '23}
}

@inproceedings{10.1145/3643832.3661891,
author = {Lee, Jingyu and Kim, Hyunsoo and Kim, Minjae and Chun, Byung-Gon and Lee, Youngki},
title = {Maestro: The Analysis-Simulation Integrated Framework for Mixed Reality},
year = {2024},
isbn = {9798400705816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643832.3661891},
doi = {10.1145/3643832.3661891},
abstract = {Mixed reality devices with near-eye displays unlock new possibilities for innovation and user experiences. Mixed reality applications require a new unified framework that enables seamless analysis of the real world and simulation of realistic virtual content. Designing such a framework faces various challenges, including huge programming efforts of analysis and simulation pipelines, and inconsistencies between real-world and virtual content caused by end-to-end processes across pipelines.This paper proposes Maestro, the analysis-simulation integrated framework for mixed reality applications. Maestro provides a programming model for effective application representation and control, aiding runtime optimization. Maestro runtime takes an object-level execution approach to minimize misalignment, integrating both simulation and analysis pipelines for applications to process individual objects based on their latency sensitivity. Our evaluation shows that Maestro improves streaming accuracy by up to 1.6\texttimes{} compared to existing frameworks and effectively expresses nine qualitatively distinct workloads expected in prospective mixed-reality applications.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services},
pages = {99–112},
numpages = {14},
keywords = {mixed reality, mobile deep learning, multi-DNN analysis, game engine, real-time simulation},
location = {Minato-ku, Tokyo, Japan},
series = {MOBISYS '24}
}

@article{10.1145/3651304,
author = {Medin, Safa C. and Li, Gengyan and Du, Ruofei and Garbin, Stephan and Davidson, Philip and Wornell, Gregory W. and Beeler, Thabo and Meka, Abhimitra},
title = {FaceFolds: Meshed Radiance Manifolds for Efficient Volumetric Rendering of Dynamic Faces},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3651304},
doi = {10.1145/3651304},
abstract = {3D rendering of dynamic face captures is a challenging problem, and it demands improvements on several fronts---photorealism, efficiency, compatibility, and configurability. We present a novel representation that enables high-quality volumetric rendering of an actor's dynamic facial performances with minimal compute and memory footprint. It runs natively on commodity graphics soft- and hardware, and allows for a graceful trade-off between quality and efficiency. Our method utilizes recent advances in neural rendering, particularly learning discrete radiance manifolds to sparsely sample the scene to model volumetric effects. We achieve efficient modeling by learning a single set of manifolds for the entire dynamic sequence, while implicitly modeling appearance changes as temporal canonical texture. We export a single layered mesh and view-independent RGBA texture video that is compatible with legacy graphics renderers without additional ML integration. We demonstrate our method by rendering dynamic face captures of real actors in a game engine, at comparable photorealism to state-of-the-art neural rendering techniques at previously unseen frame rates.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {may},
articleno = {23},
numpages = {17},
keywords = {Face Modeling, Neural Radiance Fields, Novel View Synthesis, Performance Capture, Volumetric Rendering}
}

@inproceedings{10.1145/3643832.3661414,
author = {Lee, Jingyu and Kim, Hyunsoo and Kim, Minjae and Chun, Byung-Gon and Lee, Youngki},
title = {Poster: Maestro: The Analysis-Simulation Integrated Framework for Mixed Reality},
year = {2024},
isbn = {9798400705816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643832.3661414},
doi = {10.1145/3643832.3661414},
abstract = {The recent development of DNN and hardware has created new opportunities for mixed-reality applications. These applications demand the ability to analyze the real world and simulate realistic virtual content. However, designing mixed-reality applications faces diverse challenges due to the absence of a unified framework, such as huge programming effort and inconsistencies between the real scene and virtual content induced by end-to-end latency.This paper proposes Maestro, an analysis-simulation integrated framework for mixed-reality applications. Maestro provides a programming model for effective application representation and control, aiding runtime optimization. Maestro runtime takes an object-level execution approach to minimize misalignment, integrating both simulation and analysis pipelines for applications to process individual objects based on their latency sensitivity.},
booktitle = {Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services},
pages = {670–671},
numpages = {2},
keywords = {mixed reality, mobile deep learning, multi-DNN analysis, game engine, real-time simulation},
location = {Minato-ku, Tokyo, Japan},
series = {MOBISYS '24}
}

@inproceedings{10.1145/3641233.3664728,
author = {Luhn, Philip and Liss, Adam and Willard, David},
title = {Development of Real-Time QA/QC Tools for AEC in Unity},
year = {2024},
isbn = {9798400705151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641233.3664728},
doi = {10.1145/3641233.3664728},
abstract = {Here, we are focusing on how our real-time visualizations are being used to improve the QA/QC process. While the AEC industry has utilized 3D CAD design software for years, the review process still typically involves commenting on printed 2D drawings or PDFs. We have developed a QA/QC tool that allows our engineers and project managers to review designs in real-time 3D, placing comment markers in 3D space for others to see. Built in Unity, this tool supports viewing and commenting on everything from individual CAD models and components to sprawling miles-long infrastructure projects, with all markup data being stored securely in the cloud for easy access to authorized contributors.This is opposed to the traditional method, which involved building a 3D environment, rendering a video, sending it out for review, then having the review team take screenshots and compile it in a PDF. By allowing reviewers to mark up the 3D environment directly, we are drastically speeding up the iteration process. In addition, users have better insight into the current status of revisions as markups can be configured to different stages of completion as updates are made and the project evolves. We have also added multi-user capabilities – using game engine networking functionality allows for multiple users to be in the tool at once, interacting and communicating within the same environment together in real-time. And our tool is able to be built for multiple platforms, so users can access and interact with it using web, mobile devices, VR, etc. As a result, not only does using a real-time game engine speed up the QA/QC process, it also opens new opportunities for improved communication and collaboration.},
booktitle = {ACM SIGGRAPH 2024 Talks},
articleno = {34},
numpages = {2},
keywords = {QA, QC, Unity, engineering, quality assurance, quality control, real-time rendering, visualization},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3643658.3643921,
author = {Strodick, Kathleen and Schattkowsky, Tim},
title = {Visual Scripting in Unity: A Comparative Analysis of Existing Frameworks},
year = {2024},
isbn = {9798400705618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643658.3643921},
doi = {10.1145/3643658.3643921},
abstract = {This paper explores the use of visual scripting languages in game engines, particularly in Unity, comparing different third-party frameworks. The typical applications of visual scripting in games are examined and compared, including dialog, Non-Playable Character behavior, camera movement, and more. Unity's built-in visual scripting tool, formerly Bolt, is examined in detail and compared to various third-party commercial frameworks such as Node Canvas, Playmaker, Behavior Designer and Flow Canvas.},
booktitle = {Proceedings of the ACM/IEEE 8th International Workshop on Games and Software Engineering},
pages = {44–49},
numpages = {6},
keywords = {visual scripting, visual languages, game development, unity, end user programming},
location = {Lisbon, Portugal},
series = {GAS '24}
}

@inproceedings{10.1145/3661167.3661184,
author = {Paduraru, Ciprian and Cernat, Marina and Stefanescu, Alin},
title = {Automated evaluation of game content display using deep&nbsp;learning},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661184},
doi = {10.1145/3661167.3661184},
abstract = {The gaming industry is an important part of today’s economy. Statistically, many quality issues are found by users in released products or updates. One reason for this is that testing methods from general software development cannot be transferred to test visual outputs without significant human effort. This work focuses on a major problem in this area, namely testing the correctness of the images displayed by cameras in relation to the content of the environment they are intended to see. The techniques used are a combination of state-of-the-art computer vision methods adapted to our specific use cases. Evaluation is performed in a well-known soccer game engine and shows that the proposed methods have the potential to significantly reduce manual work and development costs while improving product quality.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {421–424},
numpages = {4},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3626203.3670579,
author = {Drake, Justin and Gaffney, Niall and Freeman, Nathan and Ferlanti, Erik and Fonner, John},
title = {Human-powered AI Gym: Lessons Learned as the Test and Evaluation Team for the DARPA SHADE Program: Human-powered AI Gym},
year = {2024},
isbn = {9798400704192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626203.3670579},
doi = {10.1145/3626203.3670579},
abstract = {We present the best practices and collaborative methods used during the DARPA SHADE program, exploring the use of AI agents to advise diplomatic negotiations. In this accelerated 18-month program, we created an environment for seven research performers to collaborate in the development of an AI Gym. This AI Gym provided an area in which AI developed by the various teams could compete, negotiate, and explain their intentions as they played the game of Diplomacy, allowing us to evaluate their abilities against other digital and human agents.},
booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing},
articleno = {38},
numpages = {5},
keywords = {AI Gym, Collaborative Development, Diplomacy, Explainable AI},
location = {Providence, RI, USA},
series = {PEARC '24}
}

@inproceedings{10.1145/3578244.3583738,
author = {Nayrolles, Mathieu},
title = {Pushing the Limits of Video Game Performance: A Performance Engineering Perspective},
year = {2023},
isbn = {9798400700682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578244.3583738},
doi = {10.1145/3578244.3583738},
abstract = {Ubisoft constantly pushes the boundaries of game development to create immersive worlds that capture the imagination of millions of players worldwide. To achieve this, performance engineering plays a crucial role in ensuring that games run smoothly on various platforms and devices. In this talk, we will explore the latest advancements in the field of performance engineering for video games, focusing on runtime performance, network optimization, backend and database optimization, and cloud gaming. We will discuss how machine learning techniques enhance classical profiling and optimize game engine scheduling.Additionally, we will address the challenges of deterministic replication of assets between clients and optimizing micro-services for cloud gaming experiences. Lastly, we will touch on the importance of performance engineering for non-code aspects of game development, such as animation, textures, props, and assets.},
booktitle = {Proceedings of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {1},
numpages = {1},
keywords = {performance engineering, video games},
location = {Coimbra, Portugal},
series = {ICPE '23}
}

@inproceedings{10.1145/3573381.3597231,
author = {Tious, Amar and Vigier, Toinon and Ricordel, Vincent},
title = {Physically-based Lighting of 3D Point Clouds for Quality Assessment},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3597231},
doi = {10.1145/3573381.3597231},
abstract = {Point clouds are acknowledged as an essential data structure to represent 3D objects in many use cases, notably immersive experience settings such as Virtual, Augmented or Mixed Reality. This work is the first part of a research project on immersive Quality Assessment of point clouds in different lighting. In this report, I focus mainly on the physically-based rendering of such data in Unity 3D, and the impact of point cloud compression when considering various lighting conditions on the objects. These first observations and results will serve in the implementation of a 6DoF immersive experiment setting for subjective quality assessment.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {423–426},
numpages = {4},
keywords = {3D graphics, Physically-Based Render, Point Clouds, Unity Game Engine, Visual Quality Assessment},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3664475.3664525,
author = {Shah, Michael D.},
title = {Introduction to Scripting in Blender3D: Computational Geometry Algorithms},
year = {2024},
isbn = {9798400706837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664475.3664525},
doi = {10.1145/3664475.3664525},
abstract = {In this course students will be extending the powerful Blender 3D toolsuite through the scripting ecosystem. The goal is to get users who are familiar or otherwise just starting Blender3D to start scripting, creating add-ons, and experimenting by implementing various simple algorithms from computational geometry. Some concrete activities include installation, setup, navigating the scripting user interface, understanding how to rapidly prototype scripts, how to create larger script projects, and how to debug. Some example algorithms that will be implemented include generating geometry, computing a bounding box, and computing a convex hull with a custom user interface. This course is targeted towards artists with minimal programming experience, or programmers who want to write tools that integrate into the Blender 3D ecosystem. Attendees will benefit by learning otherwise how Blender3D can be utilized as a sandbox for research experiments, app development, rapid prototyping, or otherwise more industrial uses like building a content pipeline.},
booktitle = {ACM SIGGRAPH 2024 Courses},
articleno = {16},
numpages = {83},
location = {Denver, CO, USA},
series = {SIGGRAPH Courses '24}
}

@inproceedings{10.1145/3610543.3628795,
author = {Chalmers, Andrew and Zhao, Junhong and Khuan Hoh, Weng and Drown, James and Finnie, Simon and Yao, Richard and Lin, James and Wilmott, James and Dey, Arindam and Billinghurst, Mark and Rhee, Taehyun},
title = {A Motion-Simulation Platform to Generate Synthetic Motion Data for Computer Vision Tasks},
year = {2023},
isbn = {9798400703140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610543.3628795},
doi = {10.1145/3610543.3628795},
abstract = {We developed the Motion-Simulation Platform, a platform running within a game engine that is able to extract both RGB imagery and the corresponding intrinsic motion data (i.e., motion field). This is useful for motion-related computer vision tasks where large amounts of intrinsic motion data are required to train a model. We describe the implementation and design details of the Motion-Simulation Platform. The platform is extendable, such that any scene developed within the game engine is able to take advantage of the motion data extraction tools. We also provide both user and AI-bot controlled navigation, enabling user-driven input and mass automation of motion data collection.},
booktitle = {SIGGRAPH Asia 2023 Technical Communications},
articleno = {21},
numpages = {4},
keywords = {data generation, machine learning, motion, simulation, user study},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@article{10.1145/3549486,
author = {Evin, Inan and H\"{a}m\"{a}l\"{a}inen, Perttu and Guckelsberger, Christian},
title = {Cine-AI: Generating Video Game Cutscenes in the Style of Human Directors},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549486},
doi = {10.1145/3549486},
abstract = {Cutscenes form an integral part of many video games, but their creation is costly, time-consuming, and requires skills that many game developers lack. While AI has been leveraged to semi-automate cutscene production, the results typically lack the internal consistency and uniformity in style that is characteristic of professional human directors. We overcome this shortcoming with Cine-AI, an open-source procedural cinematography toolset capable of generating in-game cutscenes in the style of eminent human directors. Implemented in the popular game engine Unity, Cine-AI features a novel timeline and storyboard interface for design-time manipulation, combined with runtime cinematography automation. Via two user studies, each employing quantitative and qualitative measures, we demonstrate that Cine-AI generates cutscenes that people correctly associate with a target director, while providing above-average usability. Our director imitation dataset is publicly available, and can be extended by users and film enthusiasts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {223},
numpages = {23},
keywords = {cutscene, imitation, procedural cinematography, storyboard, video games}
}

@inproceedings{10.1145/3582437.3582483,
author = {de Andrade, Diogo and Fachada, Nuno},
title = {Automated Generation of Map Pieces for Snappable Meshes},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582483},
doi = {10.1145/3582437.3582483},
abstract = {Snappable Meshes is an algorithm that procedurally generates 3D environments by iteratively selecting and linking pre-built map pieces. These pieces are triangular meshes annotated by designers with connectors marking potential links, and bounding volumes indicating where overlaps should be avoided. In this article, we present a method for automatically generating connectors and bounding volumes from generic non-manifold triangular meshes for use with the Snappable Meshes algorithm, minimizing artist/designer work, while encouraging iteration of map piece design, an essential part of successful environment generation.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {2},
numpages = {10},
keywords = {Unity game engine, computational geometry, computer games, procedural content generation},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.5555/3643142.3643359,
author = {Franke, Kai and St\"{u}rmer, J. Marius and Koch, Tobias},
title = {Automated Simulation and Virtual Reality Coupling for Interactive Digital Twins},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {While there are many efforts to simulate technical systems in virtual environments and provide a visual interaction for applications such as training, authoring and analysis, the process of generating applications still requires a lot of manual work. This is particularly critical in the context of interactive Digital Twins for resilience, where uncertain events can occur and every malfunction or mistreatment of any part of the system needs to be modeled. This paper presents an approach to model such systems in a modular way by automating the generation of its components for a game engine and simulators based on a common specification. Component instances are then synchronized bidirectionally across applications to achieve interaction between the game engine and simulators. An example hydraulic system is implemented and tested to demonstrate our approach, which needs minimal manual work by using predefined components. The solution can be extended by integrating more components and simulations.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2615–2626},
numpages = {12},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inproceedings{10.1145/3658852.3659095,
author = {Mccormick, John and Pyaraka, Jagannatha Charjee},
title = {Real Robot Dance Challenge: Exploring live and online dance challenge videos for robot movement},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658852.3659095},
doi = {10.1145/3658852.3659095},
abstract = {This paper describes a proposed robotic artwork for the MOCO 2024 conference. Real Robot Dance Challenge is a human-robot art installation that investigates both live human-robot interactions as well as human movement embedded in online videos as sources for the robot's movement. Dance is extremely challenging for a robot in terms of movement goals. Compared to typical industrial robot approaches, actions may have a comparative excess expenditure of energy, and style and expressivity becoming more important than accuracy and control. This work draws on the popular trend of dance challenges as seen on platforms such as Tik Tok [1]. The short format dances, often 10 – 30 seconds long, are choreographed with the intention of showcasing skills as well as challenging others to reinterpret the dance, often done with individual stylistic changes. RRDC challenges the robot to reinterpret the movement of human movers interacting live as well as dances contained in online videos. The artwork leverages the unity game engine as a mediating platform for the robot to access human movement and repurpose it for its own body. This allows the robot's movement to be procured from multiple sources. The unity environment can also be used to connect robots of different morphologies to the movement sources enabling flexibility in the engagement between human and robot. The robot moves beyond the control of programmed actions, able to respond with interpretations of the movement offered to it.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {35},
numpages = {4},
keywords = {AI, Human Robot Interaction, Robot Dance, Robot Dance Challenge, Robot Installation, Unity},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3574131.3574442,
author = {Li, Wanwan},
title = {PM4VR: A Scriptable Parametric Modeling Interface for Conceptual Architecture Design in VR},
year = {2023},
isbn = {9798400700316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574131.3574442},
doi = {10.1145/3574131.3574442},
abstract = {In this paper, we propose PM4VR, a novel scriptable parametric modeling interface for the Unity3D game engine which can be applied to VR-driven parametric modeling designs. By simplifying prevailing advanced programming languages such as C# and Java, we propose another programming language, named Java♭, to simplify the grammar and lower the programmer’s learning curve. By implementing a series of advanced parametric modeling techniques, we integrate our Java♭ compiler virtual machine with those functionalities which can facilitate interactive parametric modeling design process on the Unity3D game engine within immersive SteamVR environments. More specifically, in this paper, we introduce the Java♭ programming language, explain the implementation details of Java♭ compiler virtual machine, and discuss the experimental results of the interactive parametric modeling on conceptual architecture designs using PM4VR. Besides, a Supplementary Material with Java♭ programming examples is included.},
booktitle = {Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
articleno = {24},
numpages = {8},
keywords = {Compiler and Virtual Machine, Conceptual Architecture Design, Parametric Modeling, Programming Language, Virtual Reality},
location = {Guangzhou, China},
series = {VRCAI '22}
}

@inproceedings{10.1145/3613904.3642555,
author = {Huang, William and Ghahremani, Sam and Pei, Siyou and Zhang, Yang},
title = {WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642555},
doi = {10.1145/3613904.3642555},
abstract = {Existing pose estimation models perform poorly on wheelchair users due to a lack of representation in training data. We present a data synthesis pipeline to address this disparity in data collection and subsequently improve pose estimation performance for wheelchair users. Our configurable pipeline generates synthetic data of wheelchair users using motion capture data and motion generation outputs simulated in the Unity game engine. We validated our pipeline by conducting a human evaluation, investigating perceived realism, diversity, and an AI performance evaluation on a set of synthetic datasets from our pipeline that synthesized different backgrounds, models, and postures. We found our generated datasets were perceived as realistic by human evaluators, had more diversity than existing image datasets, and had improved person detection and pose estimation performance when fine-tuned on existing pose estimation models. Through this work, we hope to create a foothold for future efforts in tackling the inclusiveness of AI in a data-centric and human-centric manner with the data synthesis techniques demonstrated in this work. Finally, for future works to extend upon, we open source all code in this research and provide a fully configurable Unity Environment used to generate our datasets. In the case of any models we are unable to share due to redistribution and licensing policies, we provide detailed instructions on how to source and replace said models. All materials can be found at https://github.com/hilab-open-source/wheelpose.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {944},
numpages = {25},
keywords = {Accessibility, Data Synthesis, Pose Estimation, Wheelchair Users},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3663532.3664466,
author = {Paduraru, Ciprian and Stefanescu, Alin and Jianu, Augustin},
title = {Unit Test Generation using Large Language Models for Unity Game Development},
year = {2024},
isbn = {9798400706745},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663532.3664466},
doi = {10.1145/3663532.3664466},
abstract = {Challenges related to game quality, whether occurring during initial release or after updates, can result in player dissatisfaction, media scrutiny, and potential financial setbacks. These issues may stem from factors like software bugs, performance bottlenecks, or security vulnerabilities. Despite these challenges, game developers often rely on manual playtesting, highlighting the need for more robust and automated processes in game development.  This research explores the application of Large Language Models (LLMs) for automating unit test creation in game development, with a specific focus on strongly typed programming languages like C++ and C#, widely used in the industry. The study centers around fine-tuning Code Llama, an advanced code generation model, to address common scenarios encountered in game development, including game engines and specific APIs or backends. Although the prototyping and evaluations primarily occurred within the Unity game engine, the proposed methods can be adapted to other internal or publicly available solutions. The evaluation outcomes demonstrate the effectiveness of these methods in enhancing existing unit test suites or automatically generating new tests based on natural language descriptions of class contexts and targeted methods.},
booktitle = {Proceedings of the 1st ACM International Workshop on Foundations of Applied Software Engineering for Games},
pages = {7–13},
numpages = {7},
keywords = {game development, large language models, unit testing},
location = {Porto de Galinhas, Brazil},
series = {FaSE4Games 2024}
}

@inproceedings{10.1145/3550495.3558222,
title = {Blender Scripting for Creative Coding Projects},
year = {2023},
isbn = {9781450394741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550495.3558222},
doi = {10.1145/3550495.3558222},
abstract = {This session introduces Blender as a tool for creative coding. Blender is open-source software for 3D modelling and animation that can also handle compositing, video editing, and 2D animation.},
booktitle = {SIGGRAPH Asia 2022 Courses},
articleno = {7},
numpages = {28},
location = {Daegu, Republic of Korea},
series = {SA '22}
}

@inproceedings{10.1145/3587819.3590985,
author = {Chung, Yu-Yen and Annaswamy, Thiru M. and Prabhakaran, Balakrishnan},
title = {Performance and User Experience Studies of HILLES: Home-based Immersive Lower Limb Exergame System},
year = {2023},
isbn = {9798400701481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587819.3590985},
doi = {10.1145/3587819.3590985},
abstract = {Head-Mounted Devices (HMDs) have become popular for home-based immersive gaming. However, using lower limb motion in the immersive virtual environment is still restricted. This work introduces an RGB-D camera-based motion capture system alongside a standalone HMD for Home-based Immersive Lower Limbs Exergame Systems (HILLES) in a seated pose. With the advance of neural network models, camera-based 3D body tracking accuracy is increasing. Nevertheless, the high demand for computing resources on model inference may compromise the game engine's performance. Accordingly, HILLES applies a distributed architecture to leverage the resources effectively. The system performances, such as frames per second and latency, are compared with a centralized system. For an immersive exergame, a pet walking around could raise safety issues. Hence, we also showcase that the camera system can provide an additional safety feature by combining an object detection model. Besides, another challenge in games focusing on lower limb interactions is the safe reachability of different virtual objects from a seated pose. Accordingly, in the user study, a stomping game with two reachability enhancements, including leg extension and seated navigation, is implemented based on the HILLES to evaluate and explore the gaming experience. The result shows that the system motivates the leg exercise, and the added enhancements may adjust the game difficulty. However, the enhancements may also distract users from focusing on leg exertion. The derived insight could benefit the lower limb exergame design in the future.},
booktitle = {Proceedings of the 14th ACM Multimedia Systems Conference},
pages = {62–73},
numpages = {12},
keywords = {lower limb, immersive exergame, azure kinect, microservices},
location = {Vancouver, BC, Canada},
series = {MMSys '23}
}

@inproceedings{10.1145/3641235.3664439,
author = {Maki, Nahomi},
title = {360° Animation Projects by Students with a Background in Traditional Stop-Motion Techniques},
year = {2024},
isbn = {9798400705175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641235.3664439},
doi = {10.1145/3641235.3664439},
abstract = {Innovations in media technology, exemplified by extended reality, have begun. Hence, the seamless presentation of media, such as animation, film, and games, is progressing at an impressive rate. Integrating these advanced media and experiences into educational environments has excellent potential. Similarly, students who have studied traditional stop-motion animation can discover new forms of expression and enhance their abilities by challenging themselves to incorporate advanced media. This study introduces a unique challenge of stop-motion animation in 360° as a first step toward VR for students who have learned traditional handcrafted object modeling and stop-motion animation. No game engine or VR painting application will be used to ensure that this assignment is free from technical hurdles. Instead, students will complete the project using only an omni-directional camera and standard video editing software.},
booktitle = {ACM SIGGRAPH 2024 Educator's Forum},
articleno = {19},
numpages = {2},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3641234.3671019,
author = {Lopez Mendez, Roberto and Li, Sicong and Liu, Hong Ji},
title = {The Future of Interaction with Mobile Game Characters},
year = {2024},
isbn = {9798400705168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641234.3671019},
doi = {10.1145/3641234.3671019},
abstract = {This poster showcases the future of player-NPC interaction in gaming: more natural, based on speech. The poster explains the implementation of verbal interaction with an NPC in a mobile game. Large Language Models (LLMs) open new ways of interacting in gaming, but the large size and memory footprint make deploying them on mobile a challenge. We have identified a 33M-parameter small Language Model (LM), TinyStories [Eldan and Li 2023], repurposed it to be conversational, and combined it with the standard Unity Multi-Layer Perceptron (MLP) ML-Agents [Juliani et&nbsp;al. 2020] model. The LM drives the interaction with the user, while the MLP performs the actions. A Sentence Similarity model [Reimers and Gurevych 2019] is then responsible for filtering user input to decide whether the NPC needs to communicate with the player or to perform an action. Running all these models locally on mobile inside a game engine like Unity can be very challenging in terms of optimization, but this poster demonstrates how it is possible.},
booktitle = {ACM SIGGRAPH 2024 Posters},
articleno = {59},
numpages = {2},
keywords = {language model, mobile games, speech recognition, speech-based user interaction, unity ml-agents, user interaction in games},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3625008.3625035,
author = {Mastra, Leonardo and Silva, Luiz J. S. and Raposo, Alberto and Silva, Vinicius da},
title = {Virtual Reality Dance Tracks from Skeletal Animations},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625035},
doi = {10.1145/3625008.3625035},
abstract = {This paper presents a novel approach for automatically generating Virtual Reality (VR) dance tracks, focusing on translating the movement of animated 3D models directly derived from music. Our method capitalizes on the recent advances in automated synthesis of animated 3D models from music, using this data to bridge the gap to fully automatic VR dance track creation. We introduce a novel plugin for the Unity game engine, facilitating the conversion of dance animations from the Choreomaster dataset into dance tracks for the VR game Synth Riders. This approach aims to guide dance while supporting creative movement interpretation and minimizing movement restriction. The paper offers a comprehensive review of the current state of VR dance experiences and music-to-animation synthesis, and an in-depth explanation of our plugin and its key components. Our approach has potential to enhance the creation of dance experiences in VR, reducing resource dependency and increasing versatility in dance styles. This research is a step towards fully automated VR dance track creation, potentially impacting realms of music, dance, and fitness in VR.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {248–253},
numpages = {6},
keywords = {Virtual Reality, animation, dance},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@article{10.1145/3592431,
author = {Weinrauch, Alexander and Tatzgern, Wolfgang and Stadlbauer, Pascal and Crickx, Alexis and Hladky, Jozef and Coomans, Arno and Winter, Martin and Mueller, Joerg H. and Steinberger, Markus},
title = {Effect-based Multi-viewer Caching for Cloud-native Rendering},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592431},
doi = {10.1145/3592431},
abstract = {With cloud computing becoming ubiquitous, it appears as virtually everything can be offered as-a-service. However, real-time rendering in the cloud forms a notable exception, where the cloud adoption stops at running individual game instances in compute centers. In this paper, we explore whether a cloud-native rendering architecture is viable and scales to multi-client rendering scenarios. To this end, we propose world-space and on-surface caches to share rendering computations among viewers placed in the same virtual world. We discuss how caches can be utilized on an effect-basis and demonstrate that a large amount of computations can be saved as the number of viewers in a scene increases. Caches can easily be set up for various effects, including ambient occlusion, direct illumination, and diffuse global illumination. Our results underline that the image quality using cached rendering is on par with screen-space rendering and due to its simplicity and inherent coherence, cached rendering may even have advantages in single viewer setups. Analyzing the runtime and communication costs, we show that cached rendering is already viable in multi-GPU systems. Building on top of our research, cloud-native rendering may be just around the corner.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {87},
numpages = {16},
keywords = {real-time rendering, ray tracing, distributed rendering, scalability, cloud computing}
}

@inproceedings{10.1145/3625008.3625010,
author = {Borges, Luiz Felipe Muniz Rocha and Viana, Pedro Henrique Porto and de Oliveira, Tain\~{a} Ribeiro and Martins, Thiago da Silva and Andre\~{a}o, Rodrigo Varej\~{a}o and Schimidt, Marcelo and Mestria, M\'{a}rio},
title = {Evaluating Virtual Reality Simulations for Wheel Loader Inspection},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625010},
doi = {10.1145/3625008.3625010},
abstract = {In this study, a virtual reality simulation game for the inspection of a wheel loader is proposed. It has a cost-effective way to provide training and to monitor the performance of machinery operators in a mining company. This game is used during their mandatory pre-operation safety inspection, a tool that is otherwise nonexistent for their specific needs. The simulator was developed using a game engine and built to operate with different virtual reality headsets. It aims to use a guide to perform inspection procedures on a wheel loader of a mining company. A team of professional collaborators from the mining company evaluated the simulator under several scenarios. The computational results showed that the simulator provided the employees with an initial experience before conducting an actual field inspection of the wheel loader. In this sense, the simulator increases the safety of employees and does not expose them to risky situations, owing to a deeper knowledge of inspection procedures. Moreover, the simulator has a lower cost compared to an outsourced contract and can decrease machine downtime. Finally, the simulator is effective for training because it provides professional employees with a sense of realism.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {8–16},
numpages = {9},
keywords = {Inspection, Serious game, Training simulation, Virtual environment, Virtual reality},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@inproceedings{10.1145/3638530.3654209,
author = {Maliukov, Irina and Weiss, Gera and Margalit, Oded and Elyasaf, Achiya},
title = {Evolving Assembly Code in an Adversarial Environment},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3654209},
doi = {10.1145/3638530.3654209},
abstract = {We evolve survivors for the CodeGuru competition --- assembly programs that run the longest in shared memory, by resisting attacks from adversary survivors and finding their weaknesses. For evolving top-notch solvers, we specify a Backus Normal Form (BNF) for the assembly language and synthesize the code from scratch using Genetic Programming (GP). We evaluate the survivors by running CodeGuru games against human-written winning survivors. Our evolved programs found weaknesses in the programs they were trained against and utilized them. This work has important applications for cyber-security, as we utilize evolution to detect weaknesses in survivors. The assembly BNF is domain-independent; thus, by modifying the fitness function, it can detect code weaknesses and help fix them. Finally, the CodeGuru competition offers a novel platform for analyzing GP and code evolution in adversarial environments. To support further research in this direction, we provide a thorough qualitative analysis of the evolved survivors and the weaknesses found.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {723–726},
numpages = {4},
keywords = {genetic programming, assembly, code generation, codeguru xtreme},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3664475.3664731,
author = {McLeran, Aaron},
title = {Advances in Real Time Audio Rendering - Part 1},
year = {2024},
isbn = {9798400706837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664475.3664731},
doi = {10.1145/3664475.3664731},
abstract = {Hearing is the most time-sensitive of the human senses. The technology underlying real-time audio rendering must provide control over our physical, perceptual, cultural, and aesthetic worlds within the tightest of deadlines and with perfect temporal coherence. This course offers an introduction to state-of-the-art real-time audio rendering technology. We dive into the core concepts and challenges that define the problem space and touch on similarities shared by real-time graphic rendering and non-real-time audio rendering.},
booktitle = {ACM SIGGRAPH 2024 Courses},
articleno = {06},
numpages = {14},
location = {Denver, CO, USA},
series = {SIGGRAPH Courses '24}
}

@inproceedings{10.1145/3582437.3582467,
author = {Julia, Clement and van Rozen, Riemer},
title = {ScriptButler serves an Empirical Study of PuzzleScript: Analyzing the Expressive Power of a Game DSL through Source Code Analysis},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582467},
doi = {10.1145/3582437.3582467},
abstract = {Automated Game Design (AGD) empowers game designers with languages and tools that automate game design processes. Domain-Specific Languages (DSLs) promise to deliver an expressive means for rapidly prototyping and fine-tuning interaction mechanisms that support rich emergent player experiences. However, despite the growing number of studies that center around languages for games and play, few prototypes are ever thoroughly validated and evaluated in practice. As a result, it is not yet well understood what the costs, benefits and limitations of DSL formalisms are. To find out, we investigate to what extent rules, affordances and play can be related by means of source code analysis. We study PuzzleScript, a language and online game engine with an active user community. We reverse engineer PuzzleScript’s design and propose ScriptButler, a novel tool prototype and engine for its analysis. To validate our approach, we conduct an empirical study on the quality of the source code by performing an analysis on a curated collection of 95 games. Our results show that ScriptButler can identify bugs and helps relate PuzzleScript rules to game qualities.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {1},
numpages = {11},
keywords = {PuzzleScript, automated game design, domain-specific languages, game design tools, reverse engineering, source code analysis},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3593434.3593952,
author = {Manfredi, Gilda and Erra, Ugo and Gilio, Gabriele},
title = {A Mixed Reality Approach for Innovative Pair Programming Education with a Conversational AI Virtual Avatar},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593952},
doi = {10.1145/3593434.3593952},
abstract = {Pair Programming (PP) is an Agile software development methodology that involves two developers working together on a single computer. However, the physical presence of two developers has become a challenge in recent years due to the pandemic, necessitating remote collaboration methods such as Distributed Pair Programming (DPP). DPP has been found to have similar benefits to in-person PP, but the issue of team compatibility remains unresolved. These are more evident in the educational field of Agile methodologies. To address these challenges, we developed a novel approach by creating a Mixed Reality (MR) application that enables users to learn PP with the assistance of a conversational intelligent virtual avatar. The application uses the HoloLens MR device and a Conversational Agent (CA) extension integrated into Visual Studio Code to provide suggestions for improving the code written by the user. The virtual avatar animates these suggestions, making it appear to speak and interact with the user in real time. This system aims to overcome the limitations of common DPP methods, allowing a single developer to learn and apply the PP methodology even when a human partner is unavailable.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {450–454},
numpages = {5},
keywords = {artificial intelligence, conversational agents, extended reality, pair programming},
location = {Oulu, Finland},
series = {EASE '23}
}

@article{10.1145/3651289,
author = {Ying, Zhi and Edwards, Nicholas and Kutuzov, Mikhail},
title = {Efficient Visibility Approximation for Game AI using Neural Omnidirectional Distance Fields},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3651289},
doi = {10.1145/3651289},
abstract = {Visibility information is critical in game AI applications, but the computational cost of raycasting-based methods poses a challenge for real-time systems. To address this challenge, we propose a novel method that represents a partitioned game scene as neural Omnidirectional Distance Fields (ODFs), allowing scalable and efficient visibility approximation between positions without raycasting. For each position of interest, we map its omnidirectional distance data from the spherical surface onto a UV plane. We then use multi-resolution grids and bilinearly interpolated features to encode directions. This allows us to use a compact multi-layer perceptron (MLP) to reconstruct the high-frequency directional distance data at these positions, ensuring fast inference speed. We demonstrate the effectiveness of our method through offline experiments and in-game evaluation. For in-game evaluation, we conduct a side-by-side comparison with raycasting-based visibility tests in three different scenes. Using a compact MLP (128 neurons and 2 layers), our method achieves an average cold start speedup of 9.35 times and warm start speedup of 4.8 times across these scenes. In addition, unlike the raycasting-based method, whose evaluation time is affected by the characteristics of the scenes, our method's evaluation time remains constant.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {may},
articleno = {21},
numpages = {15},
keywords = {Line of Sight, Multi-resolution Grid Encoding, Neural Implicit Representation, Omnidirectional Distance Fields, Visibility for Game AI}
}

@inproceedings{10.1145/3639701.3656306,
author = {Ekman, Johan and Solsona, Jordi and Quintero, Luis},
title = {Codeseum: Learning Introductory Programming Concepts through Virtual Reality Puzzles},
year = {2024},
isbn = {9798400705038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639701.3656306},
doi = {10.1145/3639701.3656306},
abstract = {When learning programming concepts, beginners face challenges that lead to decreased motivation, Game-Based Learning (GBL) uses game principles to make learning more engaging, and VR has been explored as a way to enhance GBL further. This paper explores the impact of Virtual Reality (VR) on learning programming, and we developed Codeseum to compare whether VR-based learning is perceived as more engaging and usable than a desktop game counterpart. The experiment with ten participants included data from questionnaires, interviews, and structured observations. The quantitative analysis indicated that VR was perceived as inducing higher focused attention, aesthetic appeal, and reward, while the thematic analysis provided discussion elements of seven themes, including interaction, engagement, and physical expressions from the participants. Overall, the desktop application had better accessibility, whereas the immersive interactions from Codeseum in VR induced higher levels of enjoyment and engagement. Our study contributes insights into the potential of VR in education, mainly teaching coding skills in engaging ways, and offers information to adopt immersive technologies in teaching practice.},
booktitle = {Proceedings of the 2024 ACM International Conference on Interactive Media Experiences},
pages = {192–200},
numpages = {9},
keywords = {GBL, Game-based learning, VR, education, immersive, learning, programming, virtual reality},
location = {Stockholm, Sweden},
series = {IMX '24}
}

@inproceedings{10.1145/3638209.3638234,
author = {Chen, Jr-Chang and Hsu, Tzu-Yang and Hsu, Chia-Ming and Hsu, Tsan-sheng},
title = {Applying Larger N-Tuple Networks to EinStein Wurfelt Nicht!},
year = {2024},
isbn = {9798400709067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638209.3638234},
doi = {10.1145/3638209.3638234},
abstract = {EinStein w\"{u}rfelt nicht! (abbr. EWN) is a stochastic game with a die rolled in each ply which makes it full of uncertainty. It does not look easy to design a good game engine using traditional alpha-beta-based approaches. In this paper, an EWN game engine is formed by combining n-tuple networks, trained by features extracted from two billion random games, with Monte Carlo Tree Search. Similar work has been proposed previously, but in this work, we compare performances on versions with n = 4, 6 and 9. We discover that 4-tuple networks add no extra strength in comparison with a plain baseline. However, a larger n does bring extra benefit. The jump of performance is noticeable from n = 4 to n = 6 and is very visible from n = 6 to n = 9. The tuple’s size n increases from 4 and 6 to 9. When n = 9, the size of the feature set is much larger than before and cannot be stored plainly in the main memory. One of our main contributions is to come out with a good encoding design taking care of various symmetry or equivalence properties so that it can be used in real-time game playing. Using our scheme, the number of distinct features is reduced to 20,244,124,980, which is 21.21% less than the original 95,440,494,357 features for 9-tuple networks.},
booktitle = {Proceedings of the 2023 6th International Conference on Computational Intelligence and Intelligent Systems},
pages = {165–172},
numpages = {8},
keywords = {EinStein w\"{u}rfelt nicht!, Monte Carlo Tree Search, n-tuple networks},
location = {Tokyo, Japan},
series = {CIIS '23}
}

@inproceedings{10.1145/3555858.3555880,
author = {Nova, Atiya and Sansalone, Stevie and Robinson, Raquel and Mirza-Babaei, Pejman},
title = {Charting the Uncharted with GUR: How AI Playtesting Can Supplement Expert Evaluation},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555880},
doi = {10.1145/3555858.3555880},
abstract = {Despite the advantages of using expert evaluation as a method within games user research (GUR) (i.e. provides stakeholders low cost, rapid feedback), it does not always accurately reflect the general player’s experience. Testing the game out with real users (also called playtesting) helps bridge this gap by giving game developers an in-depth look into the player experience. However, playtesting is resource intensive and time consuming, making it difficult to implement within the tight time frames of industry game development. AI can help to mitigate some of these issues by providing an automated way to simulate player behaviour and experience. In this paper, we introduce a tool called PathOS+—a playtesting interface which uses AI playtesting data to help enhance expert evaluation. Results from a study conducted with expert participants shows how PathOS+ could contribute to game design and assist developers and researchers in conducting expert evaluations. This is an important contribution as it provides game user researchers and designers with a fast, low-cost and effective game evaluation approach which has the potential to make game evaluation more accessible to indie and smaller game studios.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {28},
numpages = {12},
keywords = {artificial intelligence, expert evaluation, game development},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3579654.3579752,
author = {Zhang, Xinyuan and Zhang, Xinyou},
title = {Based on Navmesh to implement AI intelligent pathfinding in three-dimensional maps in UE4},
year = {2023},
isbn = {9781450398336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579654.3579752},
doi = {10.1145/3579654.3579752},
abstract = {Abstract: The development of science and technology has led to the development of the game industry. As the mainstream development engine of the current three-dimensional game, UE4 is popular with game developers for its powerful rendering technology and model processing technology. There will be a large number of non-player characters (NPC, non-player character) in the game, and these AI-controlled characters are an important way for developers to interact with users. Among them, pathfinding is the most important part of the game's AI character, which gives the AI character vitality, so that the AI character can interact with the player, chase, and automatically complete the movement to the target point. Pathfinding is the core function of an AI character, and there are many current pathfinding methods, such as Dijstra algorithm, best-first search algorithm, A-star algorithm. Most of these pathfinding algorithms are used in 2D static maps, but there is no detailed description of 3D pathfinding by pathfinding algorithms in UE4. This paper first introduces and compares several map pathfinding models, then experimentally compares different pathfinding algorithms, and then uses blueprint programming to implement AI role pathfinding in the three-dimensional map in UE4 based on the optimal performance algorithm. Finally, the overall experimental results are summarized and sorted out, and the direction for future development is pointed out.},
booktitle = {Proceedings of the 2022 5th International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {93},
numpages = {5},
keywords = {A-star, Blueprint, Navmesh, Unreal Engine 4},
location = {Sanya, China},
series = {ACAI '22}
}

@inproceedings{10.1145/3595916.3626456,
author = {On, Sung Kwon and Kim, Songhyon and Yang, Kwangjin and Lee, Younggun},
title = {Monocular 3D Pose Estimation of Very Small Airplane in the Air},
year = {2024},
isbn = {9798400702051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3595916.3626456},
doi = {10.1145/3595916.3626456},
abstract = {In this paper, a novel pose estimation algorithm is proposed specifically for maneuvering airplanes in the air. The algorithm consists of two main stages. The first stage involves semantic segmentation of a monocular input image of a flying airplane, where the entire captured area serves as feature points for the airplane, which are typically small in the image. The second stage focuses on the 3D pose estimation of the segmented image using projective registration. Since airplanes have unique characteristics and there is a scarcity of airplane-specific datasets, a custom dataset is generated for the experiments. Unreal Engine 4, a 3D computer graphics game engine renowned for its realistic simulations, is employed for this purpose. Experimental results demonstrate the suitability of the algorithm for 3D pose estimation of airplanes, providing valuable information for studying autonomous control of airplanes.},
booktitle = {Proceedings of the 5th ACM International Conference on Multimedia in Asia},
articleno = {82},
numpages = {7},
keywords = {3D pose estimation, Dataset, Monocular camera, Small airplane},
location = {Tainan, Taiwan},
series = {MMAsia '23}
}

@inproceedings{10.1145/3505270.3558374,
author = {Mulvany, Gerard T.},
title = {Because the Night - Immersive Theatre for Digital Audiences: Mapping the affordances of immersive theatre to digital interactions using game engines},
year = {2022},
isbn = {9781450392112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505270.3558374},
doi = {10.1145/3505270.3558374},
abstract = {Immersive theatre is a highly interactive format that raises the audience's agency over their experience, and, depending on their actions, provides each member with their own understanding of the overarching narrative that developed based on the spatial path they undertook. This theatre format challenges traditional theatre digitisation techniques as current archiving and reproduction methods cannot authentically replicate the affordances (or ‘actions possible’) that audiences have in the original performance. Theatres need to explore new methods of digitisation and archiving, to preserve past experiences and cater to an audience who will access content digitally. This report documents the design decisions and their impact for the interactive prototype digitisation and archival of the 2021 immersive-theatre production “Because the Night” for a digital audience. This interactive digital archive utilises game design and game engine technology to map and transfer the affordances of audiences from the original immersive theatre play to a digital platform, leveraging the game engine's innate affordances, including freedom of movement, spatialised audio, and user intractability. To deliver these research outcomes, the project captured 360-degree video of the performance, music, images of props, and 360-degree photographs of the set, all embedded into a game engine system to authentically preserve the immersive, interactive, and ambient experience of the original production. The resulting system was able to successfully map several of the performances original affordances, including environmental exploration, spatially emergent narratives, object inspection and exploration, and interactive environmental challenges. This document reports on three matters: the design process and decisions to authentically capture the affordances of the theatre production in a digital gameplay experience; the current status of this research project; and the proposed approach to testing the prototype with users.},
booktitle = {Extended Abstracts of the 2022 Annual Symposium on Computer-Human Interaction in Play},
pages = {291–296},
numpages = {6},
keywords = {Archival, Digital Theatre, Digitisation, Immersive Theatre, Interaction Design},
location = {Bremen, Germany},
series = {CHI PLAY '22}
}

@inproceedings{10.1145/3544549.3583920,
author = {Li, Ke and Rolff, Tim and Schmidt, Susanne and Bacher, Reinhard and Leemans, Wim and Steinicke, Frank},
title = {Interacting with Neural Radiance Fields in Immersive Virtual Reality},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583920},
doi = {10.1145/3544549.3583920},
abstract = {Recent advancements in the neural radiance field (NeRF) technology, in particular its extension by instant neural graphics primitives, provide tremendous opportunities for the use of real-time immersive virtual reality (VR) applications. Moreover, the recent release of an immersive neural graphics primitives framework (immersive-ngp) brings real-time, stereoscopic NeRF rendering to the Unity game engine. However, the system and application research combining NeRF and human-computer interaction in VR is still at the very beginning. In this demo, we present multiple interactive system features for immersive-ngp with design principles focusing on improving the usability and interactivity of the framework for small to medium-scale NeRF scenes. We demonstrate that these new feature implementations such as exocentric manipulation, VR tunneling effects, and immersive scene appearance editing enable novel VR-NeRF experiences, for example, for customized experiences in inspecting a particle accelerator environment.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {460},
numpages = {4},
keywords = {Immersive Virtual Reality, Neural Radiance Field},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3605495.3605798,
author = {Petrescu, David and Warren, Paul A. and Montazeri, Zahra and Otkhmezuri, Boris and Pettifer, Steve},
title = {Foveated Walking: Translational Ego-Movement and Foveated Rendering},
year = {2023},
isbn = {9798400702525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605495.3605798},
doi = {10.1145/3605495.3605798},
abstract = {The demands of creating an immersive Virtual Reality (VR) experience often exceed the raw capabilities of graphics hardware. Perceptually-driven techniques can reduce rendering costs by directing effort away from features that do not significantly impact the overall user experience while maintaining a high level of quality where it matters most. One such approach is foveated rendering, which allows for a reduction in the quality of the image in the peripheral region of the field-of-view where lower visual acuity results in users being less able to resolve fine details. 6 Degrees of Freedom tracking allows for the exploration of VR environments through different modalities, such as user-generated head or body movements. The effect of self-induced motion on rendering optimization has generally been overlooked and is not yet well understood. To explore this, we used Variable Rate Shading (VRS) to create a foveated rendering method triggered by the translational velocity of the users and studied different levels of shading Level-of-Detail (LOD). We asked 10 participants in a within-subjects design to report whether they noticed a degradation in the rendering of a rich environment when performing active ego-movement or when being passively transported through the environment. We ran a psychophysical experiment using an accelerated stochastic approximation staircase method and modified the diameter and the LOD of the peripheral region. Our results show that self-induced walking can be used to significantly improve the savings of foveated rendering by allowing for an increased size of the low-quality area in a foveated algorithm compared to the passive condition. After fitting psychometric functions showcasing the percentage of correct responses related to different shading rates in the two types of movements, we also report the threshold severity (75%) point for when participants are able to detect such degradation. We argue such metrics can inform the future design of movement-dependent foveated techniques that could reduce computational load and increase energy savings.},
booktitle = {ACM Symposium on Applied Perception 2023},
articleno = {7},
numpages = {8},
keywords = {foveated rendering, motion, psychophysics, variable rate shading},
location = {Los Angeles, CA, USA},
series = {SAP '23}
}

@inproceedings{10.1145/3578837.3578839,
author = {Horst, Robin and Naraghi-Taghi-Off, Ramtin and D\"{o}rner, Ralf},
title = {Integrating Stand-Alone New Media Technologies Such as Games and Virtual and Augmented Reality Software into Learning Management Systems: Integrating Stand-Alone Media Software into LMSs},
year = {2023},
isbn = {9781450398428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578837.3578839},
doi = {10.1145/3578837.3578839},
abstract = {Both learning management systems (LMSs) and new media technologies (NMTs, e.g., Augmented/Virtual Reality software or Serious Games) can enhance teaching and learning, however, the integration of both technologies within an E-Learning system is challenging. NMTs are often developed as bulky stand-alone applications, for example, using game engines, which make them difficult to integrate into the LMS courses. Thus, connecting middleware technologies are needed, however, various technologies exist. In this paper, we introduce three techniques (SCORM-Data, xAPI-Injection, and JSON-REST) that integrate stand-alone NMT learning software into LMSs based on existing technologies and standards from the E-Learning domain. We discuss the techniques and their underlying processes based on a prototype implementation and the results of an expert interview and point out the advantages and disadvantages of each technique to give practitioners differentiated advice on which technique to use. Although SCORM-Data has fewer technical requirements, we conclude that our JSON-REST technique was favored through applicability, simplicity, ease of integration, and efficiency.},
booktitle = {Proceedings of the 2022 6th International Conference on Education and E-Learning},
pages = {7–13},
numpages = {7},
keywords = {Authoring, Extended Realities, Game Engine Software, Learning Management Systems, Serious Games, Software Integration Processes, Stand-Alone Learning Software},
location = {Yamanashi, Japan},
series = {ICEEL '22}
}

@inproceedings{10.1145/3610540.3627004,
author = {Marks, Stefan and Gil Parga, Sebasti\'{a}n},
title = {Computer Graphics and Extended Reality Courses for the Programmophobic},
year = {2023},
isbn = {9798400703119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610540.3627004},
doi = {10.1145/3610540.3627004},
abstract = {This paper describes the challenges and solutions to teaching computer graphics as well as extended reality concepts to students from a variety of backgrounds in the context of the School of Future Environments at the Auckland University of Technology, New Zealand. Examples are provided for the content and assessment strategies for two courses, as well as a summary of student work and feedback collected over the last three years.},
booktitle = {SIGGRAPH Asia 2023 Educator's Forum},
articleno = {3},
numpages = {8},
keywords = {augmented reality, computer graphics, education, virtual reality},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@inproceedings{10.1145/3570945.3607311,
author = {Lugrin, Jean-Luc and Topel, Jessica and Gl\'{e}marec, Yann and Lugrin, Birgit and Latoschik, Marc Erich},
title = {Posture Parameters for Personality-Enhanced Virtual Audiences},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570945.3607311},
doi = {10.1145/3570945.3607311},
abstract = {This paper presents the development and preliminary evaluation of a personality enhancer behaviour model for virtual audiences to increase their realism and individualism. We conducted a systematic literature review and identified sixteen posture parameters to modify seated animations dynamically, calling them personality enhancers. We grouped them into four main categories: body, gaze, face and gesture behaviour modifiers. We implemented a unique animation modifier system on top of a game engine to apply these personality enhancers on existing pre-recorded generic seated animations. The first results with sixty participants in an online video survey show that the model can successfully simulate individuals with low and high levels of extroversion as well as with low and high levels of emotional stability.},
booktitle = {Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
articleno = {46},
numpages = {4},
keywords = {personality, virtual audiences},
location = {W\"{u}rzburg, Germany},
series = {IVA '23}
}

@inproceedings{10.1145/3631085.3631238,
author = {Borges, Rodrigo Campos and Malheiros, Marcelo de Gomensoro and Billa, Cleo Zanella and Pias, Marcelo Rita and Bicho, Alessandro de Lima},
title = {An Open-Source Framework Using WebRTC for Online Multiplayer Gaming},
year = {2024},
isbn = {9798400716270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631085.3631238},
doi = {10.1145/3631085.3631238},
abstract = {Currently, most network topologies used in online games to connect players are client-server, even for browser-based games. This work evaluates an alternative approach, describing the implementation of a hybrid peer-to-peer (P2P) architecture. We have developed the open-source Proton framework for multiplayer online games, using the browser-based Web Real-Time Communication (WebRTC) technology and running on the Unity game engine. As a case study, the same prototype game was built over both topologies: a client-server using the Photon Unity Networking (PUN) and the other using our proposed framework. We detail our implementation and then analyze networking metrics in two scenarios. Later, we show that the results are promising and assess that the WebRTC API is mature enough to serve as the basis for online multiplayer games.},
booktitle = {Proceedings of the 22nd Brazilian Symposium on Games and Digital Entertainment},
pages = {143–150},
numpages = {8},
keywords = {WebRTC, multiplayer games, peer-to-peer communication},
location = {Rio Grande (RS), Brazil},
series = {SBGames '23}
}

@inproceedings{10.1145/3613904.3642579,
author = {De La Torre, Fernanda and Fang, Cathy Mengying and Huang, Han and Banburski-Fahey, Andrzej and Amores Fernandez, Judith and Lanier, Jaron},
title = {LLMR: Real-time Prompting of Interactive Worlds using Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642579},
doi = {10.1145/3613904.3642579},
abstract = {We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR’s cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {600},
numpages = {22},
keywords = {artificial intelligence, large language model, mixed reality, spatial reasoning},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3664475.3664734,
author = {Zicarelli, David},
title = {Advances in Real Time Audio Rendering - Part 2},
year = {2024},
isbn = {9798400706837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664475.3664734},
doi = {10.1145/3664475.3664734},
abstract = {We originally referred to "real-time" audio systems to draw a distinction with "non-real-time" systems where a series of audio samples is entirely determined and computed in advance, originally because computers were not fast enough to perform the needed mathematical calculations. If you can't listen to the sound as it is produced, you won't be able to change it live and know what you're changing. Thus, the desire to turn a computer into something more like a "musical instrument" was a primary motivation in the development of real-time audio systems.},
booktitle = {ACM SIGGRAPH 2024 Courses},
articleno = {07},
numpages = {16},
location = {Denver, CO, USA},
series = {SIGGRAPH Courses '24}
}

@article{10.1145/3633463,
author = {Bai, Xiangyu and Luo, Yedi and Jiang, Le and Gupta, Aniket and Kaveti, Pushyami and Singh, Hanumant and Ostadabbas, Sarah},
title = {Bridging the Domain Gap between Synthetic and Real-World Data for Autonomous Driving},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3633463},
doi = {10.1145/3633463},
abstract = {Modern autonomous systems require extensive testing to ensure reliability and build trust in ground vehicles. However, testing these systems in the real-world is challenging due to the lack of large and diverse datasets, especially in edge cases. Therefore, simulations are necessary for their development and evaluation. However, existing open-source simulators often exhibit a significant gap between synthetic and real-world domains, leading to deteriorated mobility performance and reduced platform reliability when using simulation data. To address this issue, our Scoping Autonomous Vehicle Simulation (SAVeS) platform benchmarks the performance of simulated environments for autonomous ground vehicle testing between synthetic and real-world domains. Our platform aims to quantify the domain gap and enable researchers to develop and test autonomous systems in a controlled environment. Additionally, we propose using domain adaptation technologies to address the domain gap between synthetic and real-world data with our SAVeS+ extension. Our results demonstrate that SAVeS+ is effective in helping to close the gap between synthetic and real-world domains and yields comparable performance for models trained with processed synthetic datasets to those trained on real-world datasets of same scale. Finally, we introduce two new autonomy driving datasets with complex scenes, essential sensor data, ground truth and improved imagery. The data is generated using both open-source and commercial simulators and processed through our SAVeS+ domain adaptation pipeline. This paper highlights our efforts to quantify and address the domain gap between synthetic and real-world data for autonomy simulation. By enabling researchers to develop and test autonomous systems in a controlled environment, we hope to bring autonomy simulation one step closer to realization.1},
journal = {ACM J. Auton. Transport. Syst.},
month = {apr},
articleno = {9},
numpages = {15},
keywords = {Autonomous ground vehicles, machine learning inference, simulation, simultaneous localization and mapping (SLAM) algorithms, synthetic environments, domain adaption}
}

@inproceedings{10.1145/3616901.3616944,
author = {Zhang, Qi and Lin, Zi Qing and Liang, Xiao Tong and Zhou, Hua Ying},
title = {Realistic 3D Modeling Method of Virtual Traditional Chinese Medicine},
year = {2024},
isbn = {9798400707544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616901.3616944},
doi = {10.1145/3616901.3616944},
abstract = {In order to realize the 3D construction and simulation of virtual traditional Chinese medicine with high realism. Taking the confusing Chinese medicine Radix Phytolaccae and Ginseng slices as an example, starting from morphological identification of traditional Chinese medicine, constructing the 3D model of its appearance, using physically based rendering(PBR) technology based on its geometry, color, texture, surface and cross section for 3D realistic simulation. By rendering in the Arnold built in Maya, and the 3D game engine software Unity3d, using PBR process technology to build visual traditional Chinese medicines can accurately present the surface texture characteristics of traditional Chinese medicine and achieve a more realistic rendering effect. The production method based on PBR mapping can be applied to different 3D software platforms, and can show realistic rendering effects, which is more realistic and efficient than traditional mapping methods.},
booktitle = {Proceedings of the 2023 International Conference on Frontiers of Artificial Intelligence and Machine Learning},
pages = {195–198},
numpages = {4},
location = {Beijing, China},
series = {FAIML '23}
}

@article{10.1145/3603618,
author = {Zheng, Ce and Wu, Wenhan and Chen, Chen and Yang, Taojiannan and Zhu, Sijie and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak},
title = {Deep Learning-based Human Pose Estimation: A Survey},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3603618},
doi = {10.1145/3603618},
abstract = {Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade and has been utilized in a wide range of applications including human-computer interaction, motion analysis, augmented reality, and virtual reality. Although the recently developed deep learning-based solutions have achieved high performance in human pose estimation, there still remain challenges due to insufficient training data, depth ambiguities, and occlusion. The goal of this survey article is to provide a comprehensive review of recent deep learning-based solutions for both 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. More than 260 research papers since 2014 are covered in this survey. Furthermore, 2D and 3D human pose estimation datasets and evaluation metrics are included. Quantitative performance comparisons of the reviewed methods on popular datasets are summarized and discussed. Finally, the challenges involved, applications, and future research directions are concluded. A regularly updated project page is provided: .},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {11},
numpages = {37},
keywords = {Survey of human pose estimation, 2D and 3D pose estimation, deep learning-based pose estimation, pose estimation datasets, pose estimation metrics}
}

@inproceedings{10.1145/3549555.3549572,
author = {Hassan, Syed Zohaib and Salehi, Pegah and Riegler, Michael Alexander and Johnson, Miriam Sinkerud and Baugerud, Gunn Astrid and Halvorsen, P\r{A}L and Sabet, Saeed Shafiee},
title = {A Virtual Reality Talking Avatar for Investigative Interviews of Maltreat Children},
year = {2022},
isbn = {9781450397209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549555.3549572},
doi = {10.1145/3549555.3549572},
abstract = {Interviews conducted with the maltreated children are often the primary source of evidence in prosecution. Many alleged incidents of abuse are not prosecuted because the children’s testimony is collected in an unreliable way. Research shows the consistent poor quality of these interviews and highlights the need for better training of Child Protection Services (CPS) and police personnel who interview abused child witnesses. The currently available systems for training of CPS and police personnel are developed in a rigid way that lag behind in generating dynamic responses. Moreover, these systems require human input such as employing an actor mimicking a child or an operator controlling prerecorded child responses during the interactions. This paper demonstrates the prototype of an interview training program with an artificial intelligent Child Avatar in Virtual Reality (VR), enabling CPS and police personnel to practice interviewing with abused children. The program is developed using Unity game engine and artificial intelligence-based technologies such as dialogue models, talking visual avatars, text-to-speech, and speech-to-text components.},
booktitle = {Proceedings of the 19th International Conference on Content-Based Multimedia Indexing},
pages = {201–204},
numpages = {4},
keywords = {Virtual reality, dialogue models, maltreated children, virtual child avatar},
location = {Graz, Austria},
series = {CBMI '22}
}

@inproceedings{10.1145/3503161.3548327,
author = {Liu, Mengmeng and Ma, Zhi and Li, Tao and Jiang, Yanfeng and Wang, Kai},
title = {Long-Term Person Re-identification with Dramatic Appearance Change: Algorithm and Benchmark},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548327},
doi = {10.1145/3503161.3548327},
abstract = {For person re-identification (Re-ID) task, most of previous studies assumed that the pedestrians do not change their appearances. The works on cross-appearance Re-ID, including datasets and algorithms, are still few. Therefore, this paper contributes a cross-season appearance change Re-ID dataset, namely NKUP+, including more than 300 IDs from surveillance videos over 10 months, to support the studies of the cross-appearance Re-ID. In addition, we propose a network named M2Net, which integrates multi-modality features from the RGB images, contour images and human parsing images. By ignoring irrelevant misleading information for cross-appearance retrieval in RGB images, M2Net can learn features that are robust to appearance changes. Meanwhile, we propose a sampling strategy called RAS to contain a variety of appearances in one batch. And appearance loss and multi-appearance loss are designed to guide the network to learn both same-appearance and cross-appearance features. Finally, we evaluated our method on NKUP+/PRCC/DeepChange datasets, and the results showed that, compared with the baseline, our method renders significant improvement, leading to the state-of-the-art performance over other methods. Our dataset is available at https://github.com/nkicsl/NKUP-dataset.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {6406–6415},
numpages = {10},
keywords = {benchmark, cross-appearance, metric learning, person re-identification},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inbook{10.1145/3570361.3614082,
author = {Hu, Tianyi and Scargill, Tim and Chen, Ying and Lan, Guohao and Gorlatova, Maria},
title = {DNN-based SLAM Tracking Error Online Estimation},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570361.3614082},
abstract = {Simultaneous localization and mapping (SLAM) takes in sensor data, e.g., camera frames, and estimates the user's trajectory while creating a map of the surrounding environment. However, existing SLAM evaluation methods are not reference-free, requiring ground-truth trajectories collected from external systems that are infeasible for most scenarios. In this demo, we present Deep SLAM Error Estimator (DeepSEE), a framework that collects features from a standard visual SLAM pipeline as multivariate time series and uses an attention-based neural network to estimate the tracking error at run time. We evaluate DeepSEE in a game engine-based virtual environment, which generates the visual input for DeepSEE and provides the ground-truth trajectory. Demo participants can navigate the virtual environment to create their own trajectories and view the online pose error estimation. This demo showcases how DeepSEE can act as a quality-of-service indicator for downstream applications.},
booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
articleno = {114},
numpages = {3}
}

@inproceedings{10.1145/3623462.3624636,
author = {Gagner\'{e}, Georges and Ternova, Anastasiia},
title = {AKN_Regie: bridging digital and performing arts},
year = {2023},
isbn = {9798400708367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623462.3624636},
doi = {10.1145/3623462.3624636},
abstract = {AvatarStaging framework consists in directing avatars on a mixed theatrical stage, enabling a co-presence between the materiality of the physical actor and the virtuality of avatars controlled in real time by motion capture or specific animation players. It led to the implementation of the AKN_Regie authoring tool, programmed with the Blueprint visual language as a plugin for the Unreal Engine (UE) video game engine. The paper describes AKN_Regie main functionalities as a tool for non-programmer theatrical people. It gives insights of its implementation in the Blueprint visual language specific to UE. It details how the tool evolved along with its use in around ten theater productions. A circulation process between a non-programming point of view on AKN_Regie called Plugin Perspective and a programming acculturation to its development called Blueprint Perspective is discussed. Finally, a C++ Perspective is suggested to enhance the cultural appropriation of technological issues, bridging the gap between performing arts deeply involved in human materiality and avatars inviting to discover new worlds.},
booktitle = {Proceedings of the 20th International Conference on Culture and Computer Science: Code and Materiality},
articleno = {9},
numpages = {11},
keywords = {Avatar, Unreal Engine, digital art, motion capture, performing arts, software architecture},
location = {Lisbon, Portugal},
series = {KUI '23}
}

@article{10.1145/3657284,
author = {Shen, Meng and Tan, Zhehui and Niyato, Dusit and Liu, Yuzhi and Kang, Jiawen and Xiong, Zehui and Zhu, Liehuang and Wang, Wei and Shen, Xuemin (Sherman)},
title = {Artificial Intelligence for Web 3.0: A Comprehensive Survey},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3657284},
doi = {10.1145/3657284},
abstract = {Web 3.0 is the next generation of the Internet built on decentralized technologies such as blockchain and cryptography. It is born to solve the problems faced by the previous generation of the Internet such as imbalanced distribution of interests, monopoly of platform resources, and leakage of personal privacy. In this survey, we discuss the latest development status of Web 3.0 and the application of emerging AI technologies in it. First, we investigate the current successful practices of Web 3.0 and various components in the current Web 3.0 ecosystem and thus propose the hierarchical architecture of the Web 3.0 ecosystem from the perspective of application scenarios. The architecture we proposed contains four layers: data management, value circulation, ecological governance, and application scenarios. We dive into the current state of development and the main challenges and issues present in each layer. In this context, we find that AI technology will have great potential. We first briefly introduce the role that artificial intelligence technology may play in the development of Web 3.0. Then, we conduct an in-depth analysis of the current application status of artificial intelligence technology in the four layers of Web 3.0 and provide some insights into its potential future development directions.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {247},
numpages = {39},
keywords = {Web 3.0, artificial intelligence, blockchain, computing network}
}

@article{10.1145/3661133,
author = {Prithul, Aniruddha and Lynam, Hudson and Folmer, Eelke},
title = {Performance and Navigation Behavior of using Teleportation in VR First-Person Shooter Games},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3661133},
doi = {10.1145/3661133},
abstract = {Teleportation has been adopted as the preferred mode of navigation in many virtual reality (VR) experiences due to its ability to allow users to easily move beyond the limitations of the tracking space while minimizing the risk of inducing VR sickness. Teleportation instantly translates the user’s viewpoint to a user-selected destination and therefore eliminates any optical flow generation that could cause visual-vestibular conflict. Though teleportation is a discrete navigation method unique to the domain of VR, most VR experiences are modeled after three-dimensional experiences found on desktop/console platforms that use continuous locomotion. How the use of teleportation affects the performance and navigation behavior of its users, especially in competitive first-person shooter environments is unknown, yet it could have a significant effect on gameplay design. We conducted a user study (n=21) that compares teleportation versus continuous locomotion using a VR first-person shooter game with other players being simulated using AI agents. We found significant differences in performance, navigation behavior, and how both locomotion methods are perceived by its users. Specifically, using teleportation, players traveled farther but during combat were found to be more stationary and as a result got hit more frequently. These differences were profound and carry the potential to impact multiplayer games. We discuss possible strategies to balance gameplay.},
journal = {ACM Games},
month = {aug},
articleno = {24},
numpages = {18},
keywords = {Virtual reality, locomotion, teleportation, performance, human navigation behavior}
}

@inproceedings{10.1145/3652628.3652790,
author = {Liu, Xiaoyan},
title = {Comparative Study on Intelligent Driving Path Planning Based on Self-Implemented CNN and Transfer Learning Models},
year = {2024},
isbn = {9798400708831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652628.3652790},
doi = {10.1145/3652628.3652790},
abstract = {This paper aims to explore the performance differences between different models in the path planning task of intelligent driving by implementing convolutional neural network and comparing it with transfer learning models. We first implemented a custom CNN model and adjusted and optimized its structure and parameters in detail. Then we chose transfer learning models ResNet50, InceptionV3, InceptionResNetV2, MobileNet, and Xception, and used the Unity 3d game engine to simulate the physical operation of real cars in a 3d world and generate a large number of images to form these diverse datasets to ensure a comprehensive evaluation of the performance of these models. Finally, we used an attention mechanism module in the CNN model for further experimentation and compared its performance with our self-implemented CNN and the transfer learning models we selected. The experimental results show that the CNN model loaded with the attention mechanism module performs well in the path planning task, achieving an accuracy rate of 86.10%, which is better than our self-implemented CNN model at 85.6% and other transfer learning models, demonstrating better generalization ability and recognition accuracy.},
booktitle = {Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering},
pages = {976–980},
numpages = {5},
keywords = {Attention Mechanism, Convolutional Neural Network, Transfer Learning},
location = {Dalian, China},
series = {ICAICE '23}
}

@inproceedings{10.1145/3641236.3664419,
author = {Yuen, Deborah and Spjut, Josef},
title = {Experimenting with Artificial Intelligence: Programming Pathfinding Algorithms in C++ with Unreal Engine 5},
year = {2024},
isbn = {9798400705182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641236.3664419},
doi = {10.1145/3641236.3664419},
booktitle = {ACM SIGGRAPH 2024 Labs},
articleno = {7},
numpages = {2},
keywords = {computer games, education},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.5555/3635637.3663188,
author = {Rutherford, Alexander and Ellis, Benjamin and Gallici, Matteo and Cook, Jonathan and Lupu, Andrei and Ingvarsson, Gar\dh{}ar and Willi, Timon and Khan, Akbir and Schroeder de Witt, Christian and Souly, Alexandra and Bandyopadhyay, Saptarashmi and Samvelyan, Mikayel and Jiang, Minqi and Lange, Robert and Whiteson, Shimon and Lacerda, Bruno and Hawes, Nick and Rockt\"{a}schel, Tim and Lu, Chris and Foerster, Jakob},
title = {JaxMARL: Multi-Agent RL Environments and Algorithms in JAX},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Benchmarks play an important role in the development of machine learning algorithms, with reinforcement learning (RL) research having been heavily influenced by the available environments. However, RL environments are traditionally run on the CPU, limiting their scalability with typical academic compute. Recent advancements in JAX have enabled the wider use of hardware acceleration to overcome these computational hurdles, enabling massively parallel RL training pipelines and environments. This is particularly useful for multi-agent reinforcement learning (MARL) research. First of all, multiple agents must be considered at each environment step, adding computational burden, and secondly, the sample complexity is increased due to non-stationarity, decentralised partial observability, or other MARL challenges. In this paper, we present JaxMARL, the first open-source code base that combines ease-of-use with GPU enabled efficiency, and supports a large number of commonly used MARL environments as well as popular baseline algorithms. When considering wall clock time, our experiments show that per-run our JAX-based training pipeline is up to 12500x faster than existing approaches. %This enables efficient and thorough evaluations, with the potential to alleviate the evaluation crisis of the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. We provide code at https://github.com/flairox/jaxmarl.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {2444–2446},
numpages = {3},
keywords = {benchmarks, jax, multi-agent reinforcement learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3562939.3565616,
author = {Hiroi, Yuichi and Itoh, Yuta and Rekimoto, Jun},
title = {NeARportation: A Remote Real-time Neural Rendering Framework},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565616},
doi = {10.1145/3562939.3565616},
abstract = {While presenting a photorealistic appearance plays a major role in immersion in Augmented Virtuality environment, displaying that of real objects remains a challenge. Recent developments in photogrammetry have facilitated the incorporation of real objects into virtual space. However, reproducing complex appearances, such as subsurface scattering and transparency, still requires a dedicated environment for measurement and possesses a trade-off between rendering quality and frame rate. Our NeARportation framework combines server–client bidirectional communication and neural rendering to resolve these trade-offs. Neural rendering on the server receives the client’s head posture and generates a novel-view image with realistic appearance reproduction that is streamed onto the client’s display. By applying our framework to a stereoscopic display, we confirm that it can display a high-fidelity appearance on full-HD stereo videos at 35-40 frames per second (fps) according to the user’s head motion.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {23},
numpages = {5},
keywords = {appearance reproduction, augmented virtuality, neural rendering, real-time rendering, remote rendering},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3543758.3547569,
author = {Kaminski, Wolf Elias and Keppler, Sebastian and Israel, Johann Habakuk},
title = {Physics-Based Hand-Object-Interaction for Immersive Environments},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543758.3547569},
doi = {10.1145/3543758.3547569},
abstract = {In this work, we developed a physics-based hand-object interaction that can be used in immersive virtual environments. The resulting interaction technique enables the user to interact with objects in a pseudo-realistic way that works on the calculation of forces, friction and other factors. Even though this approach can lead to a grasping method that is suitable for arbitrary interactions and is not limited by predefined gestures and data, our prototype is limited to sliding objects on a plane. We evaluated our implementation in a user study in which users had to move an object with their virtual hands, and made a comparison with a rule-based interaction method.},
booktitle = {Proceedings of Mensch Und Computer 2022},
pages = {523–527},
numpages = {5},
keywords = {Physics-based, hand-object-interaction, hand-tracking, manual interaction, virtual reality},
location = {Darmstadt, Germany},
series = {MuC '22}
}

@inproceedings{10.1145/3603555.3609315,
author = {Fr\"{o}lke, Robin and Butz, Benjamin and Lux, Gregor and Gerken, Jens},
title = {HoloBoard: A gamified balance board experience},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603555.3609315},
doi = {10.1145/3603555.3609315},
abstract = {In this demo, we present HoloBoard, a HoloLens2-based Augmented Reality application, which gamifies traditional balance board training to make it more engaging and enjoyable. Ankle injuries are among the most common injuries in many ball sports. Balance training, such as training programs which use a balance board, can help stabilize foot joints and prevent injuries. Traditional balance training is often inadequate because there is no feedback, and the "monotonous" movements are perceived as "boring". Our application is based on the Unreal game engine and utilizes the HoloLens2 to engage the player in various ball sports related game types such as catching, heading and dodging, thereby requiring various body movements similar to traditional balance board training programs.},
booktitle = {Proceedings of Mensch Und Computer 2023},
pages = {548–550},
numpages = {3},
keywords = {augmented reality, balance exercise, exergame, sports},
location = {Rapperswil, Switzerland},
series = {MuC '23}
}

@inproceedings{10.1145/3613904.3641889,
author = {Boucher, Josiah D and Smith, Gillian and Telliel, Yunus Do\u{g}an},
title = {Is Resistance Futile?: Early Career Game Developers, Generative AI, and Ethical Skepticism},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641889},
doi = {10.1145/3613904.3641889},
abstract = {This paper presents a study that examines developer perceptions and usage of generative AI (GAI) in a summer professional development program for game development interns focused on mobile game design. GAI applications are in common usage worldwide, yet the impacts of this technology in game development remain relatively underexplored. Through a qualitative study using ethnographic interviews and participatory observation, this paper explores how GAI impacted the workflows, creative processes, and professional identities of early career game developers. We present a case of GAI integration that was not a straightforward adoption. Focusing on the interns’ resistance, negotiation, and reimagining, we show that the interns were actively developing a new professional culture both with and against generative AI. For the interns, their ethical commitments to fellow game developers and the future of their profession were as important as their practical concerns about usability, utility, and efficacy of GAI tools.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {173},
numpages = {13},
keywords = {Creativity Support, Future of GAI, Games/Play, Generative AI, Professional Communities, Programming/Development Support, Qualitative Methods},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613905.3651026,
author = {Wan, Hongyu and Zhang, Jinda and Suria, Abdulaziz Arif and Yao, Bingsheng and Wang, Dakuo and Coady, Yvonne and Prpa, Mirjana},
title = {Building LLM-based AI Agents in Social Virtual Reality},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651026},
doi = {10.1145/3613905.3651026},
abstract = {In this paper, we introduce the design and evaluation of an LLM-based AI agent for human-agent interaction in Virtual Reality (VR). Our AI agent system leverages GPT-4, a Large Language Model (LLM) to simulate human behavior. Our LLM-based agent, deployed in VRChat as a Non-playable Character (NPC), exhibits the ability to respond to a player by providing context-relevant responses followed by appropriate facial expressions and body gestures. Our preliminary evaluation yielded the most optimal parameters for generating the most plausible responses. With our system, we lay the groundwork for future development of LLM-based NPCs in VR.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {65},
numpages = {7},
keywords = {GPT-4, Generative Agents, Human-Computer Interaction, Large Language Models, Virtual Reality},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3589806.3600034,
author = {Greenman, Ben},
title = {GTP Benchmarks for Gradual Typing Performance},
year = {2023},
isbn = {9798400701764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589806.3600034},
doi = {10.1145/3589806.3600034},
abstract = {Reproducible, rigorous experiments are key to effective computing research because they provide grounding and a way to measure progress. Gradual typing is an emerging area that desperately needs such grounding. A gradual language lets programmers add types to part of a codebase while leaving the rest untyped. The critical research question is how to balance the guarantees that types provide against the run-time cost of enforcing them. Either weaker guarantees or better implementation methods could lead to answers, but without benchmarks for reproducibility there is no sound way to evaluate competing designs. The GTP Benchmark Suite is a rigorous testbed for gradual typing that supports reproducible experiments. Starting from a core suite of 21 programs drawn from a variety of applications, it enables the systematic exploration of over 40K gradually-typed program configurations via software for managing experiments and for analyzing results. Language designers have used the benchmarks to evaluate implementation strategies in at least seven major efforts since 2014. Furthermore, the benchmarks have proven useful for broader topics in gradual typing.},
booktitle = {Proceedings of the 2023 ACM Conference on Reproducibility and Replicability},
pages = {102–114},
numpages = {13},
keywords = {benchmarks, gradual typing, performance, reproducibility},
location = {Santa Cruz, CA, USA},
series = {ACM REP '23}
}

@article{10.1145/3617683,
author = {Knodt, Julian and Pan, Zherong and Wu, Kui and Gao, Xifeng},
title = {Joint UV Optimization and Texture Baking},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3617683},
doi = {10.1145/3617683},
abstract = {Level of detail has been widely used in interactive computer graphics. In current industrial 3D modeling pipelines, artists rely on commercial software to generate highly detailed models with UV maps and then bake textures for low-poly counterparts. In these pipelines, each step is performed separately, leading to unsatisfactory visual appearances for low polygon count models. Moreover, existing texture baking techniques assume the low-poly mesh has a small geometric difference from the high-poly, which is often not true in practice, especially with extremely low poly count models.To alleviate the visual discrepancy of the low-poly mesh, we propose to jointly optimize UV mappings during texture baking, allowing for low-poly models to faithfully replicate the appearance of the high-poly even with large geometric differences. We formulate the optimization within a differentiable rendering framework, allowing the automatic adjustment of texture regions to encode appearance information. To compensate for view parallax when two meshes have large geometric differences, we introduce a spherical harmonic parallax mapping, which uses spherical harmonic functions to modulate per-texel UV coordinates based on the view direction. We evaluate the effectiveness and robustness of our approach on a dataset composed of online downloaded models, with varying complexities and geometric discrepancies. Our method achieves superior quality over state-of-the-art techniques and commercial solutions.},
journal = {ACM Trans. Graph.},
month = {sep},
articleno = {2},
numpages = {20},
keywords = {Texture baking, differentiable rendering, UV optimization}
}

@article{10.1145/3533612,
author = {Gochfeld, David and Coulombe, Alex and Yeh, Yu-Jun and Lester, Robert and Fleming, Robert Barry and Meicher-Buzzi, Zachary and Tarr, Ari},
title = {A Tale of Two Productions: A Christmas Carol On Stage and in VR},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3533612},
doi = {10.1145/3533612},
abstract = {During the COVID-19 pandemic, many theatre companies began experimenting with new technologies and ways to bring their work to audiences. As theatres have resumed in-person performances, they are exploring how these new techniques can be incorporated into their productions. Recently one leading regional theatre company produced two versions of A Christmas Carol in parallel - one presented on stage, and the other entirely in virtual reality. They used virtual production tools including real-time motion capture, virtual humans, game engine rendering, and a new platform for multi-user VR experiences. We discuss the process, challenges, and creative decisions behind these shows, with an eye towards informing future theatrical productions.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {sep},
articleno = {41},
numpages = {9},
keywords = {Immersive Theatre, Real-time Performance Capture, Virtual Production, Virtual Reality}
}

@inproceedings{10.1145/3664475.3664535,
author = {Darke, A. M. and Olander, Isaac and Kim, Theodore},
title = {More Than Killmonger Locs: A Style Guide for Black Hair (in Computer Graphics)},
year = {2024},
isbn = {9798400706837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664475.3664535},
doi = {10.1145/3664475.3664535},
abstract = {We will cover recent advances and ongoing challenges in the depiction of Black hair, otherwise known as kinky, or Afro-textured hair. In computer graphics, the majority hair research has been in the depiction straight or wavy hair. As a result, many aspects of the aesthetics and mechanics of Black hair remain poorly understood. To help fill this gap, we will present Code My Crown, a free guide to creating Black digital hairstyles that we co-authored in collaboration with a community of game artists and Dove. We also cover styling guidelines for 3D models in the Open Source Afro Hair Library, and present Lifted Curls, our strand simulation technique specifically designed for Afro-textured hair. Finally, we will suggest future directions for hair research.},
booktitle = {ACM SIGGRAPH 2024 Courses},
articleno = {18},
numpages = {251},
location = {Denver, CO, USA},
series = {SIGGRAPH Courses '24}
}

@inproceedings{10.1145/3649921.3659854,
author = {Vet, Dennis and van Rozen, Riemer},
title = {The Puzzle Forecast: Tutorial Analytics Predict Trial and Error},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3659854},
doi = {10.1145/3649921.3659854},
abstract = {Puzzle tutorials are designed to teach puzzle-solving skills. For game designers, the difficulty is predicting if puzzle challenges will present players with opportunities for learning with trial and error. We aim to empower designers with tools and techniques for making those predictions by analyzing the goal chains inherent to good designs. We study PuzzleScript, an online game engine that has made the source code of high-quality puzzle tutorials available. Research on puzzles has yielded algorithms that can generate playtraces of solutions. However, until now the importance of failure traces has been mostly overlooked. As a result, there is a lack of tools with analytics that can help assess challenge. To deliver them, we propose a novel approach that enriches playtraces with verbs. We introduce TutoScript, a language for expressing goal chains in terms of verbs. By combining TutoScript with well-known search algorithms, and by mapping rules to verbs, TutoMate can enrich, analyze and visualize generated playtraces of solutions, failures and dead ends. Two case studies on Lime Rick and Block Faker demonstrate how it helps to analyze simple goal chains, and can also detect broken tutorials. Our solution takes a promising step towards generic techniques for analyzing and generating tutorials.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {77},
numpages = {8},
keywords = {analytics, automated game design, domain-specific languages, learning, puzzle tutorials, skill atoms, trial and error, verbs},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@article{10.1145/3572896,
author = {Montano-Murillo, Roberto and Hirayama, Ryuji and Martinez Plasencia, Diego},
title = {OpenMPD: A Low-Level Presentation Engine for Multimodal Particle-Based Displays},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/3572896},
doi = {10.1145/3572896},
abstract = {Phased arrays of transducers have been quickly evolving in terms of software and hardware with applications in haptics (acoustic vibrations), display (levitation), and audio. Most recently, Multimodal Particle-based Displays (MPDs) have even demonstrated volumetric content that can be seen, heard, and felt simultaneously, without additional instrumentation. However, current software tools only support individual modalities and they do not address the integration and exploitation of the multi-modal potential of MPDs. This is because there is no standardized presentation pipeline tackling the challenges related to presenting such kind of multi-modal content (e.g., multi-modal support, multi-rate synchronization at 10 KHz, visual rendering or synchronization and continuity). This article presents OpenMPD, a low-level presentation engine that deals with these challenges and allows structured exploitation of any type of MPD content (i.e., visual, tactile, audio). We characterize OpenMPD’s performance and illustrate how it can be integrated into higher-level development tools (i.e., Unity game engine). We then illustrate its ability to enable novel presentation capabilities, such as support of multiple MPD contents, dexterous manipulations of fast-moving particles, or novel swept-volume MPD content.},
journal = {ACM Trans. Graph.},
month = {apr},
articleno = {24},
numpages = {13},
keywords = {Particle-based Display, acoustic levitation, presentation engine}
}

@inproceedings{10.1145/3609987.3609993,
author = {Lamprou, Evangelos and Fidas, Christos},
title = {Investigating Applicability Heuristics of Answer Set Programming in Game Development: Use Cases and Empirical Study},
year = {2023},
isbn = {9798400708886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609987.3609993},
doi = {10.1145/3609987.3609993},
abstract = {The game industry is continuously growing and evolving, with new ways of creating games being developed. However, even with the availability of powerful game engines, developers are still forced to spend time and effort implementing common game features, such as basic AI, path-finding, and simple scene variations. This can become a serious detriment for indie game developers. The present research focuses on the application of Answer Set Programming (ASP) methods within the game development process, aiming to support rapid and cost-effective game prototyping for indie game developers. Specifically, we present a pragmatic approach to the usage of ASP for game development within certain use cases and we report on evaluation results based on feedback that was received from end-users. Analysis of results demonstrates how ASP can be used, providing new ways of thinking about game mechanics and content creation, and eventually paving the way for new game design frameworks and possibilities. On the downside, adoption of the suggested method can be difficult due to unfamiliarity with the ASP programming paradigm.},
booktitle = {Proceedings of the 2nd International Conference of the ACM Greek SIGCHI Chapter},
articleno = {6},
numpages = {5},
keywords = {Game Development Answer Set Programming Evaluation Study},
location = {Athens, Greece},
series = {CHIGREECE '23}
}

@article{10.1145/3627160,
author = {Zang, Andi and Xu, Runsheng and Trajcevski, Goce and Zhou, Fan},
title = {Data Issues in High-Definition Maps Furniture – A Survey},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2374-0353},
url = {https://doi.org/10.1145/3627160},
doi = {10.1145/3627160},
abstract = {The rapid advancements in sensing techniques, networking, and artificial intelligence (AI) algorithms in recent years have brought autonomous driving vehicles closer to common use in vehicular transportation. One of the fundamental components to enable autonomous driving functionalities are High-Definition (HD) maps – a type of map that carries highly accurate and much richer information than conventional maps. The creation and use of HD maps rely on advances in multiple disciplines, such as computer vision/object perception, geographic information systems, sensing, simultaneous localization and mapping, machine learning, etc. To date, several survey papers have been published describing the literature related to HD maps and their use in specialized contexts. In this survey, we aim to provide (1) a comprehensive overview of the issues and solutions related to HD maps and their use without attachment to a particular context; (2) a detailed coverage of the important domain knowledge of HD map furniture, from acquisition techniques and extraction approaches, through HD map–related datasets, to furniture quality assessment metrics, for the purpose of providing a comprehensive understanding of the entire workflow of HD map furniture generation, as well as its use.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = {jan},
articleno = {5},
numpages = {37},
keywords = {autonomous driving, heterogeneous datasets, high-definition maps}
}

@inproceedings{10.1145/3615452.3617937,
author = {Scargill, Tim and Hadziahmetovic, Majda and Gorlatova, Maria},
title = {Invisible Textures: Comparing Machine and Human Perception of Environment Texture for AR},
year = {2023},
isbn = {9798400703393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615452.3617937},
doi = {10.1145/3615452.3617937},
abstract = {Mobile augmented reality (AR) has a wide range of promising applications, but its efficacy is subject to the impact of environment texture on both machine and human perception. Performance of the machine perception algorithm underlying accurate positioning of virtual content, visual-inertial SLAM (VI-SLAM), is known to degrade in low-texture conditions, but there is a lack of data in realistic scenarios. We address this through extensive experiments using a game engine-based emulator, with 112 textures and over 5000 trials. Conversely, human task performance and response times in AR have been shown to increase in environments perceived as textured. We investigate and provide encouraging evidence for invisible textures, which result in good VI-SLAM performance with minimal impact on human perception of virtual content. This arises from fundamental differences between VI-SLAM-based machine perception, and human perception as described by the contrast sensitivity function. Our insights open up exciting possibilities for deploying ambient IoT devices that display invisible textures, as part of systems which automatically optimize AR environments.},
booktitle = {Proceedings of the 1st ACM Workshop on Mobile Immersive Computing, Networking, and Systems},
pages = {229–236},
numpages = {8},
keywords = {augmented reality, VI-SLAM, perception, texture},
location = {Madrid, Spain},
series = {ImmerCom '23}
}

@inproceedings{10.1145/3555858.3555898,
author = {Volokh, Sasha and Halfond, William G.J.},
title = {Static Analysis for Automated Identification of Valid Game Actions During Exploration},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555898},
doi = {10.1145/3555858.3555898},
abstract = {Automated exploration has many important applications for testing and analysis of games. Techniques for automated exploration require the capability of identifying the set of available user actions at a given game state, then performing the action selected by the exploration logic. This has been traditionally supported by having the game developer provide an API for this purpose or randomly guessing inputs. In this paper we develop a program analysis based technique for performing an automated analysis of the input-handling logic of the game code, then using this information to provide the set of player actions available at a game state (as well as the device inputs that should be simulated to perform a chosen action). We focus on developing such a technique for games built with the Unity game engine. We implemented an automatic exploration tool based on our technique and evaluated its state exploration performance for six open-source Unity games. We found that our approach is competitive with manually specified actions and is fast enough to play the games in real time.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {2},
numpages = {10},
keywords = {automated testing, game testing, program analysis},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3604479.3604503,
author = {de Oliveira, Taina Ribeiro and Gomes, Guilherme Iglesias Rocha and Martins, Thiago Silva and Borges, Luiz Felipe Muniz Rocha and Ludke, Gabriel Giesen and Andreao, Rodrigo Varejao and Cotini, Luan Guio and Viana, Pedro Henrique Porto and Schimidt, Marchelo Queiroz and Mestria, Mario},
title = {Virtual Reality System for Inspection and Training in Wheel Loader},
year = {2024},
isbn = {9798400700026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604479.3604503},
doi = {10.1145/3604479.3604503},
abstract = {In this study, a simulator is proposed for inspection and training of a wheel loader based on a first-person serious game and virtual reality. The simulator was developed using a cross-platform game engine and integrated into various virtual reality headsets. The goal of the game was to conduct maintenance inspection procedures on the wheel loader of a mining company. A group of 34 users tested the simulator and completed a survey. The results showed that the system is suitable for inspection and training of staff before they can use a real wheel loader at a mining company. The findings revealed that implementing a simulator for inspection and maintenance training can enhance the process, reducing expenses and increasing safety. In addition, methods in the real-world of training are scarcely sufficient to instruct operators in all dangerous situations. These methods are generally effective, but they do not give operators the real-life impression of all scenarios. In this sense, the serious game makes the training effective because it gives operators a sense of realism. Beyond, the simulator can be extended to include other types of procedures with several situations.},
booktitle = {Proceedings of the 24th Symposium on Virtual and Augmented Reality},
pages = {11–20},
numpages = {10},
keywords = {Industry 4.0, Inspection virtual environment, Serious game, Training, Virtual reality},
location = {Natal, RN, Brazil},
series = {SVR '22}
}

@inproceedings{10.1145/3588028.3603654,
author = {Ko, Ginam and Yoon, Jaeseok and Jung, Byungseok and Nam, SangHun},
title = {BStick: Hand-held Haptic Controller for Virtual Reality Applications},
year = {2023},
isbn = {9798400701528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588028.3603654},
doi = {10.1145/3588028.3603654},
abstract = {This study proposes Bstick-Mark2, a handheld virtual reality (VR) haptic controller that monitors and controls input from five fingers in real-time with pressure sensors and linear motors. Bstick-Mark2 is designed and fabricated to enable users to use both hands along with a head-mounted display (HMD) while freely roaming based on Bluetooth technologies. When a user holds the haptic controller with fingers, the data input from five pressure sensors is transmitted to microcontroller unit (MCU) to independently control the movements of five linear motors. The pressure and position data of linear motors are sent to a computer connected to a VR display through a Bluetooth module embedded in the controller and utilized in interaction with a virtual object and virtual hand movements using the Unity game engine. Bstick-Mark2 can withstand 22 N of force per finger to maintain the pressing force of a male’s finger and is compact to enable users to easily handle using their hands. It enables to make sensations of grabbing and controlling while interacting with VR content.},
booktitle = {ACM SIGGRAPH 2023 Posters},
articleno = {16},
numpages = {2},
keywords = {handheld controller, haptic device, virtual reality},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3505270.3558341,
author = {Vasquez Gomez, Juan Carlos},
title = {Interactive Gamification for New Experimental Music: Initial Findings},
year = {2022},
isbn = {9781450392112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505270.3558341},
doi = {10.1145/3505270.3558341},
abstract = {"Ecstasy / Light / Inertia" is an interactive narrative-driven application that proposes a novel method to experience new music in a gamified setting. It is comprised of a 3d environment that allows navigation and interaction aiming to present a narrative-driven virtual alternative to a traditional concert setting/sound art gallery. "Ecstasy / Light / Inertia" exhibits multi-layered music installations in virtual spaces built with high-quality real-life 3d scans inside the Unreal Game Engine. It additionally combines spatial audio modeling, Nordic modernist architecture/nature, and gamified interaction paradigms to pursue a setting capable of delivering an engaging musical experience accompanying a narrative-driven design. The current paper presents a summary of the justification for this project, documents the advances of the prologue and first level, and finally sets custom evaluation methodologies for the near future.},
booktitle = {Extended Abstracts of the 2022 Annual Symposium on Computer-Human Interaction in Play},
pages = {103–106},
numpages = {4},
keywords = {electroacoustic music, game audio, gamification, spatial audio},
location = {Bremen, Germany},
series = {CHI PLAY '22}
}

@article{10.1145/3565020,
author = {Ribeiro de Oliveira, Tain\~{a} and Biancardi Rodrigues, Brenda and Moura da Silva, Matheus and Antonio N. Spinass\'{e}, Rafael and Giesen Ludke, Gabriel and Ruy Soares Gaudio, Mateus and Iglesias Rocha Gomes, Guilherme and Guio Cotini, Luan and da Silva Vargens, Daniel and Queiroz Schimidt, Marcelo and Varej\~{a}o Andre\~{a}o, Rodrigo and Mestria, M\'{a}rio},
title = {Virtual Reality Solutions Employing Artificial Intelligence Methods: A Systematic Literature Review},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3565020},
doi = {10.1145/3565020},
abstract = {Although there are methods of artificial intelligence (AI) applied to virtual reality (VR) solutions, there are few studies in the literature. Thus, to fill this gap, we performed a systematic literature review of these methods. In this review, we apply a methodology proposed in the literature that locates existing studies, selects and evaluates contributions, analyses, and synthesizes data. We used Google Scholar and databases such as Elsevier's Scopus, ACM Digital Library, and IEEE Xplore Digital Library. A set of inclusion and exclusion criteria were used to select documents. The results showed that when AI methods are used in VR applications, the main advantages are high efficiency and precision of algorithms. Moreover, we observe that machine learning is the most applied AI scientific technique in VR applications. In conclusion, this paper showed that the combination of AI and VR contributes to new trends, opportunities, and applications for human-machine interactive devices, education, agriculture, transport, 3D image reconstruction, and health. We also concluded that the usage of AI in VR provides potential benefits in other fields of the real world such as teleconferencing, emotion interaction, tourist services, and image data extraction.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {214},
numpages = {29},
keywords = {Virtual reality, artificial intelligence, Industry 4.0, literature review}
}

@article{10.1145/3588433,
author = {Shahbazi, Nima and Lin, Yin and Asudeh, Abolfazl and Jagadish, H. V.},
title = {Representation Bias in Data: A Survey on Identification and Resolution Techniques},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3588433},
doi = {10.1145/3588433},
abstract = {Data-driven algorithms are only as good as the data they work with, while datasets, especially social data, often fail to represent minorities adequately. Representation Bias in data can happen due to various reasons, ranging from historical discrimination to selection and sampling biases in the data acquisition and preparation methods. Given that “bias in, bias out,” one cannot expect AI-based solutions to have equitable outcomes for societal applications, without addressing issues such as representation bias. While there has been extensive study of fairness in machine learning models, including several review papers, bias in the data has been less studied. This article reviews the literature on identifying and resolving representation bias as a feature of a dataset, independent of how consumed later. The scope of this survey is bounded to structured (tabular) and unstructured (e.g., image, text, graph) data. It presents taxonomies to categorize the studied techniques based on multiple design dimensions and provides a side-by-side comparison of their properties.There is still a long way to fully address representation bias issues in data. The authors hope that this survey motivates researchers to approach these challenges in the future by observing existing work within their respective domains.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {293},
numpages = {39},
keywords = {Responsible data science, fairness in machine learning, data equity systems, data-centric AI, AI-ready data}
}

@article{10.1145/3592427,
author = {Shacklett, Brennan and Rosenzweig, Luc Guy and Xie, Zhiqiang and Sarkar, Bidipta and Szot, Andrew and Wijmans, Erik and Koltun, Vladlen and Batra, Dhruv and Fatahalian, Kayvon},
title = {An Extensible, Data-Oriented Architecture for High-Performance, Many-World Simulation},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592427},
doi = {10.1145/3592427},
abstract = {Training AI agents to perform complex tasks in simulated worlds requires millions to billions of steps of experience. To achieve high performance, today's fastest simulators for training AI agents adopt the idea of batch simulation: using a single simulation engine to simultaneously step many environments in parallel. We introduce a framework for productively authoring novel training environments (including custom logic for environment generation, environment time stepping, and generating agent observations and rewards) that execute as high-performance, GPU-accelerated batched simulators. Our key observation is that the entity-component-system (ECS) design pattern, popular for expressing CPU-side game logic today, is also well-suited for providing the structure needed for high-performance batched simulators. We contribute the first fully-GPU accelerated ECS implementation that natively supports batch environment simulation. We demonstrate how ECS abstractions impose structure on a training environment's logic and state that allows the system to efficiently manage state, amortize work, and identify GPU-friendly coherent parallel computations within and across different environments. We implement several learning environments in this framework, and demonstrate GPU speedups of two to three orders of magnitude over open source CPU baselines and 5-33\texttimes{} over strong baselines running on a 32-thread CPU. An implementation of the OpenAI hide and seek 3D environment written in our framework, which performs rigid body physics and ray tracing in each simulator step, achieves over 1.9 million environment steps per second on a single GPU.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {90},
numpages = {13},
keywords = {game AI, reinforcement learning}
}

@inproceedings{10.1145/3577190.3614107,
author = {Abadi, Romina and Wilcox, Laurie M and Allison, Robert Scott},
title = {Using Augmented Reality to Assess the Role of Intuitive Physics in the Water-Level Task},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614107},
doi = {10.1145/3577190.3614107},
abstract = {The “Water Level Task” (WLT) is a classic cognitive task that assesses an individual’s ability to draw the water level in a tilted container. Most of the existing research has used 2D imagery and shown that adults struggle with the task. Our research investigates if the use of augmented reality (AR) improves an individual’s performance by engaging embodied interaction and natural interaction with the world, thus taking advantage of their “intuitive physics.” We created a traditional online WLT to recruit low- and high-scoring participants for the AR experiment. Using a HoloLens2 AR headset, we created two containers half-filled with water. One of the simulations featured a water surface that did not remain horizontal when the container was tilted, while in the other simulation, the water surface remained level. Participants were able to interact with the containers and were asked to indicate which simulation looked more natural. Our results revealed that individuals prone to errors in the 2D version of the task were more likely to make errors in the AR version, indicating that misconceptions about water orientation persist even in a more natural setting. However, people’s perceptions of the natural orientation of water differed in 2D and AR settings, suggesting that different perceptual and cognitive factors were involved in participants’ intuitive understanding of the natural orientation of water in the two settings. Additionally, we found that participants were insensitive to minor tilts of the water surface. Our study highlights the potential benefits of using AR to create more realistic and interactive virtual environments, which provides a basis for further study of intuitive physics and how humans interact with physical environments.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {622–630},
numpages = {9},
keywords = {Augmented Reality, Cognition, Intuitive Physics, Perception, Water-Level Task},
location = {Paris, France},
series = {ICMI '23}
}

@inproceedings{10.1145/3543873.3587593,
author = {Yang, Riyan and Li, Lin and Gan, Wensheng and Chen, Zefeng and Qi, Zhenlian},
title = {The Human-Centric Metaverse: A Survey},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587593},
doi = {10.1145/3543873.3587593},
abstract = {In the era of the Web of Things, the Metaverse is expected to be the landing site for the next generation of the Internet, resulting in the increased popularity of related technologies and applications in recent years and gradually becoming the focus of Internet research. The Metaverse, as a link between the real and virtual worlds, can provide users with immersive experiences. As the concept of the Metaverse grows in popularity, many scholars and developers begin to focus on the Metaverse’s ethics and core. This paper argues that the Metaverse should be centered on humans. That is, humans constitute the majority of the Metaverse. As a result, we begin this paper by introducing the Metaverse’s origins, characteristics, related technologies, and the concept of the human-centric Metaverse (HCM). Second, we discuss the manifestation of human-centric in the Metaverse. Finally, we discuss some current issues in the construction of HCM. In this paper, we provide a detailed review of the applications of human-centric technologies in the Metaverse, as well as the relevant HCM application scenarios. We hope that this paper can provide researchers and developers with some directions and ideas for human-centric Metaverse construction.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1296–1306},
numpages = {11},
keywords = {Metaverse, Web of Things, applications, ethics., human-centric},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3597503.3639111,
author = {Dolata, Mateusz and Lange, Norbert and Schwabe, Gerhard},
title = {Development in times of hype: How freelancers explore Generative AI?},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639111},
doi = {10.1145/3597503.3639111},
abstract = {The rise of generative AI has led many companies to hire freelancers to harness its potential. However, this technology presents unique challenges to developers who have not previously engaged with it. Freelancers may find these challenges daunting due to the absence of organizational support and their reliance on positive client feedback. In a study involving 52 freelance developers, we identified multiple challenges associated with developing solutions based on generative AI. Freelancers often struggle with aspects they perceive as unique to generative AI such as unpredictability of its output, the occurrence of hallucinations, and the inconsistent effort required due to trial-and-error prompting cycles. Further, the limitations of specific frameworks, such as token limits and long response times, add to the complexity. Hype-related issues, such as inflated client expectations and a rapidly evolving technological ecosystem, further exacerbate the difficulties. To address these issues, we propose Software Engineering for Generative AI (SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the software engineering community can provide effective guidance. This support is essential for freelancers working with generative AI and other emerging technologies.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {183},
numpages = {13},
keywords = {generative AI, AI-based systems, challenges, freelancers, hype, SE for generative AI, SE4GenAI, hype-induced SE, hype-SE, fashion, product, paradigm, novelty, qualitative research},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643834.3661525,
author = {Vaithilingam, Priyan and Arawjo, Ian and Glassman, Elena L.},
title = {Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661525},
doi = {10.1145/3643834.3661525},
abstract = {We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value that large AI models can provide design compared to past technologies. We arrive at three affordances—dynamic grounding, constructive negotiation, and sustainable motivation—that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {289–300},
numpages = {12},
keywords = {AI affordances, Design fiction, Grounding, Human AI collaboration, Language models},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3641234.3671037,
author = {McGhee, John and Bourke, Conan and Lawther, Robert and Zhou, Hao and Petersen, Rolf},
title = {3DCrewCap: Applying 3D Volumetric video capture for XR helicopter rescue crew training and simulation.},
year = {2024},
isbn = {9798400705168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641234.3671037},
doi = {10.1145/3641234.3671037},
abstract = {Helicopter rescue crew training scenarios are complex and hard to simulate in game engines with animated 3D digital characters. This challenge is compounded when simulating Realtime photorealistic animated character sequences on XR-based Head-Mounted Displays (HMD). In this research, we present a practical early-stage development of 3D volumetric video capture and playback workflow for use in helicopter rescue crew training on XR HMDs. We break down the workflow of using Gaussian Splat approaches to construct keyframed 3D animated models of the rescue crew training actions. This novel application approach provides a practical example of the photorealistic capture and XR display of helicopter rescue crew performing training scenarios.},
booktitle = {ACM SIGGRAPH 2024 Posters},
articleno = {34},
numpages = {2},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3587423.3595529,
author = {Bailey, Mike},
title = {The Vulkan Computer Graphics API},
year = {2023},
isbn = {9798400701450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587423.3595529},
doi = {10.1145/3587423.3595529},
booktitle = {ACM SIGGRAPH 2023 Courses},
articleno = {19},
numpages = {158},
location = {Los Angeles, California},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3543873.3585572,
author = {Mehanna, Naif and Rudametkin, Walter},
title = {Caught in the Game: On the History and Evolution of Web Browser Gaming},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3585572},
doi = {10.1145/3543873.3585572},
abstract = {Web browsers have come a long way since their inception, evolving from a simple means of displaying text documents over the network to complex software stacks with advanced graphics and network capabilities. As personal computers grew in popularity, developers jumped at the opportunity to deploy cross-platform games with centralized management and a low barrier to entry. Simply going to the right address is now enough to start a game. From text-based to GPU-powered 3D games, browser gaming has evolved to become a strong alternative to traditional console and mobile-based gaming, targeting both casual and advanced gamers. Browser technology has also evolved to accommodate more demanding applications, sometimes even supplanting functions typically left to the operating system. Today, websites display rich, computationally intensive, hardware-accelerated graphics, allowing developers to build ever-more impressive applications and games. In this paper, we present the evolution of browser gaming and the technologies that enabled it, from the release of the first text-based games in the early 1990s to current open-world and game-engine-powered browser games. We discuss the societal impact of browser gaming and how it has allowed a new target audience to access digital gaming. Finally, we review the potential future evolution of the browser gaming industry.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {601–609},
numpages = {9},
keywords = {browser games, game engines, web browsers, web history},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3649921.3659853,
author = {Moradi Karkaj, Arash and Nelson, Mark J. and Koutis, Ioannis and Hoover, Amy K.},
title = {Prompt Wrangling: On Replication and Generalization in Large Language Models for PCG Levels},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3659853},
doi = {10.1145/3649921.3659853},
abstract = {The ChatGPT4PCG competition calls for participants to submit inputs to ChatGPT or prompts that guide its output toward instructions to generate levels as sequences of Tetris-like block drops. Prompts submitted to the competition are queried by ChatGPT to generate levels that resemble letters of the English alphabet. Levels are evaluated based on their similarity to the target letter and physical stability in the game engine. This provides a quantitative evaluation setting for prompt-based procedural content generation (PCG), an approach that has been gaining popularity in PCG, as in other areas of generative AI. This paper focuses on replicating and generalizing the competition results. The replication experiments in the paper first aim to test whether the number of responses gathered from ChatGPT is sufficient to account for the stochasticity requery the original prompt submissions to rerun the original scripts from the competition on different machines about six months after the competition organizers. We re-run the competition, using the original scripts, but on our own machines, several months later, and with varying sample sizes. We find that results largely replicate, except that two of the 15 submissions do much better in our replication, for reasons we can only partly determine. When it comes to generalization, we notice that the top-performing prompt has instructions for all 26 target levels hardcoded, which is at odds with the PCGML goal of generating new, previously unseen content from examples. We perform experiments in a more restricted few-shot prompting scenario, and find that generalization remains a challenge for current approaches.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {76},
numpages = {8},
keywords = {Evaluating Generalization, Generalizability, Large Language Models (LLMs), Procedural content generation (PCG), Science Birds},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@inproceedings{10.1145/3555858.3555902,
author = {Trivedi, Chintan and Makantasis, Konstantinos and Liapis, Antonios and Yannakakis, Georgios N.},
title = {Game&nbsp;State Learning via Game&nbsp;Scene&nbsp;Augmentation},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555902},
doi = {10.1145/3555858.3555902},
abstract = {Having access to accurate game state information is of utmost importance for any artificial intelligence task including game-playing, testing, player modeling, and procedural content generation. Self-Supervised Learning (SSL) techniques have shown to be capable of inferring accurate game state information from the high-dimensional pixel input of game footage into compressed latent representations. Contrastive Learning is a popular SSL paradigm where the visual understanding of the game’s images comes from contrasting dissimilar and similar game states defined by simple image augmentation methods. In this study, we introduce a new game scene augmentation technique—named GameCLR—that takes advantage of the game-engine to define and synthesize specific, highly-controlled renderings of different game states, thereby, boosting contrastive learning performance. We test our GameCLR technique on images of the CARLA driving simulator environment and compare it against the popular SimCLR baseline SSL method. Our results suggest that GameCLR can infer the game’s state information from game footage more accurately compared to the baseline. Our proposed approach allows us to conduct game artificial intelligence research by directly utilizing screen pixels as input.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {48},
numpages = {4},
keywords = {computer vision, contrastive learning, game state representations, representation learning, self-supervised learning},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3613905.3644054,
author = {Liu, Pinyao and Kitson, Alexandra and Picard-Deland, Claudia and Carr, Michelle and Liu, Sijia and Lc, Ray and Zhu-Tian, Chen},
title = {Virtual Dream Reliving: Exploring Generative AI in Immersive Environment for Dream Re-experiencing},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3644054},
doi = {10.1145/3613905.3644054},
abstract = {Dreaming is a fundamental component of the human experience. Modern-day psychologists and neuroscientists use “dreamwork” to describe a variety of strategies that deepen and engage with dreams. Re-experiencing the dream as if reliving the memory, feelings, and bodily sensations from the dream is a key element shared by many dreamwork practices. In this paper, we propose the concept of "dreamwork engineering" by creating a system enabling dream re-experiencing in a virtual reality environment through generative AI. Through an autoethnographic study, the first author documented his own dreams and relived his dream experiences for two weeks. Based on our results, we propose a technology-aided dreamwork framework, where technology could potentially augment traditional dreamwork methods through spatiality and movement, interactivity and abstract anchor. We further highlight the collaborative role of technology in dreamwork and advocate that the scientific community could also benefit from dreaming and dreamwork for scientific creativity.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {569},
numpages = {11},
keywords = {Dream Re-experiencing, Dreamwork Engineering, Generative AI, Personal Insight, Scientific Creativity, Virtual Reality},
location = {
},
series = {CHI EA '24}
}

@article{10.1145/3549499,
author = {Rogoda, Kamil and Daniszewski, Piotr and Florowski, Kamil and Mathur, Rishab and Amouzgar, Kourosh and Mackenzie, James and Sauv\'{e}, Kim and Karnik, Abhijit},
title = {FoodChoices(Q): Exploring the Design of a Serious Game Proxy for Likert-style Survey Questionnaires},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549499},
doi = {10.1145/3549499},
abstract = {Likert-style questionnaires and surveys are commonly used tools for research. To alleviate survey-fatigue, researchers have explored gamification routes to increase engagement and lower drop-outs. However, these attempts still rely on direct use of questionnaire text and focus on creating engagement around the actual activity and do not fully alleviate the challenges of filling survey. In this paper, we explore an alternative approach involving the use of a serious game to capture user responses through in-game activities rather than direct questions. We chose the Food Choice Questionnaire (FCQ) and explored the design challenges of creating a serious game which deploys a sub-sample of the FCQ questions as four mini-game activities. The player actions and decisions are used to compute a result which is compared with their FCQ responses. We demonstrate the method to evaluate the equivalence of game results to the questionnaire responses. We discuss how future serious games can be designed and evaluated to generate similar outcomes while avoiding potential pitfalls through design and analysis.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {236},
numpages = {15},
keywords = {food choice questionnaire, gamification, serious games, survey fatigue}
}

@inproceedings{10.1145/3625008.3625036,
author = {Monteiro, Erico Patricio and Campos, Alexandre and Gitirana, Marcelo and Oviedo, Rafael},
title = {Virtual Wheelchair Accessibility Evaluation Tool for Virtual Environments: Design and Preliminary Tests},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625036},
doi = {10.1145/3625008.3625036},
abstract = {The growing demand for inclusivity towards accessibility in various social settings leads to stricter global standards, which emphasize the importance of having an assessment tool to gauge the virtual environment experience of wheelchair users. This proposal introduces a virtual reality manual wheelchair simulator able to capture trajectory and time metrics. The objective is to provide designers, architects, and wheelchair users with valuable data for redesign or comparison among virtual environments. The preliminary results of this study indicate promising feasibility in terms of user displacement, although certain aspects of the virtual interface, particularly those related to physics and dynamics calculations, still require improvement. Future work should address those aspects, focus on in-editor modular integration, track check-point system and a questionnaire system.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {254–258},
numpages = {5},
keywords = {accessibility, simulation, virtual reality, wheelchair},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@inproceedings{10.5555/3643142.3643159,
author = {Jawad, Alvi and Martin, Cristina Ruiz and Wainer, Gabriel},
title = {Modeling Reactive Game Agents Using the Cell-Devs Modeling Formalism},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {Intelligent game agents are vital to modern games as they add life, story, and immersion to the game environment. The requests in the gaming industry for more realism have made intelligent agents more important than ever. Modeling and simulation of game agents and their surrounding environment provide an alternate setting to study dynamic agent behavior before integration into the game engine. The Cell-DEVS formalism, an extension of Cellular Automata, allows modeling such behaviors using the rigorously formalized Discrete Event Systems Specification (DEVS) formalism. This paper explains how to model and test reactive game agents using the Cell-DEVS formalism and the CD++ toolkit. To analyze the dynamic behavior of such agents, we perform several experiments in varying system configurations. Our experimental results confirm the versatility of Cell-DEVS and the functionalities in the CD++ toolkit to model comfort-driven, exploratory, and desire-driven game agents.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {209–220},
numpages = {12},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@article{10.1145/3508361,
author = {Martin, Daniel and Malpica, Sandra and Gutierrez, Diego and Masia, Belen and Serrano, Ana},
title = {Multimodality in VR: A Survey},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3508361},
doi = {10.1145/3508361},
abstract = {Virtual reality (VR) is rapidly growing, with the potential to change the way we create and consume content. In VR, users integrate multimodal sensory information they receive to create a unified perception of the virtual world. In this survey, we review the body of work addressing multimodality in VR and its role and benefits in user experience, together with different applications that leverage multimodality in many disciplines. These works thus encompass several fields of research and demonstrate that multimodality plays a fundamental role in VR, enhancing the experience, improving overall performance, and yielding unprecedented abilities in skill and knowledge transfer.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {216},
numpages = {36},
keywords = {Virtual reality, immersive environments, multimodality}
}

@inproceedings{10.1145/3639477.3639726,
author = {Sun, Gengyi and Meidani, Mehran and Habchi, Sarra and Nayrolles, Mathieu and Mcintosh, Shane},
title = {Code Impact Beyond Disciplinary Boundaries: Constructing a Multidisciplinary Dependency Graph and Analyzing Cross-Boundary Impact},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639726},
doi = {10.1145/3639477.3639726},
abstract = {To produce a video game, engineers and artists must iterate on the same project simultaneously. In such projects, a change to the work products of any of the teams can impact the work of other teams. As a result, any analytics tasks should consider intra- and inter-dependencies within and between artifacts produced by different teams. For instance, the focus of quality assurance teams on changes that are local to a team differs from one that impacts others. To extract and analyze such cross-disciplinary dependencies, we propose the multidisciplinary dependency graph. We instantiate our idea by developing tools that extract dependencies and construct the graph at Ubisoft---a multinational video game organization with more than 18,000 employees.Our analysis of a recently launched video game project reveals that code files only make up 2.8% of the dependency graph, and code-to-code dependencies only make up 4.3% of all dependencies. We also observe that 44% of the studied source code changes impact the artifacts that are developed by other teams, highlighting the importance of analyzing inter-artifact dependencies. A comparative analysis of cross-boundary changes with changes that do not cross boundaries indicates that cross-boundary changes are: (1) impacting a median of 120,368 files; (2) with a 51% probability of causing build failures; and (3) a 67% likelihood of introducing defects. All three measurements are larger than changes that do not cross boundaries to statistically significant degrees.We also find that cross-boundary changes are: (4) more commonly associated with gameplay functionality and feature additions that directly impact the game experience than changes that do not cross boundaries, and (5) disproportionately produced by the same team (74% of the contributors are associated with that team).},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {122–133},
numpages = {12},
keywords = {interdisciplinary dependencies, build systems, impact analysis},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@article{10.1145/3675757,
author = {Jelonek, Karissa and Kunde, Siya and Simms, Nathan and Uriarte, Gerson and Duncan, Brittany},
title = {Comparison of Human-Drone Distancing Studies Across In-person and Online Modalities},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675757},
doi = {10.1145/3675757},
abstract = {Human-robot proxemics behaviors can vary based on personal, robot, and environmental factors which, along with their deployment in public-facing interactions, calls for an in-depth exploration. This paper explores the impact of altitude and safety modifications of small unmanned aerial vehicle (sUAV) on users’ comfortable interaction distance. By leveraging interaction techniques from literature like video, sound, and simulations, we explore personal space interactions in online studies (N=376) with the sUAV and the Double telepresence robot. We then compare the findings with our in-person interaction data (N=47). While in-person interactions are the ultimate goal, online methods can be used to reduce resources, allow larger sample sizes, and may lead to a more comprehensive sampling of population than would be expected from in-person studies. The lessons learned from this work are applicable broadly within the social robotics community, even outside those who are interested in proxemics interactions, to conduct future crowd-sourced experiments. The various modalities provided similar trends when compared with data from in-person studies. While the distances may not have been precise compared to those measured in the real world, these experiments are useful to detect patterns in human-robot interactions, and to conduct formative studies before committing resources to in-person testing.},
note = {Just Accepted},
journal = {J. Hum.-Robot Interact.},
month = {aug},
keywords = {human-robot interaction, proxemics, crowd sourcing, evaluation methods, interaction design process and methods, scenario-based design}
}

@inproceedings{10.1145/3613904.3642040,
author = {Kamath, Purnima and Morreale, Fabio and Bagaskara, Priambudi Lintang and Wei, Yize and Nanayakkara, Suranga},
title = {Sound Designer-Generative AI Interactions: Towards Designing Creative Support Tools for Professional Sound Designers},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642040},
doi = {10.1145/3613904.3642040},
abstract = {The practice of sound design involves creating and manipulating environmental sounds for music, films, or games. Recently, an increasing number of studies have adopted generative AI to assist in sound design co-creation. Most of these studies focus on the needs of novices, and less on the pragmatic needs of sound design practitioners. In this paper, we aim to understand how generative AI models might support sound designers in their practice. We designed two interactive generative AI models as Creative Support Tools (CSTs) and invited nine professional sound design practitioners to apply the CSTs in their practice. We conducted semi-structured interviews and reflected on the challenges and opportunities of using generative AI in mixed-initiative interfaces for sound design. We provide insights into sound designers’ expectations of generative AI and highlight opportunities to situate generative AI-based tools within the design process. Finally, we discuss design considerations for human-AI interaction researchers working with audio.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {730},
numpages = {17},
keywords = {Audio, Creative Support Tools, Generative AI, Mixed-Initiative Creative Interfaces, Sound design},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.5555/3665464.3665470,
author = {Hoot, Charles and Eloe, Nathan W. and Linville, Diana},
title = {Introducing Controlled Variability in Programming Assignments},
year = {2024},
issue_date = {April 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {6},
issn = {1937-4771},
abstract = {Evaluating students in Computer Science related fields has always been challenging. Often times programming and problem solving skills are evaluated either with a fully creative (or client driven/capstone) project or a program that generates "the right answers". As class size increases, evaluating the work product of students is more challenging. We illustrate a middle ground approach that allows students in a larger class setting to benefit from some creative choices while still ensuring a minimum level of difficulty and efficient grading processes.},
journal = {J. Comput. Sci. Coll.},
month = {may},
pages = {52–60},
numpages = {9}
}

@book{10.1145/3596711,
editor = {Whitton, Mary C.},
title = {Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
year = {2023},
isbn = {9798400708978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {Volume 2},
abstract = {When we began planning how to celebrate 50 years of SIGGRAPH Conferences, there was unanimous agreement that one of the projects should be publishing a second volume of Seminal Graphics Papers. The first volume was published in 1998 as part of the celebration of the 25th SIGGRAPH conference. Seminal Graphics Papers Volume 2, perhaps more than any other activity undertaken in this milestone year, celebrates ACM SIGGRAPH's origins and continued success as a Technical and Professional Society. This collection of papers typifies the ground-breaking research that has been the conference's hallmark since 1974. A quick scan of the chapter and the paper titles shows just how far SIGGRAPH research has pushed the boundaries of our discipline and contributed to its evolution.The ACM Digital Library team has been supportive of this Seminal Graphics Papers project from the beginning. I am pleased to let you know that both Volumes 1 and 2 of Seminal Graphics Papers are freely available from the ACM Digital Library at these URLs: Volume 1: https://dl.acm.org/doi/book/10.1145/280811Volume 2: https://dl.acm.org/doi/book/10.1145/3596711}
}

@inproceedings{10.1145/3613904.3642646,
author = {McGee, Fintan and McCall, Roderick and Baixauli, Joan},
title = {Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642646},
doi = {10.1145/3613904.3642646},
abstract = {Augmented Reality (AR) provides a safe and low-cost option for hazardous safety training that allows for the visualization of aspects that may be invisible, such as radiation. Effectively visually communicating such threats in the environment around the user is not straightforward. This work describes visually encoding radiation using the spatial awareness mesh of an AR Head Mounted Display. We leverage the AR device’s GPUs to develop a real time solution that accumulates multiple dynamic sources and uses stencils to prevent an environment being over saturated with a visualization, as well as supporting the encoding of direction explicitly in the visualization. We perform a user study (25 participants) of different visualizations and obtain user feedback. Results show that there are complex interactions and while no visual representation was statistically superior or inferior, user opinions vary widely. We also discuss the evaluation approaches and provide recommendations.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {188},
numpages = {15},
keywords = {Augmented Reality, CBRN Response Training, Spatial Awareness, Visualization},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3599640.3599669,
author = {Choi, Geonhee and Li, Wanwan},
title = {Prevailing Technologies Augmented Reality Software with Hardware for E-Entertainment and E-Learning Purposes: A Survey},
year = {2023},
isbn = {9781450399593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599640.3599669},
doi = {10.1145/3599640.3599669},
abstract = {Augmented Reality (AR) technology is rapidly growing in recent years. AR hardware, software, and devices have become more general, accessible, and attractive for users. Therefore, many developers are researching the way to effective methods to be developed in the AR field. In this survey paper, we researched advanced technologies to improve AR games, applications, and training. With advanced hardware and software development tools. We introduce advanced software with hardware development tools for AR, flowcharts of advanced AR development, and details of AR hardware with functionality, for AR applications, games, and training.},
booktitle = {Proceedings of the 9th International Conference on Education and Training Technologies},
articleno = {27},
numpages = {7},
keywords = {AR game/training/ application, AR software development.AR hardware development, Augmented Reality},
location = {Macau, China},
series = {ICETT '23}
}

@article{10.1145/3593226,
author = {Nwagu, Chukwuemeka and AlSlaity, Alaa and Orji, Rita},
title = {EEG-Based Brain-Computer Interactions in Immersive Virtual and Augmented Reality: A Systematic Review},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {EICS},
url = {https://doi.org/10.1145/3593226},
doi = {10.1145/3593226},
abstract = {Brain-computer interactions allow humans to passively or actively control computer systems using their brain activity. For more than a decade now, these interactions have been implemented and evaluated in immersive virtual environments where they prompt novel means of human interaction with systems. In this paper, we present a systematic review of 76 studies published over the last 10 years that develop and evaluate immersive virtual reality or augmented reality systems with electroencephalography-based interactions. The aim of the review is to summarize and highlight trends in technology design, research methods, current practices, techniques used in systems of this kind, challenges and opportunities that present direction for future research in this area. Our analysis uncovers useful insights, limitations, and highlights of the trends, innovations, and usability and technical challenges at the intersection of brain-computer interfaces and immersive technologies, as well as recommendations for future research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {174},
numpages = {33},
keywords = {augmented reality, brain-computer interaction, electroencephalography, immersive technology, mixed reality, systematic review, virtual reality}
}

@proceedings{10.1145/3623504,
title = {PAINT 2023: Proceedings of the 2nd ACM SIGPLAN International Workshop on Programming Abstractions and Interactive Notations, Tools, and Environments},
year = {2023},
isbn = {9798400703997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Programming environments that integrate tools, notations, and abstractions into a holistic user experience can provide programmers with better support for what they want to achieve. These programming environments can create an engaging place to do new forms of informational work---resulting in enjoyable, creative, and productive experiences with programming.  

In the workshop on Programming Abstractions and Interactive Notations, Tools, and Environments (PAINT), we want to discuss programming environments that support users in working with and creating notations and abstractions that matter to them. We are interested in the relationship between people centric notations and general-purpose programming languages and environments. How do we reflect the various experiences, needs, and priorities of the many people involved in programming --- whether they call it that or not?},
location = {Cascais, Portugal}
}

@inproceedings{10.1145/3652037.3663948,
author = {Kapoor, Shashank and Uttrani, Shashank and Paul, Gunjan and Dutt, Varun},
title = {Exploring Performance in Complex Search-and-Retrieve Tasks: A Comparative Analysis of PPO and GAIL Robots},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652037.3663948},
doi = {10.1145/3652037.3663948},
abstract = {Prior research has extensively examined supervised and unsupervised methods for search tasks, focusing primarily on individual model accuracy and performance. However, there’s a gap in understanding how these models collaborate during complex tasks. This study’s primary objective was to evaluate a simulated robot employing Proximal Policy Optimization (PPO) and another using Generative Adversarial Imitation Learning (GAIL) algorithms, working together for a search-and-retrieve task. The environment, built in Unity3D, contained 56 distractors (e.g., walls, tables, and chairs) with negative points and 112 targets (e.g., pistol, laptop, and armour) with positive points during the gameplay. A PPO robot searched for the targets within an environment with a GAIL robot player. The results demonstrated that the inclusion of the GAIL robot along with the PPO robot led to superior performance of the multi-robot team in the search-and-retrieve task. The GAIL robot outperformed the PPO robot when both players performed individually in the search-and-retrieve task. The PPO robots demonstrated comparatively poorer performance when performing alone in a task without GAIL players. Our findings highlight the importance of collaborative multi-robot search involving generative imitation learning within a simulated environment.},
booktitle = {Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {466–473},
numpages = {8},
keywords = {Deep Reinforcement Learning, Generative Adversarial Imitation Learning, Proximal Policy Optimization, Reinforcement Learning, Unity3D, Virtual Environments},
location = {Crete, Greece},
series = {PETRA '24}
}

@inproceedings{10.1145/3565970.3567693,
author = {Lim, Donghae and Shirai, Shizuka and Orlosky, Jason and Ratsamee, Photchara and Uranishi, Yuki and Takemura, Haruo},
title = {Evaluation of User Interfaces for Three-Dimensional Locomotion in Virtual Reality},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565970.3567693},
doi = {10.1145/3565970.3567693},
abstract = {Locomotion is an essential factor for interaction in virtual environments. Virtual reality allows users to move freely from the constraints of gravity or the environment, which is impossible in the real world. These experiences, in which users can move at all 6 degrees of freedom, can enable new navigation paradigms that exceed 2D ground-based movement in terms of enjoyment and interactivity. However, most existing VR locomotion interfaces have limitations because they are developed for ground-based locomotion, constraining the degrees of freedom (DoF) during movement as in the real world. This exploratory study was designed to seek the features required for three-dimensional (3D) locomotion by evaluating three types of interfaces: Slider, Teleport, and Point-Tug, which were implemented based on existing ground-based locomotion techniques. We then conducted a user study with 3D navigation tasks, using both objective and subjective measures to evaluate efficiency, overall usability, motion sickness, perceived workload, and presence. The results suggest that Slider has an advantage compared to the others in terms of usability and perceived workload since it can freely and intuitively designate the direction of movement.},
booktitle = {Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
articleno = {8},
numpages = {9},
keywords = {3d locomotion, evaluation, flying, human computer interaction, locomotion, virtual reality},
location = {Online, CA, USA},
series = {SUI '22}
}

@inproceedings{10.1145/3587423.3595537,
author = {Liu, Baoquan and Wang, Minxuan and Yang, Yifan and Shao, Yuewei},
title = {Building a Real-Time System on GPUs for Simulation and Rendering of Realistic 3D Liquid in Video Games},
year = {2023},
isbn = {9798400701450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587423.3595537},
doi = {10.1145/3587423.3595537},
abstract = {Modern video games employ a variety of sophisticated algorithms to produce groundbreaking 3D rendering of water, which are pushing the visual boundaries and interactive experience of rich virtual environments. However, simulation and rendering of a large number of water particles is very time consuming and it is very hard to achieve real-time frame rate (due to the huge computational cost required), e.g., those found in feature movies and offline products [Flip Fluids2022], or tools (e.g., Houdini and Blender). That is why most water visual effects in modern games are either simulated as only a 2D shallow-water in which simulation is calculated in 2D grid and projected onto the heightfield, as in [Fluid Flux 2022], or are baked at preprocessing stage which does not allow the player to dynamically interact with the liquid at runtime, as a result, lots of the fun of interactivity is lost.This course will discuss the state-of-the-art and production-proven techniques involved in building a real-time system on GPUs for simulation and rendering of realistic 3D liquid with millions of particles. It also discusses how to integrate the system into modern game engines (like UE5), with some show-cases of real applications in gaming environments.},
booktitle = {ACM SIGGRAPH 2023 Courses},
articleno = {4},
numpages = {23},
location = {Los Angeles, California},
series = {SIGGRAPH '23}
}

@article{10.1145/3611652,
author = {Debie, Essam and Kasmarik, Kathryn and Garratt, Matt},
title = {Swarm Robotics: A Survey from a Multi-Tasking Perspective},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3611652},
doi = {10.1145/3611652},
abstract = {The behaviour of social insects such as bees and ants has influenced the development of swarm robots. To enable robots to cooperate together, swarm robotics employs principles such as communication, coordination, and collaboration. Collaboration among multiple robots can lead to a faster task completion time compared to the utilisation of a single, complex robot. One of the key aspects of swarm robotics is that control is distributed uniformly across the robots in the swarm, which boosts the system’s resilience and fault tolerance. Through the use of the robots’ embodied sensors and actuators, this distributed control often facilitates the emergence of collective behaviours through the interaction of the robots with one another and with the environment. The purpose of this survey is to examine the reasons behind the lack of utilisation of swarm robots in multi-tasking applications, which will be accomplished by studying previous research works in the field. We examine the literature from the perspective of multi-tasking: we pay particular attention to concepts that contribute to the progress of swarm robotics for multi-tasking applications. To do this, we first examine the different studies in multi-tasking swarm robotics, covering platforms, multi-tasking scenarios, sub-task allocation methodologies, and performance metrics. We then highlight several swarm robotics related disciplines that have significant effect on the development of swarm robotics for multi-tasking problems. We propose two taxonomies: the first categorises works based on the characteristics of the scenarios being handled, whereas the second taxonomy categorises works based on the swarming strategies utilised to achieve multi-tasking capabilities. We finish with a discussion of swarm robots’ existing limitations for real-world multi-tasking applications, as well as recommendations for future research directions.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {49},
numpages = {38},
keywords = {Swarm robotics, multi-tasking, reinforcement learning}
}

@inproceedings{10.1145/3629606.3629652,
author = {He, Xi and Sun, Xing},
title = {Research on the Reconstruction of Ming Dynasty History Based on AIGC},
year = {2024},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629606.3629652},
doi = {10.1145/3629606.3629652},
abstract = {The purpose of this paper is to explore a new perspective for experiencing historical culture through AI technology. The specific content is divided into three parts: The first part introduces that we use Ming Dynasty ancient paintings as the original input images, use the Stable Diffusion model to transform the images, and then use the 3D Morphable Face Model (3DMM) based methods to output a single character image into a 3D face model. After integrating from 3D software and game engine, we obtain a complete and controllable 3D character, As a participant and experiential carrier in the reconstruction of Ming Dynasty history. The second part introduces the use of images and text as inputs, through the Stable Diffusion model to generate scene backgrounds and related items related to Ming Dynasty history. The final section introduces the historical and cultural rules as a framework presented in a game mechanism, allowing the experience to perceive the immersion of historical and cultural, rather than simply conveying historical knowledge or appreciating historical artifacts. By employing AIGC, this research reconstructs 3D characters and scenes from the ancient Ming Dynasty in China, creating a historical immersive experience co-authored by humans and AI in the form of a game. Specifically, AIGC creates assets to restore 3D characters and scenes from the ancient Ming Dynasty in China. The creators use game gameplay to restore historical rules, and players participate in interactive experiences. This approach facilitates an objective assessment of history by allowing participants to deeply engage with historical narratives, prompting critical thinking and influencing the future. Empirical studies find that this method enables participants to actively immerse themselves in historical stories, triggering thoughtful reflections on history.},
booktitle = {Proceedings of the Eleventh International Symposium of Chinese CHI},
pages = {449–454},
numpages = {6},
keywords = {AIGC, Chinese Traditional Culture, Games and Play, History, Immersion},
location = {Denpasar, Bali, Indonesia},
series = {CHCHI '23}
}

@inproceedings{10.1145/3546155.3546667,
author = {Jung, Sangwon and Buruk, O\u{g}uz 'Oz and Hamari, Juho},
title = {Altered States of Consciousness in Human-Computer Interaction: A Review},
year = {2022},
isbn = {9781450396998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546155.3546667},
doi = {10.1145/3546155.3546667},
abstract = {There has been increasing interest shown in experiences such as lucid dreams, hallucinations, or awe that arise in HCI. Altered States of Consciousness (ASC) is the umbrella term for these experiences, yet it has been subject to fragmented study, and design knowledge to help individuals working on technology-driven ASCs is lacking. This paper investigates HCI studies involving ASC artefacts through a scoping review. The findings relate to (1) ASC induction methods, (2) ASC experiences through artefacts, (3) ASC artefacts, and (4) the technology of ASC artefacts. The returned literature shows that HCI studies have mainly explored psychologically induced ASCs, and XR technologies and embodied interaction are widely used in ASC research. Meanwhile, physical artefact design including active body movements and the integration of games and play approaches featured as prospective directions. These results will contribute to the knowledge of those studying and designing ASC artefacts.},
booktitle = {Nordic Human-Computer Interaction Conference},
articleno = {69},
numpages = {13},
keywords = {Altered States of Consciousness, Digital Technology, Human-computer Interaction, Scoping Review},
location = {Aarhus, Denmark},
series = {NordiCHI '22}
}

@inproceedings{10.1145/3581783.3611709,
author = {Yang, Shuyu and Zhou, Yinan and Zheng, Zhedong and Wang, Yaxiong and Zhu, Li and Wu, Yujiao},
title = {Towards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611709},
doi = {10.1145/3581783.3611709},
abstract = {In this paper, we introduce a large Multi-Attribute and Language Search dataset for text-based person retrieval, called MALS, and explore the feasibility of performing pre-training on both attribute recognition and image-text matching tasks in one stone. In particular, MALS contains 1,510,330 image-text pairs, which is about 37.5 \texttimes{} larger than prevailing CUHK-PEDES, and all images are annotated with 27 attributes. Considering the privacy concerns and annotation costs, we leverage the off-the-shelf diffusion models to generate the dataset. To verify the feasibility of learning from the generated data, we develop a new joint Attribute Prompt Learning and Text Matching Learning (APTM) framework, considering the shared knowledge between attribute and text. As the name implies, APTM contains an attribute prompt learning stream and a text matching learning stream. (1) The attribute prompt learning leverages the attribute prompts for image-attribute alignment, which enhances the text matching learning. (2) The text matching learning facilitates the representation learning on fine-grained details, and in turn, boosts the attribute prompt learning. Extensive experiments validate the effectiveness of the pre-training on MALS, achieving state-of-the-art retrieval performance via APTM on three challenging real-world benchmarks. In particular, APTM achieves a consistent improvement of +6.96 %, +7.68%, and +16.95% Recall@1 accuracy on CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets by a clear margin, respectively. The dataset, model, and code are available at https://github.com/Shuyu-XJTU/APTM.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {4492–4501},
numpages = {10},
keywords = {attribute prompt learning, image-text alignment, multi-attribute recognition, synthetic data, text-based person retrieval},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.1145/3682072,
author = {Barua, Hrishav Bakul and Mg, Theint Haythi and Pramanick, Pradip and Sarkar, Chayan},
title = {Enabling Social Robots to Perceive and Join Socially Interacting Groups using F-formation: A Comprehensive Overview},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3682072},
doi = {10.1145/3682072},
abstract = {Social robots in our daily surroundings, like personal guides, waiter robots, home helpers, assistive robots, telepresence/teleoperation robots etc., are increasing day by day. Their usability and acceptability largely depend on their explicit and implicit interaction capability with fellow human beings. As a result, social behavior is one of the most sought-after qualities that a robot can possess. However, there is no specific aspect and/or feature that defines socially acceptable behavior and it largely depends on the situation, application, and society. In this article, we investigate one such social behavior for collocated robots. Imagine a group of people is interacting with each other and we want to join the group. We as human beings do it in a socially acceptable manner, i.e., within the group, we do position ourselves in such a way that we can participate in the group activity without disturbing/obstructing anybody. To possess such a quality, first, a robot needs to determine the formation of the group and then determine a position for itself, which we humans do implicitly. There are many theories which study group formations and proxemics; one such theory is F-formation which could be utilized for this purpose. As the types of formations can be very diverse, detecting the social groups is not a trivial task. In this article, we provide a comprehensive survey of the existing work on social interaction and group detection using f-formation for robotics and other applications. We also put forward a novel holistic survey framework combining some of the possibly more important concerns and modules relevant to this problem. We define taxonomies based on methods, camera views, datasets, detection capabilities and scale, evaluation approaches, and application areas. We discuss certain open challenges and limitations in current literature along with possible future research directions based on this framework. In particular, we discuss the existing methods/techniques and their relative merits and demerits, applications, and provide a set of unsolved but relevant problems in this domain.},
note = {Just Accepted},
journal = {J. Hum.-Robot Interact.},
month = {jul},
keywords = {F-formation, social robotics, group detection, interaction detection, machine learning, deep learning, artificial intelligence, robotics, telepresence, teleoperation, computer vision, scene monitoring, human-robot interaction}
}

@inproceedings{10.1145/3563836.3568725,
author = {Geier, Leonard and Tiedt, Clemens and Beckmann, Tom and Taeumel, Marcel and Hirschfeld, Robert},
title = {Toward a VR-Native Live Programming Environment},
year = {2022},
isbn = {9781450399104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563836.3568725},
doi = {10.1145/3563836.3568725},
abstract = {Fast feedback loops between performing code changes and seeing their outcome help developers to be productive. For development of virtual reality (VR) applications, developers use a separate device, forcing them to switch devices whenever they want to test their application, thus significantly increasing the length of the feedback loop.  
In this paper, we describe a prototypical development environment that allows writing VR applications while inside VR. Unlike previous work in this area that projected traditional 2D editors into the 3D world, we explore the use of direct manipulation in a structured editor for the general-purpose programming language Smalltalk. We present and discuss insights from a preliminary user study with four participants. Our findings demonstrate that the concept does work if users are given prior instructions, especially for smaller features where direct feedback is valuable, but ergonomics of both the hardware and our prototype have to be improved before extended programming sessions are viable.},
booktitle = {Proceedings of the 1st ACM SIGPLAN International Workshop on Programming Abstractions and Interactive Notations, Tools, and Environments},
pages = {26–34},
numpages = {9},
keywords = {live programming, programming environment, virtual reality, vr-native},
location = {Auckland, New Zealand},
series = {PAINT 2022}
}

@article{10.1145/3582273,
author = {Fulcini, Tommaso and Coppola, Riccardo and Ardito, Luca and Torchiano, Marco},
title = {A Review on Tools, Mechanics, Benefits, and Challenges of Gamified Software Testing},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3582273},
doi = {10.1145/3582273},
abstract = {Gamification is an established practice in Software Engineering to increase effectiveness and engagement in many practices. This manuscript provides a characterization of the application of gamification to the Software Testing area. Such practice in fact reportedly suffers from low engagement by both personnel in industrial contexts and learners in educational contexts. Our goal is to identify the application areas and utilized gamified techniques and mechanics, the provided benefits and drawbacks, as well as the open challenges in the field. To this purpose, we conducted a Multivocal Literature Review to identify white and grey literature sources addressing gamified software testing.We analyzed 73 contributions and summarized the most common gamified mechanics, concepts, tools, and domains where they are mostly applied. We conclude that gamification in software testing is mostly applied to the test creation phase with simple white-box unit or mutation testing tools and is mostly used to foster good behaviors by promoting the testers’ accomplishment. Key research areas and main challenges in the field are: careful design of tailored gamified mechanics for specific testing techniques; the need for technological improvements to enable crowdsourcing, cooperation, and concurrency; the necessity for empirical and large-scale evaluation of the benefits delivered by gamification mechanics.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {310},
numpages = {37},
keywords = {Software/program verification, testing and debugging gamification, software testing, Software Engineering, Systematic Literature Review, Multivocal Literature Review}
}

@article{10.1145/3665243,
author = {Tewell, Jordan and Ranasinghe, Nimesha},
title = {A Review of Olfactory Display Designs for Virtual Reality Environments},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3665243},
doi = {10.1145/3665243},
abstract = {The field of Virtual Reality continues to evolve to provide an ever-greater sense of immersion to the user. However, VR experiences are still primarily constrained through the human senses of vision and audition, with some interest in haptic (mainly vibrotactile) applications. Only recently have olfactory displays—technologies that generate and deliver scent stimuli—been examined to provide the sense of smell to the human olfactory organ in virtual environments. This article presents a classification and review of olfactory-enhanced virtual reality systems, particularly those that deployed a Head-mounted Display or Cave Automatic Virtual Environment system. In addition, the article provides a discussion of the various technological and design challenges for developing an olfactory display suitable for enhancing virtual reality experiences. Finally, the article proposes future perspectives on the field and includes a table summarizing the characteristics and features of the reviewed systems.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {276},
numpages = {35},
keywords = {Virtual reality, olfaction, odor, smell, presence, immersion, multisensory}
}

@inproceedings{10.1145/3591196.3596823,
author = {Rafner, Janet and Zana, Blanka and Dalsgaard, Peter and Biskjaer, Michael Mose and Sherson, Jacob},
title = {Picture This: AI-Assisted Image Generation as a Resource for Problem Construction in Creative Problem-Solving},
year = {2023},
isbn = {9798400701801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591196.3596823},
doi = {10.1145/3591196.3596823},
abstract = {In this paper, we explore the potential of AI-assisted visualization during the problem identification and construction phase of the creative problem-solving process. We examine this within the context of the ongoing crea.visions research project, which employs AI technologies to visualize citizens' visions of the future. Our findings underscore various factors contributing to the effectiveness of assisted visualization in this setting, such as: 1) the tool's dual role as both a visual and ideational aid, 2) the introduction of innovative collaborative elements like prompt engineering, 3) the enhancement of visual expression without requiring artistic skills, and 4) the facilitation of idea communication. We also recognize limitations related to the tool and the problem context such as abstract concepts. This study serves as a foundation for future research on AI-assisted image generation as a resource in creative problem-solving, laying the groundwork for the creation of increasingly effective and user-friendly tools.},
booktitle = {Proceedings of the 15th Conference on Creativity and Cognition},
pages = {262–268},
numpages = {7},
keywords = {GANs, creative problem solving, human-AI co-creation, stable diffusion},
location = {Virtual Event, USA},
series = {C&amp;C '23}
}

@article{10.1145/3579642,
author = {Tang, Shuncheng and Zhang, Zhenya and Zhang, Yi and Zhou, Jixiang and Guo, Yan and Liu, Shuang and Guo, Shengjian and Li, Yan-Fu and Ma, Lei and Xue, Yinxing and Liu, Yang},
title = {A Survey on Automated Driving System Testing: Landscapes and Trends},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3579642},
doi = {10.1145/3579642},
abstract = {Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {124},
numpages = {62},
keywords = {ADS testing, module-level testing, system-level testing, system security}
}

@proceedings{10.1145/3660829,
title = {Programming '24: Companion Proceedings of the 8th International Conference on the Art, Science, and Engineering of Programming},
year = {2024},
isbn = {9798400706349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lund, Sweden}
}

@inproceedings{10.1145/3674912.3674927,
author = {Sembayev, Talgat and Nurbekova, Zhanat and Tazabekova, Parassat and Baigusheva, Kanagat and Naimanova, Dinara},
title = {Enhancing educational resources through augmented reality-based assessment materials: a comprehensive study},
year = {2024},
isbn = {9798400716843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674912.3674927},
doi = {10.1145/3674912.3674927},
abstract = {This research investigates the potential for enhancing the quality of educational materials utilized in the learning process by incorporating augmented reality (AR) technology into assessment materials. The characteristics of educational resources have been enhanced by introducing interactivity within AR objects, thereby elevating the quality of the presented tasks in diverse formats. Enhancements such as joystick control, utilization of virtual buttons, various animation techniques, AR video projection, and facial recognition exercises have enriched the AR environment, rendering it more dynamic for educational purposes. The present study outlines the benefits of upgraded educational resources employed in teaching the subject of "Information and Communication Technologies" to 88 students. Additionally, the outcomes of feedback received from 155 participants who engaged in an advanced professional development course focused on AR-based assessment materials are also detailed.},
booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024},
pages = {213–220},
numpages = {8},
keywords = {assessment materials, augmented reality, educational resource, evaluation, interaction, quality},
location = {Ruse, Bulgaria},
series = {CompSysTech '24}
}

@inproceedings{10.1145/3582437.3587193,
author = {Vandenberg, Jessica and Min, Wookhee and Catete, Veronica and Boulden, Danielle and Mott, Bradford},
title = {Leveraging Game Design Activities for Middle Grades AI Education in Rural Communities},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587193},
doi = {10.1145/3582437.3587193},
abstract = {The ever pervasive nature of artificial intelligence (AI) in our world necessitates a focus on fostering an AI literate society. Young children, those aged 11 to 14, are at a critical point in developing their dispositions toward and perceptions of science, technology, engineering, and mathematics (STEM), which influences their future education and career interests. Youth in rural areas are in particular need of access to AI learning opportunities to prepare them for the future workforce; digital games may be one way to attract young, rural students to STEM education and careers. In this paper, we explore how to introduce rural middle grades students to foundational AI concepts through digital game design activities. To inform our efforts and to establish an understanding of what these student populations as well as their teachers know about AI and games, we conducted a set of interviews and focus groups. In brief, students’ awareness and understanding of AI varied significantly, whereas teachers had limited knowledge of AI. Moreover, students shared great interest in playing and designing games. In support of our findings, we are developing a set of game design activities around five core AI concepts and ensuring the activities are of interest to our rural students.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {53},
numpages = {4},
keywords = {artificial intelligence, digital games, middle grades students},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3527188.3563937,
author = {Shi, Jibing and Savery, Richard J.},
title = {Partners Who Grow Together: Collaborative Machine Learning in Video Game AI Design},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527188.3563937},
doi = {10.1145/3527188.3563937},
abstract = {The majority of research at the intersection of AI and video games focuses on developing agents capable of playing games without human input, or developing AI game enemies. The research in this paper explores a counter approach, whereby a player trains an a AI partner during game play and learns to play cooperatively with the agent. We created a 2D video game that allows the player to cooperate with an AI agent manipulated by two underlying algorithms, either with reinforcement learning or a random process. For our reinforcement learning approach we used a Q-learning table, that is updated based on the player. We found that players engaged strongly with the idea of training their own custom AI agent and believe this shows significant potential for future exploration.},
booktitle = {Proceedings of the 10th International Conference on Human-Agent Interaction},
pages = {269–271},
numpages = {3},
keywords = {artificial intelligence, q-learning, random movement, unity, video game},
location = {Christchurch, New Zealand},
series = {HAI '22}
}

@article{10.1145/3651292,
author = {Dinkov, Martin and Pattanaik, Sumanta and McMahan, Ryan P.},
title = {Perceptions of Hybrid Lighting for Virtual Reality},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3651292},
doi = {10.1145/3651292},
abstract = {Virtual reality (VR) applications are computationally demanding due to high frame rate requirements, which precludes large numbers of realtime lights from being used. In this paper, we explore a hybrid lighting approach that combines the benefits of realtime and baked lights based on the importance of each source. First, we demonstrate that the hybrid approach affords better frames per second than realtime and mixed lighting. We then present the results of an online paired-comparison study, in which participants (n = 60) compared videos of the four lighting conditions (baked, mixed, realtime, and hybrid) in terms of preference. Our results indicate that the hybrid lighting approach is better than realtime lighting in terms of graphical performance and also yields better perceptions of quality for scenes with a small to moderately large number of lights.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {may},
articleno = {5},
numpages = {20},
keywords = {Hybrid Lighting, Lightmaps, Realtime Rendering, Virtual Reality}
}

@article{10.1145/3592102,
author = {Wang, Ziqi and Kennel-Maushart, Florian and Huang, Yijiang and Thomaszewski, Bernhard and Coros, Stelian},
title = {A Temporal Coherent Topology Optimization Approach for Assembly Planning of Bespoke Frame Structures},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592102},
doi = {10.1145/3592102},
abstract = {We present a computational framework for planning the assembly sequence of bespoke frame structures. Frame structures are one of the most commonly used structural systems in modern architecture, providing resistance to gravitational and external loads. Building frame structures requires traversing through several partially built states. If the assembly sequence is planned poorly, these partial assemblies can exhibit substantial deformation due to self-weight, slowing down or jeopardizing the assembly process. Finding a good assembly sequence that minimizes intermediate deformations is an interesting yet challenging combinatorial problem that is usually solved by heuristic search algorithms. In this paper, we propose a new optimization-based approach that models sequence planning using a series of topology optimization problems. Our key insight is that enforcing temporal coherent constraints in the topology optimization can lead to sub-structures with small deformations while staying consistent with each other to form an assembly sequence. We benchmark our algorithm on a large data set and show improvements in both performance and computational time over greedy search algorithms. In addition, we demonstrate that our algorithm can be extended to handle assembly with static or dynamic supports. We further validate our approach by generating a series of results in multiple scales, including a real-world prototype with a mixed reality assistant using our computed sequence and a simulated example demonstrating a multi-robot assembly application.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {144},
numpages = {13},
keywords = {frame structure, assembly sequence planning, topology optimization}
}

@article{10.1145/3563214,
author = {Nardone, Vittoria and Muse, Biruk and Abidi, Mouna and Khomh, Foutse and Di Penta, Massimiliano},
title = {Video Game Bad Smells: What They Are and How Developers Perceive Them},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3563214},
doi = {10.1145/3563214},
abstract = {Video games represent a substantial and increasing share of the software market. However, their development is particularly challenging as it requires multi-faceted knowledge, which is not consolidated in computer science education yet. This article aims at defining a catalog of bad smells related to video game development. To achieve this goal, we mined discussions on general-purpose and video game-specific forums. After querying such a forum, we adopted an open coding strategy on a statistically significant sample of 572 discussions, stratified over different forums. As a result, we obtained a catalog of 28 bad smells, organized into five categories, covering problems related to game design and logic, physics, animation, rendering, or multiplayer. Then, we assessed the perceived relevance of such bad smells by surveying 76 game development professionals. The survey respondents agreed with the identified bad smells but also provided us with further insights about the discussed smells. Upon reporting results, we discuss bad smell examples, their consequences, as well as possible mitigation/fixing strategies and trade-offs to be pursued by developers. The catalog can be used not only as a guideline for developers and educators but also can pave the way toward better automated tool support for video game developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {88},
numpages = {35},
keywords = {Video games, bad smells, Q&amp;A forums, empirical study}
}

@article{10.1145/3533376,
author = {Sch\"{a}fer, Alexander and Reis, Gerd and Stricker, Didier},
title = {A Survey on Synchronous Augmented, Virtual, andMixed Reality Remote Collaboration Systems},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533376},
doi = {10.1145/3533376},
abstract = {Remote collaboration systems have become increasingly important in today’s society, especially during times when physical distancing is advised. Industry, research, and individuals face the challenging task of collaborating and networking over long distances. While video and teleconferencing are already widespread, collaboration systems in augmented, virtual, and mixed reality are still a niche technology. We provide an overview of recent developments of synchronous remote collaboration systems and create a taxonomy by dividing them into three main components that form such systems: Environment, Avatars, and Interaction. A thorough overview of existing systems is given, categorising their main contributions to help researchers working in different fields by providing concise information about specific topics such as avatars, virtual environment, visualisation styles, and interaction. The focus of this work is clearly on synchronised collaboration from a distance. A total of 87 unique systems for remote collaboration are discussed, including more than 100 publications and 25 commercial systems.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {116},
numpages = {27},
keywords = {Virtual reality, augmented reality, mixed reality, collaboration, remote assistance, distant cooperation, literature review}
}

@article{10.1145/3519023,
author = {Liu, Shengmei and Xu, Xiaokun and Claypool, Mark},
title = {A Survey and Taxonomy of Latency Compensation Techniques for Network Computer Games},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {11s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3519023},
doi = {10.1145/3519023},
abstract = {Computer games, one of the most popular forms of entertainment in the world, are increasingly online multiplayer, connecting geographically dispersed players in the same virtual world over a network. Network latency between players and the server can decrease responsiveness and increase inconsistency across players, degrading player performance and quality of experience. Latency compensation techniques are software-based solutions that seek to ameliorate the negative effects of network latency by manipulating player input and/or game states in response to network delays. We search, find, and survey more than 80 papers on latency compensation, organizing their latency compensation techniques into a novel taxonomy. Our hierarchical taxonomy has 11 base technique types organized into four main groups. Illustrative examples of each technique are provided, as well as demonstrated use of the techniques in commercial games.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {243},
numpages = {34},
keywords = {Video games, lag, quality of experience, user study, delay}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00043,
author = {Lai, Zhongzheng and Yuan, Dong and Chen, Huaming and Zhang, Yu and Bao, Wei},
title = {elessDT: A Digital Twin Platform for Real-Time Evaluation of Wireless Software Applications},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00043},
doi = {10.1109/ICSE-Companion58688.2023.00043},
abstract = {Wireless technology has become one of the most important parts of our daily routine. Besides being used for communication, the wireless signal has been applied to various Wireless Software Applications (WSAs). The signal fluctuation caused by the measurement system or the environmental dynamic can significantly influence WSAs' performance, making it challenging to evaluate WSAs in real-world scenarios. To overcome these challenges, we propose WirelsssDT, a wireless digital twin platform, using digital twin and real-time ray tracing technologies to emulate the wireless signals propagation and generate emulation data for real-time WSAs evaluation. In this demonstration, we evaluate a wireless indoor localisation mobile application with two typical prediction algorithms: 1) Kalman Filter-based Trilateration and 2) Deep Recurrent Neural Network, as a case study to demonstrate the capabilities of WirelessDT. The source code is available at https://github.com/codelzz/WirelessDT, and the demonstration video is available at https://youtu.be/9Kl-3jgMBUA.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {146–150},
numpages = {5},
keywords = {digital twin, wireless software evaluation, wireless signal emulation, emulation tool},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3586077,
author = {Calzado-Mart\'{\i}nez, Alberto and Garc\'{\i}a-Fern\'{a}ndez, \'{A}ngel-Luis and Ortega-Alvarado, Lidia M. and Feito-Higueruela, Francisco-Ram\'{o}n},
title = {Integrated Information System for 3D Interactive Reconstruction of an Archaeological Site},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3586077},
doi = {10.1145/3586077},
abstract = {Archaeological recording is intended to preserve as much information as possible about the finds. However, once the pieces are removed from the site, there is information regarding the original positioning of these pieces that may be lost or not accurately recorded and can be relevant for further studies. This spatial arrangement can also be crucial for subsequent piece restoration or to understand certain aspects of ancient cultures.In this article, we describe a software prototype and a methodology to virtually reconstruct an archaeological site for posterity, once it has been excavated. The system is implemented with a client-server architecture. In the server, a spatial database stores and manages the three-dimensional (3D) models of the finds, as well as several 3D site ground surface models acquired at different times during the excavation process. On the client side, a graphical interface allows the user to manipulate the find models to re-create and virtually reconstruct the original spatial arrangement of the archaeological site. Topological relationships among the finds are stored in the database to provide further spatial analysis. The result is an integrated information system that goes beyond 3D visualization, making the site last for posterity after its excavation and allowing further spatial analysis.},
journal = {J. Comput. Cult. Herit.},
month = {aug},
articleno = {44},
numpages = {23},
keywords = {Virtual reconstruction, 3D interaction, archaeological recording}
}

@inproceedings{10.1145/3510456.3514164,
author = {Ryan, Brooke and Soria, Adriana Meza and Dreef, Kaj and van der Hoek, Andr\'{e}},
title = {Reading to write code: an experience report of a reverse engineering and modeling course},
year = {2022},
isbn = {9781450392259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510456.3514164},
doi = {10.1145/3510456.3514164},
abstract = {A substantial portion of any software engineer's job is reading code. Despite the criticality of this skill in a budding software engineer, reading code---and more specifically, techniques on how to read code when integrating oneself into a large existing software project---is often neglected in the typical software engineering education. As part of a new professional Master of Software Engineering at the University of California, Irvine, we designed and delivered a "reading to write code" course from the ground up. Titled Reverse Engineering and Modeling, the course introduces students to techniques they can use to become familiar with a large code base, so they are able to make meaningful contributions. In this paper, we briefly introduce the Master program and its underlying philosophy, articulate the course's learning outcomes, present the design of the course, and provide a detailed reflection on our experiences in terms of what went well, what did not go well, what we do not know yet, and what our next steps are in preparing for the forthcoming incarnation of the course in Spring 2022. In so doing, we hope to provide a baseline together with lessons learned for others who may be interested in instituting a similar course at their institution.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {223–234},
numpages = {12},
keywords = {large open source systems, reading code, software understanding},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEET '22}
}

@inproceedings{10.1145/3540250.3549108,
author = {Bittner, Paul Maximilian and Tinnes, Christof and Schulthei\ss{}, Alexander and Viegener, S\"{o}ren and Kehrer, Timo and Th\"{u}m, Thomas},
title = {Classifying edits to variability in source code},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549108},
doi = {10.1145/3540250.3549108},
abstract = {For highly configurable software systems, such as the Linux kernel, maintaining and evolving variability information along changes to source code poses a major challenge. While source code itself may be edited, also feature-to-code mappings may be introduced, removed, or changed. In practice, such edits are often conducted ad-hoc and without proper documentation. To support the maintenance and evolution of variability, it is desirable to understand the impact of each edit on the variability. We propose the first complete and unambiguous classification of edits to variability in source code by means of a catalog of edit classes. This catalog is based on a scheme that can be used to build classifications that are complete and unambiguous by construction. To this end, we introduce a complete and sound model for edits to variability. In about 21.5ms per commit, we validate the correctness and suitability of our classification by classifying each edit in 1.7 million commits in the change histories of 44 open-source software systems automatically. We are able to classify all edits with syntactically correct feature-to-code mappings and find that all our edit classes occur in practice.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {196–208},
numpages = {13},
keywords = {feature traceability, mining version histories, software evolution, software product lines, software variability},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3615979.3656060,
author = {Kim, Ryan and Torrens, Paul M.},
title = {Building Verisimilitude in VR With High-Fidelity Local Action Models: A Demonstration Supporting Road-Crossing Experiments},
year = {2024},
isbn = {9798400703638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615979.3656060},
doi = {10.1145/3615979.3656060},
abstract = {We examine how issues of investigative and experimental parity between real-world domain science and virtual reality (VR) involving human-environment behavior might be advanced, particularly in the use case of safety science for road-crossing. Our contribution centers on a VR-based traffic flow simulation to recreate, with high fidelity relative to the real world, dynamics of hyper-local interaction between traffic, people, and the roadside environment. An initial demonstration of the system shows that 22 participants responded with high levels of presence, and with high propensity toward natural behavior across road-crossing dimensions. We report these findings even with low-resolution graphic elements. Our results highlight that high levels of user-identified situational verisimilitude (i.e., appearing authentic, particularly to the senses) can be achieved, even with low-resolution graphical depictions. The key, we argue, is the design of appropriate low-level action models to drive user embodiment relative to VR assets. We contend that this finding has wider relevance to consideration of potential channels for VR experience more generally.},
booktitle = {Proceedings of the 38th ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {119–130},
numpages = {12},
keywords = {3D interaction, AI, Behavioral tree, Embodiment, Microscopic traffic flow, Pathfinding, Pedestrian, Presence, Realism, Simulation, Task load, Virtual reality},
location = {Atlanta, GA, USA},
series = {SIGSIM-PADS '24}
}

@proceedings{10.1145/3564121,
title = {AIMLSystems '22: Proceedings of the Second International Conference on AI-ML Systems},
year = {2022},
isbn = {9781450398473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangalore, India}
}

@inproceedings{10.1145/3649921.3656984,
author = {Lyman, Brandon and Ebrahimi, Ala and Cox, James and Chan, Szeyi and Barney, Christopher and De Schutter, Bob},
title = {Cardistry: Exploring a GPT Model Workflow as an Adapted Method of Gaminiscing},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3656984},
doi = {10.1145/3649921.3656984},
abstract = {Cardistry is an application that enables users to create their own playing cards for use in evocative storytelling games. It is driven by OpenAI’s Generative Pre-trained Transformer (GPT) models that generate unique card titles, cards suits, imagery, and poetry based on the user’s input. It allows the user to preserve their digital cards in an online repository and print them for tabletop game play use. Cardistry was designed to begin exploring the question of whether widely available GPT models could be used to adapt the process of gaminiscing to make it more accessible to designers and players alike. This short paper details the concept, design, and implementation of Cardistry as a first step in exploring this research question. It explains how the adapted gaminiscing process is different from the original process, discusses the limitations of the implementation, and expresses what future research would be required to answer the research question.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {52},
numpages = {4},
keywords = {AI, GPT, card games, game design, gaminiscing, generative AI, playing cards},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@inproceedings{10.1145/3582437.3587191,
author = {Hofmann, Sarah and \"{O}zdemir, Cem and von Mammen, Sebastian},
title = {Record, Review, Edit, Apply: A Motion Data Pipeline for Virtual Reality Development &amp; Design},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587191},
doi = {10.1145/3582437.3587191},
abstract = {Although virtual reality (VR) applications, especially training environments are booming, and research is advancing, there is still a lack of easy, universally applicable workflows for VR development. To facilitate this process, we present a VR motion data pipeline. Stepping through its phases involves recording motion, storing, and refining the data through runtime evaluation. A generic representation of the motion data allows, for instance, to create automatic integration tests by simulating a variety of different players through data interpolation, or to standardize the setup of psychomotor training tasks by guiding the user to follow the predefined motion path and thereby moving objects in VR. In this paper, we elaborate on the pipeline and demonstrate its application in two examples.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {52},
numpages = {4},
keywords = {motion data, psychomotor training, testing, virtual reality},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3610602.3610605,
author = {Grow, April M. and Khosmood, Foaad},
title = {ChatGPT GameJam: Unleashing the power of Large Language Models for Game Jams},
year = {2023},
isbn = {9798400708794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610602.3610605},
doi = {10.1145/3610602.3610605},
abstract = {Recently Large Language Models (LLMs) have captivated the imagination in many different fields. OpenAI’s ChatGPT, offered the first highly accessible LLM to the users and has seen phenomenal growth since its release in November 2022. While AI tools have always been part of game design and game development, now ChatGPT offers users perhaps for the first time, the ability to create a variety of novel games either entirely or partly by writing a text prompt. ChatGPT is a game jam that embraces and explores LLM-based game production while at the same time providing some insights about the current capabilities and shortcomings of the process. In this paper we report on the process and lessons learned from organizing the first ChatGPT Game Jam – held in May 2023.},
booktitle = {Proceedings of the 7th International Conference on Game Jams, Hackathons and Game Creation Events},
pages = {51–54},
numpages = {4},
keywords = {AI, LLM, artificial intelligence, game design, game development, game jams, large language models},
location = {Virtual Event, Ukraine},
series = {ICGJ '23}
}

@inproceedings{10.1145/3672406.3672417,
author = {Kontostathis, Ioannis and Apostolidis, Evlampios and Mezaris, Vasileios},
title = {A Human-Annotated Video Dataset for Training and Evaluation of 360-Degree Video Summarization Methods},
year = {2024},
isbn = {9798400717949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672406.3672417},
doi = {10.1145/3672406.3672417},
abstract = {In this paper we introduce a new dataset for 360-degree video summarization: the transformation of 360-degree video content to concise 2D-video summaries that can be consumed via traditional devices, such as TV sets and smartphones. The dataset includes ground-truth human-generated summaries, that can be used for training and objectively evaluating 360-degree video summarization methods. Using this dataset, we train and assess two state-of-the-art summarization methods that were originally proposed for 2D-video summarization, to serve as a baseline for future comparisons with summarization methods that are specifically tailored to 360-degree video. Finally, we present an interactive tool that was developed to facilitate the data annotation process and can assist other annotation activities that rely on video fragment selection.},
booktitle = {Proceedings of the 2024 ACM International Conference on Interactive Media Experiences Workshops},
pages = {71–79},
numpages = {9},
keywords = {360-Degree video, Annotation tool, Benchmarking dataset, Video summarization},
location = {Stockholm, Sweden},
series = {IMXw '24}
}

@article{10.1145/3593237,
author = {Frau, Vittoria and Spano, Lucio Davide and Artizzu, Valentino and Nebeling, Michael},
title = {XRSpotlight: Example-based Programming of XR Interactions using a Rule-based Approach},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {EICS},
url = {https://doi.org/10.1145/3593237},
doi = {10.1145/3593237},
abstract = {Research on enabling novice AR/VR developers has emphasized the need to lower the technical barriers to entry. This is often achieved by providing new authoring tools that provide simpler means to implement XR interactions through abstraction. However, novices are then bound by the ceiling of each tool and may not form the correct mental model of how interactions are implemented. We present XRSpotlight, a system that supports novices by curating a list of the XR interactions defined in a Unity scene and presenting them as rules in natural language. Our approach is based on a model abstraction that unifies existing XR toolkit implementations. Using our model, XRSpotlight can find incomplete specifications of interactions, suggest similar interactions, and copy-paste interactions from examples using different toolkits. We assess the validity of our model with professional VR developers and demonstrate that XRSpotlight helps novices understand how XR interactions are implemented in examples and apply this knowledge in their projects.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {185},
numpages = {28},
keywords = {ar, end-user development, example-based programming, mr, vr, xr, xr modelling}
}

@article{10.1145/3517199,
author = {Zhang, Pengyi and Dou, Huanzhang and Zhang, Wenhu and Zhao, Yuhan and Qin, Zequn and Hu, Dongping and Fang, Yi and Li, Xi},
title = {A Large-Scale Synthetic Gait Dataset Towards in-the-Wild Simulation and Comparison Study},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3517199},
doi = {10.1145/3517199},
abstract = {Gait recognition has a rapid development in recent years. However, current gait recognition focuses primarily on ideal laboratory scenes, leaving the gait in the wild unexplored. One of the main reasons is the difficulty of collecting in-the-wild gait datasets, which must ensure diversity of both intrinsic and extrinsic human gait factors. To remedy this problem, we propose to construct a large-scale gait dataset with the help of controllable computer simulation. In detail, to diversify the intrinsic factors of gait, we generate numerous characters with diverse attributes and associate them with various types of walking styles. To diversify the extrinsic factors of gait, we build a complicated scene with a dense camera layout. Then we design an automatic generation toolkit under Unity3D for simulating the walking scenarios and capturing the gait data. As a result, we obtain a dataset simulating towards the in-the-wild scenario, called VersatileGait, which has more than one million silhouette sequences of 10,000 subjects with diverse scenarios. VersatileGait possesses several nice properties, including huge dataset size, diverse pedestrian attributes, complicated camera layout, high-quality annotations, small domain gap with the real one, good scalability for new demands, and no privacy issues. By conducting a series of experiments, we first explore the effects of different factors on gait recognition. We further illustrate the effectiveness of using our dataset to pre-train models, which obtain considerable performance gain on CASIA-B, OU-MVLP, and CASIA-E. Besides, we show the great potential of the fine-grained labels other than the ID label in improving the efficiency and effectiveness of models. Our dataset and its corresponding generation toolkit are available at https://github.com/peterzpy/VersatileGait.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
articleno = {17},
numpages = {23},
keywords = {Gait recognition, synthetic dataset, in the wild scenarios, fine-grained attributes, Unity3D}
}

@proceedings{10.1145/3658549,
title = {I-DO '24: Proceedings of the 2024 International Conference on Information Technology, Data Science, and Optimization},
year = {2024},
isbn = {9798400709180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Taipei, Taiwan}
}

@inproceedings{10.1145/3607822.3614539,
author = {Karatas, Eren and Sunday, Kissinger and Apak, Sude Erva and Li, Yiwei and Sun, Junwei and Batmaz, Anil Ufuk and Barrera Machuca, Mayra Donaji},
title = {"\"I consider VR Table Tennis to be my secret weapon!\": An Analysis of the VR Table Tennis Players' Experiences Outside the Lab"},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607822.3614539},
doi = {10.1145/3607822.3614539},
abstract = {Thanks to stand-alone Virtual Reality (VR) advances, users can play realistic simulations of real-life sports at their homes. In these game simulations, players control their avatars by doing the same movements as in real life (RL) while playing against a person or AI opponent, making VR sports attractive for the players. In this paper, we surveyed a popular VR table tennis game community, focusing on understanding their demographics, challenges, and experiences with skill transfers between VR and RL. Our results show that, on average, VR table tennis players are primarily men, live in Europe/Asia, and are 38 years old. We also found that the current state of VR technology affects the player’s experience and that players see VR as a convenient way to play matches but that RL is better for socialization. Finally, we identified skills like backhand and forehand strikes that players perceived to be transferred from VR to RL and vice versa. Our research findings have the potential to serve as a valuable resource for VR table tennis game developers seeking to integrate mid-air controllers into their future projects.},
booktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
articleno = {29},
numpages = {12},
keywords = {Skill Transfer, Survey, Table Tennis, Training, Virtual Reality},
location = {Sydney, NSW, Australia},
series = {SUI '23}
}

@article{10.1145/3664637,
author = {Schmidt, Leonard and Yigitbas, Enes},
title = {Development and Usability Evaluation of Transitional Cross-Reality Interfaces},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {EICS},
url = {https://doi.org/10.1145/3664637},
doi = {10.1145/3664637},
abstract = {The concept of Transitional Cross-Reality Interfaces where a user can seamlessly transition across the Reality-Virtuality-Continuum dates back to the introduction of the Magic Book in 2001 but recently gained new research momentum since head-mounted displays advanced by combining the former distinctive concepts of augmented reality and virtual reality into a single device. New technological capabilities require new ways of developing applications to satisfy user requirements. Especially in the context of operating in multiple realities, developing AR/VR applications is not a trivial task. In this context, the development of Transitional Cross-Reality Interfaces remains a complex engineering problem that requires specific methods, concepts, and tools. To address this problem and provide a systematic development approach, we propose a conceptual framework for both developing and evaluating Transitional Cross-Reality Interfaces. To evaluate our solution, we implemented a Transitional Cross-Reality Interface based on our framework and evaluated it with both the measurements provided by the framework and additional questionnaires. We found high usability and interesting transitional behavior of users indicating the usefulness of the proposed framework as an underlying software architecture for Transitional Cross-Reality Interfaces.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {263},
numpages = {32},
keywords = {AR, Cross-Reality, MR, Transitional Interface, Usability, VR}
}

@inproceedings{10.1145/3581754.3584130,
author = {Mitra, Mukund and Pati, Preetam and Sharma, Vinay Krishna and Raj, Subin and Chakrabarti, Partha Pratim and Biswas, Pradipta},
title = {Comparison of Target Prediction in VR and MR using Inverse Reinforcement Learning},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581754.3584130},
doi = {10.1145/3581754.3584130},
abstract = {Numerous research were undertaken to predict pointing targets in Graphical User Interfaces (GUI). This paper extends target prediction for Extended reality (XR) platforms through Sampling-based Maximum Entropy Inverse Reinforcement Learning (SMEIRL). The SMEIRL algorithm learns the underlying reward distribution for the pointing task. Results show that SMEIRL achieves better accuracy in both VR and MR (for example accuracy in VR and accuracy in MR at of pointing task) compared to Artificial Neural Network (ANN) and Quadratic Extrapolation (QE) during early stage of pointing task. For later stage, QE performs better (for example accuracy in VR and accuracy in MR at of the pointing task) than SMEIRL and ANN. All the three algorithms, SMEIRL, ANN and QE reported higher target prediction accuracy in MR than in VR.},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {55–58},
numpages = {4},
keywords = {Inverse Reinforcement Learning, Neural Networks, Target Prediction, Virtual and Mixed Reality},
location = {Sydney, NSW, Australia},
series = {IUI '23 Companion}
}

@inproceedings{10.1145/3625008.3625014,
author = {Luz Junior, Jonas de Ara\'{u}jo and Rodrigues, Maria Andr\'{e}ia Formico},
title = {Comparative Analysis of Facial Expression Recognition Systems for Evaluating Emotional States in Virtual Humans},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625014},
doi = {10.1145/3625008.3625014},
abstract = {The digital animation process is a complex endeavour requiring professional animators to acquire substantial expertise and technique through years of study and practice. Particularly, facial animation, where virtual humans express specific mental states or emotions with a desire for realism, is further complicated by the “Uncanny Valley” phenomenon. In this context, it is posited that pre-validated facial expressions for certain emotions could serve as references for the novice or inexperienced animators during the facial animation and posing process of their virtual humans using morph targets, also known as blend shapes or shape keys. This research presents a comparative study between two Facial Expression Recognition (FER) systems that employ pre-trained models for facial recognition applied to emotion recognition in virtual humans. Given that these systems were not designed or trained for this particular purpose but for facial recognition in real humans, this study aims to investigate their level of applicability in scenarios where virtual humans are used instead of real humans. This assessment is a critical step towards evaluating the feasibility of integrating FER models as part of a support tool for facial animation and the posing of virtual humans. Through this investigation, this research provides evidence of the reliability of applying the FER library and Deepface systems for emotion recognition in virtual humans, contributing to investigating new ways to enhance the digital animation process and overcoming the inherent complexities of facial animation.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {38–47},
numpages = {10},
keywords = {comparative analysis, emotional states, evaluation, facial expression recognition systems, virtual humans},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@inproceedings{10.1145/3536220.3558072,
author = {Sch\"{u}tze, Christian and Gro\ss{}, Andr\'{e} and Wrede, Britta and Richter, Birte},
title = {Enabling Non-Technical Domain Experts to Create Robot-Assisted Therapeutic Scenarios via Visual Programming},
year = {2022},
isbn = {9781450393898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536220.3558072},
doi = {10.1145/3536220.3558072},
abstract = {In this paper, we present a visual programming software for enabling non-technical domain experts to create robot-assisted therapy scenarios for multiple robotic platforms. Our new approach is evaluated by comparing it with Choregraphe, the standard visual programming framework for the often used robotics platforms Pepper and NAO. We could show that our approach receives higher usability values and allows users to perform better in some practical tasks, including understanding, changing and creating small robot-assisted therapy scenarios.},
booktitle = {Companion Publication of the 2022 International Conference on Multimodal Interaction},
pages = {166–170},
numpages = {5},
keywords = {end user development, non-technical domain expert, robot assisted therapie, visual programming},
location = {Bengaluru, India},
series = {ICMI '22 Companion}
}

@inproceedings{10.1145/3594441.3594454,
author = {Holder, Raymond and Carey, Mark and Walder, Patrick and Keir, Paul},
title = {MoonBase VR: Learning to program in a virtual reality game},
year = {2023},
isbn = {9798400700613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594441.3594454},
doi = {10.1145/3594441.3594454},
abstract = {The idea of computer programming is quite abstract and can be challenging for novices. Learning how to write code is like learning a foreign language, but without the ability to compare the new words with native ones. To overcome this obstacle to learning, current computer science teaching can employ many creative ways to teach coding. Visual aids can be used to transfer the basic concepts of programming across to new students; helping them visualize the functional elements of coding. Learning through gamification is a method also deployed by educators, and has been a proven technique to improve learning outcomes. In this study, our research aims were to investigate the gamification of computer programming using virtual reality. Immersive technology such as virtual reality provides a promising framework to deliver visualization and gamification to better convey foundational programming concepts. To test this theory, a virtual reality game was constructed which provided a self-directed learning path through a simple game narrative. Study participants (n=40) immersed within the virtual environment were provided with the opportunity to build a fundamental understanding of programming. The outcomes of the study showed that there is an interest to learn computer programming within a VR game such as this one.},
booktitle = {Proceedings of the 2023 8th International Conference on Information and Education Innovations},
pages = {74–80},
numpages = {7},
keywords = {Computing, Education, Gamification, Programming, Virtual Reality},
location = {Manchester, United Kingdom},
series = {ICIEI '23}
}

@inproceedings{10.1145/3555858.3555888,
author = {Bishop, Robert and Churchill, David},
title = {The Effects of Human-like Modifications to Heuristic Action Evaluation in Video Game Pathfinding},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555888},
doi = {10.1145/3555858.3555888},
abstract = {We present a series of parameterizable modifications to heuristic evaluation of actions in the A* algorithm, designed to create more human-like and dexterity-robust paths through games in the 2 dimensional platformer style. We attempt to create paths at various levels of player skill by imposing constraints onto the timing and duration of actions designed to mimic human reaction times and ability. We show that these action value modifications result in the A* search algorithm producing smoother paths, taking safer routes to avoid danger, and requiring fewer actions to be performed in a given amount of game time.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {38},
numpages = {8},
keywords = {A*, Pathfinding, Video Games},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3548608.3559319,
author = {Wang, Shiyao and Wu, Qifei},
title = {Tibetan Jiu Chess Game Algorithm based on Expert Knowledge},
year = {2022},
isbn = {9781450397179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548608.3559319},
doi = {10.1145/3548608.3559319},
abstract = {Tibetan Jiu Chess is a unique chess game with complex rules, which is suitable to be used as the object of computer games. The expert knowledge database is established through the field investigation in Lhasa, Tibet. Tibetan Jiu Chess game algorithm based on expert knowledge is proposed in this paper. The algorithm is generally divided into formation recognition module, game strategy module in layout stage and game strategy module in Battle Stage. Especially for the most important battle stage, the chess power of the algorithm has been greatly improved by combining the distilled expert knowledge and better strategies. The experiments were made between the human players and the proposed algorithm in the "Minshan Cup" Tibetan chess National Open held in Lhasa City, Tibet Province in 2021. The results show that the designed algorithm based on expert knowledge is with a higher winning rate than a one-duan grading human chess player.},
booktitle = {Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics},
pages = {842–848},
numpages = {7},
location = {Nanjing, China},
series = {ICCIR '22}
}

@article{10.1145/3660243,
author = {Pascher, Max and Goldau, Felix Ferdinand and Kronhardt, Kirill and Frese, Udo and Gerken, Jens},
title = {AdaptiX - A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {EICS},
url = {https://doi.org/10.1145/3660243},
doi = {10.1145/3660243},
abstract = {With the ongoing efforts to empower people with mobility impairments and the increase in technological acceptance by the general public, assistive technologies, such as collaborative robotic arms, are gaining popularity. Yet, their widespread success is limited by usability issues, specifically the disparity between user input and software control along the autonomy continuum. To address this, shared control concepts provide opportunities to combine the targeted increase of user autonomy with a certain level of computer assistance. This paper presents the free and open-source AdaptiX XR framework for developing and evaluating shared control applications in a high-resolution simulation environment. The initial framework consists of a simulated robotic arm with an example scenario in Virtual Reality (VR), multiple standard control interfaces, and a specialized recording/replay system. AdaptiX can easily be extended for specific research needs, allowing Human-Robot Interaction (HRI) researchers to rapidly design and test novel interaction methods, intervention strategies, and multi-modal feedback techniques, without requiring an actual physical robotic arm during the early phases of ideation, prototyping, and evaluation. Also, a Robot Operating System (ROS) integration enables the controlling of a real robotic arm in a PhysicalTwin approach without any simulation-reality gap. Here, we review the capabilities and limitations of AdaptiX in detail and present three bodies of research based on the framework. AdaptiX can be accessed at https://adaptix.robot-research.de.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {244},
numpages = {28},
keywords = {assistive robotics, augmented reality, human-robot interaction, mixed reality, shared user control, virtual reality, visual cues}
}

@inproceedings{10.1145/3544548.3581172,
author = {Kalus, Alexander and Kocur, Martin and Klein, Johannes and Mayer, Manuel and Henze, Niels},
title = {PumpVR: Rendering the Weight of Objects and Avatars through Liquid Mass Transfer in Virtual Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581172},
doi = {10.1145/3544548.3581172},
abstract = {Perceiving objects’ and avatars’ weight in Virtual Reality (VR) is important to understand their properties and naturally interact with them. However, commercial VR controllers cannot render weight. Controllers presented by previous work are single-handed, slow, or only render a small mass. In this paper, we present PumpVR that renders weight by varying the controllers’ mass according to the properties of virtual objects or bodies. Using a bi-directional pump and solenoid valves, the system changes the controllers’ absolute weight by transferring water in or out with an average error of less than 5%. We implemented VR use cases with objects and avatars of different weights to compare the system with standard controllers. A study with 24 participants revealed significantly higher realism and enjoyment when using PumpVR to interact with virtual objects. Using the system to render body weight had significant effects on virtual embodiment, perceived exertion, and self-perceived fitness.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {263},
numpages = {13},
keywords = {haptic controllers, virtual embodiment, virtual reality, weight interface, weight perception},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3597631,
author = {Zeidler, Daniel and McGinity, Matthew},
title = {Bodylab: in virtuo sculpting, painting and performing of full-body avatars},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3597631},
doi = {10.1145/3597631},
abstract = {Bodylab is a virtual reality system for creating full-body virtual reality avatars. It provides anatomically realistic mannequins as canvases, which can be painted and textured, sculpted and deformed, adorned with objects and animated with particle effects. The artist can work from a first or third person perspective, and variety of interaction modalities are made available. To assess the system, we invite costume designers and theatrical artists to provide expert reviews. We also describe the use of Bodylab in a live dance performance. We believe Bodylab to be the first virtual reality tool for in virtuo painting and sculpting of full-body avatars.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {aug},
articleno = {22},
numpages = {12},
keywords = {avatars, dance, embodiment, performance, virtual reality}
}

@inproceedings{10.1145/3625468.3647612,
author = {Alhilal, Ahmad and Wu, Ze and Tsui, Yuk Hang and Hui, Pan},
title = {FovOptix: Human Vision-Compatible Video Encoding and Adaptive Streaming in VR Cloud Gaming},
year = {2024},
isbn = {9798400704123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625468.3647612},
doi = {10.1145/3625468.3647612},
abstract = {VR cloud gaming enables users to play high-end VR games on lightweight devices by offloading rendering tasks to cloud servers. Despite video compression, high-definition video streaming requires substantial data transfer rates. Foveated rendering (FR) and video encoding (FVE) leverage the non-uniform perception of the human visual system to reduce computing and bandwidth demand. They enhance visual quality in central gaze regions and reduce it in the periphery. However, bandwidth variation may hinder the provision of smooth VR gaming experiences. We present FovOptix, a system that combines FR with adaptive FVE to deliver video stream at a lower yet adaptive bitrate while not compromising the perceived video quality. FovOptix is based on a game-agnostic open-source to ensure reproducibility and compatibility with various games. We evaluate FovOptix against benchmarks using 5G mobile network traces. FovOptix achieves a latency reduction of 3% compared to the Google standard and a significant +100% reduction compared to other solutions. Additionally, it enhances the visual quality within the player's region of interest. Consequently, FovOptix attains the highest playability and gaming scores while minimizing the severity of motion sickness. FovOptix thus offers smooth and accessible VR cloud gaming for a wider range of players.},
booktitle = {Proceedings of the 15th ACM Multimedia Systems Conference},
pages = {67–77},
numpages = {11},
keywords = {Adaptive Video Streaming, Foveated Video Encoding, Human Vision System, VR Cloud Gaming},
location = {Bari, Italy},
series = {MMSys '24}
}

@inproceedings{10.1145/3626485.3626530,
author = {Park, Habin and Lichtman, Daniel and Oyekoya, Oyewole},
title = {Exploring Virtual Reality Game Development as an Interactive Art Medium: A Case Study with the Community Game Development Toolkit},
year = {2023},
isbn = {9798400704253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626485.3626530},
doi = {10.1145/3626485.3626530},
abstract = {This research paper examines the development and utilization of "The Community Game Development Toolkit," a virtual reality (VR) game development tool, for the creation of interactive art experiences. The primary objective of the toolkit is to enable artist, students, researchers, and a diverse set of peoples to design story- and game-based art presentations without requiring prior game development expertise. By incorporating VR technology into the toolkit, artists are empowered to construct immersive and interactive art encounters. This study employs a case study approach to explore the potential of VR game development as an artistic medium, focusing on how artists utilize the Toolkit to construct art presentations. The research findings presented in this paper aim to contribute to the progressive field of VR art by demonstrating the diverse possibilities for accessible artistic creation in the field of VR. Ultimately, the study aims to inspire artists and researchers to delve into the artistic potential of VR game development and foster continued advancements within the field.},
booktitle = {Companion Proceedings of the 2023 Conference on Interactive Surfaces and Spaces},
pages = {5–9},
numpages = {5},
keywords = {2D, 3D, Accessibility, Aesthetics, Ease of Access, Immersion, Immersive Design, Interactivity, Oculus Quest Artist, Toolkit, Unity, Usability User, Virtual Reality},
location = {Pittsburgh, PA, USA},
series = {ISS Companion '23}
}

@inproceedings{10.1145/3573381.3596158,
author = {Mckendrick, Zachary and Somin, Lior and Finn, Patrick and Sharlin, Ehud},
title = {Virtual Rehearsal Suite: An Environment and Framework for Virtual Performance Practice},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596158},
doi = {10.1145/3573381.3596158},
abstract = {Contemporary performance artists use Virtual Reality (VR) tools to create immersive narratives and extend the boundaries of traditional performance mediums. As the medium evolves, performance practice is changing with it. Our work explores ways to leverage VR to support the creative process by introducing the Virtual Rehearsal Suite (VRS) that provides users with the experience of a large-scale rehearsal or performance environment while occupying limited physical space and minimal real-world obstructions. In this paper, we discuss findings from scene study experiments conducted within the VRS. In addition, we contribute our thresholding protocols a framework designed to support user transitions into and out of VR experiences. Our integrated approach to digital performance practice and creative collaboration combines traditional and contemporary acting techniques with HCI research to harness the innovative capabilities of virtual reality technologies creating accessible, immersive experiences for actors while facilitating user presence through state change protocols.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {27–39},
numpages = {13},
keywords = {Drama, Interaction Design, Performance Practice, Rehearsal, Theatre},
location = {Nantes, France},
series = {IMX '23}
}

@proceedings{10.1145/3661167,
title = {EASE '24: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Salerno, Italy}
}

@article{10.1145/3631122,
author = {Rattanarungrot, Sasithorn and Kalarat, Kosin and White, Martin and Chaisriya, Kannattha},
title = {Preserving Southern Thai Traditional Manora Dance Through Mobile Role-Playing Game Technology},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3631122},
doi = {10.1145/3631122},
abstract = {Manora or Nora has been added to the heritage list of UNESCO as Thailand's intangible cultural heritage since 2021. This performance has been presented as a regional form of entertainment and traditionally passed down through generations within certain families. Nowadays, young people are able to learn and practice Manora instructed by their masters or their successors. However, there is currently no digital content of Manora that could support awareness and enrichment of cultural safeguarding, so this article proposes the design and development of a mobile 3D game representing Manora, which is Southern Thai dance-drama performing art. The aim of this game is to increase citizens’ engagement with their intangible cultural heritage and preservation by delivering novel digital heritage content specifically designed for a role-playing mobile game called Nara. The Nara game is used to enhance the perception of Manora dance and provide new experience of playing a culture-based game through costume and its material items. The game design process takes Manora's significant identities, including the costume, dance postures, and music along with players’ requirements to create a game player character, non-player characters, storytelling, challenges, and level design. The development of the game is done on Unity game engine, which provides a mobile 3D game platform to integrate all the designed content and generate the interactive Nara game. The experimentation results conclude that utilizing the unique identity of Manora in gameplay successfully enhances the audience's experience and recognition. This result indicates the effective use of creativity and interactive multimedia technology to create attractive content and enjoyment that can encourage intangible cultural heritage preservation in an innovative manner.},
journal = {J. Comput. Cult. Herit.},
month = {jan},
articleno = {11},
numpages = {22},
keywords = {Cultural safeguarding, digital heritage content, role-playing mobile games, intangible cultural heritage preservation}
}

@proceedings{10.1145/3623462,
title = {KUI '23: Proceedings of the 20th International Conference on Culture and Computer Science: Code and Materiality},
year = {2023},
isbn = {9798400708367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3582649,
title = {ICIGP '23: Proceedings of the 2023 6th International Conference on Image and Graphics Processing},
year = {2023},
isbn = {9781450398572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chongqing, China}
}

@inproceedings{10.1145/3626252.3630770,
author = {Stefik, Andreas and Allee, Willliam and Contreras, Gabriel and Kluthe, Timothy and Hoffman, Alex and Blaser, Brianna and Ladner, Richard},
title = {Accessible to Whom? Bringing Accessibility to Blocks},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630770},
doi = {10.1145/3626252.3630770},
abstract = {The introduction of block-based programming has gradually changed the landscape of programming education, particularly for school children. Block languages today, however, have serious technical barriers to students with disabilities. For example, block languages are generally not screen reader accessible, incompatible with braille, and contain serious problems for users with motor impairments. No student with a disability should ever be denied access to learning computer science and they do not have to be. To help rectify this, we present a new approach to the design of block languages called Quorum Blocks. Quorum Blocks uses a custom hardware accelerated graphical rendering pipeline that takes into account how screen readers and other devices work under the hood. We discuss these technical details and demonstrate that accessibility support can be fully achieved without meaningfully losing either the look of modern blocks or their visual output. We present the results from focus groups that highlight the barriers students faced with a variety of disabilities when using the first version of Quorum Blocks. We focus especially on challenges with low vision users, screen reader users, or those using no mouse and only one hand to type. Block languages built using either our techniques, or on top of our libraries, would become accessible out of the box.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1286–1292},
numpages = {7},
keywords = {accessibility, block languages, computer graphics, human factors},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@proceedings{10.1145/3620665,
title = {ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
abstract = {Welcome to the second volume of ASPLOS'24: the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. This document is dedicated to the 2024 summer review cycle.We introduced several notable changes to ASPLOS this year, many of which were discussed in the previous message from program chairs in Volume 1. Here, to avoid repetition, we assume that readers have already read the latter message and will only describe differences between the current cycle and the previous one. These include: (1) developing and utilizing an automated format violation identifier script focused on uncovering disallowed vertical space manipulations that "squeeze" space; (2) incorporating authors-declared best-matching topics into our review assignment process; (3) introducing the new ASPLOS role of Program Vice Chairs to cope with the increased number of submissions and the added load caused by foregoing synchronous program committee (PC) meetings, which necessitated additional managerial involvement in online dissensions; and (4) characterizing a systematic problem that ASPLOS is facing in reviewing quantum computing submissions, describing how we addressed it, and highlighting how we believe that it should be handled in the future.Key statistics of the ASPLOS'24 summer cycle include: 409 submissions were finalized (about 1.5x more than last year's summer count and nearly 2.4x more than our spring cycle), with 107 related to accelerators/FPGAs/GPUs, 97 to machine learning, 88 to storage/memory, 80 to security, and 69 to datacenter/cloud; 179 (44%) submissions were promoted to the second review round; 54 (13.2%) papers were accepted (with 20 awarded one or more artifact evaluation badges); 33 (8.1%) submissions were allowed to submit major revisions, of which 27 were subsequently accepted during the fall cycle (with 13 awarded one or more artifact evaluation badges); 1,499 reviews were uploaded; and 5,557 comments were generated during online discussions.Analyzing the per-submission most-related broader areas of research, which we asked authors to associate with their work in the submission form, revealed that 71%, 47%, and 28% of the submissions are categorized by their authors as related to architecture, operating systems, and programming languages, respectively, with about 45% being "interdisciplinary" submissions (associated with more than one area). The full details are available in the PDF of the front matter.},
location = {La Jolla, CA, USA}
}

@article{10.1145/3578523,
author = {Yal\c{c}\i{}n, \"{O}zge Nilay and Lall\'{e}, S\'{e}bastien and Conati, Cristina},
title = {The Impact of Intelligent Pedagogical Agents’ Interventions on Student Behavior and Performance in Open-Ended Game Design Environments},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3578523},
doi = {10.1145/3578523},
abstract = {Research has shown that free-form Game-Design (GD) environments can be very effective in fostering Computational Thinking (CT) skills at a young age. However, some students can still need some guidance during the learning process due to the highly open-ended nature of these environments. Intelligent Pedagogical Agents (IPAs) can be used to provide personalized assistance in real-time to alleviate this challenge. This paper presents our results in evaluating such an agent deployed in a real-word free-form GD learning environment to foster CT in the early K-12 education, Unity-CT. We focus on the effect of repetition by comparing student behaviors between no intervention, 1-shot, and repeated intervention groups for two different errors that are known to be challenging in the online lessons of Unity-CT. Our findings showed that the agent was perceived very positively by the students and the repeated intervention showed promising results in terms of helping students make fewer errors and more correct behaviors, albeit only for one of the two target errors. Building from these results, we provide insights on how to provide IPA interventions in free-form GD environments.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {sep},
articleno = {11},
numpages = {29},
keywords = {Pedagogical agent, real-time support, game design, computational thinking, open-ended learning environments}
}

@inproceedings{10.1145/3573381.3597214,
author = {Chen, Chen and Murray, Niall and Keighrey, Conor},
title = {A Quality of Experience Evaluation of an Interactive Multisensory 2.5D Virtual Reality Art Exhibit},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3597214},
doi = {10.1145/3573381.3597214},
abstract = {In recent years, museums have become more interactive and immersive through the adaptation of technology within large scale art exhibitions. Due to these changes, new types of cultural experiences are more appealing to a younger audience. Despite these positive changes, some museum experiences are still primarily focused on visual art experiences, which remain out of reach to those with visual impairments. Such unimodal and visual dominated experiences restrict these users who depend on sensory feedback to experience the world around them. In this paper, the authors propose a novel VR experience which incorporates multisensory technologies. It allows individuals to engage and interact with a visual artwork museum experience presented as a fully immersive VR environment. Users can interact with virtual paintings and trigger sensory zones which deliver multisensory feedback to the user. These sensory zones are unique to each painting, presenting thematic audio and smells, custom haptic feedback to feel the artwork, and lastly air, light and thermal changes in an effort to engage those with visual impairments.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {170–173},
numpages = {4},
keywords = {Accessibility, Immersive Experience, Multisensory, Virtual Reality, Visual Impairment},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3638067.3638124,
author = {Adachi, Bruno Hideki and Gonzaga, J\'{u}lia Carlos and Fernandes, Paula Cintra and Silva, Saul Emanuel Delabrida and Bim, Silvia Am\'{e}lia and Boss, Silvio Luiz Bragatto},
title = {Augmented Reality in Books: An Evaluation of Alan Turing Book},
year = {2024},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638067.3638124},
doi = {10.1145/3638067.3638124},
abstract = {Augmented reality technology has become popular due to its portability on smartphones and glasses. The literature presents studies on the efficiency of using augmented reality. Despite the demonstrated efficiency, books must be prepared to be recognized and provide interaction. This paper aims to assess whether a children’s book written without the purpose of using augmented reality can be identified by the leading augmented reality SDKs. As a case study, we chose the book Alan Turing: his machines and secrets&nbsp;[4] because it is illustrated a fact that facilitates recognition. Our tests were on two of the main SDKs for augmented reality development currently found on the market. Although one of the SDKs demonstrated that approximately 76% of the book is recognizable, we have identified some human-computer interaction challenges and research opportunities discussed in this work.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
articleno = {12},
numpages = {10},
keywords = {ARCore, Augmented Reality, Books, Vuforia},
location = {Macei\'{o}, Brazil},
series = {IHC '23}
}

@proceedings{10.1145/3615886,
title = {GeoAI '23: Proceedings of the 6th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
year = {2023},
isbn = {9798400703485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Emerging advances from artificial intelligence, hardware accelerators, and data processing architectures continue to reach the geospatial information sciences, with a transformative impact in many societal challenges. Recent breakthroughs in deep learning have brought forward an automated capability to learn hierarchical representational features from massive and complex data, including text, images, and videos. In tandem, rapid innovations in sensing technologies are supporting the collection of geospatial data in even higher resolution and throughput, supporting the observation, mapping, and analysis of different events/phenomena over the earth's surface with unprecedented detail. Combined, these developments are offering potential for breakthroughs in geographic knowledge discovery, impacting decision making in areas such as humanitarian mapping, intelligent transport systems, urban expansion analysis, health data analysis and epidemiology, the study of climate change, handling natural disasters, and the general monitoring of the Earth's surface.},
location = {Hamburg, Germany}
}

@inproceedings{10.1145/3564533.3564568,
author = {Arige, Abhaya Dhathri and Lavric, Traian and Preda, Marius and Zaharia, Titus},
title = {Evaluation of simplified 3D CAD data for conveying industrial assembly instructions via Augmented reality},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564533.3564568},
doi = {10.1145/3564533.3564568},
abstract = {Augmented Reality (AR) based training is gaining momentum in industrial sectors, particularly in assembly and maintenance. Generally, the media contents used to create AR assembly instructions include audio, video, images, text, signs, 3D data, and animations. Literature suggests that 3D CAD-based AR instructions spatially registered with the real-world environment are more effective and produce better training results. However, storing, processing, and rendering 3D data can be challenging even for state-of-the-art AR devices like HoloLens2, particularly in industrial usage. To overcome these concerns, heavy 3D models can be simplified to a certain extent with a minimal impact on the user experience, that is, the quality of visualization in AR. In the present paper, we evaluate the usability of a set of simplified 3D CAD models used to convey manual assembly information to novice operators. The experiment included 14 participants, six assembly operations, and two sets of 3D CAD models (i.e., originals and simplified) and was conducted in a laboratory setting. To simulate as much as possible a real-world assembly scenario, the components, and the original corresponding 3D CAD models were obtained from a real-world industrial setup. The present paper confirms that simplified 3D CAD models can replace the original 3D CAD models within AR applications without affecting the user experience with the help of subjective evaluations.},
booktitle = {Proceedings of the 27th International Conference on 3D Web Technology},
articleno = {7},
numpages = {6},
keywords = {Augmented reality, HoloLens2, Simplified 3D CAD models, industrial assembly lines, subjective evaluations},
location = {Evry-Courcouronnes, France},
series = {Web3D '22}
}

@inproceedings{10.1145/3613904.3642957,
author = {Wienrich, Carolin and Vogt, Stephanie and D\"{o}llinger, Nina and Obremski, David},
title = {Promoting Eco-Friendly Behaviour through Virtual Reality - Implementation and Evaluation of Immersive Feedback Conditions of a Virtual CO2 Calculator},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642957},
doi = {10.1145/3613904.3642957},
abstract = {Climate change is one of the most pressing global challenges in the 21st century. Urgent actions favoring the environment’s well-being are essential to mitigate its potentially irreversible consequences. However, the delayed and often distant nature of the effects of sustainable behavior makes it challenging for individuals to connect with the issue personally. Immersive media are an opportunity to introduce innovative feedback mechanisms to highlight the urgency of behavior effects. We introduce a VR carbon calculator that visualizes users’ annual carbon footprint as CO2-filled balloons over multiple periods. In a 2 \texttimes{} 2 design, participants calculated and visualized their carbon footprint numerically or as balloons over one or three years. We found no effect of our visualization but a significant impact of the visualized period on participants’ environmental self-efficacy. These findings emphasize the importance of target-oriented design in VR behavior interventions.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {282},
numpages = {9},
keywords = {Virtual reality, intention-behavior gap, pro-environmental behavior.},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@proceedings{10.1145/3641235,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Educator's Forum},
year = {2024},
isbn = {9798400705175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@inproceedings{10.1145/3649921.3650010,
author = {Anjum, Asad and Li, Yuting and Law, Noelle and Charity, M and Togelius, Julian},
title = {The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game Designer},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3650010},
doi = {10.1145/3649921.3650010},
abstract = {This paper studies how large language models (LLMs) can act as effective, high-level creative collaborators and “muses” for game design. We model the design of this study after the exercises artists use by looking at amorphous ink splotches for creative inspiration. Our goal is to determine whether AI-assistance can improve, hinder, or provide an alternative quality to games when compared to the creative intents implemented by human designers. The capabilities of LLMs as game designers are stress tested by placing it at the forefront of the decision making process. Three prototype games are designed across 3 different genres: (1) a minimalist base game, (2) a game with features and game feel elements added by a human game designer, and (3) a game with features and feel elements directly implemented from prompted outputs of the LLM, ChatGPT. A user study was conducted and participants were asked to blindly evaluate the quality and their preference of these games. We discuss both the development process of communicating creative intent to an AI chatbot and the synthesized open feedback of the participants. We use this data to determine both the benefits and shortcomings of AI in a more design-centric role.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {18},
numpages = {15},
keywords = {AI-assisted game design, LLMs, Unity, co-creative game design, mixed-initiative creativity, procedural content generation, user study},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@article{10.1145/3558773,
author = {Liu, Huimin and Choi, Minsoo and Kao, Dominic and Mousas, Christos},
title = {Synthesizing Game Levels for Collaborative Gameplay in a Shared Virtual Environment},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3558773},
doi = {10.1145/3558773},
abstract = {We developed a method to synthesize game levels that accounts for the degree of collaboration required by two players to finish a given game level. We first asked a game level designer to create playable game level chunks. Then, two artificial intelligence (AI) virtual agents driven by behavior trees played each game level chunk. We recorded the degree of collaboration required to accomplish each game level chunk by the AI virtual agents and used it to characterize each game level chunk. To synthesize a game level, we assigned to the total cost function cost terms that encode both the degree of collaboration and game level design decisions. Then, we used a Markov-chain Monte Carlo optimization method, called simulated annealing, to solve the total cost function and proposed a design for a game level. We synthesized three game levels (low, medium, and high degrees of collaboration game levels) to evaluate our implementation. We then recruited groups of participants to play the game levels to explore whether they would experience a certain degree of collaboration and validate whether the AI virtual agents provided sufficient data that described the collaborative behavior of players in each game level chunk. By collecting both in-game objective measurements and self-reported subjective ratings, we found that the three game levels indeed impacted the collaboration gameplay behavior of our participants. Moreover, by analyzing our collected data, we found moderate and strong correlations between the participants and the AI virtual agents. These results show that game developers can consider AI virtual agents as an alternative method for evaluating the degree of collaboration required to finish a game level.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {mar},
articleno = {2},
numpages = {36},
keywords = {Game level, chunks, collaboration, AI agents, behavior trees, optimization}
}

@article{10.1145/3611027,
author = {Schmid, Andreas and Halbhuber, David and Fischer, Thomas and Wimmer, Raphael and Henze, Niels},
title = {Small Latency Variations Do Not Affect Player Performance in First-Person Shooters},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3611027},
doi = {10.1145/3611027},
abstract = {In interactive systems high latency affects user performance and experience. This is especially problematic in video games.  
A large number of studies on this topic investigated the effects of constant, high latency. However, in practice, latency is never constant but varies by up to 100 ms due to variations in processing time and delays added by polling between system components.  
In a large majority of studies, these variations in latency are neither controlled for nor reported. Thus, it is unclear to which degree small, continuous variations in latency affect user performance. If these unreported variations had a significant impact, this might cast into doubt the findings of some studies.  
To investigate how latency variation affects player performance and experience in games, we conducted an experiment with 28 participants playing a first-person shooter. Participants played with two levels of base latency (50 ms vs. 150 ms) and variation (0 ms vs. 50 ms).  
As expected, high base latency significantly reduces player performance and experience. However, we found strong evidence that small variations in latency in the order of 50 ms, do not affect player performance significantly. Thus, our findings mitigate concerns that previous latency studies might have systematically ignored a confounding effect.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {381},
numpages = {20},
keywords = {gaming, latency}
}

@inproceedings{10.1145/3613904.3642821,
author = {Rifat, Mohammad Rashidujjaman and Ayad, Reem and Asha, Ashratuz Zavin and Huang, Bingjian and Okman, Selin and Sabie, Dina and Ferdous, Hasan Shahid and Soden, Robert and Ahmed, Syed Ishtiaque},
title = {Cohabitant: The Design, Implementation, and Evaluation of a Virtual Reality Application for Interfaith Learning and Empathy Building},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642821},
doi = {10.1145/3613904.3642821},
abstract = {Lack of interfaith communication often gives rise to prejudice and group-based conflict in multi-faith societies. Nurturing this communication via interfaith learning may reduce this conflict by fostering interfaith empathy. HCI has a dearth of knowledge on interfaith coexistence and empathy building. To address this gap, we present the design, implementation, and usability of Cohabitant: a virtual reality (VR) application that promotes interfaith learning and empathy. Cohabitant’s design is theoretically underpinned by Allport’s intergroup contact theory and informed by insights from a participatory workshop we ran with members of three religious groups: Christians, Hindus, and Muslims. Our evaluation study, combining quantitative and qualitative data from 30 participants, suggests that Cohabitant may enhance general interpersonal empathy, but falls short for ethnocultural empathy. We discuss the possible design and policy implications of using this kind of VR technology for interfaith learning and empathy building.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {597},
numpages = {19},
keywords = {Empathy, Interfaith, Learning, Virtual Reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3635636.3664256,
author = {Browen, Isaac and Camarillo-Abad, Hector M and Cibrian, Franceli L. and Qi, Trudi Di},
title = {Creative Insights into Motion: Enhancing Human Activity Understanding with 3D Data Visualization and Annotation},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3664256},
doi = {10.1145/3635636.3664256},
abstract = {This paper presents a novel 3D system for human motion analysis - Motion Data Visualization and Annotation (MoViAn). Designed to provide a comprehensive visual representation of 3D human motion data, MoViAn incorporates detailed visualization of gaze direction, hand movements, and object interactions, alongside an interactive interface for efficient data annotation. A user study involving eight participants indicates that MoViAn enables users to thoroughly explore and annotate human motion data, with System Usability Scale (SUS) results demonstrating a satisfactory usability level. The contribution of this paper lies in the development of an interactive and usable data analytics tool aimed at deepening the understanding of human behaviors and intentions in various creative, cognitive, and physical activities that ultimately can facilitate the design and creation of innovative tools that enhance human life in multiple domains.},
booktitle = {Proceedings of the 16th Conference on Creativity &amp; Cognition},
pages = {482–487},
numpages = {6},
keywords = {3D visualization, Data annotation, Data exploration, Human motion analysis, Usability evaluation, User interface},
location = {Chicago, IL, USA},
series = {C&amp;C '24}
}

@inproceedings{10.1145/3652920.3653028,
author = {Zhang, Kaining and Cao, Zehong and Zheng, Xianglin and Billinghurst, Mark},
title = {Identifying Hand-based Input Preference Based on Wearable EEG},
year = {2024},
isbn = {9798400709807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652920.3653028},
doi = {10.1145/3652920.3653028},
abstract = {Understanding user input preference can improve the user experience, however automatically determining preference can be difficult. In this paper, we designed an EEG-based method for directly evaluating hand-based input preference for touch and mid-air gestures on a smartwatch. We conducted a two-phase experiment, recording EEG data from 18 participants as they performed gestures and captured their ratings (Phase 1) and preference choices (Phase 2) for each gesture. Our analysis uncovered distinct EEG patterns between preferred and non-preferred gestures, including significant differences in Power Spectral Density (PSD), Coherence (Coh), and Sample Entropy (SE) features. When participants engaged with their preferred input gestures, we identified decreased brain activity (PSD) in the central and occipital regions, reduced brain connectivity (Coh) in the delta and alpha bands, and increased brain complexity (SE) in multiple sizes. These insights offer the potential to develop rapid detection of user intent for interactive computing devices by analysing brain signals.},
booktitle = {Proceedings of the Augmented Humans International Conference 2024},
pages = {102–118},
numpages = {17},
keywords = {EEG, evaluation, interaction input, user input preference},
location = {Melbourne, VIC, Australia},
series = {AHs '24}
}

@inproceedings{10.1145/3623476.3623515,
author = {van Rozen, Riemer},
title = {Cascade: A Meta-language for Change, Cause and Effect},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623476.3623515},
doi = {10.1145/3623476.3623515},
abstract = {Live programming brings code to life with immediate and continuous feedback. To enjoy its benefits, programmers need powerful languages and live programming environments for understanding the effects of code modifications on running programs. Unfortunately, the enabling technology that powers these languages, is missing. Change, a crucial enabler for explorative coding, omniscient debugging and version control, is a potential solution.  
We aim to deliver generic solutions for creating these languages, in particular Domain-Specific Languages (DSLs). We present Cascade, a meta-language for expressing DSLs with interface- and feedback-mechanisms that drive live programming. We demonstrate run-time migrations, ripple effects and live desugaring of three existing DSLs. Our results show that an explicit representation of change is instrumental for how these languages are built, and that cause-and-effect relationships are vital for delivering precise feedback.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–162},
numpages = {14},
keywords = {bidirectional transformations, domain-specific languages, live programming, metamodels, model migration},
location = {Cascais, Portugal},
series = {SLE 2023}
}

@proceedings{10.1145/3641236,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Labs},
year = {2024},
isbn = {9798400705182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@inproceedings{10.1145/3555858.3555917,
author = {Fiadotau, Mikhail},
title = {Small, personal videogames about mental health: An informal survey of Bitsy games},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555917},
doi = {10.1145/3555858.3555917},
abstract = {The article introduces an ongoing research project dedicated to short, personal videogames about mental health made using Bitsy: a tiny, browser-based game creation tool. It introduces preliminary observations from a survey of such games on the game distribution platform Itch.io. The article outlines the themes the games address and how they communicate their authors’ personal experiences and intended messages. It also discusses the significance of such games in the landscape of today’s gaming.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {50},
numpages = {4},
keywords = {game creation, mental health, participatory culture, platform studies},
location = {Athens, Greece},
series = {FDG '22}
}

@article{10.1145/3663759,
author = {Paproki, Anthony and Salvado, Olivier and Fookes, Clinton},
title = {Synthetic Data for Deep Learning in Computer Vision &amp; Medical Imaging: A Means to Reduce Data Bias},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3663759},
doi = {10.1145/3663759},
abstract = {Deep-learning (DL) performs well in computer-vision and medical-imaging automated decision-making applications. A bottleneck of DL stems from the large amount of labelled data required to train accurate models that generalise well. Data scarcity and imbalance are common problems in imaging applications that can lead DL models towards biased decision making. A solution to this problem is synthetic data. Synthetic data is an inexpensive substitute to real data for improved accuracy and generalisability of DL models. This survey reviews the recent methods published in relation to the creation and use of synthetic data for computer-vision and medical-imaging DL applications. The focus will be on applications that utilised synthetic data to improve DL models by either incorporating an increased diversity of data that is difficult to obtain in real life, or by reducing a bias caused by class imbalance. Computer-graphics software and generative networks are the most popular data generation techniques encountered in the literature. We highlight their suitability for typical computer-vision and medical-imaging applications, and present promising avenues for research to overcome their computational and theoretical limitations.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {271},
numpages = {37},
keywords = {Synthetic data, machine learning, generative adversarial network, deep learning}
}

@proceedings{10.1145/3593434,
title = {EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oulu, Finland}
}

@proceedings{10.1145/3641520,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Real-Time Live!},
year = {2024},
isbn = {9798400705267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@inproceedings{10.1145/3675231.3675240,
author = {Hadnett-Hunter, Jacob and Lundell, Benjamin and Ellison-Taylor, Ian and Porubanova, Michaela and Alasaarela, Tapani and Olkkonen, Maria},
title = {AR-in-VR simulator: A toolbox for rapid augmented reality simulation and user research},
year = {2024},
isbn = {9798400710612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675231.3675240},
doi = {10.1145/3675231.3675240},
abstract = {While providing exciting opportunities for spatial computing, current generation augmented reality (AR) headsets face visual quality and visual comfort issues. Designing headsets that are both perceptually accurate and comfortable to use for extended periods is difficult as these devices rely on state-of-the-art developments in optics, display technology, spatial computing and graphics. Understanding the requirements and parameters needed to create great AR devices currently requires expensive and time consuming prototype development and user research. Here, we present the AR-in-VR Simulator, a tool to facilitate rapid AR device prototyping and user research of the perceptual requirements needed for AR systems by simulating various display, optics and rendering properties of AR systems and presenting them in a virtual reality (VR) headset. Our simulator is suitable for conducting perceptual research on various potential AR device properties and artefacts, such as FOV size and shape, stereoscopic display configurations, and luminance and color non-uniformity. Simulations can be presented in realistic 3D environments, passthrough VR, or even Gaussian Splat environments allowing for a range of naturalistic data collection possibilities. We further demonstrate the utility of the simulator by using it to extend prior work on the perception of partially overlapping stereoscopic AR displays to a 6DOF VR simulation, capturing more ecologically valid and natural user interaction compared to traditional stereoscope based perceptual studies.},
booktitle = {ACM Symposium on Applied Perception 2024},
articleno = {11},
numpages = {11},
keywords = {augmented reality, simulation, visual quality assessment},
location = {Dublin, Ireland},
series = {SAP '24}
}

@article{10.1145/3635705,
author = {Menapace, Willi and Siarohin, Aliaksandr and Lathuili\`{e}re, St\'{e}phane and Achlioptas, Panos and Golyanik, Vladislav and Tulyakov, Sergey and Ricci, Elisa},
title = {Promptable Game Models: Text-guided Game Simulation via Masked Diffusion Models},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/3635705},
doi = {10.1145/3635705},
abstract = {Neural video game simulators emerged as powerful tools to generate and edit videos. Their idea is to represent games as the evolution of an environment’s state driven by the actions of its agents. While such a paradigm enables users to play a game action-by-action, its rigidity precludes more semantic forms of control. To overcome this limitation, we augment game models with prompts specified as a set of natural language actions and desired states. The result—a Promptable Game Model (PGM)—makes it possible for a user to play the game by prompting it with high- and low-level action sequences. Most captivatingly, our PGM unlocks the director’s mode, where the game is played by specifying goals for the agents in the form of a prompt. This requires learning “game AI,” encapsulated by our animation model, to navigate the scene using high-level constraints, play against an adversary, and devise a strategy to win a point. To render the resulting state, we use a compositional NeRF representation encapsulated in our synthesis model. To foster future research, we present newly collected, annotated and calibrated Tennis and Minecraft datasets. Our method significantly outperforms existing neural video game simulators in terms of rendering quality and unlocks applications beyond the capabilities of the current state-of-the-art. Our framework, data, and models are available at snap-research.github.io/promptable-game-models.},
journal = {ACM Trans. Graph.},
month = {jan},
articleno = {17},
numpages = {16},
keywords = {Neural radiance fields, diffusion models, human motion generation, language modeling}
}

@proceedings{10.1145/3548659,
title = {A-TEST 2022: Proceedings of the 13th International Workshop on Automating Test Case Design, Selection and Evaluation},
year = {2022},
isbn = {9781450394529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 13th edition of the International Workshop on Automating Test Case Design, Selection and Evaluation (A-TEST 2022), co-located with and organized at ESEC/FSE 2022 during two days November 17-18, 2022 in Singapore. The A-TEST workshop aims to provide a venue for researchers and industry members alike to exchange and discuss trending views, ideas, state of the art, work in progress, and scientific results on automated testing.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3565970.3567681,
author = {Ullal, Akshith and Watkins, Alexandra and Sarkar, Nilanjan},
title = {A Multi-Objective Optimization Framework for Redirecting Pointing Gestures in Remote-Local Mixed/Augmented Reality},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565970.3567681},
doi = {10.1145/3565970.3567681},
abstract = {Collaborative augmented reality is an emerging field with the promise of simulating natural human-human interactions from remote locations. Users, represented by their photorealistic avatars, and relevant objects in their scenes can be teleported to each other's environments, with the capability of tracking their gaze, body pose, and other nonverbal behaviors using modern augmented reality devices. Pointing gestures play a key role for users to communicate about aspects related to their environment. Also, the body pose one uses during pointing, relays cues of the user's intentions and nonverbal behaviors during the interaction. Due to dissimilarities in multiple users’ environments, pointing gesture needs to be redirected since direct animation of the user's motion onto its avatar may introduce error. At the same time, the nonverbal behavior represented by the body pose also needs to be preserved for realistic interaction. While these objectives are not mutually exclusive, current approaches only solve the redirection of the gesture without preserving the body pose. In this paper, we present a systematic approach to solving the dual problem of redirection of gesture as well as preserving the body pose using a multi-objective optimization framework. The presented framework efficiently adjusts the weighting between the two objectives and gives the user the flexibility to set the minimum angular error tolerance for the pointing gesture redirection. We have tested our approach with the current state-of-the-art using both pointing gesture reference poses and redirecting against continuous gesture actions collected from an augmented reality human participant study. Results show that for a given user-defined error tolerance, our approach has a decrease of 33.5% in body pose error vs current-state-of-the-art for pointing gesture reference poses and 33.6% for pointing gestures recorded during the human participant study.},
booktitle = {Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
articleno = {9},
numpages = {11},
keywords = {Collaborative Augmented Reality (CAR), Multi-Objective Optimization (MOO), Pointing gestures, Social presence},
location = {Online, CA, USA},
series = {SUI '22}
}

@inproceedings{10.1145/3625008.3625016,
author = {Feitosa, Juliana da Costa and Conrado, Rafael Ragozoni},
title = {Physical activities gamification with smart board},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625016},
doi = {10.1145/3625008.3625016},
abstract = {Physical activities are important for a healthy and balanced life. There are many physical exercises designed to stimulate different aspects of the body, such as the cardiovascular system or each of the muscle tissues. However, there is a certain resistance to their routine performance due to their monotonous and repetitive nature. In order to overcome this barrier present in various tasks, one can resort to gamification, which uses typical elements of games, especially digital ones, to make any activities more interesting. Concepts of gamification have been used in the context of physical exercise to try to stimulate the performance of physical activity among adults, and to contribute to the maintenance of an active life among users. This was done through a gamified software made on the Unity platform to be used on the Huawei IdeaHub smartboard that runs on Windows operating system and has a camera. The software has a menu with statistics, achievements and exercises with description of the activity. The user performs exercises provided by the software, viewing information about the activity and with the camera of the IdeaHub, capturing the movements to count the exercises.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {48–55},
numpages = {8},
keywords = {gamification, physical activities, smart screen},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@article{10.1145/3675376,
author = {Kuth, Bastian and Oberberger, Max and Faber, Carsten and Baumeister, Dominik and Chajdas, Matth\"{a}us and Meyer, Quirin},
title = {Real-Time Procedural Generation with GPU Work Graphs},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3675376},
doi = {10.1145/3675376},
abstract = {We present a system for real-time procedural generation that makes use of the novel GPU programming model, work graphs. The nodes of a work graph are shaders, which dynamically generate new workloads for connected nodes. This greatly simplifies the implementation of recursive procedural algorithms on GPUs. Combined with GPU ray tracing and procedural mesh shaders, our system makes use of this graph structure to tackle various common problems of procedural generation. Our system is very easy to implement, requiring no additional data structures from what would already be available in a modern rendering engine. We demonstrate the real-time editing capabilities on representative examples. We augment the scene in the teaser image with 79,710 instances in 3.74 ms on an AMD Radeon RX 7900 XTX.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {aug},
articleno = {47},
numpages = {16},
keywords = {geometry generation, mesh shaders, ray tracing, work graphs}
}

@inproceedings{10.1145/3562939.3565619,
author = {Mutasim, Aunnoy and Batmaz, Anil Ufuk and Hudhud Mughrabi, Moaaz and Stuerzlinger, Wolfgang},
title = {Performance Analysis of Saccades for Primary and Confirmatory Target Selection},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565619},
doi = {10.1145/3562939.3565619},
abstract = {In eye-gaze-based selection, dwell suffers from several issues, e.g., the Midas Touch problem. Here we investigate saccade-based selection techniques as an alternative to dwell. First, we designed a novel user interface (UI) for Actigaze and used it with (goal-crossing) saccades for confirming the selection of small targets (i.e., &lt; 1.5-2°). We compared it with three other variants of Actigaze (with button press, dwell, and target reverse crossing) and two variants of target magnification (with button press and dwell). Magnification-dwell exhibited the most promising performance. For Actigaze, goal-crossing was the fastest option but suffered the most errors. We then evaluated goal-crossing as a primary selection technique for normal-sized targets (≥ 2°) and implemented a novel UI for such interaction. Results revealed that dwell achieved the best performance. Yet, we identified goal-crossing as a good compromise between dwell and button press. Our findings thus identify novel options for gaze-only interaction.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {18},
numpages = {12},
keywords = {Activation Methods, Eye-Gaze Tracking, Fitts’ Law, Saccade, Selection Techniques, Small Targets, Target Reverse Crossing, Throughput, Virtual Reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@proceedings{10.1145/3563836,
title = {PAINT 2022: Proceedings of the 1st ACM SIGPLAN International Workshop on Programming Abstractions and Interactive Notations, Tools, and Environments},
year = {2022},
isbn = {9781450399104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Programming environments that integrate tools, notations, and abstractions into a holistic user experience can provide programmers with better support for what they want to achieve. These programming environments can create an engaging place to do new forms of informational work—resulting in enjoyable, creative, and productive experiences with programming. 

In the workshop on Programming Abstractions and Interactive Notations, Tools, and Environments (PAINT), we want to discuss programming environments that support users in working with and creating notations and abstractions that matter to them. We are interested in the relationship between people centric notations and general-purpose programming languages and environments. How do we reflect the various experiences, needs, and priorities of the many people involved in programming—whether they call it that or not?},
location = {Auckland, New Zealand}
}

@inproceedings{10.1145/3670653.3670655,
author = {Wagner, Tobias and Colley, Mark and Breckel, Daniel and K\"{o}sel, Michael and Rukzio, Enrico},
title = {UnitEye: Introducing a User-Friendly Plugin to Democratize Eye Tracking Technology in Unity Environments},
year = {2024},
isbn = {9798400709982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670653.3670655},
doi = {10.1145/3670653.3670655},
abstract = {Eye tracking is a powerful tool for analyzing visual attention, as an input technique, or for diagnosing disorders. However, eye tracking hardware is expensive and not accessible to everyone, thus, considerably limiting real-world usage or at-home evaluations. Although webcam-based eye tracking is feasible due to advances in computer vision, its open-source implementation as an easy-to-use tool is lacking. We implemented UnitEye, a Unity plugin enabling eye tracking on desktop and laptop computers. In a technical evaluation (N=12), we tested the precision and accuracy of our system compared to a state-of-the-art eye tracker. We also evaluated the usability of UnitEye with N=5 developers. The results confirm that our system provides reliable eye tracking performance for a webcam-based system and well-integrated features contributing to ease of use.},
booktitle = {Proceedings of Mensch Und Computer 2024},
pages = {1–10},
numpages = {10},
keywords = {eye tracking, open-source, technical evaluation},
location = {Karlsruhe, Germany},
series = {MuC '24}
}

@inproceedings{10.1145/3544548.3580894,
author = {Arendttorp, Emilie Maria Nybo and Winschiers-Theophilus, Heike and Rodil, Kasper and Johansen, Freja B. K. and Rosengreen J\o{}rgensen, Mads and Kjeldsen, Thomas K. K. and Magot, Samkao},
title = {Grab It, While You Can: A VR Gesture Evaluation of a Co-Designed Traditional Narrative by Indigenous People},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580894},
doi = {10.1145/3544548.3580894},
abstract = {Recent developments in Virtual Reality (VR) applications, such as hand gesture tracking, provide new opportunities to create embodied user experiences. Numerous gesture elicitation studies have been conducted. However, in most instances they lack validation of implemented gestures, as well diversity of participant groups. Our research explores the digitalization of intangible cultural heritage in collaboration with one of the San tribes in Southern Africa. The focus is on particular gestures as embodied interactions of a VR implementation of a traditional San hunting story. In this paper, we present a gesture study, which entails an in-situ elicitation of natural gestures, a co-designed integration, a VR story implementation with grasping and three mid-air gestures, and a user evaluation. Based on our findings, we discuss the anthropological value of gesture implementations determined by an indigenous community, the local usability of a grasping gesture, and in-VR gesture elicitation, as an extension of existing methods.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {308},
numpages = {13},
keywords = {Community, Diversity, Hand Gestures, Indigenous People, Namibia, Natural Interaction, User Experiences, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3587421.3604476,
author = {Kilkenny, Thomas},
title = {Virtual Cinematography with Unreal Engine: Lean how Unreal Engine's virtual cinematography framework has given rise to new and advanced techniques in film and animation studios.},
year = {2023},
isbn = {9798400701436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587421.3604476},
doi = {10.1145/3587421.3604476},
abstract = {This talk focuses on the development and real-world applications of Virtual Cinematography tools, techniques, and workflows using Unreal Engine and accompanying real time technologies. These techniques enable the creation of highly customizable, handheld, point-and-shoot Virtual Cameras (VCams), which can be operated similarly to a live-action camera. Though in a state of constant refinement over the course of nearly two decades, the primary goal has remained consistent: recreate the feel of a cinema camera and make applicable the hard-earned skills of live-action filmmakers and creatives when working in the hybrid digital and animated space of virtual production.VCam has traditionally been a highly specific process created for a particular filmmaker by a highly skilled technical team on a given production. This work seeks to make these workflows available to all filmmakers at any level and to share the developments occurring in Unreal Engine and active productions today.},
booktitle = {ACM SIGGRAPH 2023 Talks},
articleno = {12},
numpages = {2},
keywords = {Real-time Filmmaking, Simulated Worlds, VCam, Virtual Camera, Virtual Cinematography, Virtual Production},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3604479.3604485,
author = {Alves, Alan K S and Dantas, Rummenigge R},
title = {A non-invasive middleware for games adaptation and motor rehabilitation controls},
year = {2024},
isbn = {9798400700026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604479.3604485},
doi = {10.1145/3604479.3604485},
abstract = {This work present the TeamBridge, a middleware able to perform the communication between digital games and hardware devices, like joysticks, mice and motion capture cameras. That communication does not require any modification to the game source code, allowing an old game to be adapted to work with a new hardware device. To prove this, tests were carried out with several games, including one of them being a commercial game. This middleware also allows the use of more than one device at the same time, so we can obtain more accurate information, one device can supply the deficiencies of the other. Finally, we performed tests to make sure that the middleware would not interfere with the user experience. Tests have shown that TeamBridge can receive, interpret and send information quickly, the time varies according to the device used, getting 33ms when used with Kinect, 40ms with Leap Motion and 255ms with the NED Glove.},
booktitle = {Proceedings of the 24th Symposium on Virtual and Augmented Reality},
pages = {1–10},
numpages = {10},
keywords = {Exergames, Gesture Interaction, VRPN, Virtual Reality Middleware},
location = {Natal, RN, Brazil},
series = {SVR '22}
}

@inproceedings{10.1145/3625704.3625742,
author = {Pinter, Logan and Izquierdo, Marcos and Siddiqui, Mohammad Faridul Haque},
title = {Revolutionizing Learning: An Interactive VR Application for Solids of Revolution},
year = {2023},
isbn = {9798400709142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625704.3625742},
doi = {10.1145/3625704.3625742},
abstract = {The understanding and visualization of the solids of revolution can be challenging for students due to the abstract nature of the topic. This paper presents the development and evaluation of a novel interactive Virtual Reality (VR) application framework designed to enhance students' comprehension and spatial reasoning skills related to solids of revolution. The tool offers interactive 3D visualization of arbitrary functions of standard 2D graphs and solids of revolution. The methodology involves utilizing Unity to create the 3D objects and implementing algorithms for generating surfaces of revolution. At this stage, the tool is evaluated through a comparison with other related works through comprehensive comparisons and literature review. The findings indicate that the developed application offers improved interactivity by adeptly managing complexity. Moreover, employing Unity to create interactive learning tools for visualizing 3D objects can effectively enhance students' understanding of complex mathematical concepts.},
booktitle = {Proceedings of the 7th International Conference on Education and Multimedia Technology},
pages = {35–40},
numpages = {6},
keywords = {Education, Immersive Environment, Interactive Learning, Solids of Revolution, Spatial Reasoning, VR, Virtual Reality},
location = {Tokyo, Japan},
series = {ICEMT '23}
}

@inproceedings{10.1145/3587421.3595447,
author = {Sin, Funshing and Melapudi, Vinod and Havaldar, Parag},
title = {An Animator-Friendly Chain Simulator for In-Game Animations at Blizzard Entertainment},
year = {2023},
isbn = {9798400701436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587421.3595447},
doi = {10.1145/3587421.3595447},
abstract = {At Blizzard, the Animation department produces character animations which get evaluated in our video games by the game engines. These joint-based skeletal animations are mostly produced from motion capture systems or by artists using DCCs like Maya. However, to animate the accessories of a character like cloth and hair, artists prefer to employ simulation tools to achieve physically correct motions. Our artists explored a few existing solutions (e.g., Maya’s nCloth) to simulate these accessories, but were either limited by the difficulty in setup requirements, or were unsatisfied at the amount of time it took to get visibly acceptable simulation results. So, we developed a custom joint-chain solver inspired by various publications. In this talk, we will detail some of the existing solutions that were evaluated and present their limitations. Then we will illustrate our approach that helped overcome those limitations and extend the capabilities. And finally, we will discuss some use cases of our solution using our production characters and highlight how our artists benefited from employing the simulator.},
booktitle = {ACM SIGGRAPH 2023 Talks},
articleno = {26},
numpages = {2},
keywords = {Animations, Keyframes, Maya, Physics-based Simulations},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@proceedings{10.1145/3643658,
title = {GAS '24: Proceedings of the ACM/IEEE 8th International Workshop on Games and Software Engineering},
year = {2024},
isbn = {9798400705618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GAS is an annual workshop that brings together researchers and practitioners who are keen on exchanging ideas and progressing techniques in the intersection of game engineering and software engineering.GAS explores how advanced technologies can be used to benefit the engineering of gameful systems, including entertainment games, serious games, and gamified applications. The goal of this one-day workshop is to bring together the greater community of software engineers and game engineers to encourage discussions from an interdisciplinary perspective, on the emerging research challenges around game and software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3544549.3585686,
author = {Bhowmick, Shimmila and Biswas, Nilotpal and Kalita, Pratul Chandra and Sorathia, Keyur},
title = {Wow! I Have Tiny Hands: Design And Evaluation Of Adaptive Virtual Hands For Small Object Selection Within Arms Length In Dense Virtual Environment},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585686},
doi = {10.1145/3544549.3585686},
abstract = {Object selection is essential to VR applications, but selecting small objects within arms’ reach in dense virtual environments can be challenging. We introduce Tiny Hands, which allows reducing the size of the dominant virtual hand in immersive VR to navigate efficiently and accurately select the desired object. This study compares the tiny hands technique with ray casting and pinch-to-select techniques. Our results indicate that the tiny hands technique is significantly faster and more accurate than the other two techniques. It is also significantly easier to use and learn, perceived as natural, playful, and the most preferred technique. Participants appreciated its ability to resize virtual hands for selecting small objects and ease in navigating a dense VE. They also liked its similarity to mouse and cursor-based interactions and flexibility in adjusting hand size according to various object sizes. Participants perceived tiny hands as potentially fatigued for prolonged use and reported a sense of incorrect depth perception when hand sizes are too small.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {322},
numpages = {6},
keywords = {Dense virtual environment, Object selection, Selection technique, Small objects},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3638837.3638852,
author = {Fajrianti, Evianita Dewi and Funabiki, Nobuo and Haz, Amma Liesvarastranta and Sukaridhoto, Sritrusta},
title = {A Proposal of OCR-based User Positioning Method in Indoor Navigation System Using Unity and Smartphone (INSUS)},
year = {2024},
isbn = {9798400709265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638837.3638852},
doi = {10.1145/3638837.3638852},
abstract = {An indoor navigation system helps users seamlessly navigate in complex buildings. Previously, we implemented the Indoor Navigation System using Unity and Smartphones (INSUS), where we employed QR code for accurate user positioning. However, a lot of QR code sheets need to be placed in the designated area to improve accuracy. To solve this problem, text information that is recognized by an Optical Character Recognition (OCR) software, such as room numbers, resident names, and floor levels in buildings can be used to recognize the current user position. In this paper, we propose an OCR-based user positioning method in INSUS. The text information is compared with the reference data stored in the database. To verify the performance of the proposal, we conducted a series of experiments involving various OCR models, view angles, and brightness levels. The results show that PaddleOCR achieves the average accuracy of 90.31% for various brightness levels and of 95% for different viewing angles.},
booktitle = {Proceedings of the 2023 12th International Conference on Networks, Communication and Computing},
pages = {99–105},
numpages = {7},
keywords = {Indoor Navigation System, Optical Character Recognition (OCR), Paddle-OCR, Smartphone., User Positioning},
location = {Osaka, Japan},
series = {ICNCC '23}
}

@inproceedings{10.1145/3573381.3597215,
author = {Bakk, \'{A}gnes Karolina and T\"{o}lgyesi, Borb\'{a}la and Bark\'{o}czi, M\'{a}t\'{e} and Buri, Bal\'{a}zs and Szab\'{o}, Andr\'{a}s and Tobai, Botond and Georgieva, Iva and Roth, Christian},
title = {Zenctuary VR: Simulating Nature in an Interactive Virtual Reality Application: Description of the design process of creating a garden in Virtual Reality with the aim of testing its restorative effects.},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3597215},
doi = {10.1145/3573381.3597215},
abstract = {In this paper we present the design process of a virtual reality experience the aim of which is to have a restorative effect on users. In the simulated natural site, the user can interact with some elements of the environment and can also explore the view. We describe how we tried to create a more realistic sense of nature by relying on high quality graphics, the use of free-roaming space, and naturalistic interactions. During the design process we avoided gameful interactions and instead created playful interactions, while also relying on the multimodal aspect of the virtual reality technology.&nbsp;},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {165–169},
numpages = {5},
keywords = {immersive virtual reality, nature simulation&nbsp;, restorative effect of nature, virtual restorative environments},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3649921.3649992,
author = {Holly, Michael and Habich, Lisa and Pirker, Johanna},
title = {GameDevDojo - An Educational Game for Teaching Game Development Concepts},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3649992},
doi = {10.1145/3649921.3649992},
abstract = {Computer Science (CS) has experienced significant growth and diversification in recent decades. However, there is a lack of diversity in CS learning approaches. Traditional teaching methods and hands-on learning dominate this field, with limited use of playful and interactive learning methods such as educational games. This gap is particularly evident in game development as a subfield of CS. To address this problem, we present a game-based learning approach to teach foundational concepts for game development. The paper aims to expand the educational landscape within CSE, offering a unique and engaging platform for learners to explore the intricacies of game creation by integrating gamified learning strategies. In this paper, we investigate the user’s learning experience and motivation, and the differences between traditional learning and game-based learning methods for teaching game development concepts. The study involves 57 participants in an AB test to assess learners’ motivation, user experience, and learning outcomes. The results indicate a significantly increased learning outcome for the game-based learning approach, as well as higher motivation in learning game development concepts.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {37},
numpages = {9},
keywords = {computer science education, educational games, game-based learning},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@article{10.1145/3661146,
author = {Vaiani, Giacomo and Patern\`{o}, Fabio},
title = {End-User Development for Human-Robot Interaction: Results and Trends in an Emerging Field},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {EICS},
url = {https://doi.org/10.1145/3661146},
doi = {10.1145/3661146},
abstract = {This paper presents a comprehensive survey on End-User Development for Human-Robot Interaction, examining existing literature to validate findings and identify unexplored areas for future research. It explores the importance of End-User Development in allowing non-expert users to customise robots, covering methodologies, evaluation methods, robot types, and application contexts. The findings reveal various End-User Development approaches, evaluation practices, and robots application domains, leading to discussions on the untapped potential of End-User Development in enhancing Human-Robot Interaction across diverse fields. The document aims to provide groundwork for future studies, highlighting the necessity for new evaluation standards and greater customisation in robotic technologies.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {252},
numpages = {40},
keywords = {End-User Development, Human-Robot Interaction, Literature Survey}
}

@inproceedings{10.1145/3649921.3649937,
author = {Claypool, Mark and Liu, Shengmei and Kuwahara, Atsuo and Scovell, James and Gregg, Miles and Galbiati, Federico and Eroglu, Eren},
title = {Waiting to Play - Measuring Game Load Times and their Effects on Player Quality of Experience},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3649937},
doi = {10.1145/3649921.3649937},
abstract = {Before playing, gamers must wait for the game to load. While the effects of waiting on user quality of experience is well-studied for some domains, the effects of wait times on game players is not known, nor is the impact of computer system components, such as the processor or graphics card, on game loading times. We present results from a user study that evaluates the impact of game loading time on quality of experience using a custom tool that simulates game loading and collects player ratings. Analysis of the results shows game loading time has a pronounced effect on player quality of experience, but differs based on the individual game time and game load content. Results from our subsequent measurement experiment show the potential to reduce game load times through hardware upgrades – type of processor and graphics card have significant effects on game load times, but type of storage device less so.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {2},
numpages = {11},
keywords = {QoE, gamer, games, hardware, measurement, user study},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@inproceedings{10.1145/3657547.3657557,
author = {Siebenmann, Michael and Lutfallah, Mathieu and Jetter, Dominic and Hirt, Christian and Kunz, Andreas},
title = {Investigation into Recording, Replay and Simulation of Interactions in Virtual Reality},
year = {2024},
isbn = {9798400709012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657547.3657557},
doi = {10.1145/3657547.3657557},
abstract = {Previous work has shown various authoring toolkits relying on recording, but quantitative comparisons between the 3D recording approaches remain unexplored. In this study, we introduce an authoring toolkit that allows both experts and trainees to record their actions in a virtual environment, streamlining the creation of training procedures and the evaluation of trainee actions. The toolkit was developed using two distinct methods: a state-based and an input-based approach. Within a virtual testing environment, we compared these methods across a range of interactions, focusing on three performance metrics: memory footprint, performance overhead, and replay accuracy. Contrary to initial predictions, the state-based method, after optimization, consumed less memory than the input-based approach. Both methods maintained low performance overheads during recording and replaying phases. Notably, the state-based approach achieved superior replay accuracy. In contrast, the input-based method displayed varying degrees of replay inaccuracies, partially attributable to the non-deterministic physics engine of the Unity development platform.},
booktitle = {Proceedings of the 2024 8th International Conference on Virtual and Augmented Reality Simulations},
pages = {13–21},
numpages = {9},
keywords = {Authoring Toolkit, Information Exchange, Recording},
location = {Melbourne, Australia},
series = {ICVARS '24}
}

@article{10.1145/3672089.3672101,
author = {Arnedo-Moreno, Joan and Cooper, Kendra M. L. and Lin, Dayi},
title = {Emerging Advanced Technologies for Game Engineering},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3672089.3672101},
doi = {10.1145/3672089.3672101},
abstract = {In this paper, the outcomes of the 8th International Workshop on Games and Software Engineering (GAS 2024)1 are reported. The one-day workshop has been held as part of the 46th International Conference on Software Engineering (ICSE 2024) in Lisbon, Portugal on April 14, 2024. The workshop programme includes two exciting keynotes discussing topics related to harnessing video game simulations to generate content and locate bugs, and the experience of maintaining a popular FOSS library, raylib. There are three research paper sessions. The first relates to automation in game engineering; the second explores testing and quality assurance; and the third discusses specification and quality of service. The conclusion of the workshop is anchored by a panel of four researchers, educators, and practitioners discussing the current strengths and limitations of large language models in game engineering.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {37–41},
numpages = {5}
}

@inproceedings{10.1145/3649921.3656979,
author = {Zhou, Jiayi and Martens, Chris and Cooper, Seth},
title = {Authoring Games with Tile Rewrite Rule Behavior Trees},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3656979},
doi = {10.1145/3649921.3656979},
abstract = {Game authoring can be a difficult, technical process; exploring new ways to describe games and game mechanics may help make game authoring more accessible. In this work, we present Tile Rewrite Rule Behavior Trees (TRRBTs): a concept for a domain-specific language for authoring tile-based, turn-based games. The approach combines tile rewrite rules and behavior trees. Using TRRBTs, a game’s state is represented as a grid of tiles, the behavior trees describe the overall flow of the game, and the rewrite rules at leaf nodes describe changes in game state. We include transform nodes, which apply transformations to other nodes in the behavior tree, allowing more complex mechanics to be expressed in a compact way. We demonstrate a text-based approach to using TRRBTs to create several simple games, show how the approach allows re-use of trees to build on existing games, and show how they can provide a unified representation for procedural content generation and enemy AI along with game mechanics.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {47},
numpages = {4},
keywords = {behavior trees, game authoring, rewrite rules},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@inproceedings{10.1145/3641235.3664443,
author = {Johnson, Justin},
title = {Exploring the Dichotomy of Nature and Tech Through an Interactive Media Assignment},
year = {2024},
isbn = {9798400705175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641235.3664443},
doi = {10.1145/3641235.3664443},
abstract = {In this submission I present a mixed media assignment that asks students to challenge the dichotomy of nature and technology. For this assignment, students use animation, 3D modeling, and physical making to create an interactive piece that embodies the aesthetics of nature through visuals and sound or visuals and touch. The primary goal of this assignment is to strengthen a connectedness to nature through creativity, while also challenging the students by encouraging mixing media in creative ways. The current description of this assignment is open and flexible; however, it can also be focused on a specific discipline (e.g. animation or game development) or scaled by adjusting the scope of the final deliverable. This paper will give a brief overview of the assignment including the prompt, process, materials, and examples of student work.},
booktitle = {ACM SIGGRAPH 2024 Educator's Forum},
articleno = {18},
numpages = {2},
keywords = {Immersive media, animation, interactive art, mixed media},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3635636.3664254,
author = {Rollins, Sophie and Hancock, Katherine and Ali-Diaz, Jasmin and Shahdadpuri, Nyssa and Long, Duri},
title = {Knowledge Net: Fostering Children’s Understanding of Knowledge Representations Through Creative Making and Embodied Interaction in a Museum Exhibit},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3664254},
doi = {10.1145/3635636.3664254},
abstract = {As young people increasingly use AI in their daily lives, it is imperative to foster these learners’ AI literacy. We present Knowledge Net, a collaborative tangible tabletop museum exhibit aimed at teaching users about knowledge representations, which are central to understanding AI and understudied in AI education research. In this exhibit, we center creative making and embodied interaction by allowing learners to craft the appearance, behaviors, and traits of characters in a virtual world by manipulating semantic networks. Our poster features the exhibit design and corresponding rationale, and this paper contributes an exploration of how creative making and embodied interaction can be utilized to teach young learners about knowledge representations–and AI more broadly–in informal learning environments.},
booktitle = {Proceedings of the 16th Conference on Creativity &amp; Cognition},
pages = {470–475},
numpages = {6},
keywords = {AI education, AI literacy, design research, informal learning, knowledge representation, museum exhibit, prototyping, tangible user interfaces},
location = {Chicago, IL, USA},
series = {C&amp;C '24}
}

@proceedings{10.1145/3641234,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Posters},
year = {2024},
isbn = {9798400705168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@inproceedings{10.1145/3658549.3658559,
author = {Xu, Hao-Xiang and Chang, Kai-Ni and Chiu, Chien-Sheng and Wu, Shih-Jung},
title = {Research on the design of third-person 3D action role-playing games},
year = {2024},
isbn = {9798400709180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658549.3658559},
doi = {10.1145/3658549.3658559},
abstract = {Affected by COVID-19 in recent years, many industries have suffered a lot in the near future. At the same time, computer hardware technology is still improving. The game industry has not decreased but increased under this development trend. Among the many game options, 3D action role-playing games are one of the most popular types of games today, and there are many well-known and excellent works on the market. For consumers, games are not just a tool to kill time, the entertainment they bring can also relieve people's physical and mental pressure. From the perspective of developers, we might as well study these games in depth. What sets these classic 3D action role-playing games special, and how to let people have a good gaming experience. Based on the experience of actually playing several games of the same type, I will share the structure and knowledge of studying the game. The research including implement of 3D action role-playing games, the design of program logic, the writing of plot of a play, etc., using Blender for 3D modeling, Unity3D games Developing the engine as an environment, in person making a 3D action game, and exploring the knowledge gained in the research.},
booktitle = {Proceedings of the 2024 International Conference on Information Technology, Data Science, and Optimization},
pages = {36–41},
numpages = {6},
location = {Taipei, Taiwan},
series = {I-DO '24}
}

@article{10.1145/3648233,
author = {Bonic, Sanja and Bonic, Janos and Schmid, Stefan},
title = {Broomrocket: Open Source Text-to-3D Algorithm for 3D Object Placement},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3648233},
doi = {10.1145/3648233},
abstract = {Story writers and other creative professionals often rely on concept artists to visualize and then further iterate on their work during game development and other visualization processes. This exchange and its various stages are time consuming, and there is no easy remedy for creating a walkable 3D concept art without involving a 3D artist yet. As a first step, we present Broomrocket, an open source text-to-3D algorithm for 3D concept art. Broomrocket’s contribution is an object relation and placement algorithm that transforms user input describing a 3D scene given in plain English language into actual models placed in a 3D scene. It runs locally using an existing downloaded natural language processing model and does not require third-party services unless a connection to an online 3D model distribution platform is desired. In that case, Broomrocket will search for the keywords from the user’s narrative input and desired license, and place them in the 3D scene, adding each model’s individual license to a license file for further usage.},
journal = {ACM Games},
month = {aug},
articleno = {22},
numpages = {16},
keywords = {Text-to-3D, level design, scene generation, 3D concept art, prototyping, language processing, computing}
}

@inproceedings{10.1145/3634713.3634725,
author = {G\"{u}thing, Lukas and Bittner, Paul Maximilian and Schaefer, Ina and Th\"{u}m, Thomas},
title = {Explaining Edits to Variability Annotations in Evolving Software Product Lines},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634713.3634725},
doi = {10.1145/3634713.3634725},
abstract = {Software is subject to changes and revisions during its development life cycle. For configurable software systems, changes may be made to functionality of source code as well as variability information such as code-to-feature mappings. To explain how code-to-feature mappings change in edits made to configurable software, we relate the mappings before and after an edit in terms of the sets of variants they denote. We prove our explanations to be complete and unambiguous, meaning that every pair of code-to-feature mappings is explained in terms of exactly one relation. Based on a graph formalism, we provide an algorithm for fast detection of relations during commits to version control. In an initial study, we detect relations between feature annotations in 42 real-world software product-line repositories to better understand typical changes in the evolution of configurable software. We demonstrate that our formalism can be automated and that analyzing a commit requires only 135 ms on average.},
booktitle = {Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {93–102},
numpages = {10},
keywords = {software evolution, software product lines, software variability},
location = {Bern, Switzerland},
series = {VaMoS '24}
}

@inproceedings{10.1145/3587421.3595410,
author = {Bannink, Paulus},
title = {Facial Animation at Scale: Producing facial animation for 24 hours of narrative content in “Horizon: Forbidden West”},
year = {2023},
isbn = {9798400701436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587421.3595410},
doi = {10.1145/3587421.3595410},
abstract = {This talk covers the complete pipeline and process used to generate facial animation data for all the narrative content in the latest installment of the Horizon game series, created by Guerrilla. We developed a proprietary technique for embedding per-frame timecode and source take information into all our narrative animation data as the basis of this pipeline. Then we added a fully automated facial animation solving pipeline where animators could push animation quality by improving generic FaceWare models. Any update to these models would automatically cause the re-processing of all dependent facial animation data. To manage all this data a review pipeline was set up in Atlassian Jira that provided both a high-level overview as well as granular control over the more than 15.000 narrative facial animations in the game.},
booktitle = {ACM SIGGRAPH 2023 Talks},
articleno = {25},
numpages = {2},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@article{10.1145/3622856,
author = {Renda, Alex and Ding, Yi and Carbin, Michael},
title = {Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622856},
doi = {10.1145/3622856},
abstract = {Programmers and researchers are increasingly developing surrogates of programs, models of a subset of the observable behavior of a given program, to solve a variety of software development challenges. Programmers train surrogates from measurements of the behavior of a program on a dataset of input examples. A key challenge of surrogate construction is determining what training data to use to train a surrogate of a given program.  

We present a methodology for sampling datasets to train neural-network-based surrogates of programs. We first characterize the proportion of data to sample from each region of a program's input space (corresponding to different execution paths of the program) based on the complexity of learning a surrogate of the corresponding execution path. We next provide a program analysis to determine the complexity of different paths in a program. We evaluate these results on a range of real-world programs, demonstrating that complexity-guided sampling results in empirical improvements in accuracy.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {280},
numpages = {29},
keywords = {neural networks, programming languages, surrogate models}
}

@inproceedings{10.1145/3623264.3624444,
author = {Sugimori, Ken and Mitake, Hironori and Sato, Hirohito and Hasegawa, Shoichi},
title = {Avatar Tracking Control with Featherstone's Algorithm and Newton-Euler Formulation for Inverse Dynamics},
year = {2023},
isbn = {9798400703935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623264.3624444},
doi = {10.1145/3623264.3624444},
abstract = {With a spread of inexpensive and easy-to-use motion tracking methods such as cameras and trackers for virtual realities (VRs), real-time motion-tracked avatars are becoming increasingly common in virtual environments, particularly social VRs, virtual performers (e.g., virtual YouTubers), and VR games. These applications frequently involve interactions among multiple avatars or between avatars and objects for communication or gameplay. However, at present, most applications do not sufficiently consider the effects of contact for avatars, thus leading to penetration or unnatural behavior. To achieve natural avatar motion in such scenarios, the player must perform as if contact has occurred, even though, in reality, there is no contact with the player’s body. While physics simulation can solve the contact issue, the basic use of physics simulation causes tracking delay. We therefore solve this tracking delay problem by employing Featherstone’s algorithm, a reduced-coordinate method for forward dynamics, and by utilizing the Newton-Euler formulation for inverse dynamics to compute tracking forces and torques. To evaluate the delay, we measured the difference between the joint rotations of the simulated model and the input joint rotations. The evaluation with our method indicates the difference is less than 1.4 micro radians for all joints in real time with a small computational cost. Moreover, accurate tracking enables the fixation of both feet. The proposed method provides accurate tracking and soft reactions to contact.},
booktitle = {Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games},
articleno = {16},
numpages = {10},
keywords = {avatar, character, inverse dynamics, motion capture, physics simulation},
location = {Rennes, France},
series = {MIG '23}
}

@inproceedings{10.1145/3585059.3611423,
author = {Gunay, Cengiz and Barakat, Rahaf},
title = {Immersive gamification for education: No additional benefit gained from wearing a VR headset},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611423},
doi = {10.1145/3585059.3611423},
abstract = {Virtual reality (VR) technologies have been increasingly gaining attention for use in a variety of applications. Yet, wider adoption is impeded by the discomfort of headsets, costs of adopting the technology, and the computational power required for rendering. Instead, much of VR’s benefit can be achieved with simple a “desktop VR” application using a computer monitor. However, it is not known whether this semi-immersive VR approach is as effective. In this paper, we compare the experience of VR using a headset versus on the desktop, to test the effectiveness of an immersive 3D gamification approach for education of K through 12 grades and postsecondary students. In our survey of 73 general education college students from diverse backgrounds, we found no statistically significant gains from wearing a VR headset in learning computer programming concepts. To investigate further, we followed up with a secondary study of 46 middle school students, where we confirmed our findings. Additional attitude questions in the follow-up study showed that motion sickness and wearing glasses were the most important reasons for avoiding the headset. We conclude that while immersive gamification is effective for education, we recommend the inexpensive and more convenient desktop VR approach, which is also remote-learning friendly.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {136–141},
numpages = {6},
keywords = {K-12 education, Python, VR headset, desktop VR, gamification, immersive environments, programming education, virtual reality},
location = {Marietta, GA, USA},
series = {SIGITE '23}
}

@article{10.1145/3611031,
author = {You, Ruoxin and Zhou, Yihao and Zheng, Weicheng and Zuo, Yiran and Barrera Machuca, Mayra Donaji and Tong, Xin},
title = {BlueVR: Design and Evaluation of a Virtual Reality Serious Game for Promoting Understanding towards People with Color Vision Deficiency},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3611031},
doi = {10.1145/3611031},
abstract = {People with color vision deficiency (CVD) often encounter color-related challenges in their daily life, which are difficult for those with non-CVD to comprehend fully. Therefore, we designed a Virtual Reality (VR) serious game, BlueVR, to simulate challenging scenarios encountered by people with CVD and facilitate understanding from people with non-CVD. We conducted an empirical study with thirty participants with non-CVD and six participants with CVD to evaluate the opportunities and challenges of BlueVR. Our findings suggest that BlueVR increased people with non-CVD’s understanding, awareness, and perspective-taking abilities towards people with CVD. Moreover, interviews with participants with CVD revealed that BlueVR accurately depicts their real-life discomforts and meets their expectations to improve potential social awareness. This research contributes valuable insights into the mechanisms underlying the effectiveness of VR serious games in promoting understanding and design implications for future game development.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {385},
numpages = {30},
keywords = {Color Vision Deficiency, Empathy, Serious Game, Virtual Reality}
}

@article{10.1613/jair.1.15185,
author = {Suglia, Alessandro and Konstas, Ioannis and Lemon, Oliver},
title = {Visually Grounded Language Learning: A Review of Language Games, Datasets, Tasks, and Models},
year = {2024},
issue_date = {Apr 2024},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {79},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.15185},
doi = {10.1613/jair.1.15185},
abstract = {In recent years, several machine learning models have been proposed. They are trained with a language modelling objective on large-scale text-only data. With such pretraining, they can achieve impressive results on many Natural Language Understanding and Generation tasks. However, many facets of meaning cannot be learned by “listening to the radio” only. In the literature, many Vision+Language (V+L) tasks have been defined with the aim of creating models that can ground symbols in the visual modality. In this work, we provide a systematic literature review of several tasks and models proposed in the V+L field. We rely on Wittgenstein’s idea of ‘language games’ to categorise such tasks into 3 different families: 1) discriminative games, 2) generative games, and 3) interactive games. Our analysis of the literature provides evidence that future work should be focusing on interactive games where communication in Natural Language is important to resolve ambiguities about object referents and action plans and that physical embodiment is essential to understand the semantics of situations and events. Overall, these represent key requirements for developing grounded meanings in neural models.},
journal = {J. Artif. Int. Res.},
month = {jan},
numpages = {67}
}

@inproceedings{10.1145/3620665.3640367,
author = {Davies, Michael and McDougall, Ian and Anandaraj, Selvaraj and Machchhar, Deep and Jain, Rithik and Sankaralingam, Karthikeyan},
title = {A Journey of a 1,000 Kernels Begins with a Single Step: A Retrospective of Deep Learning on GPUs},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640367},
doi = {10.1145/3620665.3640367},
abstract = {We are in age of AI, with rapidly changing algorithms and a somewhat synergistic change in hardware. MLPerf is a recent benchmark suite that serves as a way to compare and evaluate hardware. However it has several drawbacks - it is dominated by CNNs and does a poor job of capturing the diversity of AI use cases, and only represents a sliver of production AI use cases. This paper performs a longitudinal study of state-of-art AI applications spanning vision, physical simulation, vision synthesis, language and speech processing, and tabular data processing, across three generations of hardware to understand how the AI revolution has panned out. We call this collection of applications and execution scaffolding the CaSiO suite. The paper reports on data gathered at the framework level, device API level, and hardware and microarchitecture level. The paper provides insights on the hardware-software revolution with pointers to future trends.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {20–36},
numpages = {17},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3649921.3656978,
author = {Osborn, Joseph Carter and Wieser, Katiana and Brody, Miriam},
title = {RetroFit: Post-Facto Accessibility for Retro Games},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3656978},
doi = {10.1145/3649921.3656978},
abstract = {Video games are important cultural artifacts, but are made inaccessible to many people by aspects of their visual, audio, and mechanical design. Some game developers add accessibility features to new games, but without access to source code, accessibility supports cannot be added to existing games. In this work, we propose a general technique based on game object affordances and instantiate it for the specific case of supporting users with limited vision. Specifically, we show an example of a color modulation scheme for simplifying and increasing the clarity of in-game graphics based on user-provided annotations.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {46},
numpages = {4},
keywords = {accessibility, automated game design learning, scene understanding},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@inproceedings{10.1145/3631204.3631866,
author = {Grau, Oliver and Hagn, Korbinian},
title = {VALERIE22 - A photorealistic, richly metadata annotated dataset of urban environments},
year = {2023},
isbn = {9798400704543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631204.3631866},
doi = {10.1145/3631204.3631866},
abstract = {The VALERIE tool pipeline is a synthetic data generator [14] developed with the goal to contribute to the understanding of domain-specific factors that influence perception performance of DNNs (deep neural networks). This work was carried out under the German research project KI Absicherung in order to develop a methodology for the validation of DNNs in the context of pedestrian detection in urban environments for automated driving. The VALERIE22 dataset was generated with the VALERIE procedural tools pipeline providing a photorealistic sensor simulation rendered from automatically synthesized scenes. The dataset provides a uniquely rich set of metadata, allowing extraction of specific scene and semantic features (like pixel-accurate occlusion rates, positions in the scene and distance + angle to the camera). This enables a multitude of possible tests on the data and we hope to stimulate research on understanding performance of DNNs. Based on cross-domain semantic segmentation experiments, i.e. training on synthetic data and evaluation on target real world data, a comparison with several other publicly available datasets is provided, demonstrating that VALERIE22 is one of best performing synthetic datasets currently available in the open domain. 1},
booktitle = {Proceedings of the 7th ACM Computer Science in Cars Symposium},
articleno = {6},
numpages = {9},
keywords = {2D-Bounding Box Detection, AI Validation, Autonomous Driving, Object Detection, Pedestrian Detection, Synthetic Data},
location = {Darmstadt, Germany},
series = {CSCS '23}
}

@inproceedings{10.1145/3613904.3642741,
author = {Wales, Michaelah and Wheeler, Michael and Cimolino, Gabriele and Levin, Laura and Mees, Jayna and Graham, T.C. Nicholas},
title = {Process, Roles, Tools, and Team: Understanding the Emerging Medium of Virtual Reality Theatre},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642741},
doi = {10.1145/3613904.3642741},
abstract = {Virtual reality (VR) theatre artists are combining theatre production and game development practices to create live performances in VR. To date, little is known about VR theatre creators’ experiences of this process or how staging a play in VR might affect the audience’s experience. To capture the experience of developing a VR theatre production we interviewed the production team behind the VR play You Should Have Stayed Home. Members of this team felt the process was a learning experience and shared the lessons they plan to incorporate into their future work. We report on the team’s efforts to understand the VR theatre medium, how this team was constructed, and challenges that they encountered. In this paper we present the opportunities that the production team members identified for creating novel experiences for VR audiences, and their own needs as creators.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {99},
numpages = {14},
keywords = {Design Process, Drama, Intermedial Theatre, Virtual Reality Theatre},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3628034.3628040,
author = {Flageol, William and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Badri, Mourad and Monnier, Stefan},
title = {Design Pattern for Reusing Immutable Methods in Object-Oriented Languages},
year = {2024},
isbn = {9798400700408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628034.3628040},
doi = {10.1145/3628034.3628040},
abstract = {Context. Features and styles inspired by functional programming have grown in popularity in the world of object-oriented programming. Immutability is a core concept of functional programming, which brings advantages to software development. However, introducing immutability in object-oriented programming presents some challenges. Problem. One of these challenges is method overriding. When inheriting non-destructive mutators (methods used on immutable objects which return a new object instead of modifying the receiver), a naive approach generates code duplication and has scalability issues. Contribution. We analyse an example of this overriding problem and propose a solution in a new design pattern based on the factory method pattern. We also discuss the advantages and limitations of this pattern, as well as implementations in Clojure, Java, and Kotlin. We also identify and discuss the language features that mostly affect the implementation of this pattern. Conclusion. Our proposed design pattern helps mitigate some of the code duplication and scalability problems of a naive approach. However, the inclusion of a functional updating language feature is required to completely remove the scalability issues.},
booktitle = {Proceedings of the 28th European Conference on Pattern Languages of Programs},
articleno = {6},
numpages = {9},
keywords = {design patterns, functional programming, immutability, language features, object-oriented programming},
location = {Irsee, Germany},
series = {EuroPLoP '23}
}

@inproceedings{10.1145/3610538.3614650,
author = {Billinghurst, Mark},
title = {Rapid Prototyping for XR},
year = {2023},
isbn = {9798400703096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610538.3614650},
doi = {10.1145/3610538.3614650},
booktitle = {SIGGRAPH Asia 2023 Courses},
articleno = {12},
numpages = {148},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@inproceedings{10.1145/3610978.3640589,
author = {Reitmann, Stefan and Mihaylova, Tsvetomila and Topp, Elin Anna and Kyrki, Ville},
title = {Conflict Simulation for Shared Autonomy in Autonomous Driving},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640589},
doi = {10.1145/3610978.3640589},
abstract = {We present a tool for modeling conflict situations that enables simulation and testing of situation awareness in shared autonomy, in this case in an autonomous driving scenario. The flexibility of the tool allows definition of new conflict situations, integration with various control and conflict detection systems, as well as customization of Takeover Request (TOR) signals and different means of communication to the human operator. We start with one particular conflict situation - loss of lane markings, for which we demonstrate a simple conflict detection system. We conduct a preliminary user evaluation, which provides useful insights about the usability of the tool. The feedback from the participants indicates that TORs without indication feel uncomfortable and providing explicit information about the conflict situation is necessary when switching to manual control is required.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {882–887},
numpages = {6},
keywords = {autonomous driving, conflict detection, driving simulation, explainability, human-ai interaction, shared autonomy},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/3625008.3625047,
author = {de Oliveira, Raquel E M and Ferreira, Lorran R. and de Oliveira, Jauvane C.},
title = {A Pedagogical Virtual Reality Environment for Children in Early Childhood Education},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625047},
doi = {10.1145/3625008.3625047},
abstract = {The research presented here describes the use of a virtual reality system focused on the teaching-learning process in early childhood education. The system comprises of four activities that aim at developing and improving skills related to perception, motor coordination, recognition of geometric shapes, and spatial location and orientation. The interaction with the virtual environment occurs through Kinect360, enabling three-dimensional tracking of the students body in real time. The SUS evaluation method was administered to pedagogues to assess the usability of the system, and the results obtained are satisfactory and give evidence of the applicability of the system.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {204–209},
numpages = {6},
keywords = {Childhood Education, Kinect360, Virtual Environment, Virtual Reality},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@article{10.1145/3605952,
author = {Johansen, Nicklas S. and K\ae{}r, Lasse B. and Madsen, Andreas L. and Nielsen, Kristian \O{}. and Srba, Ji\v{r}\'{\i} and Tollund, Rasmus G.},
title = {Kaki: Efficient Concurrent Update Synthesis for SDN},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {0934-5043},
url = {https://doi.org/10.1145/3605952},
doi = {10.1145/3605952},
abstract = {Modern computer networks based on the software-defined networking (SDN) paradigm are becoming increasingly complex and often require frequent configuration changes in order to react to traffic fluctuations. It is essential that forwarding policies are preserved not only before and after the configuration update but also at any moment during the inherently distributed execution of such an update. We present Kaki, a Petri game based tool for automatic synthesis of switch batches which can be updated in parallel without violating a given (regular) forwarding policy like waypointing or service chaining. Kaki guarantees to find the minimum number of concurrent batches and supports both splittable and nonsplittable flow forwarding. In order to achieve optimal performance, we introduce two novel optimisation techniques based on static analysis: decomposition into independent subproblems and identification of switches that can be collectively updated in the same batch. These techniques considerably improve the performance of our tool Kaki, relying on TAPAAL’s verification engine for Petri games as its backend. Experiments on a large benchmark of real networks from the Internet Topology Zoo database demonstrate that Kaki outperforms the state-of-the-art tools Netstack and FLIP. Kaki computes concurrent update synthesis significantly faster than Netstack and compared to FLIP, it provides shorter (and provably optimal) concurrent update sequences at similar runtimes.},
journal = {Form. Asp. Comput.},
month = {oct},
articleno = {20},
numpages = {22},
keywords = {Computer networks, software defined networking, concurrent update synthesis, security policies}
}

@inproceedings{10.1145/3588028.3603667,
author = {Chen, Timothy and Then, Miguel Ying Jie and Huang, Jing-Yuan and Chen, Yang-Sheng and Han, Ping-Hsuan and Hung, Yi-Ping},
title = {sPellorama: An Immersive Prototyping Tool using Generative Panorama and Voice-to-Prompts},
year = {2023},
isbn = {9798400701528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588028.3603667},
doi = {10.1145/3588028.3603667},
abstract = {We proposed sPellorama, an immersive tool that enables Virtual Reality (VR) content creators to quickly prototype scenes based on verbal input. The system first converts voice input to text, then utilizes a text-guided panorama generation model to produce the described scene. The panorama is later applied to Skybox in Unity. Previously generated panorama will be preserved in the photosphere and ready to be viewed. The pilot study shows that our tool can enhance the process of discussion and prototyping for VR content creators.},
booktitle = {ACM SIGGRAPH 2023 Posters},
articleno = {42},
numpages = {2},
keywords = {Generative Panorama, VR Prototyping, Virtual Reality},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3546155.3546707,
author = {Sadek, Nouran and Elagroudy, Passant and Khalil, Ali and Abdennadher, Slim},
title = {The Superhero Pose: Enhancing Physical Performance in Exergames by Embodying Celebrity Avatars in Virtual Reality},
year = {2022},
isbn = {9781450396998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546155.3546707},
doi = {10.1145/3546155.3546707},
abstract = {Altered virtual reality self-representations can augment human capabilities by exploiting the Proteus effect. For example, previous work has shown that embodying an avatar that signifies super-intelligence, Einstein, enhanced the users’ performance in cognitive tasks. In this paper, we show that embodying an avatar that signifies superior athletic skills enhances the users’ physical performance. We conducted a between-subject experiment (n = 50) where participants played a soccer game while being embodied in a famous soccer avatar and in a generic avatar. We reflect on how self-esteem, embodiment illusion, and presence moderate the Proteus effect. Our results showed that participants embodied as a professional soccer celebrity performed better in the tasks and had a higher sense of embodiment compared to those with a generic avatar. Our results can be used to design engaging exergames and technological interventions to expand human capabilities.},
booktitle = {Nordic Human-Computer Interaction Conference},
articleno = {1},
numpages = {11},
keywords = {Proteus effect, avatar embodiment, body ownership illusion, stereotypes, virtual reality},
location = {Aarhus, Denmark},
series = {NordiCHI '22}
}

@proceedings{10.1145/3625008,
title = {SVR '23: Proceedings of the 25th Symposium on Virtual and Augmented Reality},
year = {2023},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rio Grande, Brazil}
}

@article{10.1145/3679021,
author = {Zachos, Aristeidis and Anagnostopoulos, Christos-Nikolaos},
title = {Using TLS, UAV and MR methodologies for 3D Modelling and Historical Recreation of Religious Heritage Monuments},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4673},
url = {https://doi.org/10.1145/3679021},
doi = {10.1145/3679021},
abstract = {Preserving and safeguarding the Cultural Heritage (CH) of our world from unforeseen hazards should be viewed as a collective responsibility for humanity. Consequently, there is a growing imperative for targeted measures aimed at conserving, restoring, and safeguarding historical assets that carry cultural significance. In recent times, Terrestrial Laser Scanning (TLS), Unmanned Aerial Vehicle (UAV) Photogrammetry, and applications in Mixed Reality (MR) have assumed a pivotal role in the mapping, recording, preservation, and promotion of Cultural Heritage. This article endeavors to present a comprehensive approach spanning from 3D surveying to the 3D representation and promotion of Religious Cultural Heritage, offering an overview of the applied methodologies. Through the integration of TLS and UAV Photogrammetry techniques, a comprehensive digital record of Panagia Ekatontapyliani, the adjoining Church of Agios Nikolaos, and the Baptistery, along with their wall paintings (hagiographies) and natural surroundings, has been obtained. This record serves as the foundation for historical documentation and recreation using the HBIM concept, paving the way for the development of diverse Mixed Reality applications. These applications aim to enhance the visibility, accessibility, and visitability of the Monument.},
note = {Just Accepted},
journal = {J. Comput. Cult. Herit.},
month = {jul},
keywords = {Terrestrial 3D scanning, UAV Photogrammetry, Religious Heritage}
}

@inproceedings{10.1145/3613904.3642440,
author = {Oh, Jeongseok and Kim, Seungju and Kim, Seungjun},
title = {LumiMood: A Creativity Support Tool for Designing the Mood of a 3D Scene},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642440},
doi = {10.1145/3613904.3642440},
abstract = {The aesthetic design of 3D scenes in game content enhances players’ experience by inducing desired emotions. Creating emotionally engaging scenes involves designing low-level features, such as color distribution, contrast, and brightness. This study presents LumiMood, an AI-driven creativity support tool (CST) that automatically adjusts lighting and post-processing to create moods for 3D scenes. LumiMood supports designers by synthesizing reference images, creating mood templates, and providing intermediate design steps. Our formative study with 10 designers identified distinct challenges in mood design based on the participants’ experience levels. A user study involving 40 designers revealed that using LumiMood benefits the designers by streamlining workflow, improving precision, and increasing mood intention accuracy. Results indicate that LumiMood supports clarifying mood concepts and improves interpretation of lighting and post-processing, thus resolving the challenges. We observe the effect of template based designing and discuss considerable factors for AI-driven CSTs for users with varying levels of experiences.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {174},
numpages = {21},
keywords = {affective computing, artificial intelligence, creativity support tool, graphics design},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3625008.3625009,
author = {Soares, Jizar M. Vorma and Fernandes, Di\'{o}genes P. and Stefani, Cassiano J. Mandelli and Santiago, Pablo and Zanatta, Alexandre L. and Rieder, Rafael},
title = {A Pilot Study of the User Experience in an Augmented Reality Mobile App to Support Teaching of Hematology},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625009},
doi = {10.1145/3625008.3625009},
abstract = {This paper presents the development and pilot study of the user experience of the AnemiaAR Mobile App, an Augmented Reality serious game to support teaching of hematology. We created two versions of the game: marker-based (using Vuforia resources) and markerless (using MRTK resources). Our counterbalanced study considered assessing the two app versions by 12 volunteers, Medicine undergraduate students, enrolled in a hematology course. At the post-test, the subjects answered the User Experience Questionnaire (UEQ) and responded to an interview about the strengths and weaknesses of each version. The results showed 17 of the 26 UEQ aspects with better evaluation means for the markerless support, suggesting a more positive user experience for this version. This panorama is corroborated by the subjective interview data, showing a user preference for markerless interaction for this AR mobile game.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {1–7},
numpages = {7},
keywords = {augmented reality, hematology, marked-based, markerless, mobile game, user experience},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@inproceedings{10.1145/3660043.3660212,
author = {Hou, Huiying},
title = {Enhancing Indoor Thermal Comfort Education: A Virtual Reality Platform Introducing Fanger's Model},
year = {2024},
isbn = {9798400716157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660043.3660212},
doi = {10.1145/3660043.3660212},
abstract = {This study introduces an immersive Virtual Learning Environment (VLE) utilizing Virtual Reality (VR) to educate building service engineering students on indoor thermal comfort. Integrated with Fanger's Predicted Mean Vote Model, the VLE offers a multi-user platform with interactive interfaces spanning theoretical concepts to real-world application. Through Unity, users manipulate virtual environment parameters, gaining practical thermal comfort assessment experience. By merging theory and practice within VR, it deepens understanding of indoor environmental quality's impact on human comfort. Leveraging Human-Computer Interaction (HCI) principles, the VLE ensures intuitive interaction using interactive system and tools, notably User Interface Toolkits (UIT). Virtual Reality enhances engagement and learning outcomes, providing a realistic, immersive environment. Incorporating Fanger's model offers a structured approach to analyze indoor thermal conditions, fostering critical thinking. This research showcases VR's potential in teaching building service engineering and indoor environmental quality.},
booktitle = {Proceedings of the 2023 International Conference on Information Education and Artificial Intelligence},
pages = {948–952},
numpages = {5},
location = {Xiamen, China},
series = {ICIEAI '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00021,
author = {Wu, Xiongfei and Ye, Jiaming and Chen, Ke and Xie, Xiaofei and Hu, Yujing and Huang, Ruochen and Ma, Lei and Zhao, Jianjun},
title = {Widget Detection-Based Testing for Industrial Mobile Games},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00021},
doi = {10.1109/ICSE-SEIP58684.2023.00021},
abstract = {The fast advances in mobile hardware and widespread smartphone usage have fueled the growth of global mobile gaming in the past decade. As a result, the need for quality assurance of mobile gaming has become increasingly pressing. While general-purpose testing methods have been developed for mobile applications, they become struggling when being applied to mobile games due to the unique characteristics of mobile games, such as dynamic loading and stunning visual effects. There comes a growing industrial demand for automated testing techniques with high compatibility (compatible with various resolutions, and platforms) and non-intrusive characteristics (without packaging external modules into the source code, e.g., POCO). To fulfill these demands, in this paper, we introduce our experience in adopting the widget detection-based testing technique WDTest, for mobile games at NetEase Games. To this end, we have constructed by far the largest graphical user interface (GUI) dataset for mobile games and conducted comprehensive evaluations on the performance of state-of-the-art widget detection techniques in the context of mobile gaming.We leverage widget detection techniques to develop WDTest, which performs automated testing using only screenshots as input. Our evaluation shows that WDTest outperforms the widely used tool Monkey in achieving three times more coverage of unique UI in gaming scenarios. Our further experiments demonstrate that WDTest can be applied to general mobile applications without additional fine-tuning. Furthermore, we conducted a thorough survey at NetEase Games to gain a comprehensive understanding of widget detection-based testing techniques and identify challenges in industrial mobile game testing. The results show that testers are overall satisfied with the compatibility testing aspect of widget detection-based testing, but not much with functionality testing. This survey also highlights several unique characteristics of mobile games, providing valuable insights for future research directions.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {173–184},
numpages = {12},
keywords = {mobile game testing, GUI testing, GUI detection, software quality assurance},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3658549.3658562,
author = {Lee, Shao-Lun and Hung, Lun-Ping and Yeh, Ming-Kuei and Lin, Yu-Chih and Tsai, Wen-Lung and Zhang, Ya-Han},
title = {Enhancing Elderly Care through a 3D Interactive Simulation System in a Day Care Center},
year = {2024},
isbn = {9798400709180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658549.3658562},
doi = {10.1145/3658549.3658562},
abstract = {This study explores how a 3D interactive field simulation system can enhance users' efficiency in understanding the equipment conditions in sunlight center areas. A growing number of users in sunlight centers play an indispensable role as crucial institutions for improving the quality of life for older people. We have observed that all existing sunlight centers lack an online simulation navigation system, requiring users to visit the site physically. The issues arising from the necessity of on-site visits are the focus of investigation and improvement in this study.},
booktitle = {Proceedings of the 2024 International Conference on Information Technology, Data Science, and Optimization},
pages = {48–56},
numpages = {9},
keywords = {3D Interactive, Field Simulation, Navigation System, Sunlight Facilities},
location = {Taipei, Taiwan},
series = {I-DO '24}
}

@article{10.1145/3549487,
author = {Iacovides, Ioanna and Cutting, Joe and Beeston, Jen and Cecchinato, Marta E. and Mekler, Elisa D. and Cairns, Paul},
title = {Close but Not Too Close: Distance and Relevance in Designing Games for Reflection},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549487},
doi = {10.1145/3549487},
abstract = {Reflection is an important precursor to attitude and behavior change, but existing advice on designing for reflection in games is mixed and requires further empirical investigation. We report on the design and evaluation (n=32) of a game to prompt student reflection on work-life balance. Participants played as themselves or a third person character (Alex). An inductive qualitative analysis of post-play interviews, and a follow-up one week later, resulted in four themes relating to how gameplay facilitated reflection: making (sensible) consequences visible; it's like MY life; the space between Alex and I; and triggers in everyday life. A deductive qualitative analysis also indicated that while both games resulted in different forms of reflection for the majority of players, those who role-played as Alex appeared more likely to experience higher levels of reflection. Through exploring the ways in which the two versions of the game succeeded, and failed, to support reflection, we highlight the importance of providing a relevant context to players (so the game feels close to their experience), and allowing them to role-play as someone other than themselves (but not too close).},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {224},
numpages = {24},
keywords = {evaluation, game design, reflection, role-play, work-life balance}
}

@inproceedings{10.1145/3590837.3590869,
author = {Kashyap, Ratnesh and Singh, Juhi and Sinha, Shweta},
title = {Recent Advancement in Education System Using AR Tools: A New Perspective},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590869},
doi = {10.1145/3590837.3590869},
abstract = {Technology in education has the potential to drive pupils to learn in a more interactive and perceptible manner. The application presented here is focused on the used materials during the teaching-learning process, such as text and photos, and displays a major 3-dimensional model or video on the screen. These applications provide the students by boosting them to peruse innovative ideas by using the graphical guide. Apart from utilizing it in school, it can be used for higher education in universities. The advantages of Augmented Reality are also discussed in comparison to old tactics such as chalkboards and textbooks. The findings demonstrate that, in general, Augmented Reality technologies offer good potential and benefits that can be applied in education. The paper also identifies AR's shortcomings, which could be solved through further research.},
booktitle = {Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
articleno = {32},
numpages = {5},
keywords = {ARCORE, Augmented Reality, Education, UNITY,AI},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@proceedings{10.1145/3641233,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Talks},
year = {2024},
isbn = {9798400705151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@article{10.5555/3636971.3636973,
author = {Cliburn, Daniel C.},
title = {Teaching and Learning with Virtual Reality},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {2},
issn = {1937-4771},
abstract = {Virtual reality (VR) is an exciting field with numerous application areas, one of which is education. It has never been easier for instructors to teach with and for students to learn about VR technologies. This paper provides an overview of many ways that VR technologies are being integrated into computing education. Software tools are described that support teaching and learning about VR, and the author's experiences teaching VR and using VR technologies are presented. The paper concludes with a discussion of some of the barriers to wide scale adoption of VR technologies in computing education.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {19–27},
numpages = {9}
}

@article{10.1145/3643771,
author = {Sun, Gengyi and Habchi, Sarra and McIntosh, Shane},
title = {RavenBuild: Context, Relevance, and Dependency Aware Build Outcome Prediction},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643771},
doi = {10.1145/3643771},
abstract = {Continuous Integration (CI) is a common practice adopted by modern software organizations. It plays an especially important role for large corporations like Ubisoft, where thousands of build jobs are submitted daily. Indeed, the cadence of development progress is constrained by the pace at which CI services process build jobs. To provide faster CI feedback, recent work explores how build outcomes can be anticipated. Although early results show plenty of promise, the distinct characteristics of Project X—a AAA video game project at Ubisoft, present new challenges for build outcome prediction. In the Project X setting, changes that do not modify source code also incur build failures. Moreover, we find that the code changes that have an impact that crosses the source-data boundary are more prone to build failures than code changes that do not impact data files. Since such changes are not fully characterized by the existing set of build outcome prediction features, state-of-the art models tend to underperform. 
 
Therefore, to accommodate the data context into build outcome prediction, we propose RavenBuild, a novel approach that leverages context, relevance, and dependency-aware features. We apply the state of-the-art BuildFast model and RavenBuild to Project X, and observe that RavenBuild improves the F1 score of the failing class by 50%, the recall of the failing class by 105%, and AUC by 11%. To ease adoption in settings with heterogeneous project sets, we also provide a simplified alternative RavenBuild-CR, which excludes dependency-aware features. We apply RavenBuild-CR on 22 open-source projects and Project X, and observe across-the-board improvements as well. On the other hand, we find that a na\"{\i}ve Parrot approach, which simply echoes the previous build outcome as its prediction, is surprisingly competitive with BuildFast and RavenBuild. Though Parrot fails to predict when the build outcome differs from their immediate predecessor, Parrot serves well as a tendency indicator of the sequences in build outcome datasets. Therefore, future studies should also consider comparing to the Parrot approach as a baseline when evaluating build outcome prediction models.},
journal = {Proc. ACM Softw. Eng.},
month = {jul},
articleno = {45},
numpages = {23},
keywords = {build outcome prediction, continuous integration, maintenance cost, mining software repositories}
}

@inproceedings{10.1145/3659994.3660312,
author = {Ascari, Elena and Cerchiai, Mauro and Gorrasi, Pasquale and Longo, Antonella and Mencagli, Gabriele and Miccoli, Francesca and Zappatore, Marco},
title = {OUTFIT: Crowdsourced Data Feeding Noise Maps in Digital Twins},
year = {2024},
isbn = {9798400706417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659994.3660312},
doi = {10.1145/3659994.3660312},
abstract = {In the realm of urban management, Digital Twins (DTs) have recently shown their potential to improve planning and sustainability. The OUTFIT PRIN 2022 project aims to optimize data streams to dynamically render Road Traffic Noise (RTN) in an urban DT model, incorporating both noise levels and citizens' perceptions. In this paper, we introduce OUTFIT and propose the methodology aimed at providing a set of tools to assist policymakers in addressing noise issues and promoting actions for improving the well-being of the citizenship. The project, aligned with Mission 1 of the Italian National Recovery and Resilience Plan (PNRR) and Horizon Europe priorities, focuses on mobility, energy, urban infrastructure, circular economy, and behavioral change. OUTFIT also aims at building a traffic-related database from crowdsourced data for developing a reliable RTN model input to start. This follows an optimization of the data streams for dynamic traffic data processing and 3D noise rendering in order to create a DT model which integrates noise, traffic, and complaints data, with a particular focus on efficient monitoring and orchestration of edge resources. In addition to the definition of a validated method for deriving traffic flows and RTN, a 3D DT with dynamic noise rendering and a set of APIs to enable the interoperability of OUTFIT system's open data will be developed as well.},
booktitle = {Proceedings of the 4th Workshop on Flexible Resource and Application Management on the Edge},
pages = {39–46},
numpages = {8},
keywords = {digital twins, crowd-sourced data, noise maps},
location = {Pisa, Italy},
series = {FRAME '24}
}

@inproceedings{10.1145/3625008.3625038,
author = {Monticelli, Jonas and Wagner, Jorge and Maciel, Anderson and Torchelsen, Rafael P and Freitas, Carla and Nedel, Luciana},
title = {Immersive Visualization Interface for Endoscopy Analytics and Debriefing},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625038},
doi = {10.1145/3625008.3625038},
abstract = {Endoscopic surgery is a complex procedure and is challenging for students to learn without risking the patient. Meanwhile, simulators have been developed to provide a safe environment for students to practice and learn, guaranteeing the patient’s safety. During a surgery simulation, many data are produced, including the patient’s vital signs and the indicators of the user’s performance, i.e., the endoscope operation. Combining both data in a timeline provides a picture of the patient’s condition. This work proposes a new perspective for immersive visualization of an endoscopic surgery simulation by integrating a virtual reality simulator with a series of panels for presenting data collected during the simulation. Additionally, after the surgery training, one may graphically analyze and reason the simulation session in a timeline without leaving the immersive environment. This practice, so-called debriefing, is usual in any medical practice. The proposed interface can potentially enhance the comprehension of a patient’s state during surgical procedures. Medical practitioners and surgeons can also use it to evaluate medical students’ proficiency, leading to possible changes in surgical training and providing new tools and resources for medical education.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {166–173},
numpages = {8},
keywords = {endoscopy simulation, immersive visualization, virtual reality},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@inproceedings{10.1145/3639701.3663645,
author = {Rajasagi, Priya and Boot, Lee and Wilson, Lucy E and King, Tristan and Zuber, James and Stockwell, Ian and Komlodi, Anita},
title = {SEEe Immersive Analytics System: Enhancing Data Analysis Experience within Complex Data Visualization Environments},
year = {2024},
isbn = {9798400705038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639701.3663645},
doi = {10.1145/3639701.3663645},
abstract = {The current state-of-the-art 2D data visualizations fall short in capturing the intricate complexity and depth of available information crucial for integrated decision-making. In response to this limitation, the Systems Exploration and Engagement environment (SEEe) emerges as a cutting-edge virtual immersive analytics data experience. We developed this system through a user-centered design process involving an interdisciplinary design and development team. Through virtual reality, SEEe seamlessly integrates geo-referenced spatial data, abstract data visualization, and qualitative data encompassing text, images, videos, and conceptual diagrams to support sensemaking from large amounts of multiformat data and integrated decision making. We aim to redefine the experience of analyzing extensive amounts of abstract data by creating an environment that accommodates both quantitative and qualitative data for visualization and analysis. How these novel immersive analytics experiences fit into data analysis workflows in various domains have not been studied widely. We carried out a user study with 10 public health graduate students to test the usability, learnability, and utility of the SEEe experience and to explore how these immersive data visualization experiences can fit into traditional data analysis processes. While SEEe is designed to be adaptable across various domains, we evaluated its performance within the public health context. The results of the evaluation affirm that SEEe is not only usable and useful but also provides a learnable environment conducive to immersive analytics.},
booktitle = {Proceedings of the 2024 ACM International Conference on Interactive Media Experiences},
pages = {408–415},
numpages = {8},
keywords = {Geo-referenced data visualization, Immersive analytics, Virtual reality},
location = {Stockholm, Sweden},
series = {IMX '24}
}

@inproceedings{10.1145/3551349.3560506,
author = {Paduraru, Ciprian and Paduraru, Miruna Gabriela and Blahovici, Andrei},
title = {Transfer learning of cars behaviors from reality to simulation applications},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560506},
doi = {10.1145/3551349.3560506},
abstract = {Creating synthetic behaviors of vehicles in simulation applications has always been challenging from a development standpoint. First, it is a real challenge to create a credible and realistic simulation while achieving the required runtime efficiency. Second, the effort required to implement it can add significant cost to the development processes. In this paper, we propose an automated way to design vehicle simulation systems by transfer learning from reality to simulators. Our methods rely on advanced deep learning technologies and datasets commonly used in the field of self-driving cars. To assess how well this approach would work in a simulation environment, experiments using the CARLA simulator are presented in the evaluation. The results show that the proposed transfer learning approach provides good results, both quantitatively and qualitatively, and is suitable for runtime evaluation even in resource-constrained simulation applications such as video games.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {212},
numpages = {8},
keywords = {Deep learning, behaviors, simulation, vehicles},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3660515.3662834,
author = {Artizzu, Valentino and Luyten, Kris and Rovelo Ruiz, Gustavo Alberto and Spano, Lucio Davide},
title = {Direct Feedforward Techniques for the ViRgilites System},
year = {2024},
isbn = {9798400706516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660515.3662834},
doi = {10.1145/3660515.3662834},
abstract = {In this poster we propose an implementation of direct feedforward for the ViRgilites system. The project defines two alternative uses, with respect to the current implementation, that only shows in an indirect way (icons, target object images, text) how to perform an interaction in the simulated environment. The first representation is a single avatar mode where the user sees a virtual avatar performing an action in the same environment as the user, while the second representation is a multiple avatar mode, where the user can choose to compare two interactions and see the avatar representations side by side in dedicated panels. We report on the initial ideas and proof-of-concepts, while we envision further modifications and a future evaluation of the final outcome.},
booktitle = {Companion Proceedings of the 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {87–88},
numpages = {2},
keywords = {feedforward, meta-design, task, toolkit, virtual reality, vocational education},
location = {Cagliari, Italy},
series = {EICS '24 Companion}
}

@proceedings{10.1145/3641521,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Immersive Pavilion},
year = {2024},
isbn = {9798400705274},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@article{10.1145/3593241,
author = {Ketoma, Vix Kemanji and Vanderdonckt, Jean and Meixner, Gerrit},
title = {Towards Flexible Authoring and Personalization of Virtual Reality Applications for Training},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {EICS},
url = {https://doi.org/10.1145/3593241},
doi = {10.1145/3593241},
abstract = {Virtual reality applications offer the promise to immerse end users in a synthetic environment where several actions could be observed, simulated, and reproduced, before transferring them to reality, which makes them particularly appropriate for training. Yet, when the training requires complex handling of information, the tasks become cognitively intensive, and developing adequate applications becomes challenging. To address this challenge, we define a method for developing head-mounted-display-based virtual reality applications for modular training tasks, composed of a training model with parameters, a step-wise approach for supporting this development, and a software framework enacting the application of this approach. Authoring such applications is expected to become more flexible and provide personalization facilities. To evaluate the impact of this method, we define a case study concerning an application for training school teachers who deal with a variety of situations in a classroom for an experiment involving N= 7 participants for a set of tasks. Pre-study and post-study acceptances reveal the impact of the software framework and a workload evaluation is conducted using the NASA TLX questionnaire.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {189},
numpages = {37},
keywords = {software development method, software framework, training, training scenario authoring, virtual reality}
}

@article{10.1145/3595294,
author = {Zhong, Zhengwu},
title = {Research on the Implementation of Advertising Design Teaching Based on Unity3D Development Platform and Web3D Technology},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3595294},
doi = {10.1145/3595294},
abstract = {In this work, the Unity3D development platform and Web3D technology are integrated into the teaching method of advertising design to get rid of the issues due to lack of communication and efficacy through design of digital advertisement. Based on this, an approach for ingenious product design from nature is proposed, with an emphasis on attaining a functional interaction of aesthetic intent and geometric features and investigating the relationships among natural systems and designers in product design from nature. The ponderings and research findings for the methodologies associated with the proposed approach are presented. This methodology is thought to significantly bring down the delivery time of ground-breaking design and development of products, both economically and technologically. The findings in comprehensive experiments demonstrates that interactive virtual technology can significantly enhance the efficacy and interaction of the whole system in the process of digital advertising design.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {81},
numpages = {14},
keywords = {Advertising design, product design from nature, Unity3D development platform, Web3D technology}
}

@article{10.1145/3647646,
author = {Long, Sebastian and Denisova, Alena and Mirza-Babaei, Pejman},
title = {From Pixels to Play: Opportunities and Challenges of a Diverse and Democratized Games Industry},
year = {2024},
issue_date = {March - April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1072-5520},
url = {https://doi.org/10.1145/3647646},
doi = {10.1145/3647646},
abstract = {This forum features game-practitioner perspectives on the interaction design process, techniques, and evaluation involved in creating playful experiences. We focus on how technology advancement, infrastructure, and constraints shape the player experience.},
journal = {Interactions},
month = {feb},
pages = {54–58},
numpages = {5}
}

@inproceedings{10.1145/3568160.3570235,
author = {Malayjerdi, Mohsen and Roberts, Andrew and Maennel, Olaf manuel and Malayjerdi, Ehsan},
title = {Combined Safety and Cybersecurity Testing Methodology for Autonomous Driving Algorithms},
year = {2022},
isbn = {9781450397865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568160.3570235},
doi = {10.1145/3568160.3570235},
abstract = {Combined safety and cybersecurity testing are critical for assessing the reliability and optimisation of autonomous driving (AD) algorithms. However, safety and cybersecurity testing is often conducted in isolation, leading to a lack of evaluation of the complex system-of-system interactions which impact the reliability and optimisation of the AD algorithm. Concurrently, practical limitations of testing include resource usage and time. This paper proposes a methodology for combined safety and cybersecurity testing and applies it to a real-world AV shuttle using digital twin, software-in-the-loop (SiL) simulation and a real-world Autonomous Vehicle (AV) test environment. The results of the safety and cybersecurity tests and feedback from the AD algorithm designers demonstrate that the methodology developed is useful for assessing the reliability and optimisation of an AD algorithm in the development phase. Furthermore, from the observed system-of-system interactions, key relationships such as speed and attack parameters can be used to optimise testing.},
booktitle = {Proceedings of the 6th ACM Computer Science in Cars Symposium},
articleno = {12},
numpages = {10},
keywords = {automotive cybersecurity, autonomous driving, safety testing},
location = {Ingolstadt, Germany},
series = {CSCS '22}
}

@inproceedings{10.1145/3613905.3650924,
author = {Tong, Wai and Xia, Meng and Qu, Huamin},
title = {Exploring Stage Lighting Education in Metaverse},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650924},
doi = {10.1145/3613905.3650924},
abstract = {This paper investigates stage lighting education in the metaverse from a practical perspective. We conducted participatory design with practitioners and stakeholders from a local university to develop a VR-based stage lighting system for the Technical Theater Arts course. Over six months, we derived a list of design requirements (e.g., Level of realism serves the purpose of learning) and developed a prototype VR system for stage lighting education. Our contributions include the establishment of design requirements for stage lighting education in the metaverse, the development of a prototype system, and insights from integrating VR in course development. This research paves the way for further exploration and refinement of VR applications in educational settings.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {148},
numpages = {6},
keywords = {Stage lighting education, participatory design, virtual reality},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3604479.3604524,
author = {Valente, Felipe R and Guimaraes, Marcelo De P and Cirilo, Elder J R and Brandao, Alexandre F and Dias, Diego R C},
title = {A framework for neuromotor and neurofunctional rehabilitation using multi-agent systems},
year = {2024},
isbn = {9798400700026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604479.3604524},
doi = {10.1145/3604479.3604524},
abstract = {Degenerative diseases and stroke disable many people, so investigating methods for rehabilitating patients is necessary. Thus, using the framework for creating virtual environments to work with neuromotor and neurofunctional rehabilitation, benefits can be achieved using virtual environments and body tracking devices. Developing such applications involves many requirements, such as constructing virtual environments, data storage, interaction with input devices, processing, and communication in rehabilitation sessions. In this paper, the proposal and development of a multi-agent framework are presented to facilitate the development of software for neuromotor and neurofunctional rehabilitation, providing advantages for developers, such as eliminating the need for code rewriting, facilitating the reuse of coding standards, and decreasing the time to develop new features.},
booktitle = {Proceedings of the 24th Symposium on Virtual and Augmented Reality},
pages = {142–150},
numpages = {9},
keywords = {body tracking, framework, multi-agent, rehabilitation},
location = {Natal, RN, Brazil},
series = {SVR '22}
}

@inproceedings{10.1145/3551349.3560513,
author = {Heidrich, David and Schreiber, Andreas and Oberd\"{o}rfer, Sebastian},
title = {Towards Generating Labeled Property Graphs for Comprehending C#-based Software Projects},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560513},
doi = {10.1145/3551349.3560513},
abstract = {C# is the most widely used programming language among XR developers. However, only a limited number of graph-based data acquisition tools exist for C# software. XR development commonly relies on reusing existing software components to accelerate development. Graph-based visualization tools can facilitate this comprehension process, e.g., by providing an overview of relationships between components. This work describes a new tool called Src2Neo that generates labeled property graphs of C#-based software projects. The stored graph follows a simple C# naming scheme and — contrary to other solutions — maps each software entity to exactly one node. The resulting graph facilitates the comprehension process by providing an easy to read representation of software components. Additionally, the generated graphs can act as a data basis for more advanced software visualizations without the need for complex data requests.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {228},
numpages = {4},
keywords = {c#, game engines, labeled property graph, software comprehension, software visualization},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@proceedings{10.1145/3663532,
title = {FaSE4Games 2024: Proceedings of the 1st ACM International Workshop on Foundations of Applied Software Engineering for Games},
year = {2024},
isbn = {9798400706745},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to FaSE4Games'24, the Foundations of Applied Software Engineering for Games workshop. We are thrilled to host this inaugural event together with the ACM International Conference on the Foundations of Software Engineering (FSE'2024) dedicated to exploring the intersections of software engineering and game development. This workshop takes place on June 19th, 2024, and is a hybrid event, ensuring participation from a diverse and international community.},
location = {Porto de Galinhas, Brazil}
}

@inproceedings{10.1145/3543507.3583329,
author = {Bi, Weichen and Ma, Yun and Tian, Deyu and Yang, Qi and Zhang, Mingtao and Jing, Xiang},
title = {Demystifying Mobile Extended Reality in Web Browsers: How Far Can We Go?},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583329},
doi = {10.1145/3543507.3583329},
abstract = {Mobile extended reality (XR) has developed rapidly in recent years. Compared with the app-based XR, XR in web browsers has the advantages of being lightweight and cross-platform, providing users with a pervasive experience. Therefore, many frameworks are emerging to support the development of XR in web browsers. However, little has been known about how well these frameworks perform and how complex XR apps modern web browsers can support on mobile devices. To fill the knowledge gap, in this paper, we conduct an empirical study of mobile XR in web browsers. We select seven most popular web-based XR frameworks and investigate their runtime performance, including 3D rendering, camera capturing, and real-world understanding. We find that current frameworks have the potential to further enhance their performance by increasing GPU utilization or improving computing parallelism. Besides, for 3D scenes with good rendering performance, developers can feel free to add camera capturing with little influence on performance to support augmented reality (AR) and mixed reality (MR) applications. Based on our findings, we draw several practical implications to provide better XR support in web browsers.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2960–2969},
numpages = {10},
keywords = {Extended Reality, Measurement, Web browser},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3603269.3610851,
author = {Gao, Kaihui and Chen, Li and Li, Dan and Liu, Vincent and Wang, Xizheng and Zhang, Ran and Lu, Lu},
title = {Demo: NetVision: Efficient Visualization Front-End for Packet-level Discrete-Event Network Simulation},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603269.3610851},
doi = {10.1145/3603269.3610851},
abstract = {Visualization of network simulation is an essential tool for network practitioners. However, the front-end of existing network simulators often fails to deliver satisfactory performance when dealing with modern network scales and interface speed. In this paper, we propose NetVision, an efficient visualization front-end of network simulation based on the Unity engine, which is commonly used for video game and virtual reality development. NetVision offers flow-level visualization of network behavior and performances. Then, through parallel optimization, NetVision supports real-time visualization for large-scale high-speed networks.},
booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference},
pages = {1179–1181},
numpages = {3},
keywords = {network simulation, visualization, parallel computing},
location = {New York, NY, USA},
series = {ACM SIGCOMM '23}
}

@inproceedings{10.1145/3652024.3665510,
author = {Sareen, Kunal and Blackburn, Stephen M. and Hamouda, Sara S. and Gidra, Lokesh},
title = {Memory Management on Mobile Devices},
year = {2024},
isbn = {9798400706158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652024.3665510},
doi = {10.1145/3652024.3665510},
abstract = {The performance of mobile devices directly affects billions of people worldwide.
 
Yet, despite memory management being key to their responsiveness, energy efficiency, and cost, mobile devices are understudied in the literature.
 
A paucity of suitable methodologies and benchmarks is likely both a cause and a consequence of this gap.
 
It also reflects the challenges of evaluating mobile devices due to: i) their inherently multi-tenanted nature, ii) the scarcity of widely-used open source workloads suitable as benchmarks, iii) the challenge of determinism and reproducibility given mobile devices' extensive use of GPS and network services, iv) the complexity of mobile performance criteria.
 

 
We study this problem using the Android Runtime (ART), which is particularly interesting because it is open sourced, garbage collected, and its market extends from the most advanced to the most basic mobile devices available, with a commensurate diversity of performance expectations.
 
Our study makes the following contributions: i) we identify pitfalls and challenges to the sound evaluation of garbage collection in ART, ii) we describe a framework for the principled performance evaluation of overheads in ART, iii) we curate a small benchmark suite comprised of widely-used real-world applications, and iv) we conduct an evaluation of these real-world workloads as well as some DaCapo benchmarks and a micro-benchmark.
 
For a modestly sized heap, we find that the lower bound on garbage collection overheads vary considerably among the benchmarks we evaluate, from 2&nbsp;% to 51&nbsp;%, and that overall, overheads are similar to those identified in recent studies of Java workloads running on OpenJDK.
 
We hope that this work will demystify the challenges of studying memory management in the Android Runtime.
 
By doing so, we hope to open up research and lead to more innovation in this highly impactful and memory-sensitive domain.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on Memory Management},
pages = {15–29},
numpages = {15},
keywords = {Android, Android Runtime, Garbage Collection, Memory Management},
location = {Copenhagen, Denmark},
series = {ISMM 2024}
}

@inproceedings{10.1145/3610540.3627010,
author = {Bennett, Gregory and Najafi, Hossein and Jackson, Lee},
title = {Pedagogical strategies for teaching Virtual Production pipelines},
year = {2023},
isbn = {9798400703119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610540.3627010},
doi = {10.1145/3610540.3627010},
abstract = {The incorporation of LED walls and virtual production tools in the film industry is a recent development that has significant pedagogical implications (BLISTEIN, 2020; FARID and TORRALBA, 2021). The use of LED walls combines physical and digital realities, potentially reducing post-production time and resource usage (ONG, 2020). Virtual production can facilitate immediate on-set decision-making and mitigate the need for certain post-production adjustments (EPIC GAMES, 2020). MIT researchers posit that such practices may also reduce the carbon footprint associated with location filming (FARID and TORRALBA, 2021). Nonetheless, further research and innovation are needed to overcome any limitations and fully harness the potential of this technology.The utilization of Unreal Engine software and other real-time tools in visual effects education aligns with industry trends and enhances student preparedness for professional practice (FLEISCHER, 2020). It has been suggested that engaging students with these tools can foster an understanding of the virtual production pipeline, thus aligning the educational curriculum with evolving industry standards (BALSAMO et al., 2021).As educators at Auckland University of Technology, we recognized the necessity of early integration of these paradigm-shifting tools into our curriculum to prepare students for impending and ongoing industry changes. In anticipation of procuring LED walls, we explored the potential of leveraging existing resources to initiate a pedagogical foray into the virtual production sphere. The available resources encompassed a large green screen studio, a motion capture studio, and virtual reality (VR) headsets and trackers.This paper offers two pedagogical responses that were deployed in courses situated in the final (third) year of undergraduate studies for an Animation, Visual Effects and Game Design Major, Bachelor of Design, at the School of Art and Design, Auckland University of Technology. One course is positioned within a motion capture minor and the other an option within a major capstone project framework.These two responses form case studies in the technical modification of existing teaching equipment and the need to push the limits of existing software and hardware resources within the attendant budgetary constraints of a tertiary education institution. This, in turn, is in the service of meeting shifting tertiary curriculum demands in response to a technologically fluid and future-focused industry.},
booktitle = {SIGGRAPH Asia 2023 Educator's Forum},
articleno = {11},
numpages = {10},
keywords = {compositing, computer graphics, education, motion capture, virtual production, virtual reality},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@inproceedings{10.1145/3586183.3606763,
author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
title = {Generative Agents: Interactive Simulacra of Human Behavior},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606763},
doi = {10.1145/3586183.3606763},
abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {22},
keywords = {Human-AI interaction, agents, generative AI, large language models},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3641234.3671043,
author = {Kim, Eun Hye and Ro, Hyocheol and Park, Hyunjin},
title = {Amplify AR HUD User-Experience with Real-World Sunlight Simulation in Virtual Scene},
year = {2024},
isbn = {9798400705168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641234.3671043},
doi = {10.1145/3641234.3671043},
abstract = {In this paper, we introduce the concept of real-world sunlight simulation in virtual scenes of Head-Up Display (HUD) application. We have adopted solar position calculation logic [Zhang, 2020] and demonstrated its capability with experiments ran on real vehicle (Figure 1). In addition, we suggest UX approaches featuring the interaction between virtual 3D objects and the simulated virtual sunlight for expanded AR experiences in HUD application.},
booktitle = {ACM SIGGRAPH 2024 Posters},
articleno = {61},
numpages = {2},
keywords = {Head-Up Display, In-Vehicle Infotainment},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@proceedings{10.1145/3631085,
title = {SBGames '23: Proceedings of the 22nd Brazilian Symposium on Games and Digital Entertainment},
year = {2023},
isbn = {9798400716270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rio Grande (RS), Brazil}
}

@inproceedings{10.1145/3586182.3616627,
author = {De Vidal Flores, Eduard and Yildirim, Caglar and Harrell, D. Fox},
title = {E4UnityIntegration-MIT: An Open-Source Unity Plug-in for Collecting Physiological Data using Empatica E4 during Gameplay},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3616627},
doi = {10.1145/3586182.3616627},
abstract = {Physiological measurement of player experience (PX) during gameplay has been of increasing interest within game research circles. A commonly-used non-invasive wearable device for physiological measurement is the Empatica E4 wristband, which offers multiple physiological metrics, ranging from electrodermal activity to heart rate. That said, the E4’s integration with popular game engines such as Unity 3D presents certain challenges due to non-obvious critical bugs in the library and limited documentation applicability within the Unity context. In this paper, we present an open-source Unity plug-in designed to mitigate the challenges associated with integrating the E4 into Unity projects: E4UnityIntegration-MIT. The plug-in exposes the E4’s API for interfacing with Unity C# scripts, thereby enabling realtime data collection and monitoring. E4UnityIntegration-MIT also provides the affordance of saving the E4 data into an external file for data analysis purposes.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {49},
numpages = {3},
keywords = {Empatica E4, Unity, affective gaming, plugin, psychophysiological measures},
location = {San Francisco, CA, USA},
series = {UIST '23 Adjunct}
}

@inproceedings{10.1145/3649405.3659525,
author = {Fachada, Nuno},
title = {Databases Without Databases: Projects for Including Database Concepts in Interdisciplinary Curricula with LINQ},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649405.3659525},
doi = {10.1145/3649405.3659525},
abstract = {We present five project assignments for teaching database concepts to undergraduate students in interdisciplinary degree programs, where teaching databases as a standalone course is not practical. The projects were developed for a CS2-level programming course, where LINQ and basic database concepts are lectured, and have been shown to effectively engage students and improve their understanding of queries and data manipulation.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
pages = {793–794},
numpages = {2},
keywords = {.net, c#, cs2, databases, linq},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inbook{10.1145/3596711.3596800,
author = {Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.},
title = {SMPL: A Skinned Multi-Person Linear Model},
year = {2023},
isbn = {9798400708978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3596711.3596800},
abstract = {We present a learned model of human body shape and posedependent shape variation that is more accurate than previous models and is compatible with existing graphics pipelines. Our Skinned Multi-Person Linear model (SMPL) is a skinned vertexbased model that accurately represents a wide variety of body shapes in natural human poses. The parameters of the model are learned from data including the rest pose template, blend weights, pose-dependent blend shapes, identity-dependent blend shapes, and a regressor from vertices to joint locations. Unlike previous models, the pose-dependent blend shapes are a linear function of the elements of the pose rotation matrices. This simple formulation enables training the entire model from a relatively large number of aligned 3D meshes of different people in different poses. We quantitatively evaluate variants of SMPL using linear or dual-quaternion blend skinning and show that both are more accurate than a Blend- SCAPE model trained on the same data. We also extend SMPL to realistically model dynamic soft-tissue deformations. Because it is based on blend skinning, SMPL is compatible with existing rendering engines and we make it available for research purposes.},
booktitle = {Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
articleno = {88},
numpages = {16}
}

@inbook{10.1145/3563659.3563668,
author = {Hartholt, Arno and Mozgai, Sharon},
title = {Platforms and Tools for SIA Research and Development},
year = {2022},
isbn = {9781450398961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3563659.3563668},
booktitle = {The Handbook on Socially Interactive Agents: 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 2: Interactivity, Platforms, Application},
pages = {261–304},
numpages = {44}
}

@inproceedings{10.1145/3568160.3570230,
author = {Stauner, Thomas and Blank, Frederik and F\"{u}rst, Michael and G\"{u}nther, Johannes and Hagn, Korbinian and Heidenreich, Philipp and Huber, Markus and Knerr, Bastian and Schulik, Thomas and Lei\ss{}, Karl-Ferdinand},
title = {SynPeDS: A Synthetic Dataset for Pedestrian Detection in&nbsp;Urban&nbsp;Traffic&nbsp;Scenes},
year = {2022},
isbn = {9781450397865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568160.3570230},
doi = {10.1145/3568160.3570230},
abstract = {We introduce the Synthetic Pedestrian Dataset (SynPeDS) which was designed to support a systematic safety analysis for pedestrian detection tasks in urban scenes. The dataset was generated synthetically with a real-time and a physically-based rendering pipeline and provides camera frames and in part associated LiDAR point clouds. It contains ground truth for semantic segmentation, instance segmentation, 2D and 3D bounding boxes, and in part, pose information and bodypart segmentation. In particular, it comes with a large amount of meta information for in-depth performance and safety analysis, e.g. addressing semantic properties of the pedestrians and their environment in the frames. Some scenarios were specifically designed to systematically cover certain safety-relevant or performance-reducing dimensions of the input space, defined in project KI&nbsp;Absicherung. The dataset does not claim to be complete or free of bias, but to support coverage and data distribution studies.},
booktitle = {Proceedings of the 6th ACM Computer Science in Cars Symposium},
articleno = {2},
numpages = {10},
keywords = {Automated driving, Pedestrian detection},
location = {Ingolstadt, Germany},
series = {CSCS '22}
}

@inproceedings{10.1145/3613904.3642658,
author = {Kalus, Alexander and Klein, Johannes and Ho, Tien-Julian and Seegets, Lee-Ann and Henze, Niels},
title = {MobileGravity: Mobile Simulation of a High Range of Weight in Virtual Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642658},
doi = {10.1145/3613904.3642658},
abstract = {Simulating accurate weight forces in Virtual Reality (VR) is an unsolved challenge. Therefore, providing real weight sensations by transferring liquid mass has emerged as a promising approach. However, key objectives conceptually interfere with each other. In particular, previous designs that support a high range of weight or high flow rate lack mobility. In this work, we present MobileGravity, a system, that decouples the weight-changing object from the liquid supply and the pump. It enables weight changes of up to 1 kg at a rate of 235 g/s and allows the user to walk around freely. Through a study with 30 participants, we show that the system enables users to perceive the weight of different virtual objects and enhances realism, as well as enjoyment.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {547},
numpages = {13},
keywords = {haptics, virtual reality, weight perception, weight simulation},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.5555/3606388.3606392,
author = {Roy, Tania},
title = {Introducing Virtual Reality to Undergraduate Students: A Hybrid Approach},
year = {2023},
issue_date = {April 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {6},
issn = {1937-4771},
abstract = {This paper documents an experience report on the design and implementation of an undergraduate course titled 'Introduction To Virtual Reality Systems' taught in two iterations: Spring 2021 and 2022. The paper describes the unique COVID-19 classroom restrictions and limited access to limited hardware. This elective course was aimed at second, third and fourth year students who had taken at least one programming course. The course focuses on the human-centered approach of designing VR experiences with emphasis on game design, software development, and social elements. The paper elaborates on course setup, teaching modalities, course content, sample assignments, and evaluation.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {28–39},
numpages = {12}
}

@inproceedings{10.1145/3657547.3657554,
author = {Alva, Yashwith D and Janawade, Namrata Shrikant and Jain, Avani and Bhasu, Nimish and V, Sarasvathi},
title = {Augmented Reality Visualization of Menu Items using Image Reconstruction},
year = {2024},
isbn = {9798400709012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657547.3657554},
doi = {10.1145/3657547.3657554},
abstract = {In today’s fast-paced world, the abundance of dining options often overwhelms customers, making decision-making challenging. Traditional menus have 2D images that lack the depth and immersion needed for an accurate visual representation of each dish. Our ground-breaking approach utilizes state-of-the-art technology, converting restaurant-uploaded 2D images into 3D models. Through our mobile application and a QR code on the table, customers can visualize dishes, empowering them to make informed decisions before ordering. As a result, this application which is developed using Unity 3D, AliceVision and AR frameworks, transforms the dining experience. Our technology improves overall customer pleasure and engagement with the eating process by providing customers with comprehensive insights on meals, therefore transforming the way customers interact with restaurant menus.},
booktitle = {Proceedings of the 2024 8th International Conference on Virtual and Augmented Reality Simulations},
pages = {35–42},
numpages = {8},
keywords = {3D Image Reconstruction, AliceVision, Augmented Reality, Photogrammetry, Unity3D, Visualization},
location = {Melbourne, Australia},
series = {ICVARS '24}
}

@inproceedings{10.1145/3656650.3656741,
author = {Andrao, Margherita and Gini, Federica and Frageri, Davide and Bucchiarone, Antonio and Cappelletti, Alessandro and Treccani, Barbara and Zancanaro, Massimo},
title = {RuleCraft: an End-User Development Hub for Education},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656741},
doi = {10.1145/3656650.3656741},
abstract = {Personalized learning methods have revolutionized education by allowing tailored teaching approaches to meet individual students’ needs. This approach, supported by educational technologies, aims to engage learners with diverse expertise levels by adapting content and methods accordingly. This demo introduces an End-User Development system that empowers teachers to create and define interactive behaviors of educational tools using Trigger-Action Programming (TAP). The system facilitates the creation of "vocabularies" specific to each learning subject and translates them into verbal primitives for trigger-action rule definition. These rules are then used in the customization of interfaces. This demo presents examples using a tangible device and a web-based educational game aimed to enrich education activities in elementary schools. Future directions include studying teachers’ appropriation of the use of TAP to customize learning material as well as adaptation in new domains and with different devices.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {105},
numpages = {3},
keywords = {End-User Development, Internet of Things, Phygital, Tailored Gamification, Tangible},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@inproceedings{10.1145/3613905.3651112,
author = {Zhao, Yijun and Pan, Jiangyu and Dong, Yan and Dong, Tianshu and Wang, Guanyun and Ying, Fangtian and Shen, Qihang and Cao, Jiacheng},
title = {Language Urban Odyssey: A Serious Game for Enhancing Second Language Acquisition through Large Language Models},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651112},
doi = {10.1145/3613905.3651112},
abstract = {Traditional second language acquisition (SLA) often lacks deep immersion in authentic environments, presenting high learning and resource challenges. To overcome this, we introduced "Language Urban Odyssey" (LUO), a serious game designed to offer an affordable language practice environment. LUO combines Large Language Models (LLMs) with game-based learning, creating an immersive and interactive experience. Players interact with AI-driven characters in a fictional city, leveraging ChatGPT 3.5’s capabilities for simulating real language use and cultural diversity. The game aims to reduce language learning barriers, ignite interest, and provide practical scenarios. Test results show LUO significantly boosts interest and proficiency in language learning. Players praise its engaging narrative, interactive dialogues, and adaptive experience. However, while LUO is beneficial, it’s crucial to recognize gamified learning’s limits; genuine language fluency still requires real-life communication practice and validation.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {219},
numpages = {7},
keywords = {Educational Games, Human-AI Interaction, Large Language Models, Second Language Acquisition},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3604479.3604508,
author = {Souza, Alyson Matheus de Carvalho and Arruda, Deborah Dantas and Sarmento, Carlos Eduardo and Bessa, Olavo F M and Ramos, Everardo A and Renn\'{o}-Costa, C\'{e}sar},
title = {Deinonychus and The Cow – Enhancing Museum Exhibitions Through the Use of Mixed Reality},
year = {2024},
isbn = {9798400700026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604479.3604508},
doi = {10.1145/3604479.3604508},
abstract = {The use of technology inside museums is a topic of growing interest. Technology-driven installations not only create new experiences for the regular visitors but also may be enough to attract the interest of new ones. When those experiences happen outside of the museum walls, they can be even stronger in delivering the museum’s message to a new audience. Based on that, the Deinonychus experience and The Cow were created. These Mixed Reality (MR) experiences were first installed in a temporary facility of the C\^{a}mara Cascudo Museum during a Science, Culture, and Technology fair at the Federal University of Rio Grande do Norte’s central campus. This paper describes the development, deployment, and testing of those installations in a real environment, with considerations for those interested in creating something similar. Finally, the paper discusses the solutions used and how they could be replicated for museums around the globe.},
booktitle = {Proceedings of the 24th Symposium on Virtual and Augmented Reality},
pages = {47–54},
numpages = {8},
keywords = {museum experience, virtual environments, virtual reality},
location = {Natal, RN, Brazil},
series = {SVR '22}
}

@inproceedings{10.1145/3660043.3660163,
author = {Xu, Jie and Zhu, Jingwen and Gan, Jinqiang and Li, Changping},
title = {Development and Application of Virtual Simulation Teaching System for Mechanical Disassembly Based on Unity3D},
year = {2024},
isbn = {9798400716157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660043.3660163},
doi = {10.1145/3660043.3660163},
abstract = {The disassembly and assembly process of reducer, engine and other mechanisms, as well as the display of parts' working process, are important components in emerging engineering education, especially for some basic courses such as “Mechanical Design” and “Principle of Machinery”. To solve the problems of the difficulty of disassembly and assembly of mechanisms, high risk factors of experiments, high cost of experimental equipment and poor teaching effect, a virtual simulation experimental platform was developed based on unity3D. The experimental platform is a fully functional and interaction-friendly experimental platform integrates teaching, experiment, curriculum design and after-class review. This platform includes built-in virtual simulation models such as decelerator, and supports the import and editing of external model resources, which fully meets students' diversified needs of cognizing parts and structure. Through the practical application of this system, students have gained a deeper understanding of the disassembly process of mechanisms and the relationships between components. This virtual simulation experimentation platform effectively enhances students’ enthusiasm for learning, engagement, and learning efficiency."},
booktitle = {Proceedings of the 2023 International Conference on Information Education and Artificial Intelligence},
pages = {669–677},
numpages = {9},
location = {Xiamen, China},
series = {ICIEAI '23}
}

@proceedings{10.1145/3604479,
title = {SVR '22: Proceedings of the 24th Symposium on Virtual and Augmented Reality},
year = {2022},
isbn = {9798400700026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Natal, RN, Brazil}
}

@inproceedings{10.1145/3649921.3659840,
author = {Normoyle, Aline and J\"{o}rg, Sophie and Hill, Jennifer},
title = {The Curation Tree: A Lightweight Behavior Tree Framework for Implementing Puzzle and Narrative Games},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3659840},
doi = {10.1145/3649921.3659840},
abstract = {In this work, we describe the Curation Tree framework that has matured as part of our game-based research and teaching. We use this framework to manage the top-level experience of our players via a text-based, authorable behavior tree system that is easy to implement, test, and re-use. We present three case studies based on Unity games built within our framework: the first involves virtual reality games involving grasping objects with the hands; the second, Treatment X, uses the framework to implement a causal learning game; and the last involves narrative games based on a large language model. Based on our experiences with this system, we reflect on its limitations and make several recommendations based on its strengths, namely to centralize game logic with behaviors, to centralize user event handling, to prefer simple behaviors over complex ones, to use text-based scripts for authoring with version control, and to separate behaviors from persistent state.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {63},
numpages = {4},
keywords = {behavior trees, software engineering, video game development},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@inproceedings{10.1145/3625468.3647625,
author = {Xu, Xiaokun and Claypool, Mark},
title = {User Study-based Models of Game Player Quality of Experience with Frame Display Time Variation},
year = {2024},
isbn = {9798400704123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625468.3647625},
doi = {10.1145/3625468.3647625},
abstract = {Computer games are often rendered with inconsistent frame timing (frame jitter), particularly in cloud-based game streaming where frames traverse network bottlenecks before being rendered. While previous studies have helped understand the Quality of Experience (QoE) with frame jitter, derived models have tended to be limited in their prediction ability for conditions not yet tested. This paper combines results from four different user studies that assess QoE based on frame jitter, the studies differing in games, game systems, and methods of induced frame time variation. Analysis of the results shows the degree to which frame jitter degrades QoE, and that playout interruption sizes matter while interrupt frequencies do not. The rich user study-based data set provides the basis for models for predicting game player QoE with frame jitter - models which should be predictive for both cloud-based game streaming and traditional games, and for a wide range of player actions and game genres.},
booktitle = {Proceedings of the 15th ACM Multimedia Systems Conference},
pages = {210–220},
numpages = {11},
location = {Bari, Italy},
series = {MMSys '24}
}

@inproceedings{10.1145/3670653.3677480,
author = {Schenkluhn, Marius and Schulz, Thimo and Weinhardt, Christof},
title = {Xperisight: Parallelizing Extended Reality Studies Without Losing Control},
year = {2024},
isbn = {9798400709982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670653.3677480},
doi = {10.1145/3670653.3677480},
abstract = {We present Xperisight, an open-source tool for Extended Reality (XR) experiment supervision. Conducting experiments with XR devices already poses many technological and organizational challenges. However, scaling an experiment to larger numbers of participants is an even more complex endeavor. Thus, Xperisight provides remote access to Unity-based XR applications to oversee multiple sessions in parallel via one unified dashboard. Without influencing subjects by their presence, experimenters can access relevant information, remotely control the devices, and be called for help if questions or errors arise. The tool can be easily integrated with zero-code setup and minimal configuration and has already been successfully applied in a first experiment effectively halving the overall required experiment time.1},
booktitle = {Proceedings of Mensch Und Computer 2024},
pages = {485–490},
numpages = {6},
keywords = {Empirical Studies, Extended Reality, Laboratory Experiments, Software Tools, Virtual Reality},
location = {Karlsruhe, Germany},
series = {MuC '24}
}

@inproceedings{10.1145/3656650.3656669,
author = {Geurts, Eva and Warson, Dieter and Rovelo Ruiz, Gustavo},
title = {Boosting Motivation in Sports with Data-Driven Visualizations in VR},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656669},
doi = {10.1145/3656650.3656669},
abstract = {In recent years, the integration of Artificial Intelligence (AI) has sparked revolutionary progress across diverse domains, with sports applications being no exception. At the same time, using real-world data sources, such as GPS, weather, and traffic data, offers opportunities to improve the overall user engagement and effectiveness of such applications. Despite the substantial advancements, including proven success in mobile applications, there remains an untapped potential in leveraging these technologies to boost motivation and enhance social group dynamics in Virtual Reality (VR) sports solutions. Our innovative approach focuses on harnessing the power of AI and real-world data to facilitate the design of such VR systems. To validate our methodology, we conducted an exploratory study involving 18 participants, evaluating our approach within the context of indoor VR cycling. By incorporating GPX files and omnidirectional video (real-world data), we recreated a lifelike cycling environment in which users can compete with simulated cyclists navigating a chosen (real-world) route. Considering the user’s performance and interactions with other cyclists, our system employs AI-driven natural language processing tools to generate encouraging and competitive messages automatically. The outcome of our study reveals a positive impact on motivation, competition dynamics, and the perceived sense of group dynamics when using real performance data alongside automatically generated motivational messages. This underscores the potential of AI-driven enhancements in user interfaces to not only optimize performance but also foster a more engaging and supportive sports environment.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {20},
numpages = {5},
keywords = {Artificial Intelligence, Asynchronous social interaction, Cycling, Large Language Models, Real-world data, Sports, User Interface, Virtual Reality, Visualizations, eXtended Reality},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@inproceedings{10.1145/3524494.3527627,
author = {Politowski, Cristiano and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Petrillo, Fabio},
title = {Towards automated video game testing: still a long way to go},
year = {2022},
isbn = {9781450392938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524494.3527627},
doi = {10.1145/3524494.3527627},
abstract = {As the complexity and scope of game development increases, playtesting remains an essential activity to ensure the quality of video games. Yet, the manual, ad-hoc nature of playtesting gives space to improvements in the process. In this study, we investigate gaps between academic solutions in the literature for automated video game testing and the needs of video game developers in the industry. We performed a literature review on video game automated testing and applied an online survey with video game developers. The literature results show a rise in research topics related to automated video game testing. The survey results show that game developers are skeptical about using automated agents to test games. We conclude that there is a need for new testing approaches that did not disrupt the developer workflow. As for the researchers, the focus should be on the testing goal and testing oracle.},
booktitle = {Proceedings of the 6th International ICSE Workshop on Games and Software Engineering: Engineering Fun, Inspiration, and Motivation},
pages = {37–43},
numpages = {7},
keywords = {automation, testing, video-game},
location = {Pittsburgh, Pennsylvania},
series = {GAS '22}
}

@inproceedings{10.5555/3635637.3662921,
author = {Goel, Shivam and Wei, Yichen and Lymperopoulos, Panagiotis and Chur\'{a}, Kl\'{a}ra and Scheutz, Matthias and Sinapov, Jivko},
title = {NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents Designed for Open Worlds},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As AI agents leave the lab and venture into the real world as autonomous vehicles, delivery robots, and cooking robots, it is increasingly necessary to design and comprehensively evaluate algorithms that tackle the "open-world''. To this end, we introduce NovelGym, a flexible and adaptable ecosystem designed to simulate gridworld environments, serving as a robust platform for benchmarking reinforcement learning (RL) and hybrid planning and learning agents in open-world contexts. The modular architecture of NovelGym facilitates rapid creation and modification of task environments, including multi-agent scenarios, with multiple environment transformations, thus providing a dynamic testbed for researchers to develop open-world AI agents.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {688–696},
numpages = {9},
keywords = {benchmarking environments, neurosymbolic learning, open world learning},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{10.1145/3581961.3609889,
author = {Gomaa, Amr and Zitt, Robin and Reyes, Guillermo},
title = {Advancing Dynamic Hand Gesture Recognition in Driving Scenarios with Synthetic Data},
year = {2023},
isbn = {9798400701122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581961.3609889},
doi = {10.1145/3581961.3609889},
abstract = {Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. Additionally, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures1, enhances gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in dataset creation, our tool accelerates the development of gesture recognition systems for automotive applications.},
booktitle = {Adjunct Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {1–6},
numpages = {6},
keywords = {Data Augmentation, Deep Learning, Gesture Recognition, Personalization, Synthetic data},
location = {Ingolstadt, Germany},
series = {AutomotiveUI '23 Adjunct}
}

@inproceedings{10.1145/3613904.3641915,
author = {Lawley, Lane and Maclellan, Christopher},
title = {VAL: Interactive Task Learning with GPT Dialog Parsing},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641915},
doi = {10.1145/3613904.3641915},
abstract = {Machine learning often requires millions of examples to produce static, black-box models. In contrast, interactive task learning (ITL) emphasizes incremental knowledge acquisition from limited instruction provided by humans in modalities such as natural language. However, ITL systems often suffer from brittle, error-prone language parsing, which limits their usability. Large language models (LLMs) are resistant to brittleness but are not interpretable and cannot learn incrementally. We present VAL, an ITL system with a new philosophy for LLM/symbolic integration. By using LLMs only for specific tasks—such as predicate and argument selection—within an algorithmic framework, VAL reaps the benefits of LLMs to support interactive learning of hierarchical task knowledge from natural language. Acquired knowledge is human interpretable and generalizes to support execution of novel tasks without additional training. We studied users’ interactions with VAL in a video game setting, finding that most users could successfully teach VAL using language they felt was natural.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {5},
numpages = {18},
keywords = {GPT, hierarchical task networks, hybrid AI, large language models (LLMs), neuro-symbolic AI},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3611314.3615919,
author = {Ranosz, Jeremi and Leszczy\'{n}ski, Cyryl and Kumor, Stanis\l{}aw and Popiel, Agnieszka and G\l{}owaczewska, Julia and Garwol, Patryk and Kaczmarek, Mateusz and Maik, Miko\l{}aj},
title = {Advancing STEM Education in Primary Schools with an Integrated System of 3D Games},
year = {2023},
isbn = {9798400703249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611314.3615919},
doi = {10.1145/3611314.3615919},
abstract = {This paper introduces a method to enhance STEM education in primary schools by integrating 3D educational games with traditional teaching tools. Our approach combines two fundamental aspects: technical, which incorporates multi-platform educational games of various genres, and social, fostering interactions between teachers, students, and software developers. This model encourages adaptive learning tailored to student needs and promotes continuous student engagement. By simplifying complex STEM concepts through gamification, our method holds promising potential to improve the quality and effectiveness of STEM education in primary schools.},
booktitle = {Proceedings of the 28th International ACM Conference on 3D Web Technology},
articleno = {20},
numpages = {5},
keywords = {3D Games, Education, Gamification, STEM},
location = {San Sebastian, Spain},
series = {Web3D '23}
}

@inproceedings{10.1145/3674746.3674760,
author = {Xu, Xiaoying and Zhang, Huayu and Zeng, Hong and Song, Aiguo and Xing, Yifei and Zhang, Jingtian},
title = {Combination of Augmented Reality Based Brain-Computer Interface and EyeTracking for Control of a Multi-Robot System},
year = {2024},
isbn = {9798400716782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674746.3674760},
doi = {10.1145/3674746.3674760},
abstract = {Even as autonomous multi-robot systems (MRS) advance rapidly, current unmanned clusters struggle to complete many real-world robotic tasks relying solely on autonomous intelligence. Therefore, the human involvement is urgently required for MRS to effectively accomplish their missions in practical applications, and the hands-on control methods have been frequently employed for the operators to input MRS control commands. Whereas, in more complex environment where the tasks require both of the operator’s hands to be engaged, the hands-on approach is insufficient to achieve dependable human-MRS interaction. Therefore, this study represents an initial exploration into a hands-free control interface utilizing augmented reality (AR) and brain-computer interface (BCI) known as an AR-based FRP-BCI, an extension of the hands-on control approach. Specifically, the operator fixate on the target command menu displayed by Microsoft HoloLens2, the control command selection function is achieved with a more natural AR-based FRP-BCI via recognizing EEG signals. During the online experiment of the AR-based FRP-BCI command selection task with the MRS, the success rate of recognization is 90.67 ± 6.80 %. The proposed AR-based FRP-BCI system enables the users to select intention targets more natural in specific real-world environments. The results highlight the capability of the hands-free AR-based FRP-BCI to enhance traditional manual MRS input devices and the establishment of a more natural interface.},
booktitle = {Proceedings of the 2024 4th International Conference on Robotics and Control Engineering},
pages = {90–95},
numpages = {6},
keywords = {Eye tracking, augmented reality, brain-computer interface, electroencephalograph, fixation related potential, multi-robot system},
location = {Edinburgh, United Kingdom},
series = {RobCE '24}
}

@inproceedings{10.1145/3582177.3582189,
author = {Cao, Yang and Gao, Yuan and Geng, Ang},
title = {Case study of 3D Interactive Design of Nanjing Tongji Gate Wall},
year = {2023},
isbn = {9781450397926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582177.3582189},
doi = {10.1145/3582177.3582189},
abstract = {Through the case study of 3D interactive design of Nanjing Tongji Gate Wall, this paper explores the application of 3D interactive design in relic visual reconstruction. This paper collects literature on Nanjing City Wall and conducts field research and interviews about Tongji Gate. On the basis of relevant data and literature, 3D interactive technology is used to design the interactive animation system of Tongji Gate. 3D interactive animation contributes to the visual reconstruction of material cultural heritage, the improvement of traditional cultural relics protection and research, the adaptation to communication and promotion in the new media era, the protection of cultural heritage, and the continuous cultural inheritance.},
booktitle = {Proceedings of the 2023 5th International Conference on Image Processing and Machine Vision},
pages = {66–72},
numpages = {7},
keywords = {3D animation, Digitization, Historic and cultural heritage, Meridian Gate, Nanjing City Wall},
location = {Macau, China},
series = {IPMV '23}
}

@inproceedings{10.1145/3635636.3656210,
author = {Juarez, Aaron J},
title = {Sampling the Glitch Spectrum: Exploring the Architecture of Digital Media},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3656210},
doi = {10.1145/3635636.3656210},
abstract = {An empirical record of glitch practice is portrayed in this pictorial. In wandering through the information environments of digital files, researchers can discover unexpected artifacts through experimental interactions. The production of these images span about seven years and range across various digital media, such as photography, video footage, 3D renders, and gaming. This work expands the concept of organic architecture to the realm of the digital medium. Inspired by nature, analogies from physical phenomena and scientific principles are used to inform the research-creation. In doing so, glitch experiments in digital files enable a cocreativity with technology which reveal their underlying structures and thereby produce a spectrum of expression. Often, uncanny resemblance to natural terrains manifest, indicating a connection through possibilities of configuration, or entropy.},
booktitle = {Proceedings of the 16th Conference on Creativity &amp; Cognition},
pages = {255–267},
numpages = {13},
location = {Chicago, IL, USA},
series = {C&amp;C '24}
}

@inproceedings{10.1145/3603421.3603422,
author = {Graichen, Lisa and Graichen, Matthias and Nudzor, Jacko},
title = {Integrating Mid-Air Gestures into an Extended Reality Application: A Methodological Approach},
year = {2023},
isbn = {9781450397469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603421.3603422},
doi = {10.1145/3603421.3603422},
abstract = {Virtual reality (VR) is increasingly being incorporated into people's everyday lives as well as professional applications, science and research. Therefore, the question arises whether there may be interaction modes that are more suitable than the standard controllers with which these devices are typically equipped. To address this question, it is necessary to conduct human-centered studies, which are typically applied in the field of human factors research. To facilitate similar research approaches we present a framework to integrating mid-air gesture interaction into VR so that users can perform gestures without touching a button, a surface or a device into an HTC Vive VR Headset. For gesture recognition, we use a Leap Motion device, which is mounted at the front of the HTC Vive. We built a basic application using virtual 3D blocks that users can rotate and move. We used Unity to implement this application. This paper describes our methodological approach to building the setup.},
booktitle = {Proceedings of the 2023 7th International Conference on Virtual and Augmented Reality Simulations},
pages = {1–6},
numpages = {6},
location = {Sydney, Australia},
series = {ICVARS '23}
}

@inproceedings{10.1145/3514197.3549671,
author = {Hartholt, Arno and Fast, Ed and Li, Zongjian and Kim, Kevin and Leeds, Andrew and Mozgai, Sharon},
title = {Re-architecting the virtual human toolkit: towards an interoperable platform for embodied conversational agent research and development},
year = {2022},
isbn = {9781450392488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514197.3549671},
doi = {10.1145/3514197.3549671},
abstract = {The research and development (R&amp;D) of intelligent virtual agents (IVAs) is inherently complex. We aim to manage this complexity by combining the best aspects of academic and commercial approaches into a principled R&amp;D platform that emphasizes interoperability, ex-tendability, re-use, and support for multiple hardware targets. This IVA platform, the Virtual Human Toolkit 2.0, is a re-architecture of our earlier work and combines a modular message passing architecture with that of a microservices architecture. This paper discusses our approach, design decisions, lessons learned, and current status of this ongoing effort. We illustrate the strengths of the architecture, how best to use commodity AI cloud services in one's own work, and how to port legacy stand-alone software to a web service.},
booktitle = {Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents},
articleno = {16},
numpages = {8},
keywords = {digital humans, embodied conversational agents, intelligent virtual agents, system architectures, toolkits, virtual humans},
location = {Faro, Portugal},
series = {IVA '22}
}

@inproceedings{10.1145/3641519.3657454,
author = {Martins, Marcelo and Morais, Lucas and Torchelsen, Rafael and Nedel, Luciana and Maciel, Anderson},
title = {Efficient Position-Based Deformable Colon Modeling for Endoscopic Procedures Simulation},
year = {2024},
isbn = {9798400705250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641519.3657454},
doi = {10.1145/3641519.3657454},
abstract = {Current endoscopy simulators oversimplify navigation and interaction within tubular anatomical structures to maintain interactive frame rates, neglecting the intricate dynamics of permanent contact between the organ and the medical tool. Traditional algorithms fail to represent the complexities of long, slender, deformable tools like endoscopes and hollow organs, such as the human colon, and their interaction. In this paper, we address longstanding challenges hindering the realism of surgery simulators, explicitly focusing on these structures. One of the main components we introduce is a new model for the overall shape of the organ, which is challenging to retain due to the complex surroundings inside the abdomen. Our approach uses eXtended Position-Based Dynamics (XPBD) with a Cosserat rod constraint combined with a mesh of tetrahedrons to retain the colon’s shape. We also introduce a novel contact detection algorithm for tubular structures, allowing for real-time performance. This comprehensive representation captures global deformations and local features, significantly enhancing simulation fidelity compared to previous works. Results showcase that navigating the endoscope through our simulated colon seemingly mirrors real-world operations. Additionally, we use real-patient data to generate the colon model, resulting in a highly realistic virtual colonoscopy simulation. Integrating efficient simulation techniques with practical medical applications arguably advances surgery simulation realism.},
booktitle = {ACM SIGGRAPH 2024 Conference Papers},
articleno = {91},
numpages = {10},
keywords = {Cosserat rods, Interactive simulation, Position-based dynamics, Real-time collision detection, Surgery simulation},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3615452.3617941,
author = {Ganj, Ashkan and Zhao, Yiqin and Galbiati, Federico and Guo, Tian},
title = {Toward Scalable and Controllable AR Experimentation},
year = {2023},
isbn = {9798400703393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615452.3617941},
doi = {10.1145/3615452.3617941},
abstract = {To understand how well a proposed augmented reality (AR) solution works, existing papers often conducted tailored and isolated evaluations for specific AR tasks, e.g., depth or lighting estimation, and compared them to easy-to-setup baselines, either using datasets or resorting to time-consuming data capturing. Conceptually simple, it can be extremely difficult to evaluate an AR system fairly and in scale to understand its real-world performance. The difficulties arise for three key reasons: lack of control of the physical environment, the time-consuming data capturing, and the difficulties to reproduce baseline results.This paper presents our design of an AR experimentation platform, ExpAR, aiming to provide scalable and controllable AR experimentation. ExpAR is envisioned to operate as a standalone deployment or a federated platform; in the latter case, AR researchers can contribute physical resources, including scene setup and capturing devices, and allow others to time share these resources. Our design centers around the generic sensing-understanding-rendering pipeline and is driven by the evaluation limitations observed in recent AR systems papers. We demonstrate the feasibility of this vision with a preliminary prototype and our preliminary evaluations suggest the importance of further investigating different device capabilities to stream in 30 FPS.},
booktitle = {Proceedings of the 1st ACM Workshop on Mobile Immersive Computing, Networking, and Systems},
pages = {237–246},
numpages = {10},
keywords = {augmented reality, experimentation platform},
location = {Madrid, Spain},
series = {ImmerCom '23}
}

@inproceedings{10.1145/3625008.3625023,
author = {Berwaldt, Natan Luiz Paetzhold and Di Domenico, Gabriel and Pozzer, Cesar Tadeu},
title = {Virtual MultiView Panels for Distant Object Interaction and Navigation in Virtual Reality},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625023},
doi = {10.1145/3625008.3625023},
abstract = {In this paper we present a technique to improve interaction and navigation in virtual reality through the use of multiple virtual cameras projecting their view to windows around the user, expanding the through-the-lens metaphor to a virtual-drone metaphor, and making it possible for users to monitor, and navigate and interact with multiple occluded locations simultaneously using these windows. With this technique, users are able to reach anywhere in the virtual environment in an easy and fast way, without causing discomfort or disorientation. We evaluate the effectiveness of the proposed technique through a user study with two different tasks and assess the task completion performance, user behavior, and preference compared to other known methods.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {88–95},
numpages = {8},
keywords = {Interaction Technique, Large Environment, Virtual Reality, Visualization},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@inproceedings{10.1145/3649921.3649943,
author = {Earle, Sam and Kokkinos, Filippos and Nie, Yuhe and Togelius, Julian and Raileanu, Roberta},
title = {DreamCraft: Text-Guided Generation of Functional 3D Environments in Minecraft},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3649943},
doi = {10.1145/3649921.3649943},
abstract = {Procedural Content Generation (PCG) algorithms enable the automatic generation of complex and diverse artifacts. However, they don’t provide high-level control over the generated content and typically require domain expertise. In contrast, text-to-3D methods allow users to specify desired characteristics in natural language, offering a high amount of flexibility and expressivity. But unlike PCG, such approaches cannot guarantee functionality, which is crucial for certain applications like game design. In this paper, we present a method for generating functional 3D artifacts from free-form text prompts in the open-world game Minecraft. Our method, DreamCraft, trains quantized Neural Radiance Fields (NeRFs) to represent artifacts that, when viewed in-game, match given text descriptions. We find that DreamCraft produces more aligned in-game artifacts than a baseline that post-processes the output of an unconstrained NeRF. Thanks to the quantized representation of the environment, functional constraints can be integrated using specialized loss terms. We show how this can be leveraged to generate 3D structures that match a target distribution or obey certain adjacency rules over the block types. DreamCraft inherits a high degree of expressivity and controllability from the NeRF, while still being able to incorporate functional constraints through domain-specific objectives.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {17},
numpages = {15},
keywords = {Minecraft, Neural Radiance Fields, Procedural Content Generation},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@proceedings{10.1145/3641517,
title = {SIGGRAPH '24: ACM SIGGRAPH 2024 Emerging Technologies},
year = {2024},
isbn = {9798400705243},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@inproceedings{10.1145/3613905.3650730,
author = {Chheang, Vuthea and Weston, Brian Thomas and Cerda, Robert William and Au, Brian and Giera, Brian and Bremer, Peer-Timo and Miao, Haichao},
title = {A Virtual Environment for Collaborative Inspection in Additive Manufacturing},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650730},
doi = {10.1145/3613905.3650730},
abstract = {Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries. A defining feature for many AM parts is the complex internal geometry enabled by the printing process. However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces. Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team. In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts. Geographically separated experts can virtually inspect and jointly discuss data. It also supports VR and non-VR users, who can be spectators in the VR environment. Various features for data exploration and inspection are developed and enhanced via real-time synchronization. We followed usability and interface verification guidelines using Nielsen’s heuristics approach. Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback. Results reveal potential benefits, applicability, and current limitations. The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {26},
numpages = {7},
keywords = {Additive Manufacturing, Collaborative VR, Digital Twins, Virtual Inspection, Virtual Reality},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3555858.3555896,
author = {Partlan, Nathan and Soto, Luis and Howe, Jim and Shrivastava, Sarthak and Seif El-Nasr, Magy and Marsella, Stacy},
title = {EvolvingBehavior: Towards Co-Creative Evolution of Behavior Trees for Game NPCs},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555896},
doi = {10.1145/3555858.3555896},
abstract = {To assist game developers in crafting game NPCs, we present EvolvingBehavior, a novel tool for genetic programming to evolve behavior trees in Unreal®Engine 4. In an initial evaluation, we compare evolved behavior to hand-crafted trees designed by our researchers, and to randomly-grown trees, in a 3D survival game. We find that EvolvingBehavior is capable of producing behavior approaching the designer’s goals in this context. Finally, we discuss implications and future avenues of exploration for co-creative game AI design tools, as well as challenges and difficulties in behavior tree evolution.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {36},
numpages = {13},
keywords = {artificial intelligence, computational co-creativity, games, genetic programming},
location = {Athens, Greece},
series = {FDG '22}
}

@article{10.1145/3564241,
author = {Tursun, Cara and Didyk, Piotr},
title = {Perceptual Visibility Model for Temporal Contrast Changes in Periphery},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/3564241},
doi = {10.1145/3564241},
abstract = {Modeling perception is critical for many applications and developments in computer graphics to optimize and evaluate content generation techniques. Most of the work to date has focused on central (foveal) vision. However, this is insufficient for novel wide-field-of-view display devices, such as virtual and augmented reality headsets. Furthermore, the perceptual models proposed for the fovea do not readily extend to the off-center, peripheral visual field, where human perception is drastically different. In this article, we focus on modeling the temporal aspect of visual perception in the periphery. We present new psychophysical experiments that measure the sensitivity of human observers to different spatio-temporal stimuli across a wide field of view. We use the collected data to build a perceptual model for the visibility of temporal changes at different eccentricities in complex video content. Finally, we discuss, demonstrate, and evaluate several problems that can be addressed using our technique. First, we show how our model enables injecting new content into the periphery without distracting the viewer, and we discuss the link between the model and human attention. Second, we demonstrate how foveated rendering methods can be evaluated and optimized to limit the visibility of temporal aliasing.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {20},
numpages = {16},
keywords = {Temporal visibility metric, temporal change perception, spatio-temporal sensitivity, imperceptible visual change}
}

@article{10.1145/3630750,
author = {Careaga, Chris and Aksoy, Ya\u{g}\i{}z},
title = {Intrinsic Image Decomposition via Ordinal Shading},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3630750},
doi = {10.1145/3630750},
abstract = {Intrinsic decomposition is a fundamental mid-level vision problem that plays a crucial role in various inverse rendering and computational photography pipelines. Generating highly accurate intrinsic decompositions is an inherently under-constrained task that requires precisely estimating continuous-valued shading and albedo. In this work, we achieve high-resolution intrinsic decomposition by breaking the problem into two parts. First, we present a dense ordinal shading formulation using a shift- and scale-invariant loss in order to estimate ordinal shading cues without restricting the predictions to obey the intrinsic model. We then combine low- and high-resolution ordinal estimations using a second network to generate a shading estimate with both global coherency and local details. We encourage the model to learn an accurate decomposition by computing losses on the estimated shading as well as the albedo implied by the intrinsic model. We develop a straightforward method for generating dense pseudo ground truth using our model’s predictions and multi-illumination data, enabling generalization to in-the-wild imagery. We present exhaustive qualitative and quantitative analysis of our predicted intrinsic components against state-of-the-art methods. Finally, we demonstrate the real-world applicability of our estimations by performing otherwise difficult editing tasks such as recoloring and relighting.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {12},
numpages = {24},
keywords = {Intrinsic decomposition, inverse rendering, mid-level vision, shading and reflectance estimation, image manipulation}
}

@inproceedings{10.1145/3549555.3549573,
author = {Carrara, Fabio and Pasco, Lorenzo and Gennaro, Claudio and Falchi, Fabrizio},
title = {Learning to Detect Fallen People in Virtual Worlds},
year = {2022},
isbn = {9781450397209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549555.3549573},
doi = {10.1145/3549555.3549573},
abstract = {Falling is one of the most common causes of injury in all ages, especially in the elderly, where it is more frequent and severe. For this reason, a tool that can detect a fall in real time can be helpful in ensuring appropriate intervention and avoiding more serious damage. Some approaches available in the literature use sensors, wearable devices, or cameras with special features such as thermal or depth sensors. In this paper, we propose a Computer Vision deep-learning based approach for human fall detection based on largely available standard RGB cameras. A typical limitation of this kind of approaches is the lack of generalization to unseen environments. This is due to the error generated during human detection and, more generally, due to the unavailability of large-scale datasets that specialize in fall detection problems with different environments and fall types. In this work, we mitigate these limitations with a general-purpose object detector trained using a virtual world dataset in addition to real-world images. Through extensive experimental evaluation, we verified that by training our models on synthetic images as well, we were able to improve their ability to generalize. Code to reproduce results is available at https://github.com/lorepas/fallen-people-detection.},
booktitle = {Proceedings of the 19th International Conference on Content-Based Multimedia Indexing},
pages = {126–130},
numpages = {5},
keywords = {object detection, scarce data, virtual worlds for synthetic data, visual fallen people detection},
location = {Graz, Austria},
series = {CBMI '22}
}

@inproceedings{10.1145/3569951.3597576,
author = {Ameh, Edith and Bowen, Anne},
title = {Youth Homelessness: Researching VR as a Tool for Empathy},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569951.3597576},
doi = {10.1145/3569951.3597576},
booktitle = {Practice and Experience in Advanced Research Computing 2023: Computing for the Common Good},
pages = {433–436},
numpages = {4},
keywords = {data science, empathy, homeless, virtual reality},
location = {Portland, OR, USA},
series = {PEARC '23}
}

@inproceedings{10.1145/3544549.3582738,
author = {Rhodes, Chris and Allmendinger, Richard and Jay, Caroline and Climent, Ricardo},
title = {Towards Developing a Virtual Guitar Instructor through Biometrics Informed Human-Computer Interaction},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3582738},
doi = {10.1145/3544549.3582738},
abstract = {Within the last few years, wearable sensor technologies have allowed us to access novel biometrics that give us the ability to connect musical gesture to computing systems. Doing this affords us to study how we perform musically and understand the process at data level. However, biometric information is complex and cannot be directly mapped to digital systems. In this work, we study how guitar performance techniques can be captured/analysed towards developing an AI which can provide real-time feedback to guitar students. We do this by performing musical exercises on the guitar whilst acquiring and processing biometric (plus audiovisual) information during their performance. Our results show: there are notable differences within biometrics when playing a guitar scale in two different ways (legato and staccato) and this outcome can be used to motivate our intention to build an AI guitar tutor.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {425},
numpages = {8},
keywords = {Biometrics, Deep learning, EMG, Game engines, Guitar, HCI, Multimodal data, Musical performance},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@article{10.1145/3617946.3617955,
author = {Bucchiarone, Antonio and Cooper, Kendra M. L. and Lin, Dayi and Smith, Adam and Wanick, Vanissa},
title = {Fostering Collaboration and Advancing Research in Software Engineering and Game Development for Serious Contexts},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617955},
doi = {10.1145/3617946.3617955},
abstract = {The potential benefits of using the engaging and interactive nature of games to achieve specific objectives have been recognized by researchers and professionals from numerous domains. Serious games have been developed to impart knowledge, skills, and awareness in areas such as education, healthcare and the environment, while gamification has been applied to enhance the engagement, motivation, and participation of users in non-game activities such as sustainability and learning. As a result, the fields of game engineering, software engineering, and user experience are increasingly converging to create innovative solutions that blend the strengths of games with real-world applications.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {46–50},
numpages = {5}
}

@inproceedings{10.1145/3564533.3564561,
author = {Maik, Miko\l{}aj and Soboci\'{n}ski, Pawe\l{} and Walczak, Krzysztof and Struga\l{}a, Dominik and G\'{o}rski, Filip and Zawadzki, Przemys\l{}aw},
title = {Flexible Photorealistic VR Training System for Electrical Operators},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564533.3564561},
doi = {10.1145/3564533.3564561},
abstract = {Virtual reality can be a very effective tool for professional training, especially when training performed in reality would pose a high risk for the trainees. However, efficient use of VR in practical industrial training requires high-quality VR training environments - 3D scenes and interactive scenarios. Because creating such a VR environment is a complex and time-consuming task, existing VR training systems generally do not offer the flexibility that would allow trainers to adjust the environments to changing training contexts and requirements. In many cases, in industrial training, however, one can benefit from the componentization and repeatability of the training content to provide such flexibility. In this paper, we propose a method of creating photorealistic virtual reality training scenes with the use of a library of training objects. Designing training objects is a relatively complex process, but once a library of objects is available, a designer can assemble new training scenes from the objects available in the library. Training scenarios can be added using an easy-to-use graphical tool.},
booktitle = {Proceedings of the 27th International Conference on 3D Web Technology},
articleno = {5},
numpages = {9},
keywords = {Content creation, Industrial training, Photorealism, VR},
location = {Evry-Courcouronnes, France},
series = {Web3D '22}
}

@inproceedings{10.1145/3663529.3663813,
author = {Bittner, Paul Maximilian and Schulthei\ss{}, Alexander and Moosherr, Benjamin and Kehrer, Timo and Th\"{u}m, Thomas},
title = {Variability-Aware Differencing with DiffDetective},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663813},
doi = {10.1145/3663529.3663813},
abstract = {Diff tools are essential in developers' daily workflows and software engineering research. Motivated by limitations of traditional line-based differencing, countless specialized diff tools have been proposed, aware of the underlying artifacts' type, such as a program's syntax or semantics. However, no diff tool is aware of systematic variability embodied in highly configurable systems such as the Linux kernel. Our software library called DiffDetective can turn any generic diff tool into a variability-aware differencer such that a changes' impact on the source code and its superimposed variability can be distinguished and analyzed. Besides graphical diff inspectors, DiffDetective provides a framework for large-scale empirical analyses of version histories, tested on a substantial body of configurable software including the Linux kernel. DiffDetective has been successfully employed to explain edits, generate clone-and-own scenarios, or evaluate diff algorithms and patch mutations.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {632–636},
numpages = {5},
keywords = {software evolution, software product lines, software variability},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3581783.3611884,
author = {Jin, Yeying and Lin, Beibei and Yan, Wending and Yuan, Yuan and Ye, Wei and Tan, Robby T.},
title = {Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611884},
doi = {10.1145/3581783.3611884},
abstract = {Visibility in hazy nighttime scenes is frequently reduced by multiple factors, including low light, intense glow, light scattering, and the presence of multicolored light sources. Existing nighttime dehazing methods often struggle with handling glow or low-light conditions, resulting in either excessively dark visuals or unsuppressed glow outputs. In this paper, we enhance the visibility from a single nighttime haze image by suppressing glow and enhancing low-light regions. To handle glow effects, our framework learns from the rendered glow pairs. Specifically, a light source aware network is proposed to detect light sources of night images, followed by the APSF (Angular Point Spread Function)-guided glow rendering. Our framework is then trained on the rendered images, resulting in glow suppression. Moreover, we utilize gradient-adaptive convolution, to capture edges and textures in hazy scenes. By leveraging extracted edges and textures, we enhance the contrast of the scene without losing important structural details. To boost low-light intensity, our network learns an attention map, then adjusted by gamma correction. This attention has high values on low-light regions and low values on haze and glow regions. Extensive evaluation on real nighttime haze images, demonstrates the effectiveness of our method. Our experiments demonstrate that our method achieves a PSNR of 30.38dB, outperforming state-of-the-art methods by 13% on GTA5 nighttime haze dataset. Our data and code is available at: https://github.com/jinyeying/nighttime_dehaze.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {2446–2457},
numpages = {12},
keywords = {apsf, edge, glow, gradient, haze, low-light, nighttime, texture},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3613904.3642105,
author = {Qin, Hua Xuan and Jin, Shan and Gao, Ze and Fan, Mingming and Hui, Pan},
title = {CharacterMeet: Supporting Creative Writers' Entire Story Character Construction Processes Through Conversation with LLM-Powered Chatbot Avatars},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642105},
doi = {10.1145/3613904.3642105},
abstract = {Support for story character construction is as essential as characters are for stories. Building upon past research on early character construction stages, we explore how conversation with chatbot avatars embodying characters powered by more recent technologies could support the entire character construction process for creative writing. Through a user study (N=14) with creative writers, we examine thinking and usage patterns of CharacterMeet, a prototype system allowing writers to progressively manifest characters through conversation while customizing context, character appearance, voice, and background image. We discover that CharacterMeet facilitates iterative character construction. Specifically, participants, including those with more linear usual approaches, alternated between writing and personalized exploration through visualization of ideas on CharacterMeet while visuals and audio enhanced immersion. Our findings support research on iterative creative processes and the growing potential of personalizable generative AI creativity support tools. We present design implications for leveraging chatbot avatars in the creative writing process.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {1051},
numpages = {19},
keywords = {Creative Writing, Creativity Support, Human-AI Collaboration, Large Language Models, Writing Assistants},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3586182.3616690,
author = {Shi, Hongning and Li, Jiajia and Xue, Lian and Song, Yajing},
title = {OperAR: Using an Augmented Reality Agent to Enhance Children's Interactive Intangible Cultural Heritage Experience of the Peking Opera},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3616690},
doi = {10.1145/3586182.3616690},
abstract = {As a traditional Chinese theatrical performance, the Peking Opera is a world intangible cultural heritage. However, appreciating and understanding the traditional arts of Peking Opera requires rich life experience and knowledge about traditional Chinese culture, which has led to its lack of recognition and support from younger Chinese audiences. To expand the audience for the traditional Chinese cultural heritage of Peking Opera and stimulate young people’s interest in learning about this art form, we introduce “OperAR” as an augmented reality game system that teaches children about the history and culture of the Peking Opera and how to perform Peking Opera using a cartoon virtual agent and a set of physical cards based on Peking Opera costumes. The pilot study results show the potential benefits of OperAR in enhancing children’s interactive experiences in virtual scenarios in addition to stimulating their interest in learning about Peking Opera.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {3},
keywords = {Augmented reality, Children, Immersive opera, Interactive design, Virtual agent},
location = {San Francisco, CA, USA},
series = {UIST '23 Adjunct}
}

@inproceedings{10.1145/3526113.3545639,
author = {Mahadevan, Karthik and Chen, Yan and Cakmak, Maya and Tang, Anthony and Grossman, Tovi},
title = {Mimic: In-Situ Recording and Re-Use of Demonstrations to Support Robot Teleoperation},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545639},
doi = {10.1145/3526113.3545639},
abstract = {Remote teleoperation is an important robot control method when they cannot operate fully autonomously. Yet, teleoperation presents challenges to effective and full robot utilization: controls are cumbersome, inefficient, and the teleoperator needs to actively attend to the robot and its environment. Inspired by end-user programming, we propose a new interaction paradigm to support robot teleoperation for combinations of repetitive and complex movements. We introduce Mimic, a system that allows teleoperators to demonstrate and save robot trajectories as templates, and re-use them to execute the same action in new situations. Templates can be re-used through (1) macros—parametrized templates assigned to and activated by buttons on the controller, and (2) programs—sequences of parametrized templates that operate autonomously. A user study in a simulated environment showed that after initial set up time, participants completed manipulation tasks faster and more easily compared to traditional direct control.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {40},
numpages = {13},
keywords = {Robot teleoperation, end-user robot programming},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{10.1145/3565066.3608246,
author = {Zafeiropoulos, Vasilis and Anastassakis, George and Orphanoudakis, Theofanis and Kalles, Dimitris and Fanariotis, Anastasios and Fotopoulos, Vassilis},
title = {The V-Lab VR Educational Application Framework: A Beacon Application of the XR2Learn Project},
year = {2023},
isbn = {9781450399241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565066.3608246},
doi = {10.1145/3565066.3608246},
abstract = {This paper presents the V-Lab, a VR application development framework for educational scenarios mainly involving scientific processes executed in laboratory environments such as chemistry and biology laboratories. This work is an extension of the Onlabs simulator which has been developed by the Hellenic Open University as a distance teaching enabler for similar subjects, helping to alleviate the need for access to the physical laboratory infrastructure; thus, shortening training periods of students in the laboratory and making their training during the periods of physical presence more productive and secure. The extensions of the Onlabs to deliver an enhanced and modular framework that can be extended to multiple educational scenarios is the work performed within the context of the European project XR2Learn (Leveraging the European XR industry technologies to empower immersive learning and training).},
booktitle = {Proceedings of the 25th International Conference on Mobile Human-Computer Interaction},
articleno = {32},
numpages = {4},
keywords = {Simulated experiments, Virtual labs},
location = {Athens, Greece},
series = {MobileHCI '23 Companion}
}

@inproceedings{10.1145/3588015.3589203,
author = {Hosp, Benedikt W. and Wahl, Siegfried},
title = {ZERO: A Generic Open-Source Extended Reality Eye-Tracking Controller Interface for Scientists},
year = {2023},
isbn = {9798400701504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588015.3589203},
doi = {10.1145/3588015.3589203},
abstract = {Virtual reality and eye-tracking technologies are nowadays standard research tools. A growing number of researchers from different disciplines are utilizing these technologies. Currently, access to eye-tracking hardware in virtual reality glasses is usually provided as APIs to call functions of the eye-tracking device. Proper implementation is device-specific and left to the user. Especially non-computer scientists are left alone with this problem, which impedes eye-tracking research in virtual reality for many scientists. This paper describes a generic open-source interface for everyone to efficiently and easily utilize common eye trackers in virtual reality. The interface is published under a friendly CC BY 4.0 license that allows for integration, modification, and extension of the code. It includes a standardized interface for several eye-tracking devices in virtual reality, is ready to be used out of the box, and allows easy addition of APIs from other manufacturers. The code is available at https://bitbucket.org/benediktwhosp/zvsl-zero},
booktitle = {Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},
articleno = {64},
numpages = {4},
keywords = {datasets, gaze detection, neural networks, text tagging},
location = {Tubingen, Germany},
series = {ETRA '23}
}

@article{10.1145/3669936,
author = {Chen, Sarah},
title = {The Toxic Cost of Cheap Usernames: Re-Applying Friedman and Resnick's Framework in Competitive Video Game Networks},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3669936},
doi = {10.1145/3669936},
abstract = {Toxicity in video games, acting in a rude, abusive, bullying, or deliberately losing manner, ruins competitive team-based video game experiences for everyone involved. Companies are experimenting with the detection, regulation, and bans of toxic players. Regulation attempts are foiled by the ease with which players can switch accounts by creating new profiles to evade consequences.This article applies the framework of Friedman and Resnick (2001), “The Social Cost of Cheap Pseudonyms”, to address the reputational and repetition cost of online pseudonyms in the niche environment of video game toxicity. Four potential solutions are discussed in the context of modern video game regulation: resource-intensive account set-up, paid dues, real-world identification, and a once-in-a-lifetime identification system that creates a permanent, traceable record of toxicity. The article covers the strengths, weaknesses, feasibility, implementation challenges, and player impact of the potential implementation of these solutions in video game networks.},
journal = {ACM Games},
month = {aug},
articleno = {13},
numpages = {17},
keywords = {Video games, toxicity, game cooperation, usernames}
}

@inproceedings{10.1145/3641519.3657432,
author = {Jim\'{e}nez Navarro, Daniel and Peng, Xi and Zhang, Yunxiang and Myszkowski, Karol and Seidel, Hans-Peter and Sun, Qi and Serrano, Ana},
title = {Accelerating Saccadic Response through Spatial and Temporal Cross-Modal Misalignments},
year = {2024},
isbn = {9798400705250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641519.3657432},
doi = {10.1145/3641519.3657432},
abstract = {Human senses and perception are our mechanisms to explore the external world. In this context, visual saccades –rapid and coordinated eye movements– serve as a primary tool for awareness of our surroundings. Typically, our perception is not limited to visual stimuli alone but is enriched by cross-modal interactions, such as the combination of sight and hearing. In this work, we investigate the temporal and spatial relationship of these interactions, focusing on how auditory cues that precede visual stimuli influence saccadic latency –the time that it takes for the eyes to react and start moving towards a visual target. Our research, conducted within a virtual reality environment, reveals that auditory cues preceding visual information can significantly accelerate saccadic responses, but this effect plateaus beyond certain temporal thresholds. Additionally, while the spatial positioning of visual stimuli influences the speed of these eye movements, as reported in previous research, we find that the location of auditory cues with respect to their corresponding visual stimulus does not have a comparable effect. To validate our findings, we implement two practical applications: first, a basketball training task set in a more realistic environment with complex audiovisual signals, and second, an interactive farm game that explores previously untested values of our key factors. Lastly, we discuss various potential applications where our model could be beneficial.},
booktitle = {ACM SIGGRAPH 2024 Conference Papers},
articleno = {129},
numpages = {12},
keywords = {Audiovisual integration, cross-modal interactions, multisensory perception, saccadic latency, virtual reality},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@article{10.1145/3675784,
author = {Tsoi, Nathan and Sterneck, Rachel and Zhao, Xuan and V\'{a}zquez, Marynel},
title = {Influence of Simulation and Interactivity on Human Perceptions of a Robot During Navigation Tasks},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675784},
doi = {10.1145/3675784},
abstract = {In Human-Robot Interaction, researchers typically utilize in-person studies to collect subjective perceptions of a robot. In addition, videos of interactions and interactive simulations (where participants control an avatar that interacts with a robot in a virtual world) have been used to quickly collect human feedback at scale. How would human perceptions of robots compare between these methodologies? To investigate this question, we conducted a 2x2 between-subjects study (N=160), which evaluated the effect of the interaction environment (Real vs. Simulated environment) and participants’ interactivity during human-robot encounters (Interactive participation vs. Video observations) on perceptions about a robot (competence, discomfort, social presentation, and social information processing) for the task of navigating in concert with people. We also studied participants’ workload across the experimental conditions. Our results revealed a significant difference in the perceptions of the robot between the real environment and the simulated environment. Furthermore, our results showed differences in human perceptions when people watched a video of an encounter versus taking part in the encounter. Finally, we found that simulated interactions and videos of the simulated encounter resulted in a higher workload than real-world encounters and videos thereof. Our results suggest that findings from video and simulation methodologies may not always translate to real-world human-robot interactions. In order to allow practitioners to leverage learnings from this study and future researchers to expand our knowledge in this area, we provide guidelines for weighing the tradeoffs between different methodologies.},
note = {Just Accepted},
journal = {J. Hum.-Robot Interact.},
month = {jul},
keywords = {Human-robot interaction, human perception, robot navigation}
}

@article{10.1145/3655599,
author = {Slavuljica, Aleksandar and Bektas, Kenan and Strecker, Jannis and Mayer, Simon},
title = {NeighboAR: Efficient Object Retrieval using Proximity- and Gaze-based Object Grouping with an AR System},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {ETRA},
url = {https://doi.org/10.1145/3655599},
doi = {10.1145/3655599},
abstract = {Humans only recognize a few items in a scene at once and memorize three to seven items in the short term. Such limitations can be mitigated using cognitive offloading (e.g., sticky notes, digital reminders). We studied whether a gaze-enabled Augmented Reality (AR) system could facilitate cognitive offloading and improve object retrieval performance. To this end, we developed NeighboAR, which detects objects in a user's surroundings and generates a graph that stores object proximity relationships and user's gaze dwell times for each object. In a controlled experiment, we asked N=17 participants to inspect randomly distributed objects and later recall the position of a given target object. Our results show that displaying the target together with the proximity object with the longest user gaze dwell time helps recalling the position of the target. Specifically, NeighboAR significantly reduces the retrieval time by 33%, number of errors by 71%, and perceived workload by 10%.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {may},
articleno = {225},
numpages = {19},
keywords = {augmented reality, cognitive offloading, eye tracking, human augmentation, mixed reality, object detection, visual search, working memory}
}

@inproceedings{10.1145/3639701.3663647,
author = {Santos, Ana and Freitas, Constan\c{c}a and Bala, Paulo and Campos, Pedro and Dionisio, Mara},
title = {Financial DreamScape: Puzzle Narrative Games for Financial Education},
year = {2024},
isbn = {9798400705038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639701.3663647},
doi = {10.1145/3639701.3663647},
abstract = {Financial literacy is essential for personal and national economic well-being, demanding attention in today’s dynamic society. However, a concerning trend persists where individuals may lack basic financial knowledge; this is common even among individuals with a high level of education. This gap is especially concerning for young adults, as it prevents them from making informed decisions, managing finances effectively, and securing a stable future. This paper presents the development and pilot evaluation of a serious puzzle game prototype to enhance financial literacy among young adults. The pilot evaluation (N=7) explores various game mechanics to deliver engaging financial education. Initial findings indicate promising potential for serious puzzle games in facilitating financial literacy. The research aims to contribute insights into pedagogical strategies and game design enhancements, fostering informed decision-making and economic stability among young adults.},
booktitle = {Proceedings of the 2024 ACM International Conference on Interactive Media Experiences},
pages = {421–425},
numpages = {5},
keywords = {Education, Financial Behaviour, Financial Literacy, Serious Games, Young Adults},
location = {Stockholm, Sweden},
series = {IMX '24}
}

@inproceedings{10.1145/3628516.3659424,
author = {Eriksson, Eva and Holflod, Kim and Toft N\o{}rg\r{a}rd, Rikke},
title = {Jamming with Culture - Piloting Cultural Game Jam with Youth},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3659424},
doi = {10.1145/3628516.3659424},
abstract = {In this paper, we report on the first pilot of a concept called Cultural Game Jams. This specific form of game jam draws on tangible (artworks) and intangible (values) cultural heritage as material for game design. The aim of the cultural game jam is to scaffold young people transitioning from consumers to creators of games, aware of cultural and societal challenges and values. The concept has so far been tested with high school students and indicate promising results for future work.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {945–950},
numpages = {6},
keywords = {CCI, Cultural Game Jam, HCI, design-based research, empowered participation, youth},
location = {Delft, Netherlands},
series = {IDC '24}
}

@article{10.1145/3688006,
author = {Choi, Minsoo and Guo, Siqi and Koilias, Alexandros and Volonte, Matias and Kao, Dominic and Mousas, Christos},
title = {Exploring the Effects of Self-correction Behavior of an Intelligent Virtual Character during a Jigsaw Puzzle Co-solving Task},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2160-6455},
url = {https://doi.org/10.1145/3688006},
doi = {10.1145/3688006},
abstract = {Although researchers have explored how humans perceive the intelligence of virtual characters, few studies have focused on the ability of intelligent virtual characters to fix their mistakes. Thus, we explored the self-correction behavior of a virtual character with different intelligence capabilities in a within-group design ( (N=23) ) study. For this study, we developed a virtual character that can solve a jigsaw puzzle whose self-correction behavior is controlled by two parameters, namely, Intelligence and Accuracy of Self-correction. Then, we integrated the virtual character into our virtual reality experience and asked participants to co-solve a jigsaw puzzle. During the study, our participants were exposed to five experimental conditions resulting from combinations of the Intelligence and Accuracy of Self-correction parameters. In each condition, we asked our participants to respond to a survey examining their perceptions of the virtual character's intelligence and awareness (private, public, and surroundings awareness) and user experiences, including trust, enjoyment, performance, frustration, and desire for future interaction. We also collected application logs, including participants’ dwell gaze data, completion times, and the number of puzzle pieces they placed to co-solve the jigsaw puzzle. The results of all the survey ratings and the completion time were statistically significant. Our results indicated that higher levels of Intelligence and Accuracy of Self-correction enhanced not only our participants’ perceptions of the virtual character's intelligence, awareness (private, public, and surroundings), trustworthiness, and performance but also increased their enjoyment and desire for future interaction with the virtual character while reducing their frustration and completion time. Moreover, we found that as the Intelligence and Accuracy of Self-correction increased, participants had to place fewer puzzle pieces and needed less time to complete the jigsaw puzzle. Lastly, regardless of the experimental condition to which we exposed our participants, they gazed at the virtual character for more time compared to the puzzle pieces and puzzle goal in the virtual environment.},
note = {Just Accepted},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {aug},
keywords = {virtual reality, virtual character, intelligence, self-correction, jigsaw puzzle, puzzle co-solving}
}

@inproceedings{10.1145/3526113.3545679,
author = {Lee, Jaewook and Natarrajan, Raahul and Rodriguez, Sebastian S. and Panda, Payod and Ofek, Eyal},
title = {RemoteLab: A VR Remote Study Toolkit},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545679},
doi = {10.1145/3526113.3545679},
abstract = {User studies play a critical role in human subject research, including human-computer interaction. Virtual reality (VR) researchers tend to conduct user studies in-person at their laboratory, where participants experiment with novel equipment to complete tasks in a simulated environment, which is often new to many. However, due to social distancing requirements in recent years, VR research has been disrupted by preventing participants from attending in-person laboratory studies. On the other hand, affordable head-mounted displays are becoming common, enabling access to VR experiences and interactions outside traditional research settings. Recent research has shown that unsupervised remote user studies can yield reliable results, however, the setup of experiment software designed for remote studies can be technically complex and convoluted. We present a novel open-source Unity toolkit, RemoteLab, designed to facilitate the preparation of remote experiments by providing a set of tools that synchronize experiment state across multiple computers, record and collect data from various multimedia sources, and replay the accumulated data for analysis. This toolkit facilitates VR researchers to conduct remote experiments when in-person experiments are not feasible or increase the sampling variety of a target population and reach participants that otherwise would not be able to attend in-person.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {51},
numpages = {9},
keywords = {Unity, Virtual reality, remote user study, toolkit, user study},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{10.1145/3576914.3588017,
author = {Hatch, Nicole and Magnussen, Walt and Tao, Jian},
title = {Efforts Towards a Digital Twin-based Testbed for Public Safety},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576914.3588017},
doi = {10.1145/3576914.3588017},
abstract = {In the chaos and high stress of a disaster scenario, first responders’ situational awareness is negatively impacted. This is a significant detriment to the fast response time and quick choices required of them and potentially poses a risk to civilians needing help. One solution to this problem is presenting first responders with relevant situational data collected by sensors in an organized and easily readable format, which can be done with a digital twin. The resulting digital twin, using MQTT to handle data transfer and created with Unreal Engine, would be helpful for both training purposes and real-time assessment.https://dl.acm.org/ccs/ccs.cfm},
booktitle = {Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
pages = {297–299},
numpages = {3},
location = {San Antonio, TX, USA},
series = {CPS-IoT Week '23}
}

@article{10.1145/3588441,
author = {Xiang, Suncheng and Qian, Dahong and Guan, Mengyuan and Yan, Binjie and Liu, Ting and Fu, Yuzhuo and You, Guanjie},
title = {Less Is More: Learning from Synthetic Data with Fine-Grained Attributes for Person Re-Identification},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3588441},
doi = {10.1145/3588441},
abstract = {Person re-identification (ReID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data&nbsp;[9], which benefits from the popularity of the synthetic data engine, has attracted great attention from the public. However, existing datasets are limited in quantity, diversity, and realisticity, and cannot be efficiently used for the ReID problem. To address this challenge, we manually construct a large-scale person dataset named FineGPR with fine-grained attribute annotations. Moreover, aiming to fully exploit the potential of FineGPR and promote the efficient training from millions of synthetic data, we propose an attribute analysis pipeline called AOST based on the traditional machine learning algorithm, which dynamically learns attribute distribution in a real domain, then eliminates the gap between synthetic and real-world data and thus is freely deployed to new scenarios. Experiments conducted on benchmarks demonstrate that FineGPR with AOST outperforms (or is on par with) existing real and synthetic datasets, which suggests its feasibility for the ReID task and proves the proverbial less-is-more principle. Our synthetic FineGPR dataset is publicly available at .},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jun},
articleno = {173},
numpages = {20},
keywords = {Person re-identification, synthetic data, efficient training}
}

@article{10.1145/3631134,
author = {Kim, Hayun and Shakeri, Maryam and Shin, Jae-Eun and Woo, Woontack},
title = {Space-adaptive Artwork Placement Based on Content Similarities for Curating Thematic Spaces in a Virtual Museum},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3631134},
doi = {10.1145/3631134},
abstract = {Virtual Reality (VR) provides curators with the tools to design immersive 3D exhibition spaces. However, manually positioning artworks in VR is labor-intensive, and most existing automated methods are limited in considering both artwork content and spatial characteristics, as well as accommodating curators’ design preferences. To address these challenges, we present a virtual exhibition authoring system that automatically generates optimized artwork placements for thematic space layouts, where exhibits are grouped by conceptual themes and arranged as clusters. Our approach clusters artworks based on five content similarity factors—color, material, description, artist, and production date—and allows curators to adjust the importance of each factor and set limits on cluster sizes according to their design goals. A genetic optimization algorithm is employed to determine the placement of artworks, using four cost functions—intra-cluster distance, inter-cluster distance, intra-cluster intervisibility, and occupancy—to evaluate the arrangement with respect to spatial characteristics. The effectiveness of our approach is demonstrated through a series of practical scenarios and an expert evaluation with curators.},
journal = {J. Comput. Cult. Herit.},
month = {jan},
articleno = {3},
numpages = {21},
keywords = {Virtual museums, artwork similarity, placement optimization, thematic space layout}
}

@inproceedings{10.1145/3617184.3623451,
author = {Wen, Siyao and Li, Dandan and Yuan, Junlong and Huang, Jingbin and Zhou, Long},
title = {Design and Implementation of a Game Session Plugin based on Unreal Engine},
year = {2023},
isbn = {9798400708800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617184.3623451},
doi = {10.1145/3617184.3623451},
abstract = {With the rise in popularity of multiplayer online games, efficient game network communication has become increasingly vital. This research paper presents a comprehensive design and implementation of a network session plug-in, focusing on framework construction, data module integration, intermediate module development, and plug-in functionality testing. The primary objective of the plug-in is to enhance the convenience and efficiency of game network communication. Extensive testing, including performance evaluation, stress testing, and functional testing, has been conducted to validate the effectiveness and feasibility of the plug-in. The results showcase the successful development of a robust network session plug-in for multiplayer online games, offering valuable insights for future plug-in advancements. This research contributes significantly to the enhancement of game network technology, ultimately improving the overall player experience in multiplayer online gaming environments.},
booktitle = {Proceedings of the 8th International Conference on Cyber Security and Information Engineering},
pages = {142–149},
numpages = {8},
keywords = {Data module integration, Framework construction, Functionality testing, Intermediate module development, Multiplayer online games, Network session plug-in},
location = {Putrajaya, Malaysia},
series = {ICCSIE '23}
}

@inproceedings{10.1145/3574131.3574443,
author = {Buhion, Deborah Rose and Dizon, Michaela Nicole and Go, Thea Ellen and Oafallas, Kenneth Neil and Joya, Patrick Jaspher and Mangune, Alexandra Cyrielle and Nerie, Sean Paulo and Del Gallego, Neil Patrick},
title = {A Comparative Study of Two Marker-Based Mobile Augmented Reality Applications for Solving 3D Anamorphic Illusion Puzzles},
year = {2023},
isbn = {9798400700316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574131.3574443},
doi = {10.1145/3574131.3574443},
abstract = {Anamorphic illusions are a class of optical illusions wherein objects are perspectively distorted in some way so that the object becomes recognizable when viewing them from a certain point of view or direction. We develop two marker-based mobile augmented reality applications to demonstrate 3D anamorphic illusions. We frame this as a puzzle-solving mechanic where users must align the anamorphic pieces through the movement of the device camera to form a distinguishable virtual model. The first AR proposed utilizes 2D printable markers (2D marker-based AR). In contrast, the second AR uses tabletop items as markers, such as cereal boxes, tin cans, action figures, and the like (3D marker-based AR). The AR applications differ regarding scene setup, user interactions, and examples of anamorphic illusions. We sliced public 3D models, and the corresponding slices were randomly distributed in a given virtual space, using a camera viewpoint where the model would become recognizable. Our proposed framework ensures that each playthrough provides a new anamorphic illusion. Early user testing results show that our 2D-marker-based AR application is more effective in showcasing anamorphic illusions.},
booktitle = {Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
articleno = {3},
numpages = {8},
keywords = {anamorphic art, anamorphic illusion, augmented reality, mobile devices, user interaction},
location = {Guangzhou, China},
series = {VRCAI '22}
}

@proceedings{10.1145/3610540,
title = {SA '23: SIGGRAPH Asia 2023 Educator's Forum},
year = {2023},
isbn = {9798400703119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3628516.3659372,
author = {Delgado, Johnny and West, Rachel and Barmpoutis, Angelos and Jang, Seung Hyuk and Stanley, Edward and Kang, Hyo},
title = {Enhancing Museum Experience with VR by Situating 3D Collections in Contex},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3659372},
doi = {10.1145/3628516.3659372},
abstract = {Recent advancements in photogrammetry and 3D LiDAR scanners have led to a noticeable increase in the creation of 3D scans of museum artifacts. This trend has opened up new possibilities for museums to create interactive and immersive experiences using virtual reality (VR). In collaboration with the Florida Natural History Museum, we have developed an interactive VR game that utilizes the extensive 3D digital collection of the Florida Museum. The goal is to enhance the traditional museum experience by immersing visitors in dynamic VR environments that showcase 3D museum collections within context. Specifically, our VR game highlights 3D scans of endangered species of underwater creatures in the ocean. Children can swim alongside these sea creatures while an AI conversational agent provides scientific insights. Our VR game was showcased to the public at the Florida Natural History Museum during a public outreach event, attracting visits from three K-12 school trips. We conducted field observations to evaluate children’s interactions with the VR game and conducted semi-structured interviews with children’s guardians as well as museum staff. The findings from our observations emphasized the importance of shared experiences among visitors, which could be facilitated by projecting the VR gameplay on a large screen to mitigate the isolated nature of the HMD VR experience limitations. Additionally, museum staff emphasized the significance of considering visitor traffic when designing VR experiences in museum settings. The findings also highlighted a preference for seated experiences over standing ones due to safety concerns related to children colliding with others and museum artifacts. This paper provides an overview of our design process and the challenges of implementing HMD VR in museum settings, offering valuable insights for future endeavors aimed at designing public VR educational experiences targeting children.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {670–675},
numpages = {6},
location = {Delft, Netherlands},
series = {IDC '24}
}

@inproceedings{10.1145/3623053.3623367,
author = {Ong, Yunyu},
title = {Quest Driven Spatial Songs: Exploring how narrative structures of Quests in Games can be mapped onto songs.},
year = {2023},
isbn = {9798400703928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623053.3623367},
doi = {10.1145/3623053.3623367},
abstract = {This paper briefly examines the relationship between music recordings and narrative structures found in video games. It highlights the immersive and time-consuming nature of game quests, where players invest substantial time exploring virtual worlds to reveal the narrative. The paper explores the potential of spatialised music experiences, particularly in the context of Dolby Atmos and the concept of degrees of freedom (3DoF and 6DoF). It investigates the application of spatial algorithms and game engines' middleware to create immersive music experiences that dynamically adapt to the listener's spatial and temporal position. The research acknowledges limited instances of studies on compositional narrative design frameworks specifically tailored for completely spatialised music, particularly regarding the virtual spatialisation of individual instruments. It recognizes that the prevalence of non-diegetic music in games, which serves to represent the player's emotional state, has impeded the exploration of spatialised music elements. Consequently, the paper proposes the adoption of narrative design frameworks derived from video games as a promising approach for songwriters to develop Quest Driven Spatial Songs (QDSS).},
booktitle = {SIGGRAPH Asia 2023 Doctoral Consortium},
articleno = {8},
numpages = {3},
keywords = {3D, 6 degrees of freedom, Interactive Music, Spatial Audio},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@inproceedings{10.1145/3588028.3603678,
author = {Verheydt, Madeline and Staben, Evan and Carncross, Kyle and Minear, Meredith},
title = {Down the Rabbit Hole: Experiencing Alice in Wonderland Syndrome through Virtual Reality},
year = {2023},
isbn = {9798400701528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588028.3603678},
doi = {10.1145/3588028.3603678},
abstract = {Alice in Wonderland Syndrome (AIWS) is a rare perceptual disorder affecting visual processing, the perception of one's body and the experience of time. This condition can be congenital or result from various insults to the brain. There is growing interest in AIWS in providing a window into how different areas of the brain work together to construct reality. We developed a virtual reality (VR) simulation of this condition as a psychoeducational tool for students in the psychological and medical sciences and care givers to experience the different perceptual distortions common in AIWS and an opportunity to reflect on the nature of perception.},
booktitle = {ACM SIGGRAPH 2023 Posters},
articleno = {17},
numpages = {2},
keywords = {Alice in Wonderland Syndrome, Education, Perception, Virtual Reality},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3669947.3669956,
author = {Jayaraj, Lionel and Reeve, Carlton},
title = {XR Immersion for teaching and learning with precise visualization in user perspective},
year = {2024},
isbn = {9798400718083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3669947.3669956},
doi = {10.1145/3669947.3669956},
abstract = {User Perspective Learning is a method that stresses the importance of teaching and learning from the user's point of view (POV). A video is usually created from the user's point of view to be viewed on desktop displays, televisions, and mobile devices. Traditional displays could be replaced by Extended Reality (XR) applications and devices in the not-too-distant future. XR is an inclusive term for VR, AR and MR. Teaching learners with the freedom to view and interact can be made much more effective by using this technology. With the help of XR technology, learners can be tracked as they practice, augment Computer-Generated Imagery (CGI) in real-time, or completely teleport to a digital environment. Over the years, handheld device processing advancements have resulted in significant improvements in portable XR technologies. Modern multi-core processors are capable of handling complex computations in a fraction of a second. The adaptation of XR technologies to mobile smart phones has made them more accessible in addition to standalone devices. Our study is aimed at discovering the potential of XR in user-perspective learning and its effect of immersion. An XR application was created for the experiment, which allows participants to navigate in a virtualized environment (VE) with a great degree of freedom. Users can opt for either handheld or head-mounted viewing through the application. By tracking the orientation of practice models, CGI was augmented accurately matching to the model's dimensions and provided a grabbing and learning experience. This provided a precise visualization. A pilot study was conducted with 71 university students to evaluate the degree of immersion. To study the immersion levels of XR technology, a statistical analysis was conducted on the collected data.},
booktitle = {Proceedings of the 2024 5th International Conference on Education Development and Studies},
pages = {1–13},
numpages = {13},
keywords = {Augmented Reality, Extended Reality, Games Development, Immersion, Teaching and Learning, Virtual Reality, Visualization},
location = {Cambridge, United Kingdom},
series = {ICEDS '24}
}

@article{10.1145/3651288,
author = {Guo, Siqi and Choi, Minsoo and Kao, Dominic and Mousas, Christos},
title = {Collaborating with my Doppelg\"{a}nger: The Effects of Self-similar Appearance and Voice of a Virtual Character during a Jigsaw Puzzle Co-solving Task},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3651288},
doi = {10.1145/3651288},
abstract = {The research community has long been interested in human interaction with embodied virtual characters in virtual reality (VR). At the same time, interaction with self-similar virtual characters, or virtual doppelg\"{a}ngers, has become a prominent topic in both VR and psychology due to the intriguing psychological effects these characters can have on people. However, studies on human interaction with self-similar virtual characters are still limited. To address this research gap, we designed and conducted a 2 (appearance: self-similar vs. non-self-similar appearance) \texttimes{} 2 (voice: self-similar vs. non-self-similar voice) within-group study (N = 25) to explore how combinations of appearance and voice factors influence participants' perception of virtual characters. During the study, we asked participants to collaborate with a virtual character in solving a VR jigsaw puzzle. After each experimental condition, we had participants complete a survey about their experiences with the virtual character. Our findings showed that 1) the virtual characters' self-similarity in appearance enhanced the sense of co-presence and perceived intelligence, but it also elicited higher eeriness; 2) the self-similar voices led to higher ratings on the characters' likability and believability; however, they also induced a more eerie sensation; and 3) we observed an interaction effect between appearance and voice factors for ratings on believability, where the virtual characters were considered more believable when their self-similarity in appearance matched that of their voices. This study provided valuable insights and comprehensive guidance for creating novel collaborative experiences with self-similar virtual characters in immersive environments.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {may},
articleno = {4},
numpages = {23},
keywords = {Collaboration, Doppelg\"{a}nger, Jigsaw Puzzle, Self-similar Appearance, Self-similar Voice, Task Co-solving, Virtual Characters, Virtual Reality}
}

@inproceedings{10.1145/3586182.3615772,
author = {Herbst, Yair and Wolf, Alon and Zelnik-Manor, Lihi},
title = {Demonstrating HUGO, a High-Resolution Tactile Emulator for Complex Surfaces},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3615772},
doi = {10.1145/3586182.3615772},
abstract = {We demonstrate HUGO, a novel device developed to deliver enhanced tactile feedback and facilitate interaction with real-world surfaces. The device aims to overcome the limitations of existing cutaneous feedback devices, which often provide a restricted range of sensations and are primarily tested on simple synthetic surfaces. HUGO was meticulously designed through a human-centered process to enable users to experience realistic touch sensations encountered in various real-world scenarios. HUGO utilizes a parallel manipulator and a pin-array mechanism that operate concurrently at a frequency of up to 200Hz to simulate both coarse and fine geometrical features. By employing a high operation frequency and decomposing the tactile feedback into distinct features, HUGO enables a more accurate replication of tactile experiences associated with different surfaces. The demonstration will showcase HUGO's capabilities in providing authentic haptic feedback. This includes facilitating social interactions, enhancing e-commerce experiences, and improving gaming interactions through realistic haptic engagement with real-world surfaces.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {57},
numpages = {4},
keywords = {Haptic Textures, Haptics, High-Resolution Haptics, Human Computer Interface, User Study},
location = {San Francisco, CA, USA},
series = {UIST '23 Adjunct}
}

@inproceedings{10.1145/3544548.3580978,
author = {Li, Wanwan and Li, Changyang and Kim, Minyoung and Huang, Haikun and Yu, Lap-Fai},
title = {Location-Aware Adaptation of Augmented Reality Narratives},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580978},
doi = {10.1145/3544548.3580978},
abstract = {The recent popularity of augmented reality (AR) devices has enabled players to participate in interactive narratives through virtual events and characters populated in a real-world environment, where different actions may lead to different story branches. In this paper, we propose a novel approach to adapt narratives to real spaces for AR experiences. Our optimization-based approach automatically assigns contextually compatible locations to story events, synthesizing a navigation graph to guide players through different story branches while considering their walking experiences. We validated the effectiveness of our approach for adapting AR narratives to different scenes through experiments and user studies.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {33},
numpages = {15},
keywords = {augmented reality, interactive narratives, path generation, storytelling},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3587424,
title = {SIGGRAPH '23: ACM SIGGRAPH 2023 Educator's Forum},
year = {2023},
isbn = {9798400701467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Los Angeles, CA, USA}
}

@inproceedings{10.1145/3623264.3624464,
author = {Prattic\`{o}, Filippo Gabriele and Checo, Irene and Visconti, Alessandro and Simeone, Adalberto and Lamberti, Fabrizio},
title = {Designing Hand-held Controller-based Handshake Interaction in Social VR and Metaverse},
year = {2023},
isbn = {9798400703935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623264.3624464},
doi = {10.1145/3623264.3624464},
abstract = {This work presents four possible designs for the handshake interaction in a Social VR-like virtual environment in which the user operates using hand-held controllers: a first design based on a graphics user interface (GUI), a second design leveraging a physical button on hand-held controllers, and two designs based on recreating the handshaking gesture by grabbing the other party’s hand and shaking it. The four designs were evaluated and compared through a user study which involved 24 participants, analyzing factors pertaining to embodiment, presence and social presence, usability, and handshake quality of experience. Results indicated that the gesture-based design was preferred, overall.},
booktitle = {Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games},
articleno = {18},
numpages = {6},
keywords = {Non-verbal communication, handshaking, virtual environments.},
location = {Rennes, France},
series = {MIG '23}
}

@inproceedings{10.1145/3675231.3675241,
author = {Takenaka, Shun and Mizuho, Takato and Narumi, Takuji and Kuzuoka, Hideaki},
title = {Effects of Human and Animal Partner-Avatars on Profile Memory in Virtual Reality},
year = {2024},
isbn = {9798400710612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675231.3675241},
doi = {10.1145/3675231.3675241},
abstract = {Advancements in virtual reality technology have facilitated users in freely changing their appearance during communication. Notably, it has become evident that the perceptual and cognitive responses of users vary when their partner uses an animal avatar compared to those when using a human avatar. This study aimed to clarify the effects of partner-avatars on memory through experiments wherein participants were tasked with memorizing the profiles spoken by the partner-avatars. A preliminary experiment validated the assumption that both the animal avatars and the virtual environments could be integrated into processes of profile memory. Subsequently, the primary experiment examined differences in performance in remembering partners’ profiles when the partners were animal avatars versus human avatars. The results showed that the animal avatar condition was marginally inferior to the human avatar condition. Moreover, the perceived intimacy and the presence of others were significantly lower in the animal avatar condition. These findings suggested that interacting with animal avatars may have specific drawbacks in terms of profile memory and engagement in social interactions within virtual environments.},
booktitle = {ACM Symposium on Applied Perception 2024},
articleno = {10},
numpages = {10},
keywords = {avatar, context-dependent memory, environmental context, social VR, virtual environment},
location = {Dublin, Ireland},
series = {SAP '24}
}

@inproceedings{10.1145/3582437.3587176,
author = {Dymek, Mikolaj and Lankoski, Petri},
title = {Ecosystems of indie porn game development: co-dependent partial organisations},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587176},
doi = {10.1145/3582437.3587176},
abstract = {Building on case study data, this paper identifies processes and actors that form an enabling ecosystem for indie porn game development consisting of: game platform technologies, asset stores, commission-based artists, F95zone, Steam, Discord, and crowdfunding platforms (e.g., Patreon). Co-creational and organisational perspectives provide a rewarding exploration of this phenomenon.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {41},
numpages = {4},
keywords = {crowdfunding, ecosystems, indie games, pornography, videogames},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@article{10.1145/3634683,
author = {Qi, Lei and Dong, Peng and Xiong, Tan and Xue, Hui and Geng, Xin},
title = {DoubleAUG: Single-domain Generalized Object Detector in Urban via Color Perturbation and Dual-style Memory},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {1551-6857},
url = {https://doi.org/10.1145/3634683},
doi = {10.1145/3634683},
abstract = {Object detection in urban scenarios is crucial for autonomous driving in intelligent traffic systems. However, unlike conventional object detection tasks, urban-scene images vary greatly in style. For example, images taken on sunny days differ significantly from those taken on rainy days. Therefore, models trained on sunny-day images may not generalize well to rainy-day images. In this article, we aim to solve the single-domain generalizable object detection task in urban scenarios, meaning that a model trained on images from one weather condition should be able to perform well on images from any other weather conditions. To address this challenge, we propose a novel Double AUGmentation (DoubleAUG) method that includes image- and feature-level augmentation schemes. In the image-level augmentation, we consider the variation in color information across different weather conditions and propose a Color Perturbation (CP) method that randomly exchanges the RGB channels to generate various images. In the feature-level augmentation, we propose to utilize a Dual-Style Memory (DSM) to explore the diverse style information on the entire dataset, further enhancing the model’s generalization capability. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art methods. Furthermore, ablation studies confirm the effectiveness of each module in our proposed method. Moreover, our method is plug-and-play and can be integrated into existing methods to further improve model performance.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
articleno = {126},
numpages = {20},
keywords = {Single-domain generalization, Object detection, DoubleAUG}
}

@inproceedings{10.1145/3605495.3605800,
author = {Bodenheimer, Bobby and Adams, Haley and Whitaker, Mirinda and Stefanucci, Jeanine and Creem-Regehr, Sarah},
title = {Perceiving Absolute Distance in Augmented Reality Displays with Realistic and Non-realistic Shadows},
year = {2023},
isbn = {9798400702525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605495.3605800},
doi = {10.1145/3605495.3605800},
abstract = {Although distance perception to Augmented Reality (AR) objects has been studied for decades, little is known about absolute distance perception with the newest available AR displays. One significant distinction in categories of head-worn AR displays is whether they are optical see-through (OST) or video see-through (VST). These two types of devices have different methods of rendering that could affect the cues available for perceiving distance. Specifically, rendering cast shadows can be challenging, especially in OST displays that rely on additive light for rendering, and there may be alternative shadow shading methods that are equally as effective for conveying cues to depth. The current study tests absolute egocentric distance judgments to targets 3-6 meters away from an observer with two types of shadows, in two types of AR displays, the Hololens 2 (OST) and the Varjo XR-3 (VST). Shadows were realistic cast shadows or non-realistic shadows in the form of a stylized ring placed beneath the object. Participants verbally reported perceived distance to spherical virtual targets presented on or above the ground, viewed through the displays in a real world classroom. We found overall distance underestimation in both devices, but that estimations were more accurate with the Hololens 2 compared to the Varjo XR-3. There was little support for a difference in accuracy of estimations between shadow conditions or position on or above the ground (confirmed by a Bayesian analysis), suggesting that non-realistic shadows may be a good option for providing additional shading cues for depth in AR.},
booktitle = {ACM Symposium on Applied Perception 2023},
articleno = {2},
numpages = {9},
keywords = {Augmented reality, Distance perception, Shadows},
location = {Los Angeles, CA, USA},
series = {SAP '23}
}

@inproceedings{10.1145/3517745.3561464,
author = {Xu, Xiaokun and Claypool, Mark},
title = {Measurement of cloud-based game streaming system response to competing TCP cubic or TCP BBR flows},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561464},
doi = {10.1145/3517745.3561464},
abstract = {Cloud-based game streaming is emerging as a convenient way to play games when clients have a good network connection. However, high-quality game streams need high bitrates and low latencies, a challenge when competing for network capacity with other flows. While some network aspects of cloud-based game streaming have been studied, missing are comparative performance and congestion responses to competing TCP flows. This paper presents results from experiments that measure how three popular commercial cloud-based game streaming systems - Google Stadia, NVidia GeForce Now, and Amazon Luna - respond and then recover to TCP Cubic and TCP BBR flows on a congested network link. Analysis of bitrates, loss rates and round-trip times show the three systems have markedly different responses to the arrival and departure of competing network traffic.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {305–316},
numpages = {12},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3613904.3642575,
author = {Jo, Hye-Young and Suzuki, Ryo and Kim, Yoonji},
title = {CollageVis: Rapid Previsualization Tool for Indie Filmmaking using Video Collages},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642575},
doi = {10.1145/3613904.3642575},
abstract = {Previsualization, previs, is essential for film production, allowing cinematographic experiments and effective collaboration. However, traditional previs methods like 2D storyboarding and 3D animation require substantial time, cost, and technical expertise, posing challenges for indie filmmakers. We introduce CollageVis, a rapid previsualization tool using video collages. CollageVis enables filmmakers to create previs through two main user interfaces. First, it automatically segments actors from videos and assigns roles using name tags, color filters, and face swaps. Second, it positions video layers on a virtual stage and allows users to record shots using mobile as a proxy for a virtual camera. These features were developed based on formative interviews by reflecting indie filmmakers’ needs and working methods. We demonstrate the system’s capability by replicating seven film scenes and evaluate the system’s usability with six indie filmmakers. The findings indicate that CollageVis allows more flexible yet expressive previs creation for idea development and collaboration.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {164},
numpages = {16},
keywords = {indie filmmaking, previsualization, storyboard},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3548777,
author = {Elor, Aviv and Whittaker, Steve and Kurniawan, Sri and Michael, Sam},
title = {BioLumin: An Immersive Mixed Reality Experience for Interactive Microscopic Visualization and Biomedical Research Annotation},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3548777},
doi = {10.1145/3548777},
abstract = {Many recent breakthroughs in medical diagnostics and drug discovery arise from deploying machine learning algorithms to large-scale data sets. However, a significant obstacle to such approaches is that they depend on high-quality annotations generated by domain experts. This study develops and evaluates BioLumin, a novel immersive mixed reality environment that enables users to virtually shrink down to the microscopic level for navigation and annotation of 3D reconstructed images. We discuss how domain experts were consulted in the specification of a pipeline to enable automatic reconstruction of biological models for mixed reality environments, driving the design of a 3DUI system to explore whether such a system allows accurate annotation of complex medical data by non-experts. To examine the usability and feasibility of BioLumin, we evaluated our prototype through a multi-stage mixed-method approach. First, three domain experts offered expert reviews, and subsequently, nineteen non-expert users performed representative annotation tasks in a controlled setting. The results indicated that the mixed reality system was learnable and that non-experts could generate high-quality 3D annotations after a short training session. Lastly, we discuss design considerations for future tools like BioLumin in medical and more general scientific contexts.},
journal = {ACM Trans. Comput. Healthcare},
month = {nov},
articleno = {44},
numpages = {28},
keywords = {Immersive technologies, mixed reality, spatial computing, magic leap, interactive visualization, biomedical visualization, human-computer interaction}
}

@article{10.1145/3649595,
author = {Karre, Sai Anirudh and Reddy, Y. Raghu and Mittal, Raghav},
title = {RE Methods for Virtual Reality Software Product Development: A Mapping Study},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3649595},
doi = {10.1145/3649595},
abstract = {Software practitioners use various methods in Requirements Engineering (RE) to elicit, analyze, and specify the requirements of enterprise products. The methods impact the final product characteristics and influence product delivery. Ad-hoc usage of the methods by software practitioners can lead to inconsistency and ambiguity in the product. With the notable rise in enterprise products, games, and so forth across various domains, Virtual Reality (VR) has become an essential technology for the future. The methods adopted for RE for developing VR products requires a detailed study. This article presents a mapping study on RE methods prescribed and used for developing VR applications including requirements elicitation, requirements analysis, and requirements specification. Our study provides insights into the use of such methods in the VR community and suggests using specific RE methods in various fields of interest. We also discuss future directions in RE for VR products.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {88},
numpages = {31},
keywords = {Software requirements, requirements elicitation, virtual reality, industrial practices}
}

@inproceedings{10.1145/3625008.3625039,
author = {Coronado, Angelo and Carvalho, Sergio and Berretta, Luciana},
title = {Game accessibility: Adaptation of a digital escape room game to improve spatial cognitive skills on blind people},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625039},
doi = {10.1145/3625008.3625039},
abstract = {There is a concept called "Universal Accessible Games" that promotes the creation of games for everybody. To be more precise, games should be accessible for all gamers through inclusive design. Therefore, this work has the intention to adapt an Escape-the-room game to be accessible for completely blind people using game accessibility guidelines obtained in a Systematic Mapping. In addition to adapting a game, the work also aims to show a connection between playing an accessible escape-the-room game and enhancing the spatial cognitive skills of blind people. Finally, the work presents 35 accessibility guidelines from the literature for adapting a video game; and after user testing, it shows the potential value of an escape-the-room game for reaching accessibility and also training spatial cognitive skills.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {174–182},
numpages = {9},
keywords = {accessibility, audio game, blind, escape room, spatial cognition, video game, visual impairment},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@inproceedings{10.1145/3564982.3564993,
author = {DIAS, Diogo Miguel P.L. and SOUSA, Joao Paulo P. and BARROSO, Barbara C.V.B. and MAGALHAES, Ines M.B.},
title = {A Novel Procedural Content Generation Algorithm for Tower Defense Games},
year = {2023},
isbn = {9781450397407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564982.3564993},
doi = {10.1145/3564982.3564993},
abstract = {The purpose of this article is to present a procedural content generation algorithm for tower defense games. The algorithm was developed for an isometric tower-defense game with roguelite elements. This algorithm can generate content that improves replayability by generating surprising new levels according to the expected gameplay in this game genre. Each time that the player starts a new game he is faced with new generated road maps, enemies, and spawn points. Finally, we made a playtest session to evaluate several aspects of the algorithm and of the game, such as the levels and enemies’ generation, gameplay, performance, and others.},
booktitle = {Proceedings of the 6th International Conference on Algorithms, Computing and Systems},
articleno = {9},
numpages = {7},
location = {Larissa, Greece},
series = {ICACS '22}
}

@inproceedings{10.1145/3656650.3656681,
author = {Gabriel, Alex and Deborde, Josselin and Hassan, Alaa},
title = {Exploring the Acceptability of HELP-XR among Instructors: A Tool for XR Pedagogical Content Creation},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656681},
doi = {10.1145/3656650.3656681},
abstract = {Virtual and Augmented Reality (VR and AR) are recognized for their potential in education, yet their widespread adoption remains limited, primarily due to challenges in content creation, especially in Extended Reality (XR). This paper presents an acceptability study of HELP XR, an authoring tool aimed at simplifying XR content creation. Following the Design Science Research Paradigm, we assessed educators’ attitudes towards technology using the Greek Computer Attitudes Scale (GCAS), followed by an evaluation of HELP XR’s acceptability using the Unified Theory of Acceptance and Use of Technology 2 (UTAUT2) framework. Our study with 14 participants revealed positive attitudes towards computers, with high ratings in effort expectancy and facilitating conditions for HELP XR. These findings indicate its potential to ease XR content creation barriers in educational settings.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {14},
numpages = {9},
keywords = {Authoring Tools, Education, Extended Reality, Technological Acceptance, Training, User Experience},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@inproceedings{10.1145/3580252.3589998,
author = {Baron, Lauren and Chheang, Vuthea and Chaudhari, Amit and Liaqat, Arooj and Chandrasekaran, Aishwarya and Wang, Yufan and Cashaback, Joshua and Thostenson, Erik and Barmaki, Roghayeh Leila},
title = {Poster: Virtual Reality Exergame for Upper Extremity Rehabilitation Using Smart Wearable Sensors},
year = {2024},
isbn = {9798400701023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580252.3589998},
doi = {10.1145/3580252.3589998},
abstract = {In this work, we propose a creative VR therapy exergame with multi-dimensional reaching tasks for upper extremity rehabilitation. Our system tracks data from the upper extremities using VR hand controllers and a flexible carbon nanotube sensor positioned on the elbow. We conducted a preliminary study (n=12, 7 F) to evaluate the exergame's therapeutic factors, including orientation (horizontal, vertical), configuration (flat, curved), and user experience. The results show a statistically significant difference in task completion time for the variable orientation, but no significance for the number of mistakes. For the resistance change generated from the carbon nanotube sleeve, the flat configuration in the vertical orientation significantly induced more elbow stretches than the other conditions. These results suggest that VR therapy can be customized to the patient without a change in the intensity and induced body stretch. Our proposed VR exergame has the potential to personalize upper extremity home-based therapy using multi-modal sensory data collection.},
booktitle = {Proceedings of the 8th ACM/IEEE International Conference on Connected Health: Applications, Systems and Engineering Technologies},
pages = {183–184},
numpages = {2},
keywords = {virtual therapy, virtual reality, smart wearable sensors, upper extremity, telerehabilitation, human-computer interaction},
location = {Orlando, FL, USA},
series = {CHASE '23}
}

@article{10.5555/3580619.3580628,
author = {Berrier, Seth and Koehle, Karl and Loken, Kimberly Long and Tetzlaff, Michael and Thomas, Tyler},
title = {Interdisciplinary Project-Driven Learning in Game Design and Development},
year = {2022},
issue_date = {November 2022},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {4},
issn = {1937-4771},
abstract = {This panel will discuss the game design and development (GDD) program at the University of Wisconsin - Stout, with an emphasis on its use of project-based learning in an interdisciplinary setting, and the student-led use of Agile practices like sprints, stand-up meetings, reviews and retrospectives. The panel will briefly discuss the history of the program at Stout before diving into the curriculum used, from the foundational course taken by all first-year students to the senior-year capstone experience. It is intended that the takeaways from the panel will have applications to computer science education in general, particularly the focus on interdisciplinary student projects.},
journal = {J. Comput. Sci. Coll.},
month = {nov},
pages = {61–66},
numpages = {6}
}

@article{10.1613/jair.1.15960,
author = {Pternea, Moschoula and Singh, Prerna and Chakraborty, Abir and Oruganti, Yagna and Milletari, Mirco and Bapat, Sayli and Jiang, Kebei},
title = {The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models},
year = {2024},
issue_date = {Aug 2024},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {80},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.15960},
doi = {10.1613/jair.1.15960},
abstract = {In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of Deep Neural Networks (DNNs). We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing (NLP). RL4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a common planning framework without either of them contributing to training or fine-tuning of the other. We further branch this class to distinguish between studies with and without natural language feedback. We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.},
journal = {J. Artif. Int. Res.},
month = {aug},
numpages = {49}
}

@inproceedings{10.1145/3620678.3624662,
author = {Haeberlen, Andreas and Phan, Linh Thi Xuan and McGuire, Morgan},
title = {Metaverse as a Service: Megascale Social 3D on the Cloud},
year = {2023},
isbn = {9798400703874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620678.3624662},
doi = {10.1145/3620678.3624662},
abstract = {We present a vision for the future of an emerging category of cloud service: the metaverse of 3D virtual worlds. Today, hundreds of millions of users are active daily in such worlds, but they are partitioned into small groups of at most a few hundred players. Each group joins a different virtual world instance, and players can only interact in 3D with others players in the same group during that session. Current platforms are designed in ways that simply cannot scale much further, and solutions from other cloud services do not generalize to the more interactive, bidirectional, and latency-sensitive interactive 3D domain. We outline some of the technical challenges that currently stand in the way of a metaverse without inherent technical limitations on the number of users in a shared experience. We argue that, although these obviously touch on many other areas of Computer Science such as computer graphics and numerical simulation, the core challenges lie squarely within the systems domain.},
booktitle = {Proceedings of the 2023 ACM Symposium on Cloud Computing},
pages = {298–307},
numpages = {10},
keywords = {Metaverse, scalability, virtual worlds},
location = {Santa Cruz, CA, USA},
series = {SoCC '23}
}

@proceedings{10.1145/3615452,
title = {ImmerCom '23: Proceedings of the 1st ACM Workshop on Mobile Immersive Computing, Networking, and Systems},
year = {2023},
isbn = {9798400703393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This Workshop aims to bring together researchers, engineers, practitioners, and policymakers from academia, industry, and government to discuss the latest research and identify future opportunities for mobile immersive computing, networking, and systems. It solicits high-quality original research and novel developments that promote innovations to advance mobile immersive content delivery, AR/VR/MR, and the metaverse. It also welcomes papers that summarize research challenges, survey existing solutions, and/or propose visionary solutions to ongoing issues in mobile XR and the metaverse.},
location = {Madrid, Spain}
}

@article{10.1145/3654702,
author = {Tong, Derek C.W. and Tan, Xuet Ying and Chen, Alex Q.},
title = {Eye Strokes: An Eye-gaze Drawing System for Mandarin Characters},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3654702},
doi = {10.1145/3654702},
abstract = {Studies showed that 61% of the elderly in Singapore are English illiterate, and it is essential to find alternatives for non-English literate patients who have literacy in the next most common language, Mandarin. Many eye typing solutions use Mandarin hanyu pinyin, a phonetic system similar to eye typing in the English language. However, most Mandarin-speaking elderly in Singapore are not familiar with Mandarin hanyu pinyin despite being able to read and write Mandarin characters, which makes eye typing redundant. We propose Eye Strokes, a technique to capture eye gaze, as an input modality to identify Mandarin characters for motor neurone disease patients to communicate. This method uses the eye gaze as strokes in a Mandarin character to predict and identify the Mandarin word the user intends to communicate. The proof-of-ideation evaluation discussed in the paper shows that our technique is feasible with promising character prediction accuracy for further investigations, although some limitations exist.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {may},
articleno = {29},
numpages = {15},
keywords = {Chinese character detection, Eye strokes, Assistive technology, Mandarin character detection}
}

@inproceedings{10.1145/3564121.3565236,
author = {Sur, Indranil and Daniels, Zachary and Rahman, Abrar and Faber, Kamil and Gallardo, Gianmarco and Hayes, Tyler and Taylor, Cameron and Gurbuz, Mustafa Burak and Smith, James and Joshi, Sahana and Japkowicz, Nathalie and Baron, Michael and Kira, Zsolt and Kanan, Christopher and Corizzo, Roberto and Divakaran, Ajay and Piacentino, Michael and Hostetler, Jesse and Raghavan, Aswin},
title = {System Design for an Integrated Lifelong Reinforcement Learning Agent for Real-Time Strategy Games},
year = {2023},
isbn = {9781450398473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564121.3565236},
doi = {10.1145/3564121.3565236},
abstract = {As Artificial and Robotic Systems are increasingly deployed and relied upon for real-world applications, it is important that they exhibit the ability to continually learn and adapt in dynamically-changing environments, becoming Lifelong Learning Machines. Continual/lifelong learning (LL) involves minimizing catastrophic forgetting of old tasks while maximizing a model’s capability to learn new tasks. This paper addresses the challenging lifelong reinforcement learning (L2RL) setting. Pushing the state-of-the-art forward in L2RL and making L2RL useful for practical applications requires more than developing individual L2RL algorithms; it requires making progress at the systems-level, especially research into the non-trivial problem of how to integrate multiple L2RL algorithms into a common framework. In this paper, we introduce the Lifelong Reinforcement Learning Components Framework (L2RLCF), which standardizes L2RL systems and assimilates different continual learning components (each addressing different aspects of the lifelong learning problem) into a unified system. As an instantiation of L2RLCF, we develop a standard API allowing easy integration of novel lifelong learning components. We describe a case study that demonstrates how multiple independently-developed LL components can be integrated into a single realized system. We also introduce an evaluation environment in order to measure the effect of combining various system components. Our evaluation environment employs different LL scenarios (sequences of tasks) consisting of Starcraft-2&nbsp;minigames and allows for the fair, comprehensive, and quantitative comparison of different combinations of components within a challenging common evaluation environment.},
booktitle = {Proceedings of the Second International Conference on AI-ML Systems},
articleno = {12},
numpages = {9},
keywords = {Integrative Component Framework, Lifelong Learning, Reinforcement Learning, Starcraft-2&nbsp;, System Design},
location = {Bangalore, India},
series = {AIMLSystems '22}
}

@inproceedings{10.1145/3617553.3617886,
author = {Gerini, Lorenzo and Delzanno, Giorgio and Guerrini, Giovanna and Solari, Fabio and Chessa, Manuela},
title = {Gamified Virtual Reality for Computational Thinking},
year = {2023},
isbn = {9798400703737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617553.3617886},
doi = {10.1145/3617553.3617886},
abstract = {In Computer Science Education, coding activities are extremely important to teach younger students the basics of programming and computational thinking. To provide an immersive experience, in this paper, we propose VRCoding, a Virtual Reality (VR)-based block coding system. VRCoding can teach computational thinking in an immersive Virtual Reality environment, exploiting passive haptics to improve interaction and give tactile feedback to the users. Passive haptics is obtained using simple physics placeholders, i.e., textured parallelepipeds, that are tracked in real-time, and aligned with the coding blocks in VR.  
The system is tested on a group of secondary school users, performing simple coding exercises with a standard monitor-based block coding environment and with the proposed VRCoding block language. Results show positive feedback concerning the sense of presence and the user experience.},
booktitle = {Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation},
pages = {13–21},
numpages = {9},
keywords = {coding, computational thinking, extended reality, gamification, passive haptics, virtual reality},
location = {San Francisco, CA, USA},
series = {Gamify 2023}
}

@inproceedings{10.1145/3582437.3587200,
author = {Harpstead, Erik and Stowers, Kimberly and Lawley, Lane and Zhang, Qiao and Maclellan, Christopher},
title = {Speculative Game Design of Asymmetric Cooperative Games to Study Human-Machine Teaming},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587200},
doi = {10.1145/3582437.3587200},
abstract = {While recent advances in Artificial Intelligence and Machine Learning have demonstrated the potential for AI systems to outperform human experts in many domains, including games, AI systems still generally lack the ability to team with humans on complex tasks. One of the barriers to addressing this challenge is a lack of shared task domains in which to do basic research to study Human-Machine Teaming strategies. In our work, we employ speculative game design to create asymmetric cooperative games that can serve as test beds to study human-machine teaming challenges. In this paper, we will describe our general approach and detail the current state of our development efforts.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {77},
numpages = {4},
keywords = {Asymmetric Cooperative Games, Human-Machine Teaming, Speculative Game Design},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3631085.3631239,
author = {Souza, Joana Gabriela Ribeiro De and Prates, Raquel Oliveira and Santos, Heloiza Aparecida},
title = {End-user Game Development Environments for Educators: Analyzing Platforms},
year = {2024},
isbn = {9798400716270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631085.3631239},
doi = {10.1145/3631085.3631239},
abstract = {Games have been used for many years in educational contexts. They are called serious games. With the increasing presence of digital games in the everyday life of people and, therefore, of children and teenagers, educators sometimes adopt these games as part of their instructional activities. As educational games have a purpose beyond entertainment, educators sometimes want to use more than off-the-shelf games but also customize or create new games to suit their needs. However, developing games is a complex task involving aspects such as programming and game design. In this paper, we investigated what game-authoring tools aimed at educators for creating serious games offer to enable the creation and extension of educational games. We used the Semiotic Inspection Method to analyze two game authoring tools for educators - Rufus and &lt;u-Adventure&gt;. We contrasted our results with a study focusing on general-purpose game authoring tools and identified specific constructs and strategies to support end-users that the platforms we analyzed adopted for the educational context, in addition to those identified in general-purpose tools.},
booktitle = {Proceedings of the 22nd Brazilian Symposium on Games and Digital Entertainment},
pages = {134–142},
numpages = {9},
keywords = {end-user development. educational games, scientific SIM},
location = {Rio Grande (RS), Brazil},
series = {SBGames '23}
}

@inproceedings{10.1145/3604479.3604504,
author = {Girardi, Romullo and Pereira Filho, Adin\^{e} Alves and Teodoro, Gabriel Pinheiro and De Oliveira, Jauvane C},
title = {TAT VR: a Virtual Reality Simulator for Military Shooting Training},
year = {2024},
isbn = {9798400700026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604479.3604504},
doi = {10.1145/3604479.3604504},
abstract = {Traditional military shooting training methods have high costs, logistics complexity, spatial-temporal constraints, and safety risk. VR-based simulation can help address these drawbacks while serving as a platform for supplementing current training approaches, providing more convenience, accessibility, and flexibility. This work aims at analyzing the application of virtual reality in the Brazilian Army’s pistol shooting training through the TAT VR simulator. This simulator was tested by a sample composed of five Army officers and one Navy officer with practical shooting experience. The results were compared with reference values obtained from related work. The data showed that TAT VR reached the reference values for the two evaluated parameters: sense of presence and effectiveness.},
booktitle = {Proceedings of the 24th Symposium on Virtual and Augmented Reality},
pages = {21–28},
numpages = {8},
keywords = {Military, Virtual reality, Virtual worlds training simulations},
location = {Natal, RN, Brazil},
series = {SVR '22}
}

@proceedings{10.1145/3524494,
title = {GAS '22: Proceedings of the 6th International ICSE Workshop on Games and Software Engineering: Engineering Fun, Inspiration, and Motivation},
year = {2022},
isbn = {9781450392938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GAS explores how the growing adoption of gameful elements in various contexts can make the design and development of new technology increasingly complex, and provides a forum to explore these issues that crosscut the software engineering and games development communities. The goal of this one day workshop is to bring together interdisciplinary researchers and practitioners to discuss emerging and new research trends, challenges, costs, and benefits for entertainment games, serious games, and the gamification of traditional (non-game) applications and activities.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3582649.3582664,
author = {Perry, Devin and Bivins, Thomas and Dehaan, Bianca and Li, Wanwan},
title = {Procedural Rhythm Game Generation in Virtual Reality},
year = {2023},
isbn = {9781450398572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582649.3582664},
doi = {10.1145/3582649.3582664},
abstract = {Rhythm games and Virtual Reality (VR) often find themselves coming together through the help of various applications designed with the intent to deliver a unique gameplay experience to individuals who are seeking an immersive virtual environment where there is background music. Popular rhythm games such as Beat Saber make the gameplay experience unique by allowing users to move and slice multicolored tiles to the beat of developer-made “beat maps.” Using music synthesis software, we have designed an application utilizing automatic beat mapping techniques to automatically synthesize rhythm games like Beat Saber in virtual reality to make the game itself more immersive and enjoyable when allowing the user to play any song that they desire, without the need to make the beat maps manually. Through a series of experiment results and user studies, we show that our proposed approach for rhythm game synthesis is immersive and enjoyable when the game objects are automatically synchronized with the background music's rhythm.},
booktitle = {Proceedings of the 2023 6th International Conference on Image and Graphics Processing},
pages = {218–222},
numpages = {5},
keywords = {beat detection, entertainment, game content synthesis, rhythm game},
location = {Chongqing, China},
series = {ICIGP '23}
}

@inproceedings{10.1145/3625704.3625757,
author = {Li, Wanwan},
title = {InsectVR: Simulating Crawling Insects in Virtual Reality for Biology Edutainment},
year = {2023},
isbn = {9798400709142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625704.3625757},
doi = {10.1145/3625704.3625757},
abstract = {In this paper, we propose InsectVR, a Virtual Reality (VR) platform designed for biology edutainment that simulates crawling insects in realistic and immersive virtual environments. InsectVR offers a unique opportunity for educators to teach biology concepts related to insects and their behavior in an engaging way. InsectVR is designed to replicate the real-world behavior of crawling insects using a realistic mathematical model called the random walk algorithm. Through InsectVR, users can observe the movement of the insects and their behavior in immersive virtual environments which can help foster users’ understanding of these amazing creatures. InsectVR uses state-of-the-art VR technology to create a highly realistic insect world, which includes different types of insects and various environmental conditions. After being tested through a series of numerical experiments and preliminary user studies, InsectVR demonstrates the potential to revolutionize biology edutainment by providing users with immersive virtual experiences.},
booktitle = {Proceedings of the 7th International Conference on Education and Multimedia Technology},
pages = {8–14},
numpages = {7},
keywords = {behaviour synthesis, insects simulation, random walk algorithm},
location = {Tokyo, Japan},
series = {ICEMT '23}
}

@article{10.1613/jair.1.13743,
author = {Liu, Ruo-Ze and Pang, Zhen-Jia and Meng, Zhou-Yu and Wang, Wenhai and Yu, Yang and Lu, Tong},
title = {On Efficient Reinforcement Learning for Full-length Game of StarCraft II},
year = {2022},
issue_date = {Dec 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {75},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13743},
doi = {10.1613/jair.1.13743},
abstract = {StarCraft II (SC2) poses a grand challenge for reinforcement learning (RL), of which the main difficulties include huge state space, varying action space, and a long time horizon. In this work, we investigate a set of RL techniques for the full-length game of StarCraft II. We investigate a hierarchical RL approach, where the hierarchy involves two. One is the extracted macro-actions from experts’ demonstration trajectories to reduce the action space in an order of magnitude. The other is a hierarchical architecture of neural networks, which is modular and facilitates scale. We investigate a curriculum transfer training procedure that trains the agent from the simplest level to the hardest level. We train the agent on a single machine with 4 GPUs and 48 CPU threads. On a 64x64 map and using restrictive units, we achieve a win rate of 99% against the difficulty level-1 built-in AI. Through the curriculum transfer learning algorithm and a mixture of combat models, we achieve a 93% win rate against the most difficult non-cheating level built-in AI (level-7). In this extended version of the paper, we improve our architecture to train the agent against the most difficult cheating level AIs (level-8, level-9, and level-10). We also test our method on different maps to evaluate the extensibility of our approach. By a final 3-layer hierarchical architecture and applying significant tricks to train SC2 agents, we increase the win rate against the level-8, level-9, and level-10 to 96%, 97%, and 94%, respectively. Our codes and models are all open-sourced now at https://github.com/liuruoze/HierNet-SC2.
To provide a baseline referring the AlphaStar for our work as well as the research and open-source community, we reproduce a scaled-down version of it, mini-AlphaStar (mAS). The latest version of mAS is 1.07, which can be trained using supervised learning and reinforcement learning on the raw action space which has 564 actions. It is designed to run training on a single common machine, by making the hyper-parameters adjustable and some settings simplified. We then can compare our work with mAS using the same computing resources and training time. By experiment results, we show that our method is more effective when using limited resources. The inference and training codes of mini-AlphaStar are all open-sourced at https://github.com/liuruoze/mini-AlphaStar. We hope our study could shed some light on the future research of efficient reinforcement learning on SC2 and other large-scale games.},
journal = {J. Artif. Int. Res.},
month = {dec},
numpages = {48}
}

@article{10.1145/3662003,
author = {Sissler, John},
title = {Enhancing Non-Player Characters in Unity 3D using GPT-3.5},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3662003},
doi = {10.1145/3662003},
abstract = {This case study presents a comprehensive integration process of OpenAI's GPT-3.5 large language model into Unity 3D to enhance non-player characters (NPCs) in video games and interactive applications. The study aims to develop an architecture and open source software framework that enables NPCs to engage in dynamic real-time interactions with players and other characters. The background and motivation for the study are provided, highlighting the existing limitations of traditional NPC programming and the potential of advanced natural language models such as GPT-3.5 to overcome these limitations. The methodology section outlines the step-by-step process, covering framework design and preparation, core architecture development, humanoid avatar integration and animation, and important feature extensions. The progression of framework design and implementation is described, emphasizing key architectural concepts, design patterns, and essential classes and interfaces. The results of the case study are discussed, focusing on the valuable insights gained and the implications for future advancements. Lessons learned from the integration process are shared, along with suggestions for potential improvements and directions for future research. This case study provides a practical resource for game developers and researchers interested in leveraging advanced natural language processing capabilities to create more immersive and interactive NPC experiences in Unity 3D environments.},
journal = {ACM Games},
month = {aug},
articleno = {25},
numpages = {16},
keywords = {Non-player characters (NPCs), Unity 3D, GPT-3.5, large language model, natural language processing, video games, interactive applications, framework design, architectural concepts, real-time interactions, immersive experiences}
}

@inproceedings{10.1145/3594806.3596559,
author = {Jaiswal, Ashish and Hebri, Aref and Pavel, Hamza Reza and Zadeh, Mohammad Zaki and Makedon, Fillia},
title = {SmartFunction: An Immersive Vr System To Assess Attention Using Embodied Cognition},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596559},
doi = {10.1145/3594806.3596559},
abstract = {In traditional neuropsychological tests, executive functions (EFs) are typically evaluated using paper and pencil or computer-based sit-down tasks. However, a new assessment framework, the Automated Test of Embodied Cognition (ATEC), has been developed to measure EFs and embodied cognition through physical tasks. This paper proposes integrating the ATEC system with virtual reality (VR) to evaluate and diagnose attention-deficit disorders using embodied cognition (EC) principles. The VR system will utilize Meta Quest 2 VR headsets and controllers with motion sensors to accurately capture users’ physical movements. The collected motion data will be transmitted to a remote server for evaluation through machine learning algorithms. By incorporating VR technology, the proposed solution provides an effective and affordable tool for assessing executive functions and attention-deficit disorders in real-life scenarios. The paper also presents the proposed solution’s system architecture, which involves using a virtual avatar and visual and auditory cues to guide users during physical tasks.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {485–490},
numpages = {6},
keywords = {ATEC, cognitive assessment, embodied cognition, executive function, machine learning, virtual reality},
location = {Corfu, Greece},
series = {PETRA '23}
}

@inproceedings{10.1145/3570945.3607331,
author = {Santos, Carlos Pereira and de Groot, Phil and Hagen, Jens and Boudry, Agathe and Mayer, Igor},
title = {CUBE: Conversational User-Interface-Based Embodiment: Developing a Digital Humans Embodiment for Conversational Agents: Design, Implementation, and Integration Challenges},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570945.3607331},
doi = {10.1145/3570945.3607331},
abstract = {Our study introduces an open general-purpose platform for the embodiment of conversational AI systems. Conversational User-interface Based Embodiment (CUBE) is designed to streamline the integration of embodied solutions into text-based dialog managers, providing flexibility for customization depending on the specific use case and application. CUBE is responsible for naturally interacting with users by listening, observing, and responding to them.A detailed account of the design and implementation of the solution is provided, as well as a thorough examination of how it can be integrated by developers and AI dialogue manager integrators. Through interviews with developers, insight was gained into the advantages of such systems. Additionally, key areas that require further research were identified in the current challenges in achieving natural interaction between the user and the embodiments. CUBE bridges some of the gaps by providing controls to further develop natural non-verbal communication.},
booktitle = {Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
articleno = {34},
numpages = {8},
keywords = {Artificial Intelligence, Digital Embodiment, User Interface, Virtual Human},
location = {W\"{u}rzburg, Germany},
series = {IVA '23}
}

@inproceedings{10.1145/3569951.3597544,
author = {Palaniappan, Sri Ranganathan and Pateel, Varun and Jijina, Sam and Young, Jeffrey and Kim, Hyesoon},
title = {Unified Co-Simulation Framework for Autonomous UAVs},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569951.3597544},
doi = {10.1145/3569951.3597544},
abstract = {Autonomous drones (UAVs) have rapidly grown in popularity due to their form factor, agility, and ability to operate in harsh or hostile environments. Drone systems come in various form factors and configurations and operate under tight physical parameters. Further, it has been a significant challenge for architects and researchers to develop optimal drone designs as open-source simulation frameworks either lack the necessary capabilities to simulate a full drone flight stack or they are extremely tedious to setup with little or no maintenance or support. In this paper, we develop and present UniUAVSim, our fully open-source co-simulation framework capable of running software-in-the-loop (SITL) and hardware-in-the-loop (HITL) simulations concurrently. The paper also provides insights into the abstraction of a drone flight stack and details how these abstractions aid in creating a simulation framework which can accurately provide an optimal drone design given physical parameters and constraints. The framework was validated with real-world hardware and is available to the research community to aid in future architecture research for autonomous systems.},
booktitle = {Practice and Experience in Advanced Research Computing 2023: Computing for the Common Good},
pages = {474–477},
numpages = {4},
keywords = {Distributed Systems, Drones, FPGA, Hardware-in-the-loop, Simulation, Software-in-the-loop, UAV},
location = {Portland, OR, USA},
series = {PEARC '23}
}

@article{10.1145/3672359.3672361,
author = {J\"{a}ger, Georg and Licht, Gero and Seyffer, Norman and Reitmann, Stefan},
title = {VR-Based Teleoperation of Autonomous Vehicles for Operation Recovery},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1094-3641},
url = {https://doi.org/10.1145/3672359.3672361},
doi = {10.1145/3672359.3672361},
abstract = {While research will enable the deployment of autonomous systems in harsh and inaccessible environments, their operation may be interrupted due to unforeseen situations. A possibility to recover operation nonetheless is to employ teleoperation. However, what requirements and criteria need to be fulfilled by such a system when deployed in safety-critical operation scenarios? How can a timely and safe operation recovery be ensured? The present work aims to report our progress in developing a research platform for addressing these and similar questions.},
journal = {Ada Lett.},
month = {jun},
pages = {25–29},
numpages = {5}
}

@inproceedings{10.1145/3617733.3617755,
author = {Pascua, Cristina and Datu, William Henry and Dela Pena, Jann Ceasar and Ocampo, Benedict Benz},
title = {Illusory: A Non-Euclidian Concept Adoption in a 3D Puzzle Game},
year = {2023},
isbn = {9798400707735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617733.3617755},
doi = {10.1145/3617733.3617755},
abstract = {Non-Euclidean, under the context of video games, is a unique game genre that focuses on the manipulation of space, and shapes. Non-Euclidean as a game genre also has a lot of untapped potential in its application but despite this, not a lot of video games are made with non-Euclidean mechanics. Next, under the context of mathematics, the discovery of non-Euclidean geometry is one of the most celebrated moments in its history, but despite this, non-Euclidean geometry is generally viewed by most people as an extremely unorthodox idea, hence, it is extremely difficult to understand. Knowing this, gamifying non-Euclidean geometry may prove to be an effective way to educate the average player regarding this topic as well as showcase the various applications in which non-Euclidean may be used in the puzzle game genre. Given this premise, this project's name is Illusory, a non-Euclidean, single-player, first-person, 3D puzzle video game will showcase the versatility of using portals in puzzle games to create illusions.},
booktitle = {Proceedings of the 2023 11th International Conference on Computer and Communications Management},
pages = {136–143},
numpages = {8},
location = {Nagoya, Japan},
series = {ICCCM '23}
}

@inproceedings{10.1145/3655755.3655756,
author = {Wu, Wen-Chuan and Wei, Zi-Jie and Zhang, Yu-Wei and Kang, Ya-Chun},
title = {Mixed Reality Smart Glasses for Local Cultural Learning: A Case Representation of Kavalan Traditional Culture},
year = {2024},
isbn = {9798400716829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3655755.3655756},
doi = {10.1145/3655755.3655756},
abstract = {With the advancement of the Internet and information technology, immersive technology is rapidly growing and widely applied in various fields such as healthcare, retail marketing, transportation, and educational training. This technology combines virtual objects, scene creation, and interactive scenarios, integrating them with technology to deliver highly sensory-stimulating immersive and interactive experiences that engender a profound sense of personal immersion. In this study, a smart glasses-based mixed reality system is designed for local cultural learning, using an illustrative case of the Kavalan traditional culture in Yilan, Taiwan. Learners, in specific locations, use smart glasses and gesture-based interactions to capture images and recognize culturally significant scenes. It then provides learners with valuable information about the cultural background of the area. Furthermore, users can engage in interactive games through gestures to experience the indigenous culture of hunting, immersing themselves in the Kavalan culture. This not only provides an immersion experience, but also strengthens content memory and learning abilities, thereby enhancing learners’ interest and effectiveness.},
booktitle = {Proceedings of the 2024 6th International Conference on Image, Video and Signal Processing},
pages = {1–7},
numpages = {7},
keywords = {Augmented Reality, Interactive Technology, Mixed Reality, Mobile Learning, Smart Glasses},
location = {Ikuta, Japan},
series = {IVSP '24}
}

@inproceedings{10.1145/3623264.3624457,
author = {De Lambilly, Auguste and Benedetti, Gabriel and Rizk, Nour and Hanqi, Chen and Huang, Siyuan and Qiu, Junnan and Louapre, David and Granier De Cassagnac, Raphael and Rohmer, Damien},
title = {Heat Simulation on Meshless Crafted-Made Shapes},
year = {2023},
isbn = {9798400703935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623264.3624457},
doi = {10.1145/3623264.3624457},
abstract = {Interactive shape crafting is an increasingly popular feature in video games, offering players a sense of freedom and personalization. In this work, we propose to combine a stochastic simulation approach to solve the heat equation on Implicit Surface, enabling crafting-ready shapes. Our simulation relies on the "Walk on Sphere" (WoS) approach allowing to solve the asymptotic solution of the heat PDE at any point in space without the need for an explicit mesh structure. To enable interactivity when the shape is moved near a heat source, we propose the integration of time-evolving modifiers. Firstly, using the separation of variables over the PDE enables the approximation of the heating evolution using an additional exponential time variation. Then, we procedurally attach local secondary heat sources to the surface for smooth cool-down. We demonstrate the effectiveness our approach on blended-material shapes generated using CSG operations, combining spatially-varying thermal diffusivity. Overall, our method offers a promising avenue for incorporating often-neglected physical interactions, such as heat-related phenomena, into video games with complex and customizable shapes.},
booktitle = {Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games},
articleno = {9},
numpages = {7},
keywords = {Crafting, Heat equation, Implicit Surface, PDE, Shape Blending, Walk on Sphere},
location = {Rennes, France},
series = {MIG '23}
}

@article{10.1145/3550291,
author = {Zhao, Yiqin and Ma, Chongyang and Huang, Haibin and Guo, Tian},
title = {LITAR: Visually Coherent Lighting for Mobile Augmented Reality},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
url = {https://doi.org/10.1145/3550291},
doi = {10.1145/3550291},
abstract = {An accurate understanding of omnidirectional environment lighting is crucial for high-quality virtual object rendering in mobile augmented reality (AR). In particular, to support reflective rendering, existing methods have leveraged deep learning models to estimate or have used physical light probes to capture physical lighting, typically represented in the form of an environment map. However, these methods often fail to provide visually coherent details or require additional setups. For example, the commercial framework ARKit uses a convolutional neural network that can generate realistic environment maps; however the corresponding reflective rendering might not match the physical environments. In this work, we present the design and implementation of a lighting reconstruction framework called LITAR that enables realistic and visually-coherent rendering. LITAR addresses several challenges of supporting lighting information for mobile AR.First, to address the spatial variance problem, LITAR uses two-field lighting reconstruction to divide the lighting reconstruction task into the spatial variance-aware near-field reconstruction and the directional-aware far-field reconstruction. The corresponding environment map allows reflective rendering with correct color tones. Second, LITAR uses two noise-tolerant data capturing policies to ensure data quality, namely guided bootstrapped movement and motion-based automatic capturing. Third, to handle the mismatch between the mobile computation capability and the high computation requirement of lighting reconstruction, LITAR employs two novel real-time environment map rendering techniques called multi-resolution projection and anchor extrapolation. These two techniques effectively remove the need of time-consuming mesh reconstruction while maintaining visual quality. Lastly, LITAR provides several knobs to facilitate mobile AR application developers making quality and performance trade-offs in lighting reconstruction. We evaluated the performance of LITAR using a small-scale testbed experiment and a controlled simulation. Our testbed-based evaluation shows that LITAR achieves more visually coherent rendering effects than ARKit. Our design of multi-resolution projection significantly reduces the time of point cloud projection from about 3 seconds to 14.6 milliseconds. Our simulation shows that LITAR, on average, achieves up to 44.1% higher PSNR value than a recent work Xihe on two complex objects with physically-based materials.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {153},
numpages = {29},
keywords = {3D vision, lighting estimation, mobile augmented reality}
}

@proceedings{10.1145/3664475,
title = {SIGGRAPH Courses '24: ACM SIGGRAPH 2024 Courses},
year = {2024},
isbn = {9798400706837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@inproceedings{10.1145/3577190.3614128,
author = {Lafuma, Louis and Bouyer, Guillaume and Goguel, Olivier and Didier, Jean-Yves Pascal},
title = {Influence of hand representation on a grasping task in augmented reality},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614128},
doi = {10.1145/3577190.3614128},
abstract = {Research has shown that modifying the aspect of the virtual hand in immersive virtual reality can convey objects properties to users. Whether we can achieve the same results in augmented reality is still to be determined since the user’s real hand is visible through the headset. Although displaying a virtual hand in augmented reality is usually not recommended, it could positively impact the user effectiveness or appreciation of the application. For this purpose, we propose an algorithm to compute virtual hand shape in AR, based on inverse kinematics and physical constraints. It allows to naturally grasp virtual objects while keeping the virtual hand on their surface. We compare the influence of this hand representation on performance and user experience in a grasping task in AR, with two control conditions: a simple virtual hand that follows the real hand and a baseline condition without a virtual hand. Results on 48 participants show that all virtual hand conditions decreased user performance, but enhanced the satisfaction with the task.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {364–372},
numpages = {9},
keywords = {Augmented Reality, Collisions, Hand Interaction, Inverse Kinematics, Manipulation, Visual feedback},
location = {Paris, France},
series = {ICMI '23}
}

@proceedings{10.1145/3623476,
title = {SLE 2023: Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 16th ACM SIGPLAN International Conference on Software Language Engineering (SLE) held in October 2023 as part of SPLASH 2023. Software Language Engineering (SLE) is a thriving research discipline targeted at establishing an engineering approach to the development, use, and maintenance of software languages, that is, of languages for the specification, modeling and tooling of software. Key topics of interest for SLE include approaches, methodologies and tools for language design and implementation with a focus on techniques for static and behavioral semantics, generative or interpretative approaches (including transformation languages and code generation) as well as meta-languages and tools (including language workbenches). Techniques enabling the testing, simulation or formal verification for language validation purposes are also of particular interest. SLE also accommodates empirical evaluation and experience reports of language engineering tools, such as user studies evaluating usability, performance benchmarks or industrial applications.},
location = {Cascais, Portugal}
}

@proceedings{10.1145/3588028,
title = {SIGGRAPH '23: ACM SIGGRAPH 2023 Posters},
year = {2023},
isbn = {9798400701528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Los Angeles, CA, USA}
}

@inproceedings{10.1145/3637989.3638019,
author = {Samonte, Mary Jane and Fiedalan, Jed Ivan and Magadan, Nicole Florence and Nuarin, Jhernika Mae},
title = {Using Augmented Reality Book as a Learning Material for Kindergarten Pupils in the Field of Science},
year = {2024},
isbn = {9798400708732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637989.3638019},
doi = {10.1145/3637989.3638019},
abstract = {Augmented Reality (AR) technology enriches real-world surroundings with additional information like text, graphics, or sound, enabling interaction with virtual elements through devices such as smartphones, tablets, and specialized headsets. This study aimed to develop an AR book for kindergarten science - understanding the physical and natural environment to offer interactive content that broadens learning opportunities beyond traditional textbooks. Convenience sampling involved kindergarten pupils aged four to five using an AR app alongside printed modules. A quasi-experimental quantitative method was employed with experimental and control groups. The pre-test and post-test phases assessed science knowledge. A two-tailed t-test was used for hypothesis testing, comparing post-test scores between experimental and control groups. This study showed that kindergarten students gained significant learning improvement in science lesson with the use of supplemental AR tool.},
booktitle = {Proceedings of the 2023 7th International Conference on Education and E-Learning},
pages = {59–65},
numpages = {7},
keywords = {Augmented Reality, Human computer interaction, Kindergarten, Quasi-Experimental, User Experience},
location = {Tokyo, Japan},
series = {ICEEL '23}
}

@inproceedings{10.1145/3565066.3608707,
author = {Li, Jiajia and Zheng, Zixia and Chai, Yaqing and Su, Shizhen and Wei, Xiemin and Shi, Hongning and Xin, Xiangyang},
title = {DianTea:Designing and Evaluating an Immersive Virtual Reality Game to Enhance Youth Tea Culture Learning},
year = {2023},
isbn = {9781450399241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565066.3608707},
doi = {10.1145/3565066.3608707},
abstract = {As an intangible cultural heritage of China, dian cha is a unique tea-drinking ritual in traditional Chinese tea culture. The ritual comprises various tea-drinking utensils and a specific tea art performance. Nowadays, many young Chinese increasingly ignore traditional Chinese cultural activities because of their high work pressures and fast-paced lives. To broaden the audience for traditional Chinese tea ceremonies and stimulate young people’s interest in learning tea culture, we propose “DianTea” as an immersive virtual reality tea-drinking game where remote players can practice the traditional dian cha ritual. The game design also includes a virtual teacher, multimodal interaction, social scene, and game-based learning. In this paper, we also perform an intersubject user study of the game’s interactive and browsing versions. The results show that the interactive game can significantly enhance learners’ knowledge acquisition, interactive experience, and sense of participation in the dian cha ritual.},
booktitle = {Proceedings of the 25th International Conference on Mobile Human-Computer Interaction},
articleno = {4},
numpages = {8},
keywords = {Embodied learning, Intangible cultural heritage, Virtual avatar, Virtual reality, dian cha},
location = {Athens, Greece},
series = {MobileHCI '23 Companion}
}

@inproceedings{10.1145/3625008.3625037,
author = {Negr\~{a}o, Matheus Dias and Ferreira, Wesley and Bohrer, Betania and Freitas, Carla M.D.S. and Maciel, Anderson and Nedel, Luciana},
title = {Design and Think-Aloud Study of an Immersive Interface for Training Health Professionals in Clinical Skills},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625037},
doi = {10.1145/3625008.3625037},
abstract = {Virtual simulation has been used for decades to improve the practice of technical skills by physicians. However, an important aspect of medical education is learning and practicing non-technical skills, which are commonly taught in role-playing sessions. Using VR simulation for such kind of training would optimize supervision time and allow for training in multiple scenarios. In this paper, we introduce a framework and application to support healthcare professionals in learning multi-step procedures and assess students’ and candidates’ readiness. The framework is an immersive system that allows users to perform general healthcare tasks in a natural environment. It does not require constant monitoring by an expert, making it accessible to larger groups of students and candidates with reduced time and personnel resources. The design also considers individual adaptation to virtual reality by providing in-game ergonomics and interaction accuracy settings. The system was tested through a user study with a pediatrician scenario and underwent a preliminary evaluation in a major hospital.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {157–165},
numpages = {9},
keywords = {HCI, Haptic Displays, Haptics, Interactive Learning, Simulation, Teaching, Wearable Devices},
location = {Rio Grande, Brazil},
series = {SVR '23}
}

@inproceedings{10.1145/3604078.3604158,
author = {Zhang, Manyu},
title = {Research on the application of AR technology in reconstructing the digital presentation of urban identity},
year = {2023},
isbn = {9798400708237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604078.3604158},
doi = {10.1145/3604078.3604158},
abstract = {Augmented reality (AR) technology is playing an important role in the ever-changing conservation of cultural heritage. As economic growth gathers pace, urban living spaces are increasingly being reorganised, and this raises the question of how people can find a sense of identity and community in the new urban settings. To enable people to fully understand their cultural heritage and help them discover a sense of identity in the changing urban social landscape. This will enable people to fully understand their cultural heritage and help them to discover a sense of identity in the changing urban social landscape. This study expanded the effect of popular science education, designed by using AR coding, and set out to evaluate its impact on people reconstructing their identity and sense of belonging to a community in a reconfigured Chinese city. Before developing the design program, using AR coding, the first step was to analyse examples of AR application and coding in existing popular science education. On completing the analysis, we tested our implemented system by carrying out a series of experiments, focussing on a survey of 918 vocational school students in China. The survey gathered data on satisfaction levels, and a statistical analysis was undertaken to determine the correlation between the competence of the students and the degree of satisfaction they reported. The results demonstrated that AR popular science education has the most significant effect on students’ overall educational satisfaction - followed by common competence, and finally information competence. The new program evidenced that it has the potential to improve student competence, and that AR coding can make a major contribution to design education.},
booktitle = {Proceedings of the 15th International Conference on Digital Image Processing},
articleno = {80},
numpages = {7},
location = {Nanjing, China},
series = {ICDIP '23}
}

@inproceedings{10.1145/3617695.3617724,
author = {Chandra, Raymond Leonardo and Berkaoui, Djamel and Castermans, Koen and Nacken, Heribert},
title = {Utilizing Virtual Reality in Higher Education Marketing through Open-Source and Open-Educational Software},
year = {2023},
isbn = {9798400708015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617695.3617724},
doi = {10.1145/3617695.3617724},
abstract = {This paper introduces MyScore, an open-source software (OSS) and open educational resource (OER) virtual reality application designed specifically for higher education institutions. One of the purposes of MyScore is to simplify the creation of personalized virtual university tours for marketing or educational purposes. To demonstrate its capabilities, MyScore was employed to create a virtual scene representing the primary area of RWTH Aachen University. Through testing among students and public showcasing, it was confirmed that the virtual scenario effectively portrays the university and offers a valuable marketing tool, especially for international students who seek to experience the campus remotely.},
booktitle = {Proceedings of the 2023 7th International Conference on Big Data and Internet of Things},
pages = {98–102},
numpages = {5},
keywords = {Education, Higher Education Marketing, Open-Education, Open-Source, Unity, Virtual Reality},
location = {Beijing, China},
series = {BDIOT '23}
}

@inproceedings{10.1145/3607199.3607211,
author = {Anwar, Md Sakib and Zuo, Chaoshun and Yagemann, Carter and Lin, Zhiqiang},
title = {Extracting Threat Intelligence From Cheat Binaries For Anti-Cheating},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607199.3607211},
doi = {10.1145/3607199.3607211},
abstract = {Rampant cheating remains a serious concern for game developers who fear losing loyal customers and revenue. While numerous anti-cheating techniques have been proposed, cheating persists in a vibrant (and profitable) illicit market. Inspired by novel insights into the economics behind cheat development and recent techniques for defending against advanced persistent threats (APTs), we propose a fully automated methodology for extracting “cheat intelligence” from widely distributed cheat binaries to produce a “memory access graph” that guides selective data randomization to yield immune game clients. We have implemented a prototype system for Android and Windows games, CheatFighter, and evaluated it on 86 cheats collected from a variety of real-world sources, including Telegram channels and online forums. CheatFighter successfully counteracts 80 of the real-world cheats in under a minute, demonstrating practical end-to-end protection against widespread cheating.},
booktitle = {Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {17–31},
numpages = {15},
keywords = {Anti-cheating, Automated client hardening, Program analysis},
location = {Hong Kong, China},
series = {RAID '23}
}

@inproceedings{10.1145/3649902.3653338,
author = {Lee, Jaeyoon and Kim, Hanseob and Kim, Gerard Jounghyun},
title = {Keep Your Eyes on the Target: Enhancing Immersion and Usability by Designing Natural Object Throwing with Gaze-based Targeting},
year = {2024},
isbn = {9798400706073},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649902.3653338},
doi = {10.1145/3649902.3653338},
abstract = {While controllers can support many generic 3D interactions in virtual reality (VR), it alone may fall short of eliciting the core experience for certain actions. One such task is the “object throwing’’, ubiquitous in many sports contents, involving intricately timed actions of aiming, arm swinging, and object releasing. With the increasing availability of eye tracking, we propose to combine gaze-based targeting with the controller swing gesture to model the object throw. The target is aimed/locked by gaze, and the throw is enacted by the controller swing/button press with the object let-gone by the button release. We compare and evaluate the proposed interface against the conventional controller-only based interaction through two typical baseball tasks – Pitcher and Outfielder. The findings indicated that the task accuracy was similar, but the gaze-based targeting allowed for faster completion. More importantly, the gaze-based method showed significantly higher usability and richer VR experience.},
booktitle = {Proceedings of the 2024 Symposium on Eye Tracking Research and Applications},
articleno = {14},
numpages = {7},
keywords = {Baseball, Eye Tracking, Gaze Interaction, Targeting, VR Sports, Virtual Reality},
location = {Glasgow, United Kingdom},
series = {ETRA '24}
}

@proceedings{10.1145/3587421,
title = {SIGGRAPH '23: ACM SIGGRAPH 2023 Talks},
year = {2023},
isbn = {9798400701436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Los Angeles, CA, USA}
}

@inproceedings{10.1145/3565970.3567696,
author = {Franzluebbers, Anton and Li, Changying and Paterson, Andrew and Johnsen, Kyle},
title = {Virtual Reality Point Cloud Annotation},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565970.3567696},
doi = {10.1145/3565970.3567696},
abstract = {This work presents a hybrid immersive headset- and desktop-based virtual reality (VR) visualization and annotation system for point clouds, oriented towards application on laser scans of plants. The system can be used to paint regions or individual points with fine detail, while using compute shaders to address performance limitations when working with large, dense point clouds. The system can either be used with an immersive VR headset and tracked controllers, or with mouse and keyboard on a 2D monitor using the same underlying rendering systems. A within-subjects user study (N=16) was conducted to compare these interfaces for annotation and counting tasks. Results showed a strong user preference for the immersive virtual reality interface, likely as a result of perceived and actual significant differences in task performance. This was especially true for annotation tasks, where users could rapidly identify, reach and paint over target regions, reaching high levels of accuracy with minimal time, but we found nuances in the ways users approached the tasks in the two systems.},
booktitle = {Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
articleno = {14},
numpages = {11},
keywords = {annotation, point clouds, virtual reality, visualization},
location = {Online, CA, USA},
series = {SUI '22}
}

@inproceedings{10.1145/3638530.3664089,
author = {Diarchangel, Korben and Francisco, Liam and Koenig, Jude and Louis, Sushil},
title = {Evolving Potential Fields for Rules of the Road Compliant Ship Driving},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664089},
doi = {10.1145/3638530.3664089},
abstract = {We use potential fields tuned by genetic algorithms to drive simulated vessels following the nautical rules of the road in a training simulation. In our work, we use novel potential fields based on the relative bearing and target angle of the vessels at risk of collision. These non-linear potential fields are difficult to tune well by hand and we therefore use genetic algorithms to optimize search through the space of potential field parameter values. The genetic algorithm seeks parameter values that optimizes vessel behavior by avoiding collisions while following the nautical rules of the road governing the behavior of vessels at sea. We conducted experiments on five major types of scenarios identified by expert mariners who provided optimal paths for vessels to follow. Results indicate that genetic algorithms find good potential field parameter values that result in better fitness and behavior than hand tuned parameters and that the path based fitness function may be key to better ship movement.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {91–92},
numpages = {2},
keywords = {genetic algorithms, simulation training, potential fields control},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3649921.3649999,
author = {Chen, Max and Smith, Gillian},
title = {Game Development as Project-Based Learning: Synthesizing Postmortems of Student-Created Mobile Games},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3649999},
doi = {10.1145/3649921.3649999},
abstract = {Game companies and degree-granting colleges with game design programs create numerous games annually, with lessons learned from the development process holding significance for developers and game developer communities. Usually, game developers write postmortems to reflect on what went right and wrong in their game development process. Existing studies on video game postmortems have been primarily of games from professional studios that take years of development effort. Our work analyzes 36 student-created game postmortems spanning 9 years, all for mobile games shipped after 3-month development cycles in a summer internship program organized by the Massachusetts Digital Games Institute (MassDigi). We identify common themes and topics in these student postmortems and contribute to a greater understanding of student game-creation practices and project-based learning for game education.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {38},
numpages = {11},
keywords = {game development, mobile games, postmortems, professional development program, project-based learning, student games},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@proceedings{10.1145/3657547,
title = {ICVARS '24: Proceedings of the 2024 8th International Conference on Virtual and Augmented Reality Simulations},
year = {2024},
isbn = {9798400709012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/3581754.3584111,
author = {Cao, Chen},
title = {Scaffolding CS1 Courses with a Large Language Model-Powered Intelligent Tutoring System},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581754.3584111},
doi = {10.1145/3581754.3584111},
abstract = {Programming skills are rapidly becoming essential for many educational paths and career opportunities. Yet, for many international students, the traditional approach to teaching introductory programming courses can be a significant challenge due to the complexities of the language, the lack of prior programming knowledge, and the language and cultural barriers. This study explores how large language models and gamification can scaffold coding learning and increase Chinese students’ sense of belonging in introductory programming courses. In this project, a gamification intelligent tutoring system was developed to adapt to Chinese international students’ learning needs and provides scaffolding to support their success in introductory computer programming courses. My research includes three studies: a formative study, a user study of an initial prototype, and a computer simulation study with a user study in progress. Both qualitative and quantitative data were collected through surveys, observations, focus group discussions and computer simulation. The preliminary findings suggest that GPT-3-enhanced gamification has great potential in scaffolding introductory programming learning by providing adaptive and personalised feedback, increasing students’ sense of belonging, and reducing their anxiety about learning programming.},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {229–232},
numpages = {4},
location = {Sydney, NSW, Australia},
series = {IUI '23 Companion}
}

@inproceedings{10.1145/3638380.3638413,
author = {Ishac, Karlos and Bourahmoune, Katia},
title = {LifeChair-X Posture Based Gaming using an IoT Cushion and Serious Game for Workplace Wellness},
year = {2024},
isbn = {9798400717079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638380.3638413},
doi = {10.1145/3638380.3638413},
abstract = {This paper presents LifeChair-X, an IoT cushion for posture-based gaming and Serious Game for improving workplace wellness in a post-pandemic world of sedentarianism. The LifeChair posture models are used as input to a game in which a user moves an avatar using postural shifts. Through consultations with medical professionals at the University of Tsukuba Hospital, we developed the game for guiding seated exercises for core training. The system is evaluated with 12 adult participants. Experiments showed the LifeChair-X average response time was within 0.15&nbsp;s (SD=0.21) of a conventional gamepad. A gameplay study demonstrated an average score of 53.50% using LifeChair-X which was less than 25% difference to a gamepad. Elevated heart rates were observed across all participants when using LifeChair-X suggesting light-intensity exercise. A system usability score of 88.96 (SD=10.12) was achieved suggesting excellent usability levels. The LifeChair-X is an effective system for exergaming and can improve workplace wellness.},
booktitle = {Proceedings of the 35th Australian Computer-Human Interaction Conference},
pages = {166–174},
numpages = {9},
keywords = {Gamification, IoT, Posture, Sensing, Serious Game},
location = {Wellington, New Zealand},
series = {OzCHI '23}
}

@article{10.1145/3657303,
author = {Pessoa, Larissa and Martins, Lia and Hsu, Meng and Freitas, Rosiane de},
title = {ZoAM GameBot: a Journey to the Lost Computational World in the Amazonia},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4673},
url = {https://doi.org/10.1145/3657303},
doi = {10.1145/3657303},
abstract = {The search for alternative teaching-learning processes that attract more interest and involvement of young people, has inspired the development of a game with a chatbot architecture based on interactive storytelling and multiple learning paths. Thus, we introduce in this article the GameBot ZoAm, developed for the Discord instant messaging and social platform. ZoAm offers a unique learning experience centered around storytelling, focusing on fundamental computing concepts and logical challenges that enhance computational thinking skills. Furthermore, the game also promotes an appreciation for Amazonian culture and folklore, with decision-making with human values. An action research study was conducted involving students from the last years of the end of elementary school. The research utilized a heuristic analysis based on the Gameplay Heuristics (PLAY) by Desurvire and Wiberg (ANO), and the evaluation model proposed by Korhonen and Koivisto (ANO) for mobile devices. The analysis employed a reduced and merged set of heuristics from these models, suited for the gamebot’s context, focusing on I) Usability, II) Gameplay and Immersion, and III) Mobility. Regarding the reliability coefficient used to evaluate the survey applied to students after playing the gamebot, Cronbach’s Alpha and Guttman Lambda-6 (G6(smc)) coefficients were applied. These metrics were chosen to ensure the internal consistency and reliability of survey items, reflecting on how effectively the questions measured the focuses proposed by the heuristic analysis. The findings indicate that the game has the potential to facilitate the assimilation of the integrated concepts and sustain student interest throughout gameplay.},
note = {Just Accepted},
journal = {J. Comput. Cult. Herit.},
month = {may},
keywords = {conversation simulation applications, educational digital games, narratives and storytelling, computational thinking}
}

@inproceedings{10.1145/3613905.3650965,
author = {Bekta\c{s}, Kenan and Pandjaitan, Adrian and Strecker, Jannis and Mayer, Simon},
title = {GlassBoARd: A Gaze-Enabled AR Interface for Collaborative Work},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650965},
doi = {10.1145/3613905.3650965},
abstract = {Recent research on remote collaboration focuses on improving the sense of co-presence and mutual understanding among the collaborators, whereas there is limited research on using non-verbal cues such as gaze or head direction alongside their main communication channel. Our system – GlassBoARd – permits collaborators to see each other’s gaze behavior and even make eye contact while communicating verbally and in writing. GlassBoARd features a transparent shared Augmented Reality interface that is situated in-between two users, allowing face-to-face collaboration. From the perspective of each user, the remote collaborator is represented as an avatar that is located behind the GlassBoARd and whose eye movements are contingent on the remote collaborator’s instant eye movements. In three iterations, we improved the design of GlassBoARd and tested it with two use cases. Our preliminary evaluations showed that GlassBoARd facilitates an environment for conducting future user experiments to study the effect of sharing eye gaze on the communication bandwidth.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {181},
numpages = {8},
keywords = {CSCW, augmented reality, eye tracking, gaze, non-verbal cues, presence, remote collaboration},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3675231.3675243,
author = {Chakraborty, Soumyajit and Kane, Amanda and Gagnon, Holly and McNamara, Timothy and Bodenheimer, Bobby},
title = {Comparative Effectiveness of an Omnidirectional Treadmill versus Natural Walking for Navigating in Virtual Environments},
year = {2024},
isbn = {9798400710612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675231.3675243},
doi = {10.1145/3675231.3675243},
abstract = {Omnidirectional treadmills provide one solution for locomoting through large virtual environments in confined physical spaces. Through two experiments, this paper evaluated locomotion on an omnidirectional treadmill (Cyberith Virtualizer Elite 2) by comparing it to natural walking in an open physical space. In Experiment 1, participants judged distances and completed a path integration task using the treadmill and natural walking. Participants walked further on the treadmill but had larger angular errors during path integration, potentially due to increased cybersickness. Experiment 2 varied path lengths during path integration and found that longer paths led to higher cybersickness scores but did not affect performance. The paper offers interpretations and suggestions for using omnidirectional treadmills in virtual reality.},
booktitle = {ACM Symposium on Applied Perception 2024},
articleno = {3},
numpages = {10},
keywords = {Cybersickness, Distance Estimation, Natural Walking, Omnidirectional Treadmill, Path Integration, Treadmill Walking},
location = {Dublin, Ireland},
series = {SAP '24}
}

@inproceedings{10.1145/3505270.3558348,
author = {Bozgeyikli, Evren and Gomes, Victor},
title = {Googly Eyes: Displaying User's Eyes on a Head-Mounted Display for Improved Nonverbal Communication},
year = {2022},
isbn = {9781450392112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505270.3558348},
doi = {10.1145/3505270.3558348},
abstract = {Virtual reality (VR) has been becoming increasingly prevalent in daily lives of humans in various settings. One of the concerns for this technology in becoming mainstream is the isolation caused by today's occlusive headsets. An aspect of isolation is related to restricted nonverbal communication between headset-wearing and outside users. To address this gap, we developed “Googly Eyes”, a system that acts for the outside users as a window into the eyes of the headset-wearing user. On a smartphone display that was attached to a FOVE head-mounted display (HMD), user's eye movements were visualized in real time using eye-tracking data. Our main goal is to increase social communication between HMD and non-HMD users, hence increase social presence; and increase enjoyment through improved communication. This paper includes the motivation, design rationale, and implementation details of the Googly Eyes along with preliminary results from a pilot study session and preliminary user studies with a small cohort.},
booktitle = {Extended Abstracts of the 2022 Annual Symposium on Computer-Human Interaction in Play},
pages = {253–260},
numpages = {8},
keywords = {Collaboration, Real-Time Eye Rendering, Social Presence, Virtual Reality},
location = {Bremen, Germany},
series = {CHI PLAY '22}
}

@article{10.5555/3648699.3649067,
author = {Scheikl, Paul Maria and Gyenes, Bal\'{a}zs and Younis, Rayan and Haas, Christoph and Neumann, Gerhard and Wagner, Martin and Mathis-Ullrich, Franziska},
title = {LapGym: an open source framework for reinforcement learning in robot-assisted laparoscopic surgery},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Recent advances in reinforcement learning (RL) have increased the promise of introducing cognitive assistance and automation to robot-assisted laparoscopic surgery (RALS). However, progress in algorithms and methods depends on the availability of standardized learning environments that represent skills relevant to RALS. We present LapGym, a framework for building RL environments for RALS that models the challenges posed by surgical tasks, and sofa_env, a diverse suite of 12 environments. Motivated by surgical training, these environments are organized into 4 tracks: Spatial Reasoning, Deformable Object Manipulation &amp; Grasping, Dissection, and Thread Manipulation. Each environment is highly parametrizable for increasing difficulty, resulting in a high performance ceiling for new algorithms. We use Proximal Policy Optimization (PPO) to establish a baseline for model-free RL algorithms, investigating the effect of several environment parameters on task difficulty. Finally, we show that many environments and parameter configurations reect well-known, open problems in RL research, allowing researchers to continue exploring these fundamental problems in a surgical context. We aim to provide a challenging, standard environment suite for further development of RL for RALS, ultimately helping to realize the full potential of cognitive surgical robotics. LapGym is publicly accessible through GitHub (https://github.com/ScheiklP/lap_gym).},
journal = {J. Mach. Learn. Res.},
month = {mar},
articleno = {368},
numpages = {42},
keywords = {reinforcement learning, robot-assisted surgery, environment suite, software framework, deformable object simulation}
}

@inproceedings{10.1145/3628516.3655815,
author = {Alsebayel, Ghada and Nasri, Mahsa and Myers, Caleb P. and Troiano, Giovanni M and Hatamimajoumerd, Elaheh and Ostadabbas, Sarah and Allison, Kristen and Harteveld, Casper},
title = {ArticuMotion: Towards Assessing Motor Speech Disorders via Gamification},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3655815},
doi = {10.1145/3628516.3655815},
abstract = {Assessing speech disorders in early childhood is challenging, and Speech-Language Pathologists (SLPs) play a key role in addressing such a challenge. However, tools that support speech assessment are often not child-friendly, and SLPs struggle to keep young patients engaged. To compensate, we introduce &nbsp;ArticuMotion, a child-friendly app that supports the assessment of speech disorders while engaging children in gamified experiences. We use participatory design to co-create ArticuMotion with SLPs and test the resulting product in a user study with nine preschool children. ArticuMotion has promise as a gamified assessment for motor speech disorders and shows a potential avenue for designing clinical tools that are useful while being child-friendly.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {232–247},
numpages = {16},
keywords = {Children, Games for health, Gamification, Speech assessment},
location = {Delft, Netherlands},
series = {IDC '24}
}

@inproceedings{10.1145/3583133.3590644,
author = {Masek, Martin and Luke, Kelly and Snell, Jacob and Wheat, Daniel and Lam, Chiou Peng},
title = {Interactive Evolutionary Computation for Strategy Discovery in Multi-Phase Operations},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3590644},
doi = {10.1145/3583133.3590644},
abstract = {Complex adversarial operations typically involve the allocation of finite resources to meet a set of objectives over a number of phases. This poses a challenge for AI-based strategy discovery. A strategy for one phase cannot be developed in isolation as the resources available in any one phase are dependent on the outcome of previous phases. Our proposed solution is to combine an evolutionary algorithm search with human-guided evaluation. The approach uses simulation-based fitness evaluation, where a human operator can view the fittest solution after every set number of generations. The operator can 'lock in' strategies for particular phases, and 'suggest' alternative strategies to guide further evolution. Key to our approach is a representation encoding that allows relative proportions of resources to be represented where actual levels may not be known a priori. We evaluate our solution on a three-phase scenario of a real-time strategy game and compare the effectiveness of strategies that were purely human-devised, purely evolved, and those resulting from the human-evolution collaboration. The collaborative approach shows promising results in being able to find an optimum solution earlier.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {743–746},
numpages = {4},
keywords = {interactive evolution, operations research, real time strategy game},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3675231.3675242,
author = {Chakraborty, Soumyajit and Finney, Hunter and Gagnon, Holly and Creem-Regehr, Sarah and Stefanucci, Jeanine and Bodenheimer, Bobby},
title = {Inter-Pupillary Distance Mismatch Does Not Affect Distance Perception in Action Space},
year = {2024},
isbn = {9798400710612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675231.3675242},
doi = {10.1145/3675231.3675242},
abstract = {Most modern head-mounted displays (HMDs) do not support the full range of adult inter-pupillary distances (IPDs) (i.e., 45 – 80 mm) due to technological limitations. Prior work indicates that the mismatch between a user’s actual IPD and the IPD set in the HMD (“IPD mismatch”) can affect distance and size judgments in near space (0 – 2 m). Therefore, users with IPDs outside of the supported HMD IPD range may not perceive virtual environments (VEs) accurately. Across three experiments, we investigated whether IPD mismatch significantly affects peoples’ distance judgments at longer distances (4 – 7 m). In two of the experiments, we recruited participants with IPDs smaller than the minimum supported IPD of the HTC Vive Pro HMD. They estimated distances in action space using verbal estimation (Experiment 1) and blind walking (Experiment 2) measures in indoor VEs. We found that: (i) distances were underestimated in action space, and (ii) IPD mismatch had minimal to no effect on their distance judgments. In a third experiment, we investigated whether we could generalize our findings to participants with an IPD within the supported HMD IPD range. We were able to replicate our previous findings. Overall, our findings suggest that IPD mismatch in an HMD may not be a major factor in distance underestimation in action space in VEs.},
booktitle = {ACM Symposium on Applied Perception 2024},
articleno = {8},
numpages = {9},
keywords = {Action Space, Blind Walking, Distance Perception, Inter-Pupillary Distance (IPD), Verbal Estimation},
location = {Dublin, Ireland},
series = {SAP '24}
}

@inproceedings{10.1145/3607822.3618008,
author = {Mandic, Sara and Tracy, Rhys and Sra, Misha},
title = {ARFit: Pose-based Exercise Feedback with Mobile AR},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607822.3618008},
doi = {10.1145/3607822.3618008},
abstract = {Fitness encompasses a diverse array of activities, including gym sessions, home workouts, and various other forms of physical exercise. The importance of proper muscle movement remains consistent across all these settings. Ensuring people do the physical movements correctly, avoid injury, gain insights and maintain motivation is traditionally accomplished with the help of an expert instructor. In the absence of an expert, the most common at-home training methods are books, videos, or apps but they provide limited feedback. In this work, we introduce ARFit, an augmented reality application that uses pose-tracking technology to capture user exercise movements and provide feedback to ensure accurate posture.},
booktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
articleno = {45},
numpages = {3},
keywords = {augmented reality, exercise, feedback, fitness},
location = {Sydney, NSW, Australia},
series = {SUI '23}
}

@inproceedings{10.1145/3611659.3617197,
author = {Powley, Benjamin T. and Anslow, Craig and De R\'{o}iste, Mair\'{e}ad and Marshall, Stuart},
title = {Immersive visualization for ecosystem services analysis},
year = {2023},
isbn = {9798400703287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611659.3617197},
doi = {10.1145/3611659.3617197},
abstract = {Ecosystem services are benefits provided to humans by ecosystems through the natural processes and conditions which occur&nbsp;[7]. Interviews with land use scientists identified problems with currently available software applied to their ecosystem services analysis. A user centred design process is adopted and a visualization system, Immersive ESS Visualizer, is presented for visualizing data relating to ecosystem services analysis. Immersive ESS Visualizer is designed for both experts and non-experts and allows users to compare data visualized with multiple hand-manipulated maps. Users can glide over a landscape with data layers draped to analyse areas of interest. Immersive ESS Visualizer could augment a process for presenting ecosystem services analysis results.},
booktitle = {Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},
articleno = {64},
numpages = {2},
keywords = {Ecosystems Services, Virtual Reality, Visualization},
location = {Christchurch, New Zealand},
series = {VRST '23}
}

@proceedings{10.1145/3610538,
title = {SA '23: SIGGRAPH Asia 2023 Courses},
year = {2023},
isbn = {9798400703096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3569009.3571843,
author = {Uhl, Jakob Carl and Schrom-Feiertag, Helmut and Regal, Georg and Hirsch, Linda and Weiss, Yannick and Tscheligi, Manfred},
title = {When Realities Interweave: Exploring the Design Space of Immersive Tangible XR},
year = {2023},
isbn = {9781450399777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569009.3571843},
doi = {10.1145/3569009.3571843},
abstract = {Tangible devices and interaction in Extended Reality (XR) increase immersion and enable users to perform tasks more intuitively, accurately and joyfully across the reality-virtuality continuum. Upon reviewing the literature, we noticed no clear trend for a publication venue, as well as no standard in evaluating the effects of tangible XR. To position the topic of tangible XR in the TEI community, we propose a hands-on studio, where participants will bring in their own ideas for tangible XR from their application fields, and develop prototypes with the cutting-edge technology and a selection of virtual assets provided. Additionally, we will collectively reflect upon evaluation methods on tangible XR, and aim to find a consensus of a core evaluation suite. With this, we aim to foster a practical understanding and spark new developments in tangible XR and its use cases within the TEI community.},
booktitle = {Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {67},
numpages = {4},
keywords = {extended reality, fiducial markers, mixed reality, tangible interaction, virtual reality},
location = {Warsaw, Poland},
series = {TEI '23}
}

@inproceedings{10.1145/3626705.3631781,
author = {Martins, Diogo and Neves, Martim and Marques, Bernardo and Br\'{a}S, Susana and Fernandes, Jos\'{e} Maria},
title = {Immerse Yourself in a Fight Against your Fears: A Vision for using Virtual Reality Serious Games and Physiology Assessment in Phobia Treatment},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626705.3631781},
doi = {10.1145/3626705.3631781},
abstract = {The evolution and miniaturization of technology allows the development of portable solutions for physiology assessment, and the chance to mimic real world settings using Virtual Reality (VR). This work proposes an end-to-end framework for assisting Exposure Therapy (ET) with VR serious games and physiology monitoring. It relies on a generic architecture changing the VR stimuli, i.e., the phobia being considered, while the remaining modules can be re-used for data collection and analysis by a therapist team. It also integrates customizable bio-feedback, triggering specific events or adjusting the amount of stimuli based on the physiological response. The VR serious games were assessed with 16 participants before being integrated into the architecture. The framework’s capacity to acquire bio-signals and other metrics in a synchronous manner was evaluated in two user studies with a total of 23 participants.},
booktitle = {Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
pages = {477–479},
numpages = {3},
keywords = {Exposure Therapy, Phobia Treatment, Physiology Assessment, Virtual Reality},
location = {Vienna, Austria},
series = {MUM '23}
}

@inproceedings{10.1145/3517428.3544817,
author = {Li, Ziming and Connell, Shannon and Dannels, Wendy and Peiris, Roshan},
title = {SoundVizVR: Sound Indicators for Accessible Sounds in Virtual Reality for Deaf or Hard-of-Hearing Users},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517428.3544817},
doi = {10.1145/3517428.3544817},
abstract = {Sounds provide vital information such as spatial and interaction cues in virtual reality (VR) applications to convey more immersive experiences to VR users. However, it may be a challenge for deaf or hard-of-hearing (DHH) VR users to access the information given by sounds, which could limit their VR experience. To address this limitation, we present “SoundVizVR”, which explores visualizing sound characteristics and sound types for several types of sounds in VR experience. SoundVizVR&nbsp;uses Sound-Characteristic Indicators to visualize loudness, duration, and location of sound sources in VR and Sound-Type Indicators to present more information about the type of the sound. First, we examined three types of Sound-Characteristic Indicators (On-Object Indicators, Full Mini-Maps and Partial Mini-Maps) and their combinations in a study with 11 DHH participants. We identified that the combination of Full Mini-Map technique and On-Object Indicator was the most preferred visualization and performed best at locating sound sources in VR. Next, we explored presenting more information about the sounds using text and icons as Sound-Type Indicators. A second study with 14 DHH participants found that all Sound-Type Indicator combinations were successful at locating sound sources.},
booktitle = {Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {5},
numpages = {13},
keywords = {accessibility, audio visualization, deaf and hard-of-hearing, virtual reality},
location = {Athens, Greece},
series = {ASSETS '22}
}

@inproceedings{10.1145/3613904.3642909,
author = {Lipkowitz, Gabriel and Desimone, Joseph},
title = {Palette-PrintAR: augmented reality design and simulation for multicolor resin 3D printing},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642909},
doi = {10.1145/3613904.3642909},
abstract = {While 3D printing affords designers unprecedented geometrical complexity, fewer interactive design tools for multimaterial platforms exist. Recent work in resin 3D printing specifically promises fast, multicolor printing by growing fluidic channels concurrent with the object itself, infusing different resins spatioselectively into the vat; however, no design tools have been developed enabling users to interact with such novel personal fabrication machines in situ. Here, we introduce an augmented reality-based design tool allowing users to engage with this multicolor fabrication method so as to "paint" growing 3D objects. We define the design process and mode of user interaction with our tool, Palette-PrintAR, which integrates situated 3D model manipulation with real-time computational fluid dynamics simulation and computer vision-based tracking and analysis. We detail our 3D printer hardware add-on implementation and AR software architecture, along with characterizing the design flexibilities and limitations of our AR-based multicolor fabrication method.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {349},
numpages = {12},
keywords = {3D printing, augmented reality, interactive fabrication},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3565970.3568184,
author = {Zhang, Jingjing and Lee, Gun A. and Hoermann, Simon and Zhang, Wendy and Piumsomboon, Thammathip},
title = {Virtual Triplets: Human-Agent Shared Control of Virtual Avatars},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565970.3568184},
doi = {10.1145/3565970.3568184},
abstract = {We propose Virtual Triplets, a collaborative Virtual Reality system capable of human-human and human-agent interaction where users can choose to switch their control between two or more virtual avatars, while the virtual agent would take over the control of the free, unpossessed avatars. We developed a use-case scenario where a single human instructor concurrently supervises two students in a virtual classroom setting. Each student is assigned a personal instructor avatar where the instructor could switch the possession between these two avatars. When the human instructor attends to one of the students, the virtual agent controls the unpossessed avatar to assume the supervision role for another student. Virtual Triplets supports recording and playback of the user’s actions. For example, the human instructor’s demonstration shown to one student could be recorded and played back on a different avatar for another student. Moreover, the human instructor can choose to possess a top-down view instead of an individual avatar to have an overview of all the workspaces and give commands to the virtual instructor’s avatar. Our goal is to enable parallelism in a collaborative environment to improve the user experience when multitasking is required for repetitive activities.},
booktitle = {Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
articleno = {36},
numpages = {2},
location = {Online, CA, USA},
series = {SUI '22}
}

@inproceedings{10.1145/3613904.3642243,
author = {Stemasov, Evgeny and Wagner, Tobias and Askari, Ali and Janek, Jessica and Rajabi, Omid and Schikorr, Anja and Frommel, Julian and Gugenheimer, Jan and Rukzio, Enrico},
title = {DungeonMaker: Embedding Tangible Creation and Destruction in Hybrid Board Games through Personal Fabrication Technology},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642243},
doi = {10.1145/3613904.3642243},
abstract = {Hybrid board games (HBGs) augment their analog origins digitally (e.g., through apps) and are an increasingly popular pastime activity. Continuous world and character development and customization, known to facilitate engagement in video games, remain rare in HBGs. If present, they happen digitally or imaginarily, often leaving physical aspects generic. We developed DungeonMaker, a fabrication-augmented HBG bridging physical and digital game elements: 1) the setup narrates a story and projects a digital game board onto a laser cutter; 2) DungeonMaker assesses player-crafted artifacts; 3) DungeonMaker’s modified laser head senses and moves player- and non-player figures, and 4) can physically damage figures. An evaluation (n = 4 \texttimes{} 3) indicated that DungeonMaker provides an engaging experience, may support players’ connection to their figures, and potentially spark novices’ interest in fabrication. DungeonMaker provides a rich constellation to play HBGs by blending aspects of craft and automation to couple the physical and digital elements of an HBG tightly.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {328},
numpages = {20},
keywords = {3D-Printers, Board Games, Craft Games, Fabrication Games, Hybrid Board Games, Laser Cutters, Personal Fabrication, Playful Fabrication},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3611659.3615705,
author = {Kim, You-Jin and Lu, Joshua and H\"{o}llerer, Tobias},
title = {Dynamic Theater: Location-Based Immersive Dance Theater, Investigating User Guidance and Experience},
year = {2023},
isbn = {9798400703287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611659.3615705},
doi = {10.1145/3611659.3615705},
abstract = {Dynamic Theater explores the use of augmented reality (AR) in immersive theater as a platform for digital dance performances. The project presents a locomotion-based experience that allows for full spatial exploration. A large indoor AR theater space was designed to allow users to freely explore the augmented environment. The curated wide-area experience employs various guidance mechanisms to direct users to the main content zones. Results from our 20-person user study show how users experience the performance piece while using a guidance system. The importance of stage layout, guidance system, and dancer placement in immersive theater experiences are highlighted as they cater to user preferences while enhancing the overall reception of digital content in wide-area AR. Observations after working with dancers and choreographers, as well as their experience and feedback are also discussed.},
booktitle = {Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},
articleno = {27},
numpages = {11},
keywords = {Immersive Theater, Mobile Augmented Reality, User Study, Wide-Area},
location = {Christchurch, New Zealand},
series = {VRST '23}
}

@inproceedings{10.1145/3649902.3653943,
author = {D\ae{}hlen, Are and Heldal, Ilona and Rehman, Abdul and Ali, Qasim and Katona, Jozsef and K\H{o}v\'{a}ri, Attila and Stefanut, Teodor and Ferreira, Paula Da Costa and Costescu, Cristina},
title = {Towards More Accurate Help: Informing Teachers how to Support NDD Children by Serious Games and Eye Tracking Technologies},
year = {2024},
isbn = {9798400706073},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649902.3653943},
doi = {10.1145/3649902.3653943},
abstract = {The vision behind this research is to develop a platform that supports children with neurodevelopmental disorders (NDD) in handling their difficulties. This will be done by informing their teachers about each child's NDD specificity, personal ability, and learning progress through standardized tasks that allow tailoring the tasks to personalized requirements. The aim of this work in progress is to 1) examine the role of using eye tracking (ET) technologies to inform teachers to support NDD children through a platform, 2) provide an example for using ET data, and 3) show how ET data can be combined with Serious Games (SG) output in a pipeline. The results show the necessary requirements for the experimental setup with a focus on informing the teachers, the influence of inherent limitations of the participant pool, and illustrate how the ET and SG results can be used to communicate status for sustained attention.},
booktitle = {Proceedings of the 2024 Symposium on Eye Tracking Research and Applications},
articleno = {69},
numpages = {7},
keywords = {eye tracking, neurodevelopmental disorder, serious games, teaching},
location = {Glasgow, United Kingdom},
series = {ETRA '24}
}

@article{10.1145/3599729,
author = {Chen, Shuangmin and Xu, Rui and Xu, Jian and Xin, Shiqing and Tu, Changhe and Yang, Chenglei and Lu, Lin},
title = {QuickCSGModeling: Quick CSG Operations Based on Fusing Signed Distance Fields for VR Modeling},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {7},
issn = {1551-6857},
url = {https://doi.org/10.1145/3599729},
doi = {10.1145/3599729},
abstract = {The latest advancements in Virtual Reality (VR) enable the creation of 3D models within a holographic immersive simulation environment. In this article, we create QuickCSGModeling, a user-friendly mid-air interactive modeling system. We first prepare a dataset consisting of diverse components and precompute the discrete signed distance function&nbsp;(SDF) for each component. During the modeling phase, users can freely design complicated shapes with a pair of VR controllers. Based on the discrete SDF representation, any CSG-like operation (union, intersection, and subtraction) can be performed voxel-wisely. Also, we maintain a single dynamic SDF for the whole scene, whose zero-level set surface exactly encodes the most recent constructed shape. Both SDF fusion and surface extraction are implemented via GPU for a smooth user experience. A total of 34 volunteers were asked to create their favorite models using QuickCSGModeling. With a simple training, most of them can create a fascinating shape or even a descriptive scene quickly. We also discuss how to extend our system to create articulated models with hinges, where an adaptive cube subdivision has to be enforced to improve the reconstruction accuracy around the hinge part, followed by a Dual Contouring-based surface extraction.1},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {mar},
articleno = {189},
numpages = {18},
keywords = {Virtual reality, SDF fusion, boolean operation, modeling}
}

@inproceedings{10.1145/3625468.3647617,
author = {De Fr\'{e}, Matthias and van der Hooft, Jeroen and Wauters, Tim and De Turck, Filip},
title = {Scalable MDC-Based Volumetric Video Delivery for Real-Time One-to-Many WebRTC Conferencing},
year = {2024},
isbn = {9798400704123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625468.3647617},
doi = {10.1145/3625468.3647617},
abstract = {The production and consumption of video content has become a staple in the current day and age. With the rise of virtual reality (VR), users are now looking for immersive, interactive experiences which combine the classic video applications, such as conferencing or digital concerts, with newer technologies. By going beyond 2D video into a 360 degree experience the first step was made. However, a 360 degree video offers only rotational movement, making interaction with the environment difficult. Fully immersive 3D content formats, such as light fields and volumetric video, aspire to go further by enabling six degrees-of-freedom (6DoF), allowing both rotational and positional freedom. Nevertheless, the adoption of immersive video capturing and rendering methods has been hindered by their substantial bandwidth and computational requirements, rendering them in most cases impractical for low latency applications. Several efforts have been made to alleviate these problems by introducing specialized compression algorithms and by utilizing existing 2D adaptation methods to adapt the quality based on the user's available bandwidth. However, even though these methods improve the quality of experience (QoE) and bandwidth limitations, they still suffer from high latency which makes real-time interaction unfeasible. To address this issue, we present a novel, open source [32], one-to-many streaming architecture using point cloud-based volumetric video. To reduce the bandwidth requirements, we utilize the Draco codec to compress the point clouds before they are transmitted using WebRTC which ensures low latency, enabling the streaming of real-time 6DoF interactive volumetric video. Content is adapted by employing a multiple description coding (MDC) strategy which combines sampled point cloud descriptions based on the estimated bandwidth returned by the Google congestion control (GCC) algorithm. MDC encoding scales more easily to a larger number of users compared to performing individual encoding. Our proposed solution achieves similar real-time latency for both three and nine clients (163 ms and 166 ms), which is 9% and 19% lower compared to individual encoding. The MDC-based approach, using three workers, achieves similar visual quality compared to a per client encoding solution, using five worker threads, and increased quality when the number of clients is greater than 20.},
booktitle = {Proceedings of the 15th ACM Multimedia Systems Conference},
pages = {121–131},
numpages = {11},
keywords = {Adaptive Streaming, Multiple Description Coding, Virtual Conferencing, Virtual Reality, Volumetric Video, WebRTC},
location = {Bari, Italy},
series = {MMSys '24}
}

@inproceedings{10.1145/3599609.3599635,
author = {Al Hamzy, Mohamed Suleum Salim and Zhang, Shijin and Huang, Hong and Li, Wanwan},
title = {Creative NFT-Copyrighted AR Face Mask Authoring Using Unity3D Editor},
year = {2023},
isbn = {9798400708398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599609.3599635},
doi = {10.1145/3599609.3599635},
abstract = {In this paper, we extend well-designed 3D face masks into AR face masks and demonstrate the possibility of transforming this into an NFT-copyrighted AR face mask that helps authenticate the ownership of the AR mask user so as to improve creative control, brand identification, and ID protection. The output of this project will not only potentially validate the value of the NFT technology but also explore how to combine the NFT technology with the AR technology so as to be applied to the e-commerce and e-business aspects of the multimedia industry.},
booktitle = {Proceedings of the 2023 7th International Conference on E-Commerce, E-Business and E-Government},
pages = {174–180},
numpages = {7},
keywords = {AR Face Mask, Brand Identification, Creative Control, ID Protection, NFT-Copyright},
location = {Plymouth, United Kingdom},
series = {ICEEG '23}
}

@inproceedings{10.1145/3607822.3614543,
author = {Takami, Taiki and Saito, Taiga and Kameoka, Takayuki and Kajimoto, Hiroyuki},
title = {ExtEdge: Haptic Augmentation of Visual Experiences of a Smartphone by Electro-Tactile Sensation Through the Edges},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607822.3614543},
doi = {10.1145/3607822.3614543},
abstract = {We present ExtEdge, a design and method for augmenting the visual experiences of a smartphone using electro-tactile stimulation. We aim to explore new interactions through tactile augmentation of visual information, such as virtual characters, at the edges of the screen. ExtEdge presents distributed tactile sensations through electrical stimulations from electrode arrays mounted on both edges of the smartphone. This distributed tactile stimulus fits the image at the edges of the screen and provides a more realistic presentation. We first investigated two-point discrimination thresholds in the contact areas of the grasp—middle finger, ring finger, thumb, and thenar eminence—as a performance evaluation of the distributed tactile presentation and found that our design was adequate. We then conducted a user study with some application scenarios and confirmed that ExtEdge enriches visual information.},
booktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
articleno = {7},
numpages = {8},
keywords = {Electro-Tactile Interface, Interaction Technique, Mobile Haptics, Spatial Augmented Reality},
location = {Sydney, NSW, Australia},
series = {SUI '23}
}

@proceedings{10.1145/3548771,
title = {Gamify 2022: Proceedings of the 1st International Workshop on Gamification of Software Development, Verification, and Validation},
year = {2022},
isbn = {9781450394543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the Program Committee, we are pleased to present the proceedings of the 1st International Workshop on Gamification in Software Development, Verification, and Validation (Gamify 2022). The workshop is co-located with the 2022 edition of the ESEC/FSE conference, held in Singapore.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3582437.3587181,
author = {Prins, Vincent L. and Prins, Jelmer and Preuss, Mike and G\'{o}mez-Maureira, Marcello A.},
title = {StoryWorld: Procedural Quest Generation Rooted in Variety &amp; Believability},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587181},
doi = {10.1145/3582437.3587181},
abstract = {Procedural Content Generation (PCG) in games has become more common in recent years. Procedural narrative, however, is still tricky, as designers fear that applying PCG to narrative means that they lose control over the system. In this paper, we attempt to tackle procedural generation of quests, a specific type of narrative, like in the Role Playing Games genre. To this end, we present StoryWorld, an Event-driven simulation of Items, Locations, Characters, and their Memories and Desires. Its modularity makes it easy to add new items and events. Future work could see its memory system made more realistic.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {44},
numpages = {4},
keywords = {RPG, game AI, narrative, procedural content generation, procedural quests},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3563657.3596085,
author = {Risseeuw, Clarice and Martinez Castro, Jose Francisco and Barla, Pascal and Karana, Elvin},
title = {FlavoMetrics: Towards a Digital Tool to Understand and Tune Living Aesthetics of Flavobacteria},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596085},
doi = {10.1145/3563657.3596085},
abstract = {Integrating microorganisms into artefacts is a growing area of interest for HCI designers. However, the time, resources, and knowledge required to understand complex microbial behaviour limits designers from creatively exploring temporal expressions in living artefacts, i.e., living aesthetics. Bridging biodesign and computer graphics, we developed FlavoMetrics, an interactive digital tool that supports biodesigners in exploring Flavobacteria's living aesthetics. This open-source tool enables designers to virtually inoculate bacteria and manipulate stimuli to tune Flavobacteria's living colour in a digital environment. Six biodesigners evaluated the tool and reflected on its implications for their practices, for example, in (1) understanding spatio-temporal qualities of microorganisms beyond 2D, (2) biodesign education, and (3) the experience prototyping of living artefacts. With FlavoMetrics, we hope to inspire novel HCI tools for accessible and time- and resource-efficient biodesign as well as for better alignment with divergent microbial temporalities in living with living artefacts.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {2079–2092},
numpages = {14},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/3546932.3546998,
author = {Trasobares, Jose Ignacio and Domingo, \'{A}frica and Arcega, Lorena and Cetina, Carlos},
title = {Evaluating the benefits of software product lines in game software engineering},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3546998},
doi = {10.1145/3546932.3546998},
abstract = {Video game development is one of the fastest-growing industries in the world. The use of software product lines (SPLs) has proven to be effective in developing different types of software at a lower cost, in less time, and with higher quality. There are recent research efforts that propose to apply SPLs in the domain of video games. Video games present characteristics that differentiate their development from the development of classic software; for example, game developers perceive more difficulties than other non-game developers when reusing code. In this paper, we evaluate if the adoption of an SPL in game software engineering (GSE) can generate the same benefits as in classic software engineering (CSE) considering the case study of Kromaia. As in other disciplines dealing with human behaviour, empirical research allows for building a reliable knowledge base in software engineering. We present an experiment comparing two development approaches, Clone and Own (CaO) and an SPL in terms of correctness, efficiency, and satisfaction when subjects develop elements of a commercial video game. The results indicate that the elements developed using the SPL are more correct than those developed with CaO but do not indicate significant improvement in efficiency or satisfaction. Our findings suggest that SPLs in GSE may play a different role than the one they have played for decades in CSE. Specifically, SPLs can be relevant to generating new video game content or to balancing video game difficulty.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {120–130},
numpages = {11},
keywords = {empirical comparison, game software engineering, software product line engineering},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3606094.3606141,
author = {Zhao, Lijun},
title = {Study on the Recent Development of Metaverse Application in College Teaching},
year = {2023},
isbn = {9798400700422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606094.3606141},
doi = {10.1145/3606094.3606141},
abstract = {This paper mainly investigates the application of metaverse in college teaching. Firstly, we compare the understanding and expression of the concept, characteristics and supporting technology of metaverse in the research results from different scholars. Considering that there is no authoritative definition of the metaverse, the characteristics and supporting technologies recognized by many studies are summarized. The purpose is to find an indirect consensus on the concept of the metaverse. Secondly, the existing literatures related to metaverse teaching is extracted and sorted out from different angles, including general theoretical research, application exploration research and practical progress. We also point out the existing problems such as more theoretical research than practical application, lack of peer cooperation among scholars and institutions, and wide application difficulties because of practical obstacles. This paper holds that although metaverse teaching is still in its infancy, it has a very broad development prospect because it has greatly broken through the limitations of traditional blended teaching or online teaching in terms of teaching resources, teaching environment and teaching mode. Future research should focus on solving the current predicaments in legal, ethical, technical, cost and other detailed aspects, as well as appropriate forward-looking researches on the teaching of the next generation of metaverse.},
booktitle = {Proceedings of the 2023 8th International Conference on Distance Education and Learning},
pages = {30–35},
numpages = {6},
keywords = {College Teaching, Metaverse, latest progress},
location = {Beijing, China},
series = {ICDEL '23}
}

@inproceedings{10.1145/3594806.3596522,
author = {Hebri, Aref and Acharya, Sneh and Theofanidis, Michail and Makedon, Fillia},
title = {A Teleoperation Framework for Robots Utilizing Control Barrier Functions in Virtual Reality},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596522},
doi = {10.1145/3594806.3596522},
abstract = {This paper describes a novel shared control teleoperation framework for mobile robots that utilizes Control Barrier Functions (CBFs) as filtering mechanism to prevent a human operator from making dangerous actions. The proposed framework demonstrates the potential to create a CBF controller that enables users with no prior knowledge of robotics to safely tele-navigate mobile robots with limited situational awareness. As formal methods, we utilize a hand-crafted CBF, which acts as a repulsive field to describe unsafe regions withing the robot’s vicinity. The implementation of the application was deemed possible by creating a Virtual Reality (VR) simulation in the Unity Engine with the SUMMIT-XL STEEL mobile base as an experimental platform. Preliminary experimental results show the ability of the framework to enable safe teleoperation.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {408–412},
numpages = {5},
keywords = {control barrier functions, robots, shared control, simulation, teleoperation, virtual reality},
location = {Corfu, Greece},
series = {PETRA '23}
}

@proceedings{10.1145/3610543,
title = {SA '23: SIGGRAPH Asia 2023 Technical Communications},
year = {2023},
isbn = {9798400703140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3613905.3650768,
author = {Lee, Seung-Jun and Sutthiwanna, Siripon and Lee, Joon Hyub and Bae, Seok-Hyung},
title = {DirActor: Creating Interaction Illustrations by Oneself through Directing and Acting Simultaneously in VR},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650768},
doi = {10.1145/3613905.3650768},
abstract = {In HCI research papers, interaction illustrations are essential to vividly expressing user scenarios arising from novel interactions. However, creating these illustrations through drawing or photography can be challenging, especially when they involve human figures. In this study, we propose the DirActor system that helps researchers create interaction illustrations in VR that can be used as-is or post-processed, by becoming both the director and the actor simultaneously. We reproduced interaction illustrations from past ACM CHI Best and Honorable Mention papers using the proposed system to showcase its usefulness and versatility.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {109},
numpages = {6},
keywords = {bare hands, interaction illustration, virtual reality, voice command},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3561833.3561844,
author = {Rao, A Eashaan and Chimalakonda, Sridhar and Agrahari, Vartika},
title = {BlockList: A Game to Teach Basic Linked Lists Operations To Novice Programmers},
year = {2022},
isbn = {9781450397759},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561833.3561844},
doi = {10.1145/3561833.3561844},
abstract = {Linked list is a commonly used data structure due to its efficient memory utilization, faster access by using non-contiguous memory, and pointer manipulations. It is used to implement various data structures such as stacks, queues, and trees. However, novice programmers often face problems understanding basic linked list operations such as insertion and deletion because it involves low-level memory-related operations. Existing works aim to address this problem by focusing on teaching low-level implementation. Therefore, to help novice programmers, our idea is to introduce linked list concepts using abstract analogies might increase their comprehension. In this regard, we propose a game called BlockList, a drag and drop game that incorporates several patterned blocks synonymous with linked list objects. The game’s main objective is to allow the player to think of the sequence of insertions and deletions of blocks by arranging them according to a particular pattern to achieve a specific goal. To evaluate the effectiveness of BlockList, we conducted a quantitative survey among 15 first-year undergraduate students who are new to programming. 84.88% of the participants agreed that the game helped them to comprehend the basic concepts of linked list operations.},
booktitle = {Proceedings of the 15th Annual ACM India Compute Conference},
pages = {35–40},
numpages = {6},
keywords = {data structures, educational games, linked lists, unity 3D},
location = {Jaipur, India},
series = {COMPUTE '22}
}

@inproceedings{10.1145/3586182.3616662,
author = {Ito, Shunta and Nakanishi, Yasuto},
title = {AmplifiedCoaster: Virtual Roller Coaster Experience using Motorized Ramps and Personal Mobility Vehicle},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3616662},
doi = {10.1145/3586182.3616662},
abstract = {A novel virtual reality (VR) ride consisting of a head-mounted display (HMD) and an electric wheelchair was developed for a virtual roller coaster experience. The VR ride was integrated with an electric wheeled ramp to amplify the perception of virtual ascent and descent. The ascending and descending motion on a slope was replicated continuously with varying curvatures. The system features an electric wheelchair and two motorized ramps with a fixed 10-degree angle, which the wheelchair can drive on and off to simulate ascending and descending in VR. Moreover, by adjusting the relative speed and position of the wheelchair and ramps, we can simulate pitch angle curvature in VR. We are investigating several methods for providing multiple ascent and descent experiences, and introduce one of them.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {11},
numpages = {3},
keywords = {Attraction, Electric Wheelchair, Motion Platform, Personal Mobility Vehicle, Redirected Walking, Virtual Reality},
location = {San Francisco, CA, USA},
series = {UIST '23 Adjunct}
}

@inproceedings{10.1145/3562939.3565664,
author = {Choi, Minsoo and Jiang, Yeling and Breidi, Farid and Mousas, Christos and Akdere, Mesut},
title = {A Mixed Reality Platform for Collaborative Technical Assembly Training},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565664},
doi = {10.1145/3562939.3565664},
abstract = {We have developed a mixed reality (MR)-based platform for basic mechanical engineering concepts as a learning environment for collaborative assembly tasks. In our platform, multiple co-located users interact with virtual objects simultaneously, and during that time, the platform collects data related to participants’ collaboration and team behavior. We implemented four main sections in the platform including setup, introduction, training, and assessment. The platform provides the opportunity for users to interact with virtual objects while also acquiring technical knowledge. Specifically, for the technical component of the platform, users are asked to assemble a hydraulic pump by manipulating and fitting various parts and pieces into a provided pre-assembled blueprint. We conducted a preliminary expert panel review composed of three experts and received positive feedback and suggestions for further development of the platform.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {38},
numpages = {2},
keywords = {mixed reality, technical assembly, training},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3627050.3631570,
author = {Fujita, Mayu and Kurasaki, Shodai and Kanaoka, Akira},
title = {Securing Cross Reality: Unraveling the Risks of 3D Object Disguise on Head Mount Display},
year = {2024},
isbn = {9798400708541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627050.3631570},
doi = {10.1145/3627050.3631570},
abstract = {This research explores the potential risks associated with disguising 3D objects in Cross Reality (XR) environments, which encompass both Virtual Reality (VR) and Augmented Reality (AR) technologies. XR is gaining attention for its immersive experiences, primarily facilitated by Head Mounted Displays (HMDs). Unlike traditional devices, XR provides a more substantial visual impact due to its immersive nature. The study focuses on disguising 3D objects within XR environments, a novel area of research. Various disguises, such as superimposition and transparency, are discussed, then potential security threats and risks are outlined. Preliminary experiments are conducted to demonstrate the technical feasibility of these disguises, and the need for further user studies and experimentation with complex shapes is acknowledged. The research highlights the emerging threat of 3D object disguises in XR and emphasizes the importance of continued investigation in this field.},
booktitle = {Proceedings of the 13th International Conference on the Internet of Things},
pages = {281–286},
numpages = {6},
keywords = {3D Object Disguise, Cross Reality (XR), Security Risks},
location = {Nagoya, Japan},
series = {IoT '23}
}

@inproceedings{10.1145/3586183.3606753,
author = {Hammad, Noor and Harpstead, Erik and Hammer, Jessica},
title = {The View from MARS: Empowering Game Stream Viewers with Metadata Augmented Real-time Streaming},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606753},
doi = {10.1145/3586183.3606753},
abstract = {We present MARS (Metadata Augmented Real-time Streaming), a system that enables game-aware streaming interfaces for Twitch. Current streaming interfaces provide a video stream of gameplay and a chat channel for conversation, but do not allow viewers to interact with game content independently from the steamer or other viewers. With MARS, a Unity game’s metadata is rendered in real-time onto a Twitch viewer’s interface. The metadata can then power viewer-side interfaces that are aware of the streamer’s game activity and provide new capacities for viewers. Use cases include providing contextual information (e.g. clicking on a unit to learn more), improving accessibility (e.g. slowing down text presentation speed), and supporting novel stream-based game designs (e.g. asymmetric designs where the viewers know more than the streamer). We share the details of MARS’ architecture and capabilities in this paper, and showcase a working prototype for each of our three proposed use cases.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {62},
numpages = {13},
keywords = {Game Design, Live Streaming, Twitch, Unity Game Development},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3649902.3653335,
author = {Stark, Philipp and Jung, Alexander J. and Hahn, Jens-Uwe and Kasneci, Enkelejda and G\"{o}llner, Richard},
title = {Using Gaze Transition Entropy to Detect Classroom Discourse in a Virtual Reality Classroom},
year = {2024},
isbn = {9798400706073},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649902.3653335},
doi = {10.1145/3649902.3653335},
abstract = {This paper explores gaze entropy as a metric for detecting classroom discourse events in a virtual reality (VR) classroom. Using data from a laboratory experiment with N = 240 secondary school students, we distinguished between events of teacher-centered classroom discourse (question, hand raising, answer) and teacher explanation by analyzing their transition and stationary gaze entropy. Employing multi-level regression models, both entropy measures effectively discriminated between the two events and distinguished different levels of classroom participation as indicated by the degree of hand-raising by virtual students. Furthermore, using both measures in a logistic regression model, the potential of gaze entropy could be demonstrated by predicting the two events with 67% accuracy. By analyzing transition and stationary entropy, the study attempts to uncover different gaze patterns associated with learning events in a virtual classroom. The results contribute to the research and development of VR scenarios that help to simulate effective learning environments.},
booktitle = {Proceedings of the 2024 Symposium on Eye Tracking Research and Applications},
articleno = {23},
numpages = {11},
keywords = {Classroom Discourse, Event Detection, Eye Tracking, Gaze Entropy, Virtual Reality},
location = {Glasgow, United Kingdom},
series = {ETRA '24}
}

@inproceedings{10.1145/3613905.3651103,
author = {Spittle, Becky and Panda, Payod and Tankelevitch, Lev and Inkpen, Kori and Tang, John and Junuzovic, Sasa and Qi, Qianqian and Sweeney, Pat and Wilson, Andrew D and Buxton, William A.S. and Sellen, Abigail and Rintel, Sean},
title = {Comparing the Agency of Hybrid Meeting Remote Users in 2D and 3D Interfaces of the Hybridge System},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651103},
doi = {10.1145/3613905.3651103},
abstract = {Hybridge is an experimental system for exploring the design of remote inclusion for hybrid meetings. In-room users see remote participants on individual displays positioned around a table, and remotes see video feeds from the room integrated into a digital twin of the meeting room. Remotes can choose where to appear in and view the meeting room from. We designed two digital interfaces for remote attendees, one using a 2D canvas, and the other using a 3D digital twin of the room as the medium of interaction. To decide which interface to use for future evaluation, we conducted a within-subjects comparison of 24 groups completing survival tasks. We found that 3D outperformed 2D in the participants’ perceived sense of awareness, sense of agency, and physical presence. The majority of participants also subjectively preferred 3D over 2D. We discuss design recommendations based on usage patterns and participant comments, and plans for further research.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {82},
numpages = {12},
keywords = {2D, 3D, agency, asymmetry, collaboration, design, effectiveness, hybrid, immersive, inclusion, meetings, spatiality, videoconferencing, virtual},
location = {
},
series = {CHI EA '24}
}

@article{10.1145/3664213,
author = {Lin, Rem RunGu and Hu, Botao Amber and Ke, Koo Yongen and Wu, Wei and Zhang, Kang},
title = {Cell Space: Augmented Awareness of Intercorporeality},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
url = {https://doi.org/10.1145/3664213},
doi = {10.1145/3664213},
abstract = {Cell Space is an interactive art experience that integrates collocated augmented reality, neurofeedback, and contact improvisation, challenging the conventional boundaries between audience and performer, and blending the physical with the virtual, organic with technological. Inspired by McLuhan's concept of media as an extension of the human body, and The AR Art Manifesto's vision of "we becoming the Media," this project reimagines participants as dynamic cells of a shared bio-digital ecosystem within the built environment. This immersive experience expands the concept of intercorporeality, moving beyond mere physical interaction to a shared bio-digital domain, thereby fostering a deeply interconnected environment of Human-Machine-Space Symbiosis. The paper contributes to the discourse on performing arts by presenting a new paradigm for applying AR and neurofeedback in contact improvisation, creating a new layer of perception and fostering intercorporeal interactions by reinforcing both individual and collaborative intentionality of its participants. Ultimately, Cell Space serves as a reflective mirror to the potential futures of our increasingly interconnected existence, encouraging participants to explore a transformative landscape that resonates with the complex interplay between their physical and digital identities and their surroundings.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {jul},
articleno = {53},
numpages = {10},
keywords = {Augmented Reality, Contact Improvisation, Human-Machine-Space Symbiosis, Intentionality, Interactive Performing Art, Intercorporeality, Intersubjectivity, Neurofeedback}
}

@inproceedings{10.1145/3544549.3583903,
author = {Bordegoni, Monica and Carulli, Marina and Spadoni, Elena and Gallace, Alberto and Clerici, Monica and Liu, Ruiyi and Paoli, Matteo and Restifo Pilato, Simone},
title = {Exploring the impact of VR and embodiment on environmental literacy},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583903},
doi = {10.1145/3544549.3583903},
abstract = {Reduction of global bee populations is one of the relevant topics in the list of environmental issues defined in the United Nations Sustainable Development Goals Agenda. Virtual Reality technologies can play a strategic role in raising awareness and education for sustainability by allowing users to visualize, demonstrate and emphasize information, making it more accessible. Virtual Reality can also elicit emotional involvement, which is important for influencing user behavior. The authors have designed and developed an educational Virtual Reality application called Colonies, which allows users to understand the issues related to the bee problem and empathize with bee colonies. The experience also offers suggestions for more sustainable behaviors.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {453},
numpages = {4},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3550355.3552440,
author = {Casamayor, Rodrigo and Arcega, Lorena and P\'{e}rez, Francisca and Cetina, Carlos},
title = {Bug localization in game software engineering: evolving simulations to locate bugs in software models of video games},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552440},
doi = {10.1145/3550355.3552440},
abstract = {Video games have characteristics that differentiate their development and maintenance from classic software development and maintenance. These differences have led to the coining of the term Game Software Engineering to name the emerging subfield that intersects Software Engineering and video games. One of these differences is that video game developers perceive more difficulties than other non-game developers when it comes to locating bugs. Our work proposes a novel way to locate bugs in video games by means of evolving simulations. As the baseline, we have chosen BLiMEA, which targets classic software engineering and uses bug reports and the defect localization principle to locate bugs. We also include Random Search as a sanity check in the evaluation. We evaluate the approaches in a commercial video game (Kromaia). The results for F-measure range from 46.80%. to 70.28% for five types of bugs. Our approach improved the results of the baseline by 20.29% in F-measure. To the best of our knowledge, this is the first approach that is designed specifically for bug localization in video games. A focus group with professional video game developers has confirmed the acceptance of our approach. Our approach opens a new research direction for bug localization for both game software engineering and possibly classic software engineering.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {356–366},
numpages = {11},
keywords = {bug localization, model-driven engineering, search-based software engineering, video games},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3613904.3642083,
author = {Stemasov, Evgeny and Demharter, Simon and R\"{a}dler, Max and Gugenheimer, Jan and Rukzio, Enrico},
title = {pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642083},
doi = {10.1145/3613904.3642083},
abstract = {Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF). These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process. However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models. We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF. In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, recommendations) and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills. We implemented pARam for HoloLens 2 and evaluated it (n = 20), comparing XR and desktop conditions. Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam. We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {337},
numpages = {22},
keywords = {3D-modeling, Customizer Interfaces, Design Customization, In-Situ Design, In-Situ Modeling, Mixed Reality, Parametric Designs, Personal Fabrication, Remixing, pARam},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3555858.3555879,
author = {Barthet, Matthew and Khalifa, Ahmed and Liapis, Antonios and Yannakakis, Georgios},
title = {Generative Personas That Behave and Experience Like Humans},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555879},
doi = {10.1145/3555858.3555879},
abstract = {Using artificial intelligence (AI) to automatically test a game remains a critical challenge for the development of richer and more complex game worlds and for the advancement of AI at large. One of the most promising methods for achieving that long-standing goal is the use of generative AI agents, namely procedural personas, that attempt to imitate particular playing behaviors which are represented as rules, rewards, or human demonstrations. All research efforts for building those generative agents, however, have focused solely on playing behavior which is arguably a narrow perspective of what a player actually does in a game. Motivated by this gap in the existing state of the art, in this paper we extend the notion of behavioral procedural personas to cater for player experience, thus examining generative agents that can both behave and experience their game as humans would. For that purpose, we employ the Go-Explore reinforcement learning paradigm for training human-like procedural personas, and we test our method on behavior and experience demonstrations of more than 100 players of a racing game. Our findings suggest that the generated agents exhibit distinctive play styles and experience responses of the human personas they were designed to imitate. Importantly, it also appears that experience, which is tied to playing behavior, can be a highly informative driver for better behavioral exploration.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {34},
numpages = {10},
keywords = {affective computing, go-explore, imitation learning, player persona, reinforcement learning},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3613904.3642905,
author = {Zojaji, Sahba and Matviienko, Andrii and Leite, Iolanda and Peters, Christopher},
title = {Join Me Here if You Will: Investigating Embodiment and Politeness Behaviors When Joining Small Groups of Humans, Robots, and Virtual Characters},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642905},
doi = {10.1145/3613904.3642905},
abstract = {Politeness and embodiment are pivotal elements in human-agent interactions. While many previous works advocate the positive role of embodiment in enhancing these interactions, it remains unclear how embodiment and politeness affect individuals joining groups. In this paper, we explore how politeness behaviors (verbal and nonverbal) exhibited by three distinct embodiments (humans, robots, and virtual characters) influence individuals’ decisions to join a group of two agents in a controlled experiment (N=54). We assessed agent effectiveness regarding persuasiveness, perceived politeness, and participants’ trajectories when joining the group. We found that embodiment does not significantly impact agent persuasiveness and perceived politeness, but politeness does. Direct and explicit politeness strategies have a higher success rate in persuading participants to join the group at the furthest side. Lastly, participants adhered to social norms when joining at the furthest side, maintained a greater physical distance from humans, chose longer paths, and walked faster when interacting with humans.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {595},
numpages = {16},
keywords = {Free-standing conversational groups, Group dynamics, Humans, Politeness, Robots, Trajectory, Virtual characters, social norms},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3550082.3564196,
author = {Wang, Yixin and Pai, Yun Suen and Minamizawa, Kouta},
title = {It’s Me: VR-based Journaling for Improved Cognitive Self-Regulation},
year = {2022},
isbn = {9781450394628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550082.3564196},
doi = {10.1145/3550082.3564196},
abstract = {Journaling is a well-known evidence-based strategy for practicing better self-regulation and self-awareness in daily life. It is also an effective way for reducing the effects of negative emotions like anxiety and depression. We explore the combination of virtual reality (VR) and a body-tracking mirror as a cognitive activity in the form of VR journaling. VR-based journaling aims to build cognitive reappraisal, self-reflection, and autonomic emotion regulation in a sustainable way. The user performs in front of the mirror, using body-tracking to help them keep a daily journal. Considering that looking into the mirror is already a daily habit for most people, it does not require the users to form a new habit when trying to do journaling. Furthermore, it activates creativity by encouraging the user to use their body language as a story-telling tool.},
booktitle = {SIGGRAPH Asia 2022 Posters},
articleno = {45},
numpages = {2},
keywords = {Avatar, Emotion Regulation, Empathy, Journaling, Mood, Reflection, Virtual Reality},
location = {Daegu, Republic of Korea},
series = {SA '22}
}

@article{10.1145/3677324,
author = {Shahu, Ambika and W\"{o}lfer, Martin and Michahelles, Florian},
title = {Carbon Rebellion: Empowerment using Data-Driven Narratives},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677324},
doi = {10.1145/3677324},
abstract = {Climate change communication demands narratives that resonate widely. ”Carbon Rebellion,” an Interactive Digital Narrative, exemplifies this by blending personal anecdotes with data-driven content, fostering deep engagement. This research highlights the power of narratives in articulating the gravity of climate change, emphasizing the need for collective action and policy-level interventions. By juxtaposing individual choices against the backdrop of broader societal implications, the narrative seeks to both inspire and empower individuals. The findings reveal that such narratives can elicit strong emotional responses, catalyze critical thinking, and drive proactive climate action. The narrative’s emphasis on community-based efforts highlights the pivotal role of collective endeavors in addressing this global challenge.},
note = {Just Accepted},
journal = {ACM J. Comput. Sustain. Soc.},
month = {aug},
keywords = {Climate change, CO2 footprint, Sustainability, Personal impact, Serious games, Narratives, Storytelling, Personal Accountability, Climate Consciousnes}
}

@inproceedings{10.1145/3611314.3616065,
author = {Salazar, Mikel and Louka, Michael Nicholas},
title = {CoEditAR: A Framework for Collaborative Interaction in WebXR-enabled Spatial Computing},
year = {2023},
isbn = {9798400703249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611314.3616065},
doi = {10.1145/3611314.3616065},
abstract = {CoEditAR is an open-source framework focused on facilitating the creation, sharing and discovery of WebXR-based, interoperable experiences. To achieve this goal, we are working on a semantic model (and language) that allows the contextual description of the real world and interactive virtual elements on top of them. In this way, it is possible for end users to automatically find new experiences by simply navigating through a physical location, overcoming the limitations of current URL-based methods and simplifying the transition towards interaction paradigms based on Spatial Computing.},
booktitle = {Proceedings of the 28th International ACM Conference on 3D Web Technology},
articleno = {25},
numpages = {2},
keywords = {3D Content Creation, Interoperability, Multi-modal 3D Interaction paradigms, Semantic Web, Spatial Computing, WebXR},
location = {San Sebastian, Spain},
series = {Web3D '23}
}

@inproceedings{10.1145/3582437.3587208,
author = {Merino, Timothy and Charity, M and Togelius, Julian},
title = {Interactive Latent Variable Evolution for the Generation of Minecraft Structures},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587208},
doi = {10.1145/3582437.3587208},
abstract = {The open-world sandbox game Minecraft is well-known for applying a wide array of procedural content generation techniques to create unique and expansive game environments. However, procedurally generated buildings are absent in the Minecraft world, thus players must build their own structures to flesh out their worlds. This build process can be extremely time-consuming and appeals to more creatively-inclined players. To aid players in this process, we introduce a tool combining interactive evolution with latent variable evolution to evolve procedurally generated Minecraft structures to a player’s aesthetic choices. We employ two separate neural network models to generate structures: a 3D generative model for generating the structure design and an encoding model for applying Minecraft textures to the structure’s voxels. We evaluate this tool with a user study incorporating an online interface that allows participants to select, evolve, and guide a population of these generated 3D structures towards a specific design goal.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {67},
numpages = {8},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3582437.3587203,
author = {Morrell, Edward and Kultima, Annakaisa and Grufstedt, Ylva and Kauppinen, Tomi},
title = {Promotypes - Prototyping Games for a University Game Production Pipeline},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587203},
doi = {10.1145/3582437.3587203},
abstract = {In this paper, we reflect upon the development process of three videogame ‘promotypes’, prototypes designed to promote the game production pipeline service in development at Aalto University. In the process of developing example games for potential internal clients of the university, we aimed to set out realistic project goals for given resources to better communicate the scope and potential level of the delivered outcomes. Our lessons in this project were, at large, general lessons in game development, including how motivation and proficiency with production tools, alongside the anticipated expectations of clients and players for polished games, affected the game development processes. In addition, we explore how the pursuit for academic accuracy and the model of a non-intensive process of academic game development posits additional challenges in the development of a realistically balanced production pipeline.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {73},
numpages = {8},
keywords = {academic game development, design decisions, educational games, game design post-mortem, game development process},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3586183.3606829,
author = {Mukashev, Dinmukhammed and Ranasinghe, Nimesha and Nittala, Aditya Shekhar},
title = {TactTongue: Prototyping ElectroTactile Stimulations on the Tongue},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606829},
doi = {10.1145/3586183.3606829},
abstract = {The tongue is a remarkable human organ with a high concentration of taste receptors and an exceptional ability to sense touch. This work uses electro-tactile stimulation to explore the intricate interplay between tactile perception and taste rendering on the tongue. To facilitate this exploration, we utilized a flexible, high-resolution electro-tactile prototyping platform that can be administered in the mouth. We have created a design tool that abstracts users from the low-level stimulation parameters, enabling them to focus on higher-level design objectives. Through this platform, we present the results of three studies. Our first study evaluates the design tool’s qualitative and formative aspects. In contrast, the second study measures the qualitative attributes of the sensations produced by our device, including tactile sensations and taste. In the third study, we demonstrate the ability of our device to sense touch input through the tongue when placed on the hard palate region in the mouth. Finally, we present a range of application demonstrators that span diverse domains, including accessibility, medical surgeries, and extended reality. These demonstrators showcase the versatility and potential of our platform, highlighting its ability to enable researchers and practitioners to explore new ways of leveraging the tongue’s unique capabilities. Overall, this work presents new opportunities to deploy tongue interfaces and has broad implications for designing interfaces that incorporate the tongue as a sensory organ.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {105},
numpages = {14},
keywords = {Epidermal Interfaces, Fingernail devices, Haptics, On-Body Interaction, Vibrotactile Actuation, Wearables},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3625468.3652192,
author = {De Fr\'{e}, Matthias and van der Hooft, Jeroen and Wauters, Tim and De Turck, Filip},
title = {Demonstrating Adaptive Many-to-Many Immersive Teleconferencing for Volumetric Video},
year = {2024},
isbn = {9798400704123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625468.3652192},
doi = {10.1145/3625468.3652192},
abstract = {In today's world, the use of video conferencing applications has risen significantly. However, with the introduction of affordable head-mounted displays (HMDs), users are now seeking new immersive and engaging experiences that enhance the 2D video conferencing applications with a third dimension. Immersive video formats such as light fields and volumetric video aim to enhance the experience by allowing for six degrees-of-freedom (6DoF), resulting in users being able to look and walk around in the virtual space. We present a novel, open source, many-to-many streaming architecture using point cloud-based volumetric video. To ensure bitrates that satisfy contemporary networks, the Draco codec encodes the point clouds before they are transmitted using web real-time communication (WebRTC), all while ensuring that the end-to-end latency remains acceptable for real-time communication. A multiple description coding (MDC)-based quality adaptation approach ensures that the pipeline can support a large number of users, each with varying network conditions.In this demo, participants will be seated around a table and will engage in a virtual conference using an HMD, with each participant being captured using a single depth camera. To showcase the quality effectiveness of the MDC-based adaptation algorithm, a dashboard is used to monitor the status of the application and control the bandwidth available to each participant. The available bandwidth and position of the user are taken into account to dynamically assign a quality level to each participant, ensuring a higher quality experience compared to having a uniform quality level for each point cloud object.},
booktitle = {Proceedings of the 15th ACM Multimedia Systems Conference},
pages = {453–458},
numpages = {6},
keywords = {Adaptive Streaming, Multiple Description Coding, Virtual Conferencing, Virtual Reality, Volumetric Video, WebRTC},
location = {Bari, Italy},
series = {MMSys '24}
}

@inproceedings{10.1145/3562939.3565677,
author = {Heo, Yong Hae and Kim, Seongho and Um, Juwon and An, Gyubin and Kim, Sang-Youn},
title = {Haptic Interaction Module for VR Fishing Leisure Activity},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565677},
doi = {10.1145/3562939.3565677},
abstract = {This paper presents a tiny haptic interaction module which generates high resistive torque for VR fishing leisure activity. The presented haptic interaction module was developed by magnetic rheological fluids and optimizing its structure. The measured haptic torque was varied from 0.3 N·cm to 2.4 N·cm as the applied voltage increased from 0 V to 5 V. The performance of the proposed actuator was qualitatively evaluated by constructing virtual fishing environment where a user can feel not only the weight of a target object but also its motion.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {61},
numpages = {2},
keywords = {Haptics, MR actuator, VR leisure activity, Virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3613905.3650830,
author = {Sehrt, Jessica and Yilmaz, Ugur and Kosch, Thomas and Schwind, Valentin},
title = {Closing the Loop: The Effects of Biofeedback Awareness on Physiological Stress Response Using Electrodermal Activity in Virtual Reality},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650830},
doi = {10.1145/3613905.3650830},
abstract = {This paper presents the results of a user study examining the impact of biofeedback awareness on the effectiveness of stress management, utilizing Electrodermal Activity (EDA) as the primary metric within an immersive Virtual Reality (VR). Employing a between-subjects design (N=30), we probed whether informing individuals of their capacity to manipulate the VR environment’s weather impacts their physiological stress responses. Our results indicate lower EDA levels of participants who were informed of their biofeedback control than those participants who were not informed about their biofeedback control. Interestingly, the participants who were informed about the control over the environment also manifested variations in their EDA responses. Participants who were not informed of their ability to control the weather showed decreased EDA measures until the end of the biofeedback phase. This study enhances our comprehension of the significance of awareness in biofeedback in immersive settings and its potential to augment stress management techniques.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {76},
numpages = {7},
keywords = {Awareness, Biofeedback, Electrodermal Activity, Stress, Virtual Reality},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3551349.3561160,
author = {Rafi, Tahmid and Zhang, Xueling and Wang, Xiaoyin},
title = {PredART: Towards Automatic Oracle Prediction of Object Placements in Augmented Reality Testing},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3561160},
doi = {10.1145/3551349.3561160},
abstract = {While the emerging Augmented Reality (AR) technique allows a lot of new application opportunities, from education and communication to gaming, current augmented apps often have complaints about their usability and/or user experience due to placement errors of virtual objects. Therefore, identifying noticeable placement errors is an important goal in the testing of AR apps. However, placement errors can only be perceived by human beings and may need to be confirmed by multiple users, making automatic testing very challenging. In this paper, we propose PredART, a novel approach to predict human ratings of virtual object placements that can be used as test oracles in automated AR testing. PredART is based on automatic screenshot sampling, crowd sourcing, and a hybrid neural network for image regression. The evaluation on a test set of 480 screenshots shows that our approach can achieve an accuracy of 85.0% and a mean absolute error, mean squared error, and root mean squared error of 0.047, 0.008, and 0.091, respectively.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {77},
numpages = {13},
keywords = {Augmented Reality, Placement Error, Virtual Objects},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3623809.3623853,
author = {Boucaud, Fabien and Pelachaud, Catherine and Thouvenin, Indira},
title = {"\"It patted my arm\": Investigating Social Touch from a Virtual Agent"},
year = {2023},
isbn = {9798400708244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623809.3623853},
doi = {10.1145/3623809.3623853},
abstract = {Endowing socially interactive virtual agents with the social touch modality could improve their emotional communication abilities and help them bond with human users. Touch is however a sensitive channel of communication that can have negative effects if used inappropriately. We present a first implementation of a system enabling an agent to perform social touch-based interactions and a preliminary study of the system dedicated to determining the factors that come into play for the social acceptability of an agent-initiated touch. The relationship between social touch with a virtual agent and sense of embodiment is also discussed. Based on the results of the study, we propose social (macro) and practical (micro) insights into how to produce meaningful, coherent and acceptable touch decisions (when to touch or not depending on the situation).},
booktitle = {Proceedings of the 11th International Conference on Human-Agent Interaction},
pages = {72–80},
numpages = {9},
keywords = {Embodiment, Social Acceptability, Social Touch, User Study, Virtual Reality},
location = {Gothenburg, Sweden},
series = {HAI '23}
}

@inproceedings{10.1145/3610978.3640681,
author = {Marques, Bernardo and Junqueira, Gon\c{c}alo and Alves, Jo\~{a}o and Pedrosa, Eurico},
title = {Mobile Robots Meet Augmented Reality Technologies: Transforming Human-Robot Interaction in Industry 4.0 Scenarios},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640681},
doi = {10.1145/3610978.3640681},
abstract = {Mobile robots combine digital technologies and automation, enabling a new era of operations in Industry 4.0 scenarios. These robots can navigate through assembly lines and logistics warehouse autonomously, making them catalysts for increased efficiency and productivity. Despite this, various challenges arise, including maintaining multiple robots up and running over large spaces, handling navigation during unexpected situations, ensuring operators safety, among others. To help overcome these, Augmented Reality (AR), a key pillar of Industry 4.0 can be used to assist with Human-Robot Interaction (HRI), contributing to a more productive and optimized logistics environment. This work proposes a framework for enhancing the understanding of the status of logistics robots through the use of a mobile AR tool. Its goal is to provide operators with an effective way of obtaining more information about the surrounding robots, as well as remote control them. The framework was iteratively tested during the various phases of design and development, allowing to integrate the feedback collected in each stage.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {740–744},
numpages = {5},
keywords = {augmented reality, human-robot interaction, industry 4.0, mobile robots},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/3526113.3545656,
author = {Wen, Elliott and Kaluarachchi, Tharindu Indrajith and Siriwardhana, Shamane and Tang, Vanessa and Billinghurst, Mark and Lindeman, Robert W. and Yao, Richard and Lin, James and Nanayakkara, Suranga},
title = {VRhook: A Data Collection Tool for VR Motion Sickness Research},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545656},
doi = {10.1145/3526113.3545656},
abstract = {Despite the increasing popularity of VR games, one factor hindering the industry’s rapid growth is motion sickness experienced by the users. Symptoms such as fatigue and nausea severely hamper the user experience. Machine Learning methods could be used to automatically detect motion sickness in VR experiences, but generating the extensive labeled dataset needed is a challenging task. It needs either very time consuming manual labeling by human experts or modification of proprietary VR application source codes for label capturing. To overcome these challenges, we developed a novel data collection tool, VRhook, which can collect data from any VR game without needing access to its source code. This is achieved by dynamic hooking, where we can inject custom code into a game’s run-time memory to record each video frame and its associated transformation matrices. Using this, we can automatically extract various useful labels such as rotation, speed, and acceleration. In addition, VRhook can blend a customized screen overlay on top of game contents to collect self-reported comfort scores. In this paper, we describe the technical development of VRhook, demonstrate its utility with an example, and describe directions for future research.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {82},
numpages = {9},
keywords = {Automatic Data Collection, Machine Learning, Motion Sickness, VR},
location = {Bend, OR, USA},
series = {UIST '22}
}

@article{10.1145/3582927,
author = {Zendle, David and Flick, Catherine and Deterding, Sebastian and Cutting, Joe and Gordon-Petrovskaya, Elena and Drachen, Anders},
title = {The Many Faces of Monetisation: Understanding the Diversity and Extremity of Player Spending in Mobile Games via Massive-scale Transactional Analysis},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3582927},
doi = {10.1145/3582927},
abstract = {With the rise of microtransactions, particularly in the mobile games industry, there has been ongoing concern that games reliant on these obtain substantial revenue from a small proportion of heavily involved individuals, to an extent that may be financially burdensome to these individuals. Yet despite substantive grey literature and speculation on this topic, there is little robust data available. We explore the revenue distribution in microtransaction-based mobile games using a transactional dataset of $4.7B in in-game spending drawn from 69,144,363 players of 2,873 mobile games over the course of 624 days. We find diverse revenue distributions in mobile games, ranging from a “uniform” cluster, in which all spenders invest approximately similar amounts, to “hyper-Pareto” games, in which a large proportion of revenue (approximately 38%) stems from 1% of spenders alone. Specific kinds of games are typified by higher spending: The more a game relies on its top 1% for revenue generation, the more these individuals tend to spend, with simulated gambling products (“social casinos”) at the top. We find a small subset of games across all genres, clusters, and age ratings in which the top 1% of gamers are highly financially involved—spending an average of $66,285 each in the 624 days under evaluation in the most extreme case. We discuss implications for future studies on links between gaming and wellbeing.},
journal = {ACM Games},
month = {mar},
articleno = {4},
numpages = {28},
keywords = {In-app spending, monetisation, video games, mobile games, mobile gaming}
}

@inproceedings{10.1145/3544548.3580754,
author = {Lee, Benjamin and Satyanarayan, Arvind and Cordeil, Maxime and Prouzeau, Arnaud and Jenny, Bernhard and Dwyer, Tim},
title = {Deimos: A Grammar of Dynamic Embodied Immersive Visualisation Morphs and Transitions},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580754},
doi = {10.1145/3544548.3580754},
abstract = {We present Deimos, a grammar for specifying dynamic embodied immersive visualisation morphs and transitions. A morph is a collection of animated transitions that are dynamically applied to immersive visualisations at runtime and is conceptually modelled as a state machine. It is comprised of state, transition, and signal specifications. States in a morph are used to generate animation keyframes, with transitions connecting two states together. A transition is controlled by signals, which are composable data streams that can be used to enable embodied interaction techniques. Morphs allow immersive representations of data to transform and change shape through user interaction, facilitating the embodied cognition process. We demonstrate the expressivity of Deimos in an example gallery and evaluate its usability in an expert user study of six immersive analytics researchers. Participants found the grammar to be powerful and expressive, and showed interest in drawing upon Deimos’ concepts and ideas in their own research.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {810},
numpages = {18},
keywords = {Immersive Analytics, animated transitions, data visualisation, embodied interaction, grammar, user study},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3613905.3650761,
author = {Zheng, Nianzhao and Li, Jialong and Li, Nianyu and Zhang, Mingyue and Cai, Jinyu and Tei, Kenji},
title = {Exploring Optimal eHMI Display Location for Various Vehicle Types: A VR User Study},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650761},
doi = {10.1145/3613905.3650761},
abstract = {External human-machine interfaces (eHMI) are anticipated to enhance pedestrian interactions with automated vehicles (AVs) and increase social acceptance. While previous studies have focused on enhancing eHMI design by exploring various modalities and color preferences, identifying the optimal eHMI location on vehicles remains a significant challenge. Existing research has begun to explore different display locations for eHMI, but a thorough examination of the factors related to vehicle type, such as vehicle size, that influence pedestrian preferences is still lacking. Therefore, we conducted a Virtual Reality user study to assess pedestrian reactions to various AVs equipped with eHMI in different positions. Our findings reveal that larger vehicles are perceived as less safe by pedestrians, yet their increased visibility from a distance, due to longer light bands, impacts pedestrians’ crossing decisions. Additionally, the location and height of the eHMI relative to eye level are crucial, with inappropriate positioning leading to diminished effectiveness.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {145},
numpages = {7},
keywords = {display location, eHMI, vehicle type, vehicle-pedestrian interaction},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3582437.3587214,
author = {Lankoski, Petri and Dymek, Mikolaj},
title = {Towards a History of Finnish and Swedish Game Industry Platforms},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587214},
doi = {10.1145/3582437.3587214},
abstract = {This paper looks at the history of game industry platforms in Finland and Sweden between 1979 and 2020 via 745 games. Both are relatively small countries where developers perform exceptionally well in a global market context. Developers and games developed in both countries are rather similar with some notable differences: Finnish developers focused on mobile games on Symbian in the 2000s, whereas Swedish developers focused on PC and console games, continuing a PC focus during the 2010s. The number of game companies has increased rapidly in Finland and Sweden since 2010 but peaked in Finland in 2014. From a platform studies perspective our data highlights rewarding historical insights about the dynamics of game industry platforms in Finland and Sweden with dimensions such as influence by demo scene, price of hardware/software (computers), mathematics education, third-party game engines, and finally higher education programmes in game development, consequently framing the data in socio-material perspectives on game industry platforms as application ecologies.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {57},
numpages = {4},
keywords = {Finland, Game industry history, Sweden, platforms},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3630970.3631060,
author = {Cardona-Rivera, Jose Luis and Cardona-Reyes, Hector and Alvarez-Rodriguez, Francisco Javier and Munoz-Arteaga, Jaime},
title = {Virtual Reality Environment for Surgical Skills Practice for Medical Students},
year = {2024},
isbn = {9798400716577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630970.3631060},
doi = {10.1145/3630970.3631060},
abstract = {This research is centered on the development of a virtual reality (VR) environment designed for practicing continuous simple suturing techniques, offering a solution to the challenges faced by medical students, including the constraints imposed by the pandemic, limited physical space, and shortages of necessary materials. The study encompasses five distinct phases: Research, Design, Development, Testing and Evaluation, and Analysis and Conclusions, each methodically pursued to attain the predetermined research objectives. The research project was executed through an experimental study, involving the administration of tests and the collection of both quantitative and qualitative data. Eleven fourth-semester medical students from the Autonomous University of Aguascalientes actively participated in the study, conducting tests and evaluating the performance and utility of the VR environment as an educational tool. The outcomes were notably positive, with participants affirming the utility and engagement value of the VR environment for knowledge acquisition. They underscored its remarkable capacity to provide a lifelike suturing practice experience, even within a virtual environment. This project introduces the initial iteration of the VR environment, showcasing its effectiveness as an educational instrument for suturing techniques. The findings substantiate its practicality, demonstrating that VR environments present an effective alternative to overcome the limitations inherent in surgical practice.},
booktitle = {Proceedings of the XI Latin American Conference on Human Computer Interaction},
articleno = {14},
numpages = {6},
keywords = {interactive environments, learning tool, medical students, suturing, virtual reality},
location = {Puebla, Mexico},
series = {CLIHC '23}
}

@inproceedings{10.1145/3544549.3585636,
author = {Liu, Bo and Wang, Wenyu and Zhang, Yuqing and Huang, Rui and Raiti, John},
title = {Lullaland: A Multisensory Virtual Reality Experience to Reduce Stress},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585636},
doi = {10.1145/3544549.3585636},
abstract = {People with health anxiety often experience feelings of nervousness or panic while in high-stress environments. While various techniques such as aromatherapy, and therapeutic video games have been shown to be effective in reducing anxiety, few studies have explored the use of a combination of these techniques in a multisensory approach. In this paper, we propose "Lullaland", a therapeutic virtual reality game that incorporates a wireless diffuser to provide an immersive, multisensory experience designed to help reduce anxiety in medical waiting rooms.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {198},
numpages = {6},
keywords = {HCI, aromatherapy, virtual reality, wearable},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3613905.3637114,
author = {de Souza, Paula Maia and Santos, Vin\'{\i}cius Matheus Romualdo and Alves, Bianca Alessandra de Souza and Proen\c{c}a, Fernando Roberto and Motti, Vivian Genaro and Rodrigues, Kamila Rios Da Hora and Neris, Vania Paula de Almeida},
title = {Case Study: End users in recovery from substance use disorders as designers and developers of digital games with therapeutic potential},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3637114},
doi = {10.1145/3613905.3637114},
abstract = {We propose a design process and a web system to support the design and development of digital games with therapeutic potential by people in recovery from Substance Use Disorders (SUDs). The proposal aims to generate more autonomy for end users as a means of scaling game design for therapy. This case study aims to evaluate the design process and the web system. We also assessed the autonomy of end users (patients). We employ a case study, as a methodological approach, conducting six meetings with design activities and the development of digital games with patients in recovery from SUDs and their therapists. Research data were collected through field diaries, observation, questionnaires, and interviews. Results suggest that participants were more autonomous with the support of the web system. This paper also features a discussion of the authors’ perception of the activities, design implications, and recommendations for future research in the same domain.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {510},
numpages = {6},
keywords = {Game design, alcohol and drugs, therapeutic games., vulnerable population},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3544549.3583892,
author = {Herbst, Yair and Wolf, Alon and Zelnik-Manor, Lihi},
title = {HUGO, a High-Resolution Tactile Emulator for Complex Surfaces},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583892},
doi = {10.1145/3544549.3583892},
abstract = {Many of our activities rely on tactile feedback perceived through mechanoreceptors in our skin. While visual and auditory devices provide immersive experiences, cutaneous feedback devices are typically limited in the range of sensations they provide and are hence usually used and tested on relatively simple synthetic surfaces. In this paper we demonstrate HUGO, a device designed in a human-centered process, triggering the mechanoreceptors sensitive to pressure, low-frequency vibrations, and high-frequency vibrations, enabling one to experience touch of surfaces “in-the-wild". The device is based on a parallel manipulator and a pin-array, that operate simultaneously at 200Hz and emulate coarse and fine geometrical features, respectively. The decomposition into coarse and fine features, alongside the high operation frequency, enable simulation of virtual surfaces. HUGO will be showcased in multiple applications such as social interactions, e-commerce and gaming to allow haptic interaction with real-world surfaces.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {458},
numpages = {4},
keywords = {Haptic Textures, Haptics, High-Resolution Haptics, Human Computer Interface, User Study},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3613904.3642615,
author = {Nair, Vishnu and Zhu, Hanxiu 'Hazel' and Song, Peize and Wang, Jizhong and Smith, Brian A.},
title = {Surveyor: Facilitating Discovery Within Video Games for Blind and Low Vision Players},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642615},
doi = {10.1145/3613904.3642615},
abstract = {Video games are increasingly accessible to blind and low vision (BLV) players, yet many aspects remain inaccessible. One aspect is the joy players feel when they explore environments and make new discoveries, which is integral to many games. Sighted players experience discovery by surveying environments and identifying unexplored areas. Current accessibility tools, however, guide BLV players directly to items and places, robbing them of that experience. Thus, a crucial challenge is to develop navigation assistance tools that also foster exploration and discovery. To address this challenge, we propose the concept of exploration assistance in games and design Surveyor, an in-game exploration assistance tool that enhances discovery by tracking where BLV players look and highlighting unexplored areas. We designed Surveyor using insights from a formative study and compared Surveyor’s effectiveness to approaches found in existing accessible games. Our findings reveal implications for facilitating richer play experiences for BLV users within games.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {10},
numpages = {15},
keywords = {blind-accessible video games, blindness and low vision, exploration within virtual environments},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3658852.3658864,
author = {Pajala-Assefa, Hanna},
title = {Choreographing in VR: Introducing ‘Substitute Performers’ as Informants in the Choreographic Process},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658852.3658864},
doi = {10.1145/3658852.3658864},
abstract = {This paper is situated at the intersection of digital choreography and human-centered interaction within the scope of virtual reality. Both disciplines are discussed under the frame of contemporary artistic practice, an endeavor to make an artistic work while appropriating skills in crafting and composition. The paper focuses on the creation practice in and for virtual reality and the making of an artwork rooted in movement and with a choreographic idea of embedding an improvisatory score into the interaction design. Therefore, the thinking of choreography expands into the design of the interactions in the virtual realm and falls into the subfield of digital choreography.To reflect on the agent-entangled practice of digital choreography within VR and to trace the roles of various performative, epistemic agents active within the creative process, the author introduces a notion of substitute performers. The methods of phenomenology of lived mediated experience and digital choreography are synthesized to optimize the artistic endeavor to design for diverse bodies. These are then combined with various ethnographic methods to provide insights into moving and dancing bodies in a mediated performative realm. These intertwined methods were an elemental part of the creative process of the VR artwork Skeleton Conductor XR Art which was designed to induce pleasure through improvisational engagement and cultivate kinaesthetic awareness.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {13},
numpages = {8},
keywords = {Digital Choreography, Embodied design practice, Expanded Ethnography, Improvisatory systems},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3548771.3561409,
author = {DeCusatis, Casimer and Alvarico, Erin and Dirahoui, Omar},
title = {Gamification of cybersecurity training},
year = {2022},
isbn = {9781450394543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548771.3561409},
doi = {10.1145/3548771.3561409},
abstract = {A large fraction of cybercrimes could be prevented with improved cybersecurity awareness training. We have developed a virtual cybersecurity escape room based on the three-dimensional Unity game development platform. This application is based on the proven Octalysis gamification framework, which has been shown to improve user engagement and knowledge retention. Following a discussion of the application design, this position paper presents playtesting results, work in progress, and experimental quantification based on eight gamification metrics.},
booktitle = {Proceedings of the 1st International Workshop on Gamification of Software Development, Verification, and Validation},
pages = {10–13},
numpages = {4},
keywords = {Cybersecurity, Escape Room, Gamification, Octalysis},
location = {Singapore, Singapore},
series = {Gamify 2022}
}

@inproceedings{10.1145/3613904.3642648,
author = {Ding, Yaohan and Jia, Lesong and Du, Na},
title = {One Size Does Not Fit All: Designing and Evaluating Criticality-Adaptive Displays in Highly Automated Vehicles},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642648},
doi = {10.1145/3613904.3642648},
abstract = {To promote drivers’ overall experiences in highly automated vehicles, we designed three objective criticality-adaptive displays: IO display highlighting Influential Objects, CO display highlighting Critical Objects, and ICO display highlighting Influential and Critical Objects differently. We conducted an online video-based survey study with 295 participants to evaluate them in varying traffic conditions. Results showed that low-trust propensity participants found ICO display more useful while high-trust propensity participants found CO displays more useful. When interacting with vulnerable road users (VRUs), participants had higher situational awareness (SA) but worse non-driving related task (NDRT) performance. Aging and CO displays also led to slower NDRT reactions. Nonetheless, older participants found displays more useful. We recommend providing different criticality-adaptive displays based on drivers’ trust propensity, age, and NDRT choice to enhance driving and NDRT performance and suggest carefully treating objects of different categories in traffic.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {92},
numpages = {15},
keywords = {adaptive display, automated vehicles, criticality, individual differences, situational awareness, traffic density, trust, usability.},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3550469.3555378,
author = {Han, Shangchen and Wu, Po-Chen and Zhang, Yubo and Liu, Beibei and Zhang, Linguang and Wang, Zheng and Si, Weiguang and Zhang, Peizhao and Cai, Yujun and Hodan, Tomas and Cabezas, Randi and Tran, Luan and Akbay, Muzaffer and Yu, Tsz-Ho and Keskin, Cem and Wang, Robert},
title = {UmeTrack: Unified multi-view end-to-end hand tracking for VR},
year = {2022},
isbn = {9781450394703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550469.3555378},
doi = {10.1145/3550469.3555378},
abstract = {Real-time tracking of 3D hand pose in world space is a challenging problem and plays an important role in VR interaction. Existing work in this space are limited to either producing root-relative (versus world space) 3D pose or rely on multiple stages such as generating heatmaps and kinematic optimization to obtain 3D pose. Moreover, the typical VR scenario, which involves multi-view tracking from wide field of view (FOV) cameras is seldom addressed by these methods. In this paper, we present a unified end-to-end differentiable framework for multi-view, multi-frame hand tracking that directly predicts 3D hand pose in world space. We demonstrate the benefits of end-to-end differentiabilty by extending our framework with downstream tasks such as jitter reduction and pinch prediction. To demonstrate the efficacy of our model, we further present a new large-scale egocentric hand pose dataset that consists of both real and synthetic data. Experiments show that our system trained on this dataset handles various challenging interactive motions, and has been successfully applied to real-time VR applications.},
booktitle = {SIGGRAPH Asia 2022 Conference Papers},
articleno = {50},
numpages = {9},
keywords = {hand tracking, motion capture, virtual reality},
location = {Daegu, Republic of Korea},
series = {SA '22}
}

@inproceedings{10.1145/3616195.3616208,
author = {Bouillot, Nicolas and Piquet, Thomas and Gilbert, Pierre},
title = {Audiodice: an open hardware design of a distributed dodecahedron loudspeaker orchestra},
year = {2023},
isbn = {9798400708183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616195.3616208},
doi = {10.1145/3616195.3616208},
abstract = {We present a new speaker array composed of five spherical speakers with 12 independent channels each. The prototype is open source and design choices are motivated here. It is designed to be a flexible device allowing a wide range of use cases, as described in more detail in the paper: simultaneous rendering with surround speaker arrays, artistic installations and acoustical measurements. The sources in the repository include filter impulse response for frequency response correction. The measurement methodology, based on sine sweeps, is documented and allows the reader to reproduce the measurement and correction. Finally, the paper describes several use cases for which feedback is provided, and demonstrates the versatility, mobility, and ease of deployment provided by our proposed implementation.},
booktitle = {Proceedings of the 18th International Audio Mostly Conference},
pages = {154–160},
numpages = {7},
keywords = {audio spatialization, deployability, open source hardware, reuse, speaker array},
location = {Edinburgh, United Kingdom},
series = {AM '23}
}

@inproceedings{10.1145/3613905.3637128,
author = {Bursztein, Elie and Brown, Karla J and Sanderson, Leonie M and Kelley, Patrick Gage},
title = {Leveraging Virtual Reality to Enhance Diversity, Equity and Inclusion training at Google},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3637128},
doi = {10.1145/3613905.3637128},
abstract = {Virtual reality (VR) has emerged as a promising educational training method, offering a more engaging and immersive experience than traditional approaches. In this case study, we explore its effectiveness for diversity, equity, and inclusion (DEI) training, with a focus on how VR can help participants better understand and appreciate different perspectives. We describe the design and development of a VR training application that aims to raise awareness about unconscious biases and promote more inclusive behaviors in the workplace. We report initial findings based on the feedback of Google employees who took our training and found that VR appears to be an effective way to enhance DEI training. In particular, participants reported that VR training helped them better recognize biases and how to effectively respond to them. However, our findings also highlight some challenges with VR-based DEI training, which we discuss in terms of future research directions.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {522},
numpages = {7},
keywords = {DEI training, diversity, equity, inclusion, interactive learning, virtual reality, workplace},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3631085.3631236,
author = {Rossato, Lana Bertoldo and Bombardelli, Leonardo Boaventura and Tavares, Anderson Rocha},
title = {Evolutionary Tabletop Game Design: A Case Study in the Risk Game},
year = {2024},
isbn = {9798400716270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631085.3631236},
doi = {10.1145/3631085.3631236},
abstract = {Creating and evaluating games manually is an arduous and laborious task. Procedural content generation can aid by creating game artifacts, but usually not an entire game. Evolutionary game design, which combines evolutionary algorithms with automated playtesting, has been used to create novel board games with simple equipment; however, the original approach does not include complex tabletop games with dice, cards, and maps. This work proposes an extension of the approach for tabletop games, evaluating the process by generating variants of Risk, a military strategy game where players must conquer map territories to win. We achieved this using a genetic algorithm to evolve the chosen parameters, as well as a rules-based agent to test the games and a variety of quality criteria to evaluate the new variations generated. Our results show the creation of new variations of the original game with smaller maps, resulting in shorter matches. Also, the variants produce more balanced matches, maintaining the usual drama. We also identified limitations in the process, where, in many cases, where the objective function was correctly pursued, but the generated games were nearly trivial. This work paves the way towards promising research regarding the use of evolutionary game design beyond classic board games.},
booktitle = {Proceedings of the 22nd Brazilian Symposium on Games and Digital Entertainment},
pages = {161–170},
numpages = {10},
keywords = {Evolutionary Game Design, Genetic Algorithm, Risk},
location = {Rio Grande (RS), Brazil},
series = {SBGames '23}
}

@inproceedings{10.1145/3615886.3627746,
author = {Liang, Haolin and Newsam, Shawn},
title = {Plane Segmentation in Outdoor Imagery: Unsupervised Training Using Synthetic Data},
year = {2023},
isbn = {9798400703485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615886.3627746},
doi = {10.1145/3615886.3627746},
abstract = {Plane segmentation in monocular images plays an important role in understanding the geometry of a 3D environment. However, most previous plane segmentation work focuses mostly on indoor environments since it is hard to create precise planar annotations of outdoor environments for training. In this work, we proposed a framework to automatically generate synthetic outdoor planar annotations data for any given 3D building model, that can be used for outdoor plane segmentation and geometry understanding tasks. We also conduct experiments to evaluate the applicability of the synthetic data to real-world scenarios. We show that plane segmentation models trained on synthetic outdoor data can also be used to understand real-world outdoor geometry.},
booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {95–101},
numpages = {7},
keywords = {3D computer vision, plane segmentation, synthetic data},
location = {Hamburg, Germany},
series = {GeoAI '23}
}

@inproceedings{10.1145/3526114.3558633,
author = {Zhu, Junyi and Lei, Yuxuan and Shah, Aashini and Schein, Gila and Ghaednia, Hamid and Schwab, Joseph and Harteveld, Casper and Mueller, Stefanie},
title = {Monitoring Muscle Engagement via Electrical Impedance Tomography for Unsupervised Physical Rehabilitation},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558633},
doi = {10.1145/3526114.3558633},
abstract = {In this demo, we present MuscleRehab, a virtual reality rehabilitation system that tracks user’s motion via optical motion tracking and user’s muscle engagement via electrical impedance tomography (EIT) and visualizes the data on a virtual muscle-skeleton avatar. By deploying MuscleRehab, We investigate if monitoring and visualizing muscle engagement during unsupervised physical rehabilitation improves the execution accuracy of therapeutic exercises by showing users whether they target the right muscle groups. The results indicate that monitoring and visualizing muscle engagement can improve both the therapeutic exercise accuracy for users during rehabilitation, and post-rehabilitation evaluation for physical therapists. We introduce each element of MuscleRehab system, and demonstrate our custom wearable EIT devices with phantom and on-body setups.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {110},
numpages = {3},
keywords = {EIT, health sensing, muscle engagement., physical rehabilitation},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3584931.3606976,
author = {Kim, Jieun and Sandhaus, Hauke and Fussell, Susan R.},
title = {VR Job Interview Using a Gender-Swapped Avatar},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584931.3606976},
doi = {10.1145/3584931.3606976},
abstract = {Virtual Reality (VR) has emerged as a potential solution for mitigating bias in a job interview by hiding the applicants’ demographic features. The current study examines the use of a gender-swapped avatar in a virtual job interview that affects the applicants’ perceptions and their performance evaluated by recruiters. With a mixed-method approach, we first conducted a lab experiment (N=8) exploring how using a gender-swapped avatar in a virtual job interview impacts perceived anxiety, confidence, competence, and ability to perform. Then, a semi-structured interview investigated the participants’ VR interview experiences using an avatar. Our findings suggest that using gender-swapped avatars may reduce the anxiety that job applicants will experience during the interview. Also, the affinity diagram produced seven key themes highlighting the advantages and limitations of VR as an interview platform. These findings contribute to the emerging field of VR-based recruitment and have practical implications for promoting diversity and inclusion in the hiring process.},
booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {154–159},
numpages = {6},
keywords = {Gender Bias, Gender-Swapped Avatar, Mixed-method, VR Job Interview},
location = {Minneapolis, MN, USA},
series = {CSCW '23 Companion}
}

@inproceedings{10.1145/3613905.3650753,
author = {Kim, Chan Mi and Van Rompay, Thomas and Ludden, Geke},
title = {Outside In: Creating Digital Nature Tailored To The Needs of Intensive Care Unit Patients},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650753},
doi = {10.1145/3613905.3650753},
abstract = {Intensive care unit (ICU) environments play a crucial role in supporting patients’ recovery, and nature experiences, particularly their visual elements, are commonly used in ICUs to stimulate relaxation. Fueled by digital technology, applications of virtual nature have emerged to bring nature to environments without direct access, including windowless ICU rooms. Despite its healing potential, there is a lack of consensus and strategy in designing virtual nature catering to the diverse patients’ needs. This study investigates how to create virtual nature for intended effects promoting relaxation. Informed by a framework explaining the working mechanism underlying relaxation in nature, we introduce Digital Nature, a visual stimulation featuring a 24-hour streaming video of constantly changing virtual scenes. We describe how we incorporated an evidence-based approach into the design process of Digital Nature. A pilot study is planned to validate the effectiveness of Digital Nature, and ideas for further design implications are discussed.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {260},
numpages = {7},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3613905.3650988,
author = {Lee, Sang-Hyun and Lee, Joon Hyub and Bae, Seok-Hyung},
title = {Bimanual Interactions for Surfacing Curve Networks in VR},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650988},
doi = {10.1145/3613905.3650988},
abstract = {We propose an interactive system for authoring 3D curve and surface networks using bimanual interactions in virtual reality (VR) inspired by physical wire bending and film wrapping. In our system, the user can intuitively author 3D shapes by performing a rich vocabulary of interactions arising from a minimal gesture grammar based on hand poses and firmness of hand poses for constraint definition and object manipulation. Through a pilot test, we found that the user can quickly and easily learn and use our system and become immersed in 3D shape authoring.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {60},
numpages = {7},
keywords = {B\'{e}zier curve network, VR, bimanual gesture, surface modeling},
location = {
},
series = {CHI EA '24}
}

@article{10.1145/3571249,
author = {Das, Ria and Tenenbaum, Joshua B. and Solar-Lezama, Armando and Tavares, Zenna},
title = {Combining Functional and Automata Synthesis to Discover Causal Reactive Programs},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571249},
doi = {10.1145/3571249},
abstract = {We present a new algorithm that synthesizes functional reactive programs from observation data. The key novelty is to iterate between a functional synthesis step, which attempts to generate a transition function over observed states, and an automata synthesis step, which adds any additional latent state necessary to fully account for the observations. We develop a functional reactive DSL called Autumn that can express a rich variety of causal dynamics in time-varying, Atari-style grid worlds, and apply our method to synthesize Autumn programs from data. We evaluate our algorithm on a benchmark suite of 30 Autumn programs as well as a third-party corpus of grid-world-style video games. We find that our algorithm synthesizes 27 out of 30 programs in our benchmark suite and 21 out of 27 programs from the third-party corpus, including several programs describing complex latent state transformations, and from input traces containing hundreds of observations. We expect that our approach will provide a template for how to integrate functional and automata synthesis in other induction domains.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {56},
numpages = {31},
keywords = {automata, causal, reactive, synthesis}
}

@inproceedings{10.1145/3524494.3527628,
author = {Ullmann, Gabriel C. and Politowski, Cristiano and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Petrillo, Fabio},
title = {What makes a game high-rated? towards factors of video game success},
year = {2022},
isbn = {9781450392938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524494.3527628},
doi = {10.1145/3524494.3527628},
abstract = {As the video game market grows larger, it becomes harder for games to stand out from the crowd. Launching a successful game encompasses different factors, some of which are not well-known. In this paper, we investigate some factors that affect game scores, considering high-rated video games from a dataset of 200 projects. Results show that smaller team sizes are often linked to higher scores. On the other hand, the level of freedom given to developers, as well as genre, graphical perspective, game modes and platforms do not correlate to score. Additionally, teams from successful games also experience more crunch time while fewer problems with schedule and budget allocation. Further analysis shows that team, technical, and game design factors should be the main focus of the game developers.},
booktitle = {Proceedings of the 6th International ICSE Workshop on Games and Software Engineering: Engineering Fun, Inspiration, and Motivation},
pages = {16–23},
numpages = {8},
keywords = {development, project, software, success, video-games},
location = {Pittsburgh, Pennsylvania},
series = {GAS '22}
}

@inproceedings{10.1145/3675231.3675233,
author = {Bourgaize, Sheryl and Cinelli, Michael and Raimbaud, Pierre and Hoyet, Ludovic and Olivier, Anne-H\'{e}l\`{e}ne},
title = {Collision avoidance behaviours of young adult walkers: Influence of a virtual pedestrian's age-related appearance and gait profile},
year = {2024},
isbn = {9798400710612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675231.3675233},
doi = {10.1145/3675231.3675233},
abstract = {Populating virtual words with realistic pedestrian behaviours is challenging but essential to ensure natural interactions with users. In pedestrian encounters, successful collision avoidance requires adapting speed and/or locomotor trajectory based on situational- and personal-characteristics. Personal characteristics such as the age of a pedestrian can easily be observed, but whether it affects avoidance behaviours is unknown. This is however an important question to ensure realism and variety of the simulations. The purpose of the current study was to examine the influence of a virtual pedestrian’s (VP) age-related appearance and gait profile on avoidance behaviours during a circumvention task. We expected that older adult (OA) gait characteristics and/or appearance would result in more cautious behaviours. Young adults (YA; n=17, 23.6 ± 2.7yrs) navigated a virtual street using a HMD. Individuals walked 8m towards a goal, while avoiding an approaching VP who would approach and steer towards the participant’s left, right, or continue straight, while exhibiting different age-related appearances and gait profiles: 1. YA appearance, YA gait; 2. OA appearance, OA gait; 3. OA appearance, YA gait; and 4. YA appearance, OA gait. Results indicate that clearance was larger when the appearance of VP resembled an OA, and when the VP walked like an OA compared to a YA. Larger clearance distances observed with OA characteristics may be due to societal norms associated with the principle of parental respect as well as a cautious strategy for any potential instability (wavering) in balance commonly observed in OA. This research sheds light on how age-related cues influence pedestrian interactions, with implications for the design of populated virtual environments.},
booktitle = {ACM Symposium on Applied Perception 2024},
articleno = {14},
numpages = {10},
keywords = {Collision Avoidance, Human locomotion, Older Adults, Proxemics, Virtual environments, Visual perception},
location = {Dublin, Ireland},
series = {SAP '24}
}

@inproceedings{10.1145/3623264.3624438,
author = {Ferstl, Ylva},
title = {Generating Emotionally Expressive Look-At Animation},
year = {2023},
isbn = {9798400703935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623264.3624438},
doi = {10.1145/3623264.3624438},
abstract = {Humanoid characters in video games are generally animated using motion capture technology, enabling high-quality, realistic animation. This animation has to remain interactive and allow characters to react to their environment; one important component of this adaptation are look-at animations which direct the character’s torso towards an object or person of interest. Look-at animations are generated procedurally in order to handle any desired target direction, however, this procedural animation can have a robotic quality and negatively affect the overall perceived realism of the character. In this work, we present a neural network controller for generating look-at animations that are equally as appealing as motion capture while requiring minimal memory. Moreover, our controller can generate animations stylized by emotion, allowing characters to react to look-at targets depending on their context, and this style expressiveness is shown to be on par with motion captured samples.},
booktitle = {Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games},
articleno = {15},
numpages = {6},
keywords = {computer animation, expressive agents, gaze animation, machine learning},
location = {Rennes, France},
series = {MIG '23}
}

@inproceedings{10.5555/3545946.3598620,
author = {Lin, Fanqi and Huang, Shiyu and Pearce, Tim and Chen, Wenze and Tu, Wei-Wei},
title = {TiZero: Mastering Multi-Agent Football with Curriculum Learning and Self-Play},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent football poses an unsolved challenge in AI research. Existing work has focused on tackling simplified scenarios of the game, or else leveraging expert demonstrations. In this paper, we develop a multi-agent system to play the full 11 vs. 11 game mode, without demonstrations. This game mode contains aspects that present major challenges to modern reinforcement learning algorithms; multi-agent coordination, long-term planning, and non-transitivity. To address these challenges, we present TiZero; a self-evolving, multi-agent system that learns from scratch. TiZero introduces several innovations, including adaptive curriculum learning, a novel self-play strategy, and an objective that optimizes the policies of multiple agents jointly. Experimentally, it outperforms previous systems by a large margin on the Google Research Football environment, increasing win rates by over 30%. To demonstrate the generality of TiZero's innovations, they are assessed on several environments beyond football; Overcooked, Multi-agent Particle-Environment, Tic-Tac-Toe and Connect-Four.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {67–76},
numpages = {10},
keywords = {google research football, large-scale training, multi-agent reinforcement learning, self-play},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3613904.3642859,
author = {Bensch, Leonie and Nilsson, Tommy and Wulkop, Jan and Demedeiros, Paul and Herzberger, Nicolas Daniel and Preutenborbeck, Michael and Gerndt, Andreas and Flemisch, Frank and Dufresne, Florian and Albuquerque, Georgia and Cowley, Aidan},
title = {Designing for Human Operations on the Moon: Challenges and Opportunities of Navigational HUD Interfaces},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642859},
doi = {10.1145/3613904.3642859},
abstract = {Future crewed missions to the Moon will face significant environmental and operational challenges, posing risks to the safety and performance of astronauts navigating its inhospitable surface. Whilst head-up displays (HUDs) have proven effective in providing intuitive navigational support on Earth, the design of novel human-spaceflight solutions typically relies on costly and time-consuming analogue deployments, leaving the potential use of lunar HUDs largely under-explored. This paper explores an alternative approach by simulating navigational HUD concepts in a high-fidelity Virtual Reality (VR) representation of the lunar environment. In evaluating these concepts with astronauts and other aerospace experts (n=25), our mixed methods study demonstrates the efficacy of simulated analogues in facilitating rapid design assessments of early-stage HUD solutions. We illustrate this by elaborating key design challenges and guidelines for future lunar HUDs. In reflecting on the limitations of our approach, we propose directions for future design exploration of human-machine interfaces for the Moon.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {718},
numpages = {21},
keywords = {astronaut, augmented reality, head-up display, human factors, human space flight, human-system exploration, lunar exploration, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3561212.3561216,
author = {Frank, Matthias and Perinovic, Djordje},
title = {Matching auditory and visual room size, distance, and source orientation in virtual reality},
year = {2022},
isbn = {9781450397018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561212.3561216},
doi = {10.1145/3561212.3561216},
abstract = {Matching visual and auditory cues in virtual reality (VR) is important to provide plausibility and create the impression of presence in a scene. This paper presents an experiment in VR, in which participants match acoustic and visual room size, distance, and orientation of a directive sound source in a simulated concert hall. The simulation is fully interactive and allows the participants to move with 6 degrees of freedom. For all three parameters, the experiment was done in both directions: adjusting the acoustic parameters to given visual settings and adjusting the visual parameters to given acoustic settings. The results show that the adjustment generally works in both directions. However, for distance the auditory adjustment works better and does not reveal the typical compression. Regarding room size, results agree with just noticeable differences in reverberation time known from real-world experiments.},
booktitle = {Proceedings of the 17th International Audio Mostly Conference},
pages = {80–83},
numpages = {4},
keywords = {interactive experiment, room acoustics, virtual reality},
location = {St. P\"{o}lten, Austria},
series = {AM '22}
}

@inproceedings{10.1145/3623462.3630609,
author = {Alaa, Mostafa and Hammouda, Nada and Abdennadher, Slim},
title = {The effect of culture in educational games for school students},
year = {2023},
isbn = {9798400708367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623462.3630609},
doi = {10.1145/3623462.3630609},
abstract = {In this study, we aimed to determine whether incorporating a student’s cultural background into gamified educational content would positively impact their learning achievements and overall experience. To do so, we created two versions of a game - one that reflected Egyptian culture and one that did not - while keeping the learning content consistent. Through experimentation and analysis, our findings showed that students who played the culturally relevant Egyptian version demonstrated greater learning achievements and overall satisfaction compared to those who played the other version.},
booktitle = {Proceedings of the 20th International Conference on Culture and Computer Science: Code and Materiality},
articleno = {19},
numpages = {10},
keywords = {Culture, E-Learning, Education, Gamification, Serious Games},
location = {Lisbon, Portugal},
series = {KUI '23}
}

@inproceedings{10.1145/3627611.3627625,
author = {Overdijk, Maarten and Oostdijk, Aaron and Van Dam, Rob},
title = {Circulation in Virtual Media Architecture: Exploring the Spatial Logic of a Continuous Circulation Space},
year = {2024},
isbn = {9798400716355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627611.3627625},
doi = {10.1145/3627611.3627625},
abstract = {In this Pictorial we describe a design experiment aimed at prototyping and exploring a virtual media architecture. We describe how we developed a prototype of a continuous circulation space that allows grounded movement, visual continuity and transitions between modes of the reality-virtuality continuum using an HMD (head-mounted display) and a technique called Stencil Rendering. Our prototype, situated in an architectural environment and supported by a narrative that provides a direction and context of use, enables us to explore the logic of circulation from direct experience. Throughout this Pictorial we identify and reflect on several principles of this logic and define the circulation space as a composite of actual and virtual that unfolds interactively as the visitor moves through it. Our approach shows a way to experiment with the notion of virtual media architecture and to speculate about its possible use.},
booktitle = {Proceedings of the 6th Media Architecture Biennale Conference},
pages = {133–145},
numpages = {13},
location = {Toronto, ON, Canada},
series = {MAB '23}
}

@inproceedings{10.1145/3544549.3585860,
author = {Becker, Leonie and Nilsson, Tommy and Demedeiros, Paul and Rometsch, Flavie},
title = {Augmented Reality in Service of Human Operations on the Moon: Insights from a Virtual Testbed},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585860},
doi = {10.1145/3544549.3585860},
abstract = {Future astronauts living and working on the Moon will face extreme environmental conditions impeding their operational safety and performance. While it has been suggested that Augmented Reality (AR) Head-Up Displays (HUDs) could potentially help mitigate some of these adversities, the applicability of AR in the unique lunar context remains underexplored. To address this limitation, we have produced an accurate representation of the lunar setting in virtual reality (VR) which then formed our testbed for the exploration of prospective operational scenarios with aerospace experts. Herein we present findings based on qualitative reflections made by the first 6 study participants. AR was found instrumental in several use cases, including the support of navigation and risk awareness. Major design challenges were likewise identified, including the importance of redundancy and contextual appropriateness. Drawing on these findings, we conclude by outlining directions for future research aimed at developing AR-based assistive solutions tailored to the lunar setting.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {43},
numpages = {8},
keywords = {astronaut, augmented reality, head-up display, human factors, human space flight, lunar exploration, virtual reality},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@proceedings{10.1145/3656650,
title = {AVI '24: Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {AVI 2024 is the 17th edition of the International Conference on Advanced Visual Interfaces, held in Arenzano, Genoa (IT), in cooperation with ACM, ACM SIGCHI, ACM SIGMM, and ACM SIGWEB.Every two years since 1992, AVI has gathered a vast international community of experts with a wide range of backgrounds. Throughout three decades, AVI has gained and holds a prestigious position among International HCI conferences, boasting a dedicated nucleus of returning participants, but also providing a venue for young researchers to show their achievements and establish contacts with senior community members.AVI 2024 presents a broad and sound scientific program covering traditional AVI topics on information and data visualization, interaction with multimodal user interfaces, augmented and virtual reality, while also addressing emerging topics including the application of generative artificial intelligence in HCI design and evaluation.The program features the presentation of 21 long research papers and 28 short papers selected through a rigorous double-blind reviewing process and organized into sessions on 13 main topics. Furthermore, it includes the presentation of 48 poster papers, 9 demo papers, and 11 doctoral consortium papers, selected through a single-blind reviewing process. Finally, the rich and vibrant program includes 3 keynote talks, 3 tutorials, and 10 workshops addressing some of the most exciting issues in HCI.Submissions to AVI 2024 came from 34 different countries distributed in descending order in Europe, Asia, North America, South America, and Africa.},
location = {Arenzano, Genoa, Italy}
}

@inproceedings{10.1145/3573381.3597218,
author = {Ennadifi, Elias and Ravet, Thierry and Mancas, Matei and El Amine Mokhtari, Mohammed and Gosselin, Bernard},
title = {Enhancing VR Gaming Experience using Computational Attention Models and Eye-Tracking},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3597218},
doi = {10.1145/3573381.3597218},
abstract = {This study explores the potential of enhancing interaction experiences, such as virtual reality (VR) games, through the use of computational attention models. Our proposed approach utilizes a saliency map generated by attention models to dynamically adjust game difficulty levels and to help in the game level design, resulting in a more immersive and engaging experience for users. To inform the development of this approach, we present an experimental setup that is able tp collect data in a VR environment and intends to be able to validate the adaptation of attention models to this domain. Through this work, we aim to create a framework for VR game design that leverages attention models to offer a new level of immersion and engagement for users. We believe our contributions have significant potential to enhance VR experiences and advance the field of game design.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {194–198},
numpages = {5},
keywords = {Computational attention models, Difficulty adjustment, Eye-tracking, Salience map, Virtual reality},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3582515.3609553,
author = {Nguyen, Quynh and Pretolesi, Daniele and Gallhuber, Katja},
title = {Collaborative Scenario Builder: A VR Co-Design Tool for Medical First Responders},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582515.3609553},
doi = {10.1145/3582515.3609553},
abstract = {The rising occurrence of natural and human-made disasters emphasises the urgent need for effective training of medical first responders (MFRs). Virtual Reality (VR) has recently been used to enhance traditional MFR training. However, to ensure that VR training improves disaster preparedness, it is not only crucial for MFR stakeholders to actively participate in the design process. It may also be beneficial to place the co-design process in VR so that novice co-designers establish a profound, hands-on understanding of VR as a training tool. Thus, we introduce the Collaborative Scenario Builder (CSB), a prototype for MFRs without technical and designerly expertise with which to co-design scenarios for virtual simulation training in VR. An evaluation with 33 MFR participants indicates that CSB is usable and provides participants with an embodied understanding of VR, leading to new perspectives in their collaborative design considerations. Thus, CSB contributes to a co-design workflow with MFR co-designers that ensures that created VR training tools are needed and beneficial for MFRs so that they can provide better aid to people in the face of disasters.},
booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
pages = {342–350},
numpages = {9},
keywords = {Co-Design, Medical First Responders, Virtual Reality, Virtual Simulation Training},
location = {Lisbon, Portugal},
series = {GoodIT '23}
}

@inproceedings{10.1145/3587423.3595476,
author = {Kenwright, Benjamin},
title = {Real-Time Ray-Tracing with Vulkan for the Impatient},
year = {2023},
isbn = {9798400701450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587423.3595476},
doi = {10.1145/3587423.3595476},
abstract = {The author accepts no responsibility for the accuracy, completeness or quality of the information provided, nor for ensuring that it is up to date. Liability claims against the author relating to material or non-material damages arising from the information provided being used or not being used or from the use of inaccurate and incomplete information are excluded if there was no intentional or gross negligence on the part of the author. The author expressly retains the right to change, add to or delete parts of the text or the whole text without prior notice or to withdraw the information temporarily or permanently.},
booktitle = {ACM SIGGRAPH 2023 Courses},
articleno = {14},
numpages = {244},
location = {Los Angeles, California},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3573381.3596155,
author = {Robotham, Thomas and Singla, Ashutosh and Raake, Alexander and Rummukainen, Olli S. and Habets, Emanu\"{e}l A. P.},
title = {Influence of Multi-Modal Interactive Formats on Subjective Audio Quality and Exploration Behavior},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596155},
doi = {10.1145/3573381.3596155},
abstract = {This study uses a mixed between- and within-subjects test design to evaluate the influence of interactive formats on the quality of binaurally rendered 360° spatial audio content. Focusing on ecological validity using real-world recordings of 60&nbsp;s duration, three independent groups of subjects () were exposed to three formats: audio only (A), audio with 2D visuals (A2DV), and audio with head-mounted display (AHMD) visuals. Within each interactive format, two sessions were conducted to evaluate degraded audio conditions: bit-rate and Ambisonics order. Our results show a statistically significant effect (p &lt; .05) of format only on spatial audio quality ratings for Ambisonics order. Exploration data analysis shows that format A yields little variability in exploration, while formats A2DV and AHMD yield broader viewing distribution of 360° content. The results imply audio quality factors can be optimized depending on the interactive format.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {115–128},
numpages = {14},
keywords = {Behavior, Exploration, Method, Multi-modal, Quality, Spatial Audio},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3652212.3652215,
author = {Herforth, Johannes G\"{u}nter and Botev, Jean},
title = {A3Cplus: Efficient Anatomically Accurate Avatar Creation},
year = {2024},
isbn = {9798400706189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652212.3652215},
doi = {10.1145/3652212.3652215},
abstract = {Virtual reality applications are witnessing increased adoption in mental and physical health fields, from rehabilitation therapies to psychological studies. The more advanced the application, the greater the demand to incorporate realistic, custom avatars based on the participant's physical characteristics to enhance embodiment. Current solutions focus on creating such avatars by using expensive camera arrays to capture a 3D representation, which requires technical skills and actively involves the participant in the process. However, equipment and space requirements, setup complexity for non-technical operators, and physical challenges for participants often lead to difficulties and high costs for consistent adherence. This paper presents A3Cplus, a tool to efficiently generate anatomically accurate avatars based solely on a small amount of participant phenotypic data. An optimized processing pipeline uses this data to manipulate specialized blend shapes automatically and mold a generic model into the correct dimensions. We provide illustrative examples of using our tool and discuss its general applicability to immersive avatar-based virtual environments that require a high degree of accuracy and embodiment.},
booktitle = {Proceedings of the 16th International Workshop on Immersive Mixed and Virtual Environment Systems},
pages = {8–14},
numpages = {7},
keywords = {Avatar Creation, Blend Shapes, Clinical and Therapeutic Applications, Immersion, Virtual Reality},
location = {Bari, Italy},
series = {MMVE '24}
}

@inproceedings{10.1145/3610977.3634995,
author = {Jiang, Xinkai and Mattes, Paul and Jia, Xiaogang and Schreiber, Nicolas and Neumann, Gerhard and Lioutikov, Rudolf},
title = {A Comprehensive User Study on Augmented Reality-Based Data Collection Interfaces for Robot Learning},
year = {2024},
isbn = {9798400703225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610977.3634995},
doi = {10.1145/3610977.3634995},
abstract = {Future versatile robots need the ability to learn new tasks and behaviors from demonstrations. Recent advances in virtual and augmented reality position these technologies as great candidates for the efficient and intuitive collection of large sets of demonstrations. While there are different possible approaches to control a virtual robot there has not yet been an evaluation of these control interfaces in regards to their efficiency and intuitiveness. These characteristics become particularly important when working with non-expert users and complex manipulation tasks. To this end, this work investigates five different interfaces to control a virtual robot in a comprehensive user study across various virtualized tasks in an AR setting. These interfaces include Hand Tracking, Virtual Kinesthetic Teaching, Gamepad and Motion Controller. Additionally, this work introduces Kinesthetic Teaching as a novel interface to control virtual robots in AR settings, where the virtual robot mimics the movement of a real robot manipulated by the user. This study reveals valuable insights into their usability and effectiveness. It shows that the proposed Kinesthetic Teaching interface significantly outperforms other interfaces in both objective and subjective metrics based on success rate, task completeness, and completion time and User Experience Questionnaires (UEQ+).},
booktitle = {Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {333–342},
numpages = {10},
keywords = {augmented reality (ar), learning from demonstration, robot interface},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@proceedings{10.1145/3675231,
title = {SAP '24: ACM Symposium on Applied Perception 2024},
year = {2024},
isbn = {9798400710612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dublin, Ireland}
}

@article{10.1145/3594725,
author = {Ferraris, Christopher and Davis, Tom and Gatzidis, Christos and Hargood, Charlie},
title = {Digital Cultural Items in Space: The Impact of Contextual Information on Presenting Digital Cultural Items},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3594725},
doi = {10.1145/3594725},
abstract = {Cultural heritage practitioners continue to engage with ever-changing technological opportunities, and digital cultural items (DCIs) offer the potential for engaging interactive experiences. As DCIs become more prevalent, we are motivated to seek new presentation opportunities from the medium and understand its affordances with regard to contextual information. In this publication, through a series of Speak Aloud tasks with (n = 15) participants, we explore how contextual information can improve user experiences with DCIs. The aforementioned study’s results demonstrate that the inclusion of contextual information when presenting a DCI can, in fact, improve a visitor’s understanding of a DCI’s size and scale plus also the perceived realism of a DCI. Moreover, we observe that contextual information, and its recommended addition, supports the generation of a narrative by the visitor audience. In conclusion, we advise on how contextual information can improve the relationship between a visitor and a DCI, toward interacting with a DCI in a manner very similar to that of its analog counterpart.},
journal = {J. Comput. Cult. Herit.},
month = {oct},
articleno = {70},
numpages = {15},
keywords = {UX, cultural heritage, interaction design, digital items, experience design}
}

@article{10.1145/3604252,
author = {Ghaemi, Zeinab and Ens, Barrett and Engelke, Ulrich and Jenny, Bernhard},
title = {Drawing Connections: Designing Situated Links for Immersive Maps},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {MHCI},
url = {https://doi.org/10.1145/3604252},
doi = {10.1145/3604252},
abstract = {We explore the design of situated visual links in outdoor augmented reality (AR) for connecting miniature buildings on a virtual map to their real-world counterparts. We first distill design criteria from prior work, then conduct two user studies to evaluate a set of proposed link designs to better understand users' preferences for different design choices of the links. In two user studies we evaluated, respectively, a set of link geometries in a virtual environment and a refined AR prototype in two different outdoor environments. The studies reveal that links help in identifying buildings in the environments. Participants prefer straight rather than curved links, simple and thin links to avoid information occlusion, and links and maps aligned with their direction of view. We recommend using a consistent color with a strong contrast to the background color for all links in a scene. To improve visibility, the diameter of links should grow with distance to the viewer and optional animated stripes can be placed on links. The findings of this study have the potential to bolster the development of various situated visualization applications, such as those used in urban planning, tourism, smart agriculture, and other fields.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {sep},
articleno = {205},
numpages = {26},
keywords = {augmented reality, immersive visualization, mixed reality, situated visualization, visual links}
}

@inproceedings{10.1145/3505270.3558322,
author = {Kadish, David and Sarkheyli-H\"{a}gele, Arezoo and Font, Jose and H\"{a}gele, Georg and Niehorster, Diederick C. and Pederson, Thomas},
title = {Towards Situation Awareness and Attention Guidance in a Multiplayer Environment using Augmented Reality and Carcassonne},
year = {2022},
isbn = {9781450392112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505270.3558322},
doi = {10.1145/3505270.3558322},
abstract = {Augmented reality (AR) games are a rich environment for researching and testing computational systems that provide subtle user guidance and training. In particular computer systems that aim to augment a user’s situation awareness benefit from the range of sensors and computing power available in AR headsets. The main focus of this work-in-progress paper is the introduction of the concept of the individualized Situation Awareness-based Attention Guidance (SAAG) system used to increase humans’ situating awareness and the augmented reality version of the board game Carcassonne for validation and evaluation of SAAG. Furthermore, we present our initial work in developing the SAAG pipeline, the generation of game state encodings, the development and training of a game AI, and the design of situation modeling and eye-tracking processes.},
booktitle = {Extended Abstracts of the 2022 Annual Symposium on Computer-Human Interaction in Play},
pages = {133–139},
numpages = {7},
keywords = {Carcassonne, augmented reality, knowledge representation, situation awareness},
location = {Bremen, Germany},
series = {CHI PLAY '22}
}

@proceedings{10.1145/3603421,
title = {ICVARS '23: Proceedings of the 2023 7th International Conference on Virtual and Augmented Reality Simulations},
year = {2023},
isbn = {9781450397469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, Australia}
}

@proceedings{10.1145/3611314,
title = {Web3D '23: Proceedings of the 28th International ACM Conference on 3D Web Technology},
year = {2023},
isbn = {9798400703249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Sebastian, Spain}
}

@inproceedings{10.1145/3613905.3650920,
author = {Li, Zhi and Yao, Yuan},
title = {Sleep No Mosquitoes: Compensating Distance-Aware Compression in a VR Audio Game},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650920},
doi = {10.1145/3613905.3650920},
abstract = {Can you catch mosquitoes at night? Sleep No Mosquitoes is a VR game with audio as the main interactive element. The intention is to explore how to compensate for the problem of distance perception compression in the VR environment through interesting mechanisms. The game is divided into entertainment mode and experimental mode, inviting 21 players and 10 experimental participants to test the factors that affect distance perception in VR. According to the analysis combining quantitative and qualitative methods, we found that three methods - dynamic head movement, dynamic audio, and sound effects of body movement - can improve players’ efficiency in catching mosquitoes, and effectively compensate for the distortion of distance perception in VR.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {293},
numpages = {6},
keywords = {Distance Compression, Game Design, Virtual Reality, Virtual audio game},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3613904.3642511,
author = {Chen, Junjian and Wang, Yuqian and Luximon, Yan},
title = {CamTroller: An Auxiliary Tool for Controlling Your Avatar in PC Games Using Natural Motion Mapping},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642511},
doi = {10.1145/3613904.3642511},
abstract = {Natural motion mapping enhances the gaming experience by reducing the cognitive burden and increasing immersion. However, many players still use the keyboard and mouse in recent commercial PC games. To solve the conflict between complex avatar motion and the limited interaction system, we introduced CamTroller, an auxiliary tool for commercial one-to-one avatar mapping PC games following the concept of a NUI (natural user interface). To validate this concept, we selected PUBG as the application scenario and developed a proof-of-concept system to help players achieve a better experience by naturally mapping selected human motions to the avatars in games through an RGB webcam. A within-subject study with 18 non-professional players practiced common operation (Basic), professional player’s operation (Pro), and CamTroller. Results showed that the performance of CamTroller was as good as the Pro and significantly higher than Basic. Also, the subjective evaluation showed that CamTroller achieved significantly higher intuitiveness than Basic and Pro.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {108},
numpages = {17},
keywords = {NUI, PC game, intuitive interaction, motion tracking, natural mapping},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3544548.3581441,
author = {Ashby, Trevor and Webb, Braden K and Knapp, Gregory and Searle, Jackson and Fulda, Nancy},
title = {Personalized Quest and Dialogue Generation in Role-Playing Games: A Knowledge Graph- and Language Model-based Approach},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581441},
doi = {10.1145/3544548.3581441},
abstract = {Procedural content generation (PCG) in video games offers unprecedented opportunities for customization and user engagement. Working within the specialized context of role-playing games (RPGs), we introduce a novel framework for quest and dialogue generation that places the player at the core of the generative process. Drawing on a hand-crafted knowledge base, our method grounds generated content with in-game context while simultaneously employing a large-scale language model to create fluent, unique, accompanying dialogue. Through human evaluation, we confirm that quests generated using this method can approach the performance of hand-crafted quests in terms of fluency, coherence, novelty, and creativity; demonstrate the enhancement to the player experience provided by greater dynamism; and provide a novel, automated metric for the relevance between quest and dialogue. We view our contribution as a critical step toward dynamic, co-creative narrative frameworks in which humans and AI systems jointly collaborate to create unique and user-specific playable experiences.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {290},
numpages = {20},
keywords = {English, GPT-2, MMORPG, NPC dialogue, RPG, World of Warcraft, computational creativity, dynamic quest generation, human-AI co-creativity, human-computer interaction, knowledge graph, knowledge-grounded text generation, language model, large-scale language models, narrative, natural language processing, procedural content generation, quest, quests, text generation, transformers, video games},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3564533,
title = {Web3D '22: Proceedings of the 27th International Conference on 3D Web Technology},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Evry-Courcouronnes, France}
}

@inproceedings{10.1145/3572921.3572939,
author = {Muir, Mackenzie and Gallagher, Cael Aidin and Conroy, David and Brown, Ross Andrew and Schrank, Christoph Eckart and T\"{u}rkay, Selen},
title = {Development of an Immersive Visualisation System for the 3D Learning of Complex Rock Structures},
year = {2023},
isbn = {9798400700248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572921.3572939},
doi = {10.1145/3572921.3572939},
abstract = {Geological folds are complex wave-like 3D deformation structures. The identification and measurement of characteristic geometric elements of folds (e.g., wavelength, amplitude, orientation of fold axis and fold axial plane) in rocks constitute core skills of field-based Earth scientists and prove to be challenging teaching topics for novice undergraduate geologists. This paper presents an immersive, interactive VR tool for visualising geological folds for educational purposes. We describe a novel geological folding visualisation system, designed to scaffold students in learning this complex spatial concept. Through a user-centered design with geologists, a new tool for digitally visualising geological folds for low financial and computational cost was developed. We describe its major components and visualisation approach, along with its interaction approach for providing fold visualisations. We further report findings from a usability testing (N=11) and report future design considerations.},
booktitle = {Proceedings of the 34th Australian Conference on Human-Computer Interaction},
pages = {248–253},
numpages = {6},
keywords = {Geovisualisations, Immersive Learning},
location = {Canberra, ACT, Australia},
series = {OzCHI '22}
}

@inproceedings{10.1145/3656650.3656679,
author = {Aoki, Tatsuma and Sakurai, Sho and Hirota, Koichi and Nojima, Takuya},
title = {Development and Application of a Simplified Bite Force Measuring Device},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656679},
doi = {10.1145/3656650.3656679},
abstract = {In orthodontic treatment for children, a method has been recommended to strengthen bite force by chewing a tube in a certain rhythm for five minutes every day. However, ensuring that children keep the training daily is difficult due to repetitive, monotonous movements. Additionally, it is not easy for guardians to keep monitoring and ensure that children are doing the training properly, biting the tube with appropriate force. Therefore, we aim to develop an interactive serious game that utilizes bite force as an input device, allowing children to continue training properly and allowing guardians to monitor their training situation easily. This study introduces a simplified tube-based bite force measurement device to assess and record the force of the bites, as well as training games using the device.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {44},
numpages = {5},
keywords = {Bite Force, Gamification, Interfaces for Children, Mastication Training, Measurement Device},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@inproceedings{10.1145/3610978.3640677,
author = {Qian, Zhiqin and Orlov Savko, Liubove and Neubauer, Catherine and Gremillion, Gregory and Unhelkar, Vaibhav},
title = {Measuring Variations in Workload during Human-Robot Collaboration through Automated After-Action Reviews},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640677},
doi = {10.1145/3610978.3640677},
abstract = {Human collaborator's workload plays a central role in human-robot collaboration. Algorithms designed to minimize cognitive workload enhance fluent human-robot teamwork. Time series data of workload is vital for both designing and assessing these algorithms. However, accurately quantifying and measuring cognitive workload, particularly at high temporal resolution, poses a substantial challenge. Towards addressing this challenge, we explore the potential of after-action reviews (AARs) as a tool for gauging workload during human-robot collaboration. First, through a case study, we present and demonstrate AutoAAR for measuring human workload post-task at a high temporal resolution. Second, through a user study, we quantify the validity and utility of measurements derived using AutoAAR for human-robot teamwork. The paper concludes with guidelines and future directions to extend this method to measure other internal states, such as trust and intent.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {852–856},
numpages = {5},
keywords = {data collection, human internal states, methods},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/3611643.3613867,
author = {Liang, Xiaoyun and Qi, Jiayi and Gao, Yongqiang and Peng, Chao and Yang, Ping},
title = {AG3: Automated Game GUI Text Glitch Detection Based on Computer Vision},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613867},
doi = {10.1145/3611643.3613867},
abstract = {With the advancement of device software and hardware performance, and the evolution of game engines, an increasing number of emerging high-quality games are captivating game players from all around the world who speak different languages. However, due to the vast fragmentation of the device and platform market, a well-tested game may still experience text glitches when installed on a new device with an unseen screen resolution and system version, which can significantly impact the user experience. In our testing pipeline, current testing techniques for identifying multilingual text glitches are laborious and inefficient. In this paper, we present AG3, which offers intelligent game traversal, precise visual text glitch detection, and integrated quality report generation capabilities. Our empirical evaluation and internal industrial deployment demonstrate that AG3 can detect various real-world multilingual text glitches with minimal human involvement.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1879–1890},
numpages = {12},
keywords = {Deep Learning, Software Testing, Visual Test Oracle},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@proceedings{10.1145/3649921,
title = {FDG '24: Proceedings of the 19th International Conference on the Foundations of Digital Games},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Worcester, MA, USA}
}

@article{10.1145/3604258,
author = {Rahman, Adil and Heo, Seongkook},
title = {Frapp\'{e}: An Ultra Lightweight Mobile UI Framework for Rapid API-based Prototyping and Environmental Deployment},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {MHCI},
url = {https://doi.org/10.1145/3604258},
doi = {10.1145/3604258},
abstract = {QR codes have been used as an inexpensive means to connect users to digital platforms such as websites and mobile applications. However, despite their ubiquity, QR codes are limited in purpose and can only redirect users to the URL contained within it, thereby making their use heavily network dependent which can be unsuitable for use in ephemeral scenarios and areas with limited connectivity. In this paper, we introduce Frapp\'{e}, a framework capable of deploying ultra lightweight UIs to mobile devices directly through QR codes, without requiring any network connectivity. This is achieved by decomposing the UI into metadata and storing it inside the QR code, while offloading the UI functionality to API calls. We also introduce enFrapp\'{e}, a WYSIWYG tool for building Frapp\'{e} UIs. We demonstrate the lightweight nature of our framework through a technical evaluation, whereas the usability of our UI builder tool is demonstrated through a user study.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {sep},
articleno = {211},
numpages = {23},
keywords = {functional prototyping, lightweight mobile UI framework, mobile UI builder, mobile user interface, rapid application prototyping}
}

@inproceedings{10.1145/3544549.3585843,
author = {Desnoyers-Stewart, John and Stepanova, Ekaterina R. and Liu, Pinyao and Kitson, Alexandra and Pennefather, Patrick Parra and Ryzhov, Vladislav and Riecke, Bernhard E.},
title = {Embodied Telepresent Connection (ETC): Exploring Virtual Social Touch Through Pseudohaptics},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585843},
doi = {10.1145/3544549.3585843},
abstract = {In search of missing tactile sensations in telepresent virtual reality (VR), we set off to explore how we might be able to apply pseudo-haptics to elicit an embodied illusion of social touch. We developed ETC (Embodied Telepresent Connection), a prototype that allows two participants to interact remotely through pseudo-haptic touch via their abstract aura avatars. ETC consists of a set of interaction patterns aimed to elicit an illusion of interpersonal touch using embodied metaphors, simulated physics, integration of visuals and sounds, biosignal representation, and embedded social connotations. We showed ETC at 5 events and 3 workshops, observing participants and gathering feedback from them. Here, we report on our design process and initial observations relating to the experience of virtual social touch. We observed that pseudo-haptics applied to social interaction afford a subtle and somewhat uncanny experience of virtual touch with the potential for intimate embodied connection.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {97},
numpages = {7},
keywords = {embodiment, pseudo-haptics, social connection, social touch, virtual reality},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3637907.3637963,
author = {Zhong, Chaocheng and Zhan, Zehui and Zhang, Huajun and Jigeer, Aerman},
title = {Effect of Physical and Virtual Operations Sequences on Students’ Knowledge Acquisition, Learning Motivation, Engagement, and Cognitive Load in a C-STEAM Project},
year = {2024},
isbn = {9798400716676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637907.3637963},
doi = {10.1145/3637907.3637963},
abstract = {The aim of this study is to examine the impact of physical manipulation (PM) and virtual manipulation (VM) on students' learning experiences throughout the Timber Arch Lounge Bridge culture course, especially focusing on different stages of the course. A quasi-experimental approach was employed, wherein two teaching sequences were designed: "real first, then virtual" and "virtual first," utilizing physical teaching resource "Luban Lock" and virtual simulation teaching resource "E bridge APP" as examples. These two resource utilization modes were employed in teaching experiments, and the effects of the teaching sequences on students' knowledge acquisition, learning motivation, course engagement, and cognitive load were compared. The findings indicate that there were no significant disparities in knowledge acquisition and cognitive load between the distinct sequences of physical and virtual operations. Nevertheless, the "virtual first, then real" sequence substantially augmented students' motivation and engagement in the course. This study serves as a valuable reference for the development of teaching methods that integrate the virtual and physical realms. Specifically, it highlights how judicious use of virtual teaching at various stages of instruction can effectively amplify students' enthusiasm and motivation for learning. Future research could delve deeper into exploring different permutations of "virtual and real" sequences to optimize the utilization of teaching resources.},
booktitle = {Proceedings of the 2023 6th International Conference on Educational Technology Management},
pages = {112–120},
numpages = {9},
keywords = {Course Engagement, Learning Motivation, Timber Arch Lounge Bridge, Virtual-Real Combination},
location = {Guangzhou, China},
series = {ICETM '23}
}

@inproceedings{10.1145/3643833.3656135,
author = {Sabra, Mohd and Vinayaga-Sureshkanth, Nisha and Sharma, Ari and Maiti, Anindya and Jadliwala, Murtuza},
title = {De-anonymizing VR Avatars using Non-VR Motion Side-channels},
year = {2024},
isbn = {9798400705823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643833.3656135},
doi = {10.1145/3643833.3656135},
abstract = {Virtual Reality (VR) technology offers an immersive audio-visual experience to users through which they can interact with a digitally represented 3D space (i.e., a virtual world) using a headset device. By (visually) transporting users from their physical world to realistic virtual spaces, VR systems enable interactive and true-to-life versions of traditional applications such as gaming, remote conferencing and virtual tourism. However, VR applications also present significant user-privacy challenges. This paper studies a new type of privacy threat targeting VR users which attempts to connect their activities visible in the virtual world to their physical state sensed in the real world. Specifically, this paper analyzes the feasibility of carrying out a de-anonymization or identification attack on VR users by correlating visually observed movements of users' avatars in the virtual world with some auxiliary data (e.g., motion sensor data from mobile/wearable devices) representing their context/state in the physical world. To enable this attack, the paper proposes a novel framework which first employs a learning-based activity classification approach to translate the disparate visual movement data and motion sensor data into an activity-vector to ease comparison, followed by a filtering and identity ranking phase outputting an ordered list of potential identities corresponding to the target visual movement data. A comprehensive empirical evaluation of the proposed framework is conducted to study the feasibility of such a de-anonymization attack.},
booktitle = {Proceedings of the 17th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {54–65},
numpages = {12},
keywords = {de-anonymization, motion, side channel, virtual reality},
location = {Seoul, Republic of Korea},
series = {WiSec '24}
}

@inproceedings{10.1145/3580252.3586975,
author = {Baron, Lauren and Chheang, Vuthea and Chaudhari, Amit and Liaqat, Arooj and Chandrasekaran, Aishwarya and Wang, Yufan and Cashaback, Joshua and Thostenson, Erik and Barmaki, Roghayeh Leila},
title = {Virtual Therapy Exergame for Upper Extremity Rehabilitation Using Smart Wearable Sensors},
year = {2024},
isbn = {9798400701023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580252.3586975},
doi = {10.1145/3580252.3586975},
abstract = {Virtual reality (VR) has been utilized for several applications and has shown great potential for rehabilitation, especially for home therapy. However, these systems solely rely on information from VR hand controllers, which do not fully capture the individual movement of the joints. In this paper, we propose a creative VR therapy exergame for upper extremity rehabilitation using multidimensional reaching tasks while simultaneously capturing hand movement from the VR controllers and elbow joint movement from a non-invasive fabric-based wearable sensor made by coating knit fabric with carbon nanotube. We conducted a preliminary study with non-clinical participants (n = 12, 7 F). In a 2 \texttimes{} 2 within-subjects study (orientation (vertical, horizontal) \texttimes{} configuration (flat, curved)), we evaluated the effectiveness and enjoyment of the exergame. The results show that there was a statistically significant difference in terms of task completion time between the two orientations. However, no significant differences were found in the number of mistakes in both orientation and configuration of the virtual exergame. This can lead to customizing therapy while maintaining the same level of intensity. The results of the resistance change generated from the wearable sensor revealed that the flat configuration in the vertical orientation induced more elbow stretches than the other conditions. Finally, we reported the subjective measures based on questionnaires for usability and user experience in different study conditions. In conclusion, the proposed VR exergame has the potential as a multi-modal sensory tool for personalized upper extremity home-based therapy and telerehabilitation.},
booktitle = {Proceedings of the 8th ACM/IEEE International Conference on Connected Health: Applications, Systems and Engineering Technologies},
pages = {92–101},
numpages = {10},
keywords = {virtual therapy, virtual reality, smart wearable sensors, upper extremity, telerehabilitation, human-computer interaction},
location = {Orlando, FL, USA},
series = {CHASE '23}
}

@inproceedings{10.1145/3628516.3655795,
author = {Stefanidi, Evropi and Wassmann, Jonathan Luis Benjamin and Wo\'{z}niak, Pawe\l{} W. and Spellmeyer, Gunnar and Rogers, Yvonne and Niess, Jasmin},
title = {MoodGems: Designing for the Well-being of Children with ADHD and their Families at Home},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3655795},
doi = {10.1145/3628516.3655795},
abstract = {Many technologies for ADHD children and their caregivers focus on symptom management rather than overall well-being, often without involving them as technology co-designers and co-users. To explore how to design systems that integrate into their home and routines, we contribute the iterative design of MoodGems, a situated, modular, and portable set of physical displays, that allows children to record and share their data with their families. We conducted an online formative evaluation (n = 22) with ADHD children, parents, therapists, and HCI experts. Our work demonstrates the potential of technologies affording both individual and joint tracking to allow children to navigate and reflect on their experiences and emotions, and support family communication and children’s autonomy. The evaluation also uncovered necessary refinements in the system’s design. We contribute design insights towards technologies that empower ADHD children and integrate into their homes, and discuss therapists’ role in technologies that address ADHD families’ lived experiences.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {480–494},
numpages = {15},
keywords = {ADHD, children, emotion regulation, empowerment, family, neurodivergent, neurodiversity, reflection, well-being},
location = {Delft, Netherlands},
series = {IDC '24}
}

@inproceedings{10.1145/3613904.3642341,
author = {Bu, Fanjun and Li, Stacey and Goedicke, David and Colley, Mark and Sharma, Gyanendra and Ju, Wendy},
title = {Portobello: Extending Driving Simulation from the Lab to the Road},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642341},
doi = {10.1145/3613904.3642341},
abstract = {In automotive user interface design, testing often starts with lab-based driving simulators and migrates toward on-road studies to mitigate risks. Mixed reality (XR) helps translate virtual study designs to the real road to increase ecological validity. However, researchers rarely run the same study in both in-lab and on-road simulators due to the challenges of replicating studies in both physical and virtual worlds. To provide a common infrastructure to port in-lab study designs on-road, we built a platform-portable infrastructure, Portobello, to enable us to run twinned physical-virtual studies. As a proof-of-concept, we extended the on-road simulator XR-OOM with Portobello. We ran a within-subjects, autonomous-vehicle crosswalk cooperation study (N=32) both in-lab and on-road to investigate study design portability and platform-driven influences on study outcomes. To our knowledge, this is the first system that enables the twinning of studies originally designed for in-lab simulators to be carried out in an on-road platform.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {256},
numpages = {13},
keywords = {Driving Simulations, Human-Autonomous Vehicle Interaction},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3607822.3614534,
author = {Oberfrank, Lukas and Kruse, Lucie and Steinicke, Frank},
title = {Sphere Saber: A Virtual Reality Exergame to Study Age-Related Differences in Selective Visual Attention and Information Processing},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607822.3614534},
doi = {10.1145/3607822.3614534},
abstract = {Virtual reality (VR) applications enable users to perform physical exercises from home. Due to the absence of a real trainer, visual cues are typically used to provide important information about game mechanics or the current tasks. In this paper, we investigate age-related differences regarding visual perception and processing of these game cues during a VR exergame. Therefore, we conducted a comparative user study with nine older and nine younger adults, in which participants had to hit balls of different colors, which approached them. Our results show that younger participants performed better in the game than older participants, especially if a dual-task with additional information was presented. This implies an effect of age on both, task performance, as well as information processing and memory. Eye-tracking data suggests that these differences are not caused by not seeing the visual information signs that were presented, but rather differences in the processing. Furthermore, older adults reported a higher feeling of presence and lower simulator sickness scores. These insights provide important implications for the development of VR exergames for different age groups, especially if additional game information needs to be displayed.},
booktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
articleno = {28},
numpages = {10},
keywords = {Exergames, ageing, memory, older adults, virtual reality, visual attention},
location = {Sydney, NSW, Australia},
series = {SUI '23}
}

@inproceedings{10.1145/3609987.3610008,
author = {Sylaiou, Stella and Gkagka, Evangelia and Fidas, Christos and Vlachou, Elia and Lampropoulos, Giorgos and Plytas, Antonis and Nomikou, Vani},
title = {Use of XR technologies for fostering visitors' experience and inclusion at industrial museums},
year = {2023},
isbn = {9798400708886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609987.3610008},
doi = {10.1145/3609987.3610008},
abstract = {The study presents the research conducted and the solutions adopted to integrate Extended Reality applications in situ and online to enhance visitors' experience of the Tomato Industrial Museum "D. Nomikos" with emphasis on the needs of people with hearing impairments. This paper outlines the methodological, technological, and museological approaches undertaken to integrate XR in ways that will foster the engagement with the narratives of people who worked at the premises and describe the production process. Storytelling and narration are key elements of the museums’ museological design (Sylaiou and Dafiotis, 2020) and therefore including visitors with hearing impairments through XR technologies that allow users to perceive audio tours and verbal accounts through real-time visualization of spoken language by an especially adapted speech to text functionality. Moreover, the paper explores the challenges faced throughout the effort to engage audiences and help them relate with the industrial heritage through exploring the exhibition by generating interest through multimodal resources accessible to all. To engage viewers, we developed interactive applications so that visitors will undertake to fulfill workers' tasks as in role-playing. The result of our approach is a balance between coming to close contact through digital narratives with the people of the time, ensuring inclusion of people with hearing impairments, while offering interactive visualizations of the exhibited machinery functions.},
booktitle = {Proceedings of the 2nd International Conference of the ACM Greek SIGCHI Chapter},
articleno = {21},
numpages = {5},
keywords = {industrial museum, HCI, AR},
location = {Athens, Greece},
series = {CHIGREECE '23}
}

@inproceedings{10.1145/3582437.3582470,
author = {Ibrahim, Mursyid and Sweetser, Penny and Ozdowska, Anne},
title = {Tutorial Level Design Guidelines for 2D Fighting Games},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582470},
doi = {10.1145/3582437.3582470},
abstract = {Fighting games can have barriers to entry as a result of the competency and skill needed to understand the mechanics and objectives of play. One of the key challenges in fighting game design is to teach players how to attain competency. The most common teaching strategy employed in many fighting games is to include a tutorial level. However, there is a lack of research on how fighting game tutorial levels should be designed to support learning for new players. In this paper, we propose design guidelines for video game tutorials, based on the Cognitive Theory of Multimedia Learning and video game design theory. We developed a fighting game tutorial, based on our design guidelines. We evaluated our design against a popular, recent fighting game, Guilty Gear Strive, in a user study with 10 players new to the genre. Our evaluation showed that our design improved on the in-game tutorial, in terms of supporting player learning. We also demonstrated that our design guidelines can provide useful insights into how to provide learning support in fighting game tutorials.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {10},
numpages = {11},
keywords = {Design Guidelines, Game Design, Human Computer Interaction, User Study},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3611643.3616326,
author = {Karimipour, Nima and Pham, Justin and Clapp, Lazaro and Sridharan, Manu},
title = {Practical Inference of Nullability Types},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616326},
doi = {10.1145/3611643.3616326},
abstract = {NullPointerExceptions (NPEs), caused by dereferencing null, fre- quently cause crashes in Java programs. Pluggable type checking is highly effective in preventing Java NPEs. However, this approach is difficult to adopt for large, existing code bases, as it requires manually inserting a significant number of type qualifiers into the code. Hence, a tool to automatically infer these qualifiers could make adoption of type-based NPE prevention significantly easier.  
We present a novel and practical approach to automatic inference of nullability type qualifiers for Java. Our technique searches for a set of qualifiers that maximizes the amount of code that can be successfully type checked. The search uses the type checker as a black box oracle, easing compatibility with existing tools. However, this approach can be costly, as evaluating the impact of a qualifier requires re-running the checker. We present a technique for safely evaluating many qualifiers in a single checker run, dramatically reducing running times. We also describe extensions to make the approach practical in a real-world deployment.  
We implemented our approach in an open-source tool Null- AwayAnnotator, designed to work with the NullAway type checker. We evaluated NullAwayAnnotator’s effectiveness on both open- source projects and commercial code. NullAwayAnnotator re- duces the number of reported NullAway errors by 69.5% on average. Further, our optimizations enable NullAwayAnnotator to scale to large Java programs. NullAwayAnnotator has been highly effective in practice: in a production deployment, it has already been used to add NullAway checking to 160 production modules totaling over 1.3 million lines of Java code.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1395–1406},
numpages = {12},
keywords = {inference, null safety, pluggable type systems, static analysis},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3639701.3656314,
author = {Delachambre, Johanna and Wu, Hui-Yin and Vizcay, Sebastian and Di Meo, Monica and Lagniez, Fr\'{e}d\'{e}rique and Morfin-Bourlat, Christine and Baillif, St\'{e}phanie and Kornprobst, Pierre},
title = {AMD Journee: A Patient Co-designed VR Experience to Raise Awareness Towards the Impact of AMD on Social Interactions},
year = {2024},
isbn = {9798400705038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639701.3656314},
doi = {10.1145/3639701.3656314},
abstract = {We present a virtual reality (VR) experience designed to raise awareness towards the impact of low-vision conditions on social interactions for patients. Specifically, we look at age-related macular degeneration (AMD) that results in the loss of central visual field acuity (a.k.a. a scotoma), which hinders AMD patients from perceiving facial expressions and gestures, and can bring about awkward interactions, misunderstandings, and feelings of isolation. Using VR, we co-designed an experience composed of four scenes from the life of AMD patients through structured interviews with the patients and orthoptists. The experience takes the perspective of a patient, and throughout the scenarios, provides voiceovers on their feelings, the challenges they face, how they adapt to their situation, and also bits of advice on how their quality of life was improved through considerate actions from people in their social circles. A virtual scotoma is designed to follow the gaze of the user using the HTC Vive Focus 3 headset with an eye-tracking module. Setting out from a formal definition of awareness, we evaluate our experience on three components of awareness – knowledge, engagement, and empathy – through established questionnaires, continuous measures of gaze and skin conductance, and qualitative feedback. Carrying out a experiment with 29 participants, we found not only that our experience had a positive and strong impact on the awareness of participants towards AMD, but also that the scotoma and events had observable influences on gaze activity and emotions. We believe this work outlines the advantages of immersive technologies for public awareness towards conditions such as AMD, and opens avenues to conducting studies with fine-grained, multimodal analysis of user behaviour for designing more engaging experiences.},
booktitle = {Proceedings of the 2024 ACM International Conference on Interactive Media Experiences},
pages = {17–29},
numpages = {13},
keywords = {Virtual reality, age-related macular degeneration, awareness, emotion analysis, gaze analysis},
location = {Stockholm, Sweden},
series = {IMX '24}
}

@inproceedings{10.1145/3549737.3549803,
author = {Papazoglou Chalikias, Anastasios and Kouslis, Elias and Sarakatsanos, Orestis and Boikou, Andromachi and Papadopoulos, Stefanos-Iordanis and Koutlis, Christos and Papadopoulos, Symeon and Nikolopoulos, Spiros and Kompatsiaris, Ioannis and Gavilan, David and Shin, Dongjoe and Chen, Yu and Fran\c{c}ois, Axl and Jauk, Michaela and Sutherland, Jamie},
title = {Novel Paradigms of Human-Fashion Interaction},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549803},
doi = {10.1145/3549737.3549803},
abstract = {In this paper, we discuss how we can re-imagine the interaction between users (i.e. fashion designers and consumers) and fashion items, by researching and developing technologies that allow virtual try-on of garments. In this direction we a) generate personal 3D avatars of the user, b) automatically simulate the interaction between 3D user avatars and digital garments (i.e. size fitting and visualization of interactions during body movements), and c) extract fashion insights from user preference data and generate fashion recommendations. Building on recent advances in the fields of artificial intelligence, computer vision and interactive devices, we try to modernize the way people create, consume and experience fashion items by offering novel Human-Fashion-Interaction (HFI) applications that enhance the creative process of garment design, revolutionize the way people interact with fashion in social media, and simulate the physical in-store experience for online shopping.},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {62},
numpages = {11},
keywords = {virtual reality, human-computer interaction, fashion, augmented reality, artificial intelligence},
location = {Corfu, Greece},
series = {SETN '22}
}

@inproceedings{10.1145/3526114.3561351,
author = {Tsai, Ching-Yi and Sun, Chen-Kuo and Cheng, Lung-Pan},
title = {Garnish into Thin Air},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3561351},
doi = {10.1145/3526114.3561351},
abstract = {We propose Garnish into Thin Air, dynamic and three-dimensional food presentation with acoustic levitation. In contrast to traditional plating on the dishes, we make the whole garnishing process an interactive experience to stimulate users’ appetite by leveraging acoustic levitation’s capacity to decorate edibles dynamically in mid-air. To achieve Garnish into Thin Air, our system is built to orchestrate a range of edible materials, such as flavored droplets, edible beads, and rice paper cutouts. We demonstrate Garnish into Thin Air with two examples, including a glass of cocktail named “The Floral Party” and a plate of dessert called “The Winter Twig”.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {102},
numpages = {3},
keywords = {Ultrasonic Transducer, Human Food Interaction, Garnishing, Acoustic Levitation},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3616961.3617807,
author = {Khosrawi-Rad, Bijan and Shahda, Farah and Robra-Bissantz, Susanne},
title = {Towards Pedagogical Conversational Agents as Creativity Drivers in Virtual Worlds},
year = {2023},
isbn = {9798400708749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616961.3617807},
doi = {10.1145/3616961.3617807},
abstract = {Pedagogical Conversational Agents (PCAs) such as chatbots and voice assistants can be used to help learners study through intelligent dialog. For example, educators can use PCAs to facilitate creative brainstorming processes. PCAs as brainstorming facilitators allow learning groups to network with each other and generate or evaluate ideas with the support of the PCA. There is still a lack of implementations and studies on PCAs for creativity support in 3D-based virtual worlds in the literature, although they offer many potentials, such as higher immersion. In this paper, we present a Unity-based virtual world for brainstorming accompanied by two PCAs. Our paper aims to guide developers and educators on deploying PCAs in virtual worlds, and we strive to expand the knowledge base in this area.},
booktitle = {Proceedings of the 26th International Academic Mindtrek Conference},
pages = {313–316},
numpages = {4},
keywords = {Virtual World, Pedagogical Conversational Agent, Moderator, Education, Brainstorming},
location = {Tampere, Finland},
series = {Mindtrek '23}
}

@inproceedings{10.1145/3613904.3642518,
author = {Lin, Yilong and Zhang, Peng and Ofek, Eyal and Je, Seungwoo},
title = {ArmDeformation: Inducing the Sensation of Arm Deformation in Virtual Reality Using Skin-Stretching},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642518},
doi = {10.1145/3613904.3642518},
abstract = {With the development of virtual reality (VR) technology, research is being actively conducted on how incorporating multisensory feedback can create the illusion that virtual avatars are perceived as an extension of the body in VR. In line with this research direction, we introduce ArmDeformation, a wearable device employing skin-stretching to enhance virtual forearm ownership during arm deformation illusion. We conducted five user studies with 98 participants. Using a developed tabletop device, we confirmed the optimal number of actuators and the ideal skin-stretching design effectively increases the user’s body ownership. Additionally, we explored the maximum visual threshold for forearm bending and the minimum detectable bending direction angle when using skin-stretching in VR. Finally, our study demonstrates that using ArmDeformation in VR applications enhances user realism and enjoyment compared to relying on visual feedback alone.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {407},
numpages = {18},
keywords = {Body Illusion, Body Ownership, Skin-stretching, Virtual Reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3582437.3582448,
author = {Chen, Max and Solovey, Erin and Smith, Gillian},
title = {Impact of BCI-Informed Visual Effect Adaptation in a Walking Simulator},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582448},
doi = {10.1145/3582437.3582448},
abstract = {In this paper, we explore the use of brain-computer interface (BCI)-adapted visual effects to support atmosphere in a walking simulator, and investigate its impact on player-reported immersive experience. While players were using a keyboard or joystick controller to control the basic movement of a character, their mental state was accessed by a non-invasive BCI technique called functional near-infrared spectroscopy (fNIRS) to implicitly adjust the visual effects. Specifically, when less brain activity is detected, the players’ in-game vision becomes blurry and distorted, recreating the impression of losing focus. With this biological indication, we designed a BCI-controlled game, in which the vision becomes blurry and distorted when less brain activity is detected, recreating the impression of losing focus. To analyze the player’s experience, we conducted a within-subjects study where participants played both a BCI-controlled and non-BCI-controlled game and completed a questionnaire after each session. We then conducted a semi-structured interview to investigate player perceptions of the impact the BCI had on their experiences. The results showed that players had slightly improved immersion in the BCI-adaptive game, with the temporal dissociation score significantly different. Players also reported the BCI-adaptive visual effects are realistic and natural, and they enjoyed using BCI as a supplemental control.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {5},
numpages = {8},
keywords = {BCI, Immersive Experience, Personalized Experience, Visual Effect, Walking Simulator, fNIRS},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inbook{10.1145/3563659.3563663,
author = {Traum, David},
title = {Socially Interactive Agent Dialogue},
year = {2022},
isbn = {9781450398961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3563659.3563663},
booktitle = {The Handbook on Socially Interactive Agents: 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 2: Interactivity, Platforms, Application},
pages = {45–76},
numpages = {32}
}

@inproceedings{10.1145/3582700.3583955,
author = {Korkiakoski, Mikko and Antila, Anssi and Annamaa, Jouni and Sheikhi, Saeid and Alavesa, Paula and Kostakos, Panos},
title = {Hack the Room: Exploring the potential of an augmented reality game for teaching cyber security},
year = {2023},
isbn = {9781450399845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582700.3583955},
doi = {10.1145/3582700.3583955},
abstract = {There is a need for creating new educational paths for beginners as well as experienced students for cyber security. Recently, ethical hacking gamification platforms like Capture the Flag (CTF) have grown in popularity, providing newcomers with entertaining and engaging material that encourages the development of offensive and defensive cyber security skills. However, augmented reality (AR) applications for the development of cyber security skills remain mostly an untapped resource. The purpose of this work-in-progress study is to investigate whether CTF games in AR might improve learning in information security and increase security situational awareness (SA). In particular, we investigate how AR gamification influences training and overall experience in the context of ethical hacking tasks. To do this, we developed a Unity-based ethical hacking game in which participants complete CTF-style objectives. The game requires the player to execute basic Linux terminal commands, such as listing files in folders and reading data stored on virtual machines. Each gameplay session lasts up to twenty minutes and consists of three objectives. The game may be altered or made more challenging by modifying the virtual machines. In a pilot, our game was tested with six individuals separated into two groups: an expert group (N=3) and a novice group (N=3). The questionnaire given to the expert group examined their SA during the game, whereas the questionnaire administered to the novice group measured learning and remembering certain things they did in the game. In this paper we discuss our observations from the pilot.},
booktitle = {Proceedings of the Augmented Humans International Conference 2023},
pages = {349–353},
numpages = {5},
keywords = {educational games, cyber security, augmented reality},
location = {Glasgow, United Kingdom},
series = {AHs '23}
}

@inproceedings{10.1145/3543758.3549890,
author = {Greve, Daniel and Tiator, Marcel and Kreischer, Christian and Geiger, Christian},
title = {Personalized Motion Analysis with Consideration of Body Segment Shapes},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543758.3549890},
doi = {10.1145/3543758.3549890},
abstract = {Our aim is the configuration and evaluation of individual exoskeletons. To do so, we decided to analyze individual biomechanical models of 3D&nbsp;scanned human bodies and subsequent motions in OpenSim (OSim). Other than most existing approaches, our biomechanical model is reduced to be well suited for the fundamental assessment of exoskeletons. To create the model, parameters are derived from body segment shapes. Further, motion trajectories from various tracking systems can be integrated due to the utilization of Mirevi Motion Hub (MMH). Tools are provided in order to cut the scanned body into segments, to automatically calculate mass as well as inertia properties and to integrate them in an OSim&nbsp;model. In addition, we propose a conversion from the MMH file format into an OSim readable file format. Overall, we consider our solution as flexible, as we only assume a mesh as output of the body scan and as different tracking systems can be leveraged by using the MMH middleware. Future work will address the evaluation of our framework.},
booktitle = {Proceedings of Mensch Und Computer 2022},
pages = {467–471},
numpages = {5},
keywords = {Tracking, Segmentation, Human Model, Exoskeleton, Body Scanning},
location = {Darmstadt, Germany},
series = {MuC '22}
}

@inproceedings{10.1145/3610602.3610606,
author = {Park, Solip and Kultima, Annakaisa and Ono, Kenji and Choi, Buho},
title = {Cross-cultural Online Game Jams: Fostering cultural competencies through jams in game education setting},
year = {2023},
isbn = {9798400708794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610602.3610606},
doi = {10.1145/3610602.3610606},
abstract = {This paper discusses cross-cultural online game jams in a game educational setting, drawing on the experience gained from the project "Games Now! Online Jam" (henceforth "GNOJ") conducted by Aalto University in Finland during the Covid-19 pandemic in 2021-2022. Each GNOJ lasted a week, and jammers utilized various online tools, cloud services, communication platforms, and open-source software. Ninety jammers from Finland, Sweden, South Korea, and Japan participated virtually from their home countries. Through post-survey and observation data, we found that jammers highly valued the cultural learning experience offered by jamming. Notably, they encountered unexpected surprises stemming from the diverse local game development practices and different conceptual and terminological connotations across countries during the jam. But jammers displayed proactive engagement in overcoming such cultural differences, with a heightened motivation to learn other languages, cultures, and local game development practices across the world. These findings highlight the pedagogical benefits that cross-cultural online jams can bring to game education. By fostering cultural awareness and competencies in understanding local nuances in game development and communication styles, such initiatives can help future (and current) game developers to effectively prepare multinational work environments and cooperative workflow with remote teams spanning multiple time zones.},
booktitle = {Proceedings of the 7th International Conference on Game Jams, Hackathons and Game Creation Events},
pages = {1–9},
numpages = {9},
keywords = {Online Jams, Game Jams, Game Education, Cultural Awareness, Competences},
location = {Virtual Event, Ukraine},
series = {ICGJ '23}
}

@inproceedings{10.1145/3543758.3547516,
author = {Bublak, Thomas and Bofferding, Marie and Olson, Christopher and Jonas, Henk and Rademeier, Jannis and Hansen, Christian},
title = {A Virtual Environment for Emergency Ultrasound Training during Cardiopulmonary Resuscitation},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543758.3547516},
doi = {10.1145/3543758.3547516},
abstract = {We propose a virtual reality (VR) communication training environment to train cardiopulmonary resuscitation (CPR) and ultrasound (US) imaging simultaneously. Existing simulators only allow separate training of the two procedures. Our method separates the users spatially, as they each work on their own manikin. One manikin is part of a US simulator commonly used in medical training, and the other manikin is a CPR manikin. Our VR application simulates the users working on the same virtual patient. This allows for an immersive training experience in which communication skills, which are especially important in such critical situations, can be practiced by medical professionals. The prototype has the potential to provide an important foundation for advanced VR-based emergency medical training.},
booktitle = {Proceedings of Mensch Und Computer 2022},
pages = {608–610},
numpages = {3},
keywords = {cardiopulmonary resuscitation (CPR), Virtual reality (VR), Ultrasound (US), Point-of-care Ultrasound (POCUS)},
location = {Darmstadt, Germany},
series = {MuC '22}
}

@inproceedings{10.1145/3574131.3574432,
author = {Luo, Yiming and Wang, Jialin and Pan, Yushan and Luo, Shan and Irani, Pourang and Liang, Hai-Ning},
title = {Teleoperation of a Fast Omnidirectional Unmanned Ground Vehicle in the Cyber-Physical World via a VR Interface},
year = {2023},
isbn = {9798400700316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574131.3574432},
doi = {10.1145/3574131.3574432},
abstract = {This paper addresses the relations between the artifacts, tools, and technologies that we make to fulfill user-centered teleoperations in the cyber-physical environment. We explored the use of a virtual reality (VR) interface based on customized concepts of Worlds-in-Miniature (WiM) to teleoperate unmanned ground vehicles (UGVs). Our designed system supports teleoperators in their interaction with and control of a miniature UGV directly on the miniature map. Both moving and rotating can be done via body motions. Our results showed that the miniature maps and UGV represent a promising framework for VR interfaces.},
booktitle = {Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
articleno = {26},
numpages = {8},
keywords = {World-in-Miniature, Virtual Reality, Teleoperation, Interface Design, Human-robot Interaction},
location = {Guangzhou, China},
series = {VRCAI '22}
}

@inproceedings{10.1145/3550340.3564230,
author = {Elcott, Sharif and Lewis, J.P. and Kanazawa, Noritsugu and Bregler, Christoph},
title = {Training-Free Neural Matte Extraction for Visual Effects},
year = {2022},
isbn = {9781450394659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550340.3564230},
doi = {10.1145/3550340.3564230},
abstract = {Alpha matting is widely used in video conferencing as well as in movies, television, and social media sites. Deep learning approaches to the matte extraction problem are well suited to video conferencing due to the consistent subject matter (front-facing humans), however training-based approaches are somewhat pointless for entertainment videos where varied subjects (spaceships, monsters, etc.) may appear only a few times in a single movie – if a method of creating ground truth for training exists, just use that method to produce the desired mattes. We introduce a training-free high quality neural matte extraction approach that specifically targets the assumptions of visual effects production. Our approach is based on the deep image prior, which optimizes a deep neural network to fit a single image, thereby providing a deep encoding of the particular image. We make use of the representations in the penultimate layer to interpolate coarse and incomplete "trimap" constraints. Videos processed with this approach are temporally consistent. The algorithm is both very simple and surprisingly effective.},
booktitle = {SIGGRAPH Asia 2022 Technical Communications},
articleno = {12},
numpages = {4},
keywords = {visual effects., deep learning, Alpha matting},
location = {Daegu, Republic of Korea},
series = {SA '22}
}

@inproceedings{10.1145/3660043.3660129,
author = {Ma, Yao and Chen, Jun and Feng, Kaiye and Ou, Sumin and Shao, Jie and Huang, Yancheng},
title = {Development and application of electric power subject training system based on virtual reality technology},
year = {2024},
isbn = {9798400716157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660043.3660129},
doi = {10.1145/3660043.3660129},
abstract = {In recent years, with the continuous development and application of high and new computer technology in the power industry, power enterprises have begun to focus on the reform and innovation of staff training. At present, in view of the low efficiency of traditional electric power training, poor experience and other drawbacks have been increasingly exposed to the status quo, based on cloud learning platform, intelligent algorithm application and virtual reality technology of electric power training methods have been successfully developed. This paper introduces the advanced nature and superiority of VR power cloud learning training system, studies the management platform and core technology to realize the system, and analyzes the application advantages and examples of the system in power class training. This system provides users with immersive training experience by simulating real power equipment and test scenes; Provide courseware and teaching resources for users to create independent choice of learning methods, also provide a new direction for the talent training of the electric power industry.},
booktitle = {Proceedings of the 2023 International Conference on Information Education and Artificial Intelligence},
pages = {483–487},
numpages = {5},
location = {Xiamen, China},
series = {ICIEAI '23}
}

@inproceedings{10.1145/3610977.3634961,
author = {H\'{e}nard, Aymeric and Peillard, Etienne and Rivi\`{e}re, J\'{e}r\'{e}my and Kubicki, S\'{e}bastien and Coppin, Gilles},
title = {Human perception of swarm fragmentation},
year = {2024},
isbn = {9798400703225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610977.3634961},
doi = {10.1145/3610977.3634961},
abstract = {In the context of robot swarms, fragmentation refers to a breakdown in communication and coordination among the robots. This fragmentation can lead to issues in the swarm self-organisation, especially the loss of efficiency or an inability to perform their tasks. Human operators influencing the swarm could prevent fragmentation. To help them in this task, it is necessary to study the ability of humans to perceive and anticipate fragmentation. This article studies the perception of different types of fragmentation occurring in swarms depending on their behaviour selected amongst swarming, flocking, expansion and densification. Thus, we characterise human perception thanks to two metrics based on the distance separating fragmented groups and the separation speed. The experimentation protocol consists of a binary discrimination task in which participants have to assess the presence of fragmentation. The results show that detecting fragmentation for expansion behaviour and anticipating fragmentation, in general, are challenging. Moreover, they show that humans rely on separation distance and speed to infer the presence or absence of fragmentation. Our study paves the way for new research that will provide information to humans to better anticipate and efficiently prevent the occurrence of swarm fragmentation.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {250–258},
numpages = {9},
keywords = {human perception, human-robot interaction, robotic swarm, swarm intelligence},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/3526114.3558692,
author = {Song, Seokwoo and Kwon, Doil},
title = {Bodyweight Exercise based Exergame to Induce High Intensity Interval Training},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558692},
doi = {10.1145/3526114.3558692},
abstract = {Exergames have been proposed as an attractive way of making exercise fun; however, most of them do not reach the recommended intensity. Although HCI research has explored how exergame can be designed to follow High Intensity Interval Training (HIIT) that is effective exercise consisting of intermittent vigorous activity and short rest or low-intensity exercise, there are limited studies on designing bodyweight exercise (BWE) based exergame to follow HIIT. In this paper, we propose BWE based exergame to encourage users to maintain high intensity exercise. Our initial study (n=10) showed that the exergame had a significant effect on enjoyment, while the ratio of incorrect posture (ex., squat) also increased due to participants’ concentration on the exergame, which imply future design implications of BWE based exergames.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {6},
numpages = {4},
keywords = {HIIT, Exergame, Bodyweight Exercise},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3603421.3603436,
author = {Miller, Andrew Jerald and Kalafatis, Stavros},
title = {Mixed Reality Equipment Training: A Pilot Study Exploring the Potential Use of Mixed Reality to Train Users on Technical Equipment},
year = {2023},
isbn = {9781450397469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603421.3603436},
doi = {10.1145/3603421.3603436},
abstract = {The increasing availability and capability of Extended Reality (XR) systems has led to swift augmentation of many daily tasks. Research in Mixed Reality (MR), an XR branch allowing simultaneous interaction with virtual and real objects, has shown great potential for enhancing training such as learning how to use technical equipment. In this work, a pilot study was conducted to evaluate how users perceived instruction details given in a traditional written format or MR application. Participants were asked to set up a complex resin-type 3D printer following one of the instruction formats, having no prior experience with any equipment used. Performance was evaluated in regard to efficiency, precision, perceived difficulty, task comprehension, and experience preferences. This work resulted in clear analysis of how details in written and MR instructions are perceived, which can be used to more effectively leverage MR visuals to convey information in training applications.},
booktitle = {Proceedings of the 2023 7th International Conference on Virtual and Augmented Reality Simulations},
pages = {105–113},
numpages = {9},
keywords = {Training, Mixed Reality, Human Computer Interaction},
location = {Sydney, Australia},
series = {ICVARS '23}
}

@inproceedings{10.1145/3590837.3590921,
author = {Legierski, Jaroslaw and Rachwal, Kajetan and Sowinski, Piotr and Niewolski, Wojciech and Ratuszek, Przemyslaw and Kopertowski, Zbigniew and Paprzycki, Marcin and Ganzha, Maria},
title = {Towards Edge-Cloud Architectures for Personal Protective Equipment Detection},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590921},
doi = {10.1145/3590837.3590921},
abstract = {Detecting Personal Protective Equipment in images and video streams is a relevant problem in ensuring the safety of construction workers. In this contribution, an architecture enabling live image recognition of such equipment is proposed. The solution is deployable in two settings – edge-cloud and edge-only. The system was tested on an active construction site, as a part of a larger scenario, within the scope of the ASSIST-IoT H2020 project. To determine the feasibility of the edge-only variant, a model for counting people wearing safety helmets was developed using the YOLOX method. It was found that an edge-only deployment is possible for this use case, given the hardware infrastructure available on site. In the preliminary evaluation, several important observations were made, that are crucial to the further development and deployment of the system. Future work will include an in-depth investigation of performance aspects of the two architecture variants.},
booktitle = {Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
articleno = {84},
numpages = {6},
keywords = {worker safety, image recognition, edge-cloud continuum architectures, PPE detection},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@inproceedings{10.1145/3544549.3583825,
author = {Reinke, Nikolai Lukas and Wursthorn, Tobias and Jessen, Jan},
title = {MEDUSA - A View-Tracking Pong Game},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583825},
doi = {10.1145/3544549.3583825},
abstract = {Abstract: The purpose of this study was to develop a prototype game mechanic that limits the player’s sense of sight and to investigate how quickly players adapt to this mechanic as well as the effects of the mechanic on player competitiveness. We seek to incentivise players to track the current game-state as a mental image rather than getting visual input consistently. This could lead to them experiencing the game more intensely. In this study, a group of participants played a modified version of the classic game ’Pong’ on a touchscreen device while the game visually tracked the openness of their eyes. While this paper only represents a preliminary proof-of-concept and no full-scale study, initial results show meaningful differences in player action - acclimatisation and strategic adaption are clearly visible, even in very small datasets.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {594},
numpages = {7},
keywords = {Attention-Tracking, Attractiveness in connection with playing time, Change in screen time usage, Learnability, Pong},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3561613.3561632,
author = {Zhang, Manyu},
title = {“Presence” and “Empathy” — Design and Implementation Emotional Interactive Storytelling for Virtual Character},
year = {2022},
isbn = {9781450397315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561613.3561632},
doi = {10.1145/3561613.3561632},
abstract = {One of the key motivators for participating in Virtual Reality (VR) is the opportunity to and appeal of becoming immersed in a virtual environment. One avenue that is anticipated to have significant expansion is storytelling through VR, as it offers novel and absorbing experiences. To develop a design interactive storytelling program using VR-based coding, examples of VR application and coding storytelling were analyzed. Base on this analysis, we developed one design interactive storytelling featuring a virtual environment that supports the facilitation of such experiences. In this paper, we introduce expands the interactive storytelling structure, both in general and for VR. The current interactive storytelling systems are extended via emotional modeling and tracking. The components being proposed are to supplement the story segments with information about the response anticipated from users, a modeled emotional path for the individual emotional categories linked to the story, and an internal system to track emotions, in a bid to predict the users’ present emotional condition. We also show the results of the implementation with the 43 students (age 18-28) that demonstrate the emotional expression for the use of interactive storytelling. The results showed that virtual interactive storytelling, the usability of the system and the impact of plot development on inference and story understanding.},
booktitle = {Proceedings of the 5th International Conference on Control and Computer Vision},
pages = {120–126},
numpages = {7},
keywords = {artificial intelligence, Virtual actor, Storytelling design, Emotional storytelling, Digital interactive storytelling},
location = {Xiamen, China},
series = {ICCCV '22}
}

@inproceedings{10.1145/3568294.3580124,
author = {Garcia Hernandez, Nadia Vanessa and Buccelli, Stefano and Laffranchi, Matteo and de Michieli, Lorenzo},
title = {Mixed Reality-based Exergames for Upper Limb Robotic Rehabilitation},
year = {2023},
isbn = {9781450399708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568294.3580124},
doi = {10.1145/3568294.3580124},
abstract = {Robotic rehabilitation devices are showing strong potential for intensive, task-oriented, and personalized motor training. Integrating Mixed Reality (MR) technology and tangible objects in these systems allow the creation of attractive, stimulating, and personalized hybrid environments. Using a gamification approach, MR-based robotic training can increase patients' motivation, engagement, and experience. This paper presents the development of two Mixed Reality-based exergames to perform bimanual exercises assisted by a shoulder rehabilitation exoskeleton and using tangible objects. The system design was completed by adopting a user-centered iterative process. The system evaluates task performance and cost function metrics from the kinematic analysis of the hands' movement. A preliminary evaluation of the system is presented, which shows the correct operation of the system and the fact that it stimulates the desired upper limb movements.},
booktitle = {Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {447–451},
numpages = {5},
keywords = {exergames, mixed reality, robotic rehabilitation, upper limb exoskeleton},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@inproceedings{10.1145/3564533.3564574,
author = {Saunier, Lea and Panouilleres, Muriel and Fetita, Catalin and Preda, Marius},
title = {Visual Rehabilitation for Learning Disorders in Virtual Reality: Visual Rehabilitation for Learning Disorder in VR},
year = {2022},
isbn = {9781450399142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564533.3564574},
doi = {10.1145/3564533.3564574},
abstract = {Current dyslexia rehabilitations methods, although efficient, suffer from the lack of adherence from young patients due to their repetitive and arduous tasks. Digital Therapeutics (DT) have grown exponentially in the last decade, and could be a stepping stone for dyslexia therapy. Making full use of new technologies, they offer new treatments for various disorders. The advancement and diffusion of Virtual Reality (VR) technologies are a new step in the therapeutic domain, notably for the treatment of neurological troubles. In this paper we propose a hybrid VR interface using eye-tracking (ET) and Brain-Computer Interface (BCI) with a gamified application for the rehabilitation of dyslexia. This prototype was designed in collaboration with medical professionals to create a gamified set of exercises adapted in 3D for dyslexia rehabilitation. The interface VR-ET-BCI serves as a monitoring device for the patient and a therapy evaluator for the practitioner. As of today, it lacks yet the clinical trials to show validated results, but an increase in motivation and adherence to therapy is expected.},
booktitle = {Proceedings of the 27th International Conference on 3D Web Technology},
articleno = {17},
numpages = {4},
keywords = {Visual Rehabilitation, Virtual Reality, Human-Computer Interface, Eye-Tracking, Electroencephalogram, Digital Therapeutics},
location = {Evry-Courcouronnes, France},
series = {Web3D '22}
}

@inproceedings{10.1145/3613905.3650911,
author = {Ji, Ruihua and Chang, Zhuang and Wang, Shuxia and Billinghurst, Mark},
title = {Exploring Effective Real-Time Ergonomic Guidance Methods for Immersive Virtual Reality Workspace},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650911},
doi = {10.1145/3613905.3650911},
abstract = {Studying and working in Virtual Reality (VR) provides an immersive experience and enables free body movement for input. However, poor working posture in VR can result in discomfort and even lead to musculoskeletal problems over time. In this work, we explore how to use Mediapipe Blazepose to provide effective real-time ergonomic posture guidance methods in a virtual office environment using (1) auditory, (2) visual, and (3) combined auditory-visual cues. We do this in a within-subject design user study comparing these three conditions and a baseline condition where no guidance is provided. Our results indicate that all three guidance conditions significantly improved users’ ergonomic posture maintenance compared with the baseline condition. However, we found that the combined auditory-visual guidance is the most effective and preferred method. We also discuss multi-modal interaction methods, privacy concerns, and directions for future research.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {139},
numpages = {6},
keywords = {Ergonomic guidance, Immersive Workspace, Virtual Reality},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3611314.3615916,
author = {Soboci\'{n}ski, Pawe\l{} and Walczak, Krzysztof and Jenek, Tomasz},
title = {Building Virtual User Interfaces of Household Appliances for Marketing},
year = {2023},
isbn = {9798400703249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611314.3615916},
doi = {10.1145/3611314.3615916},
abstract = {User experience is one of the most significant factors that impact users’ opinions about products. This also applies to the user interfaces of household appliances. However, realistic interactive presentation of user interfaces of such appliances in a virtual form remotely over the Internet is challenging. In this paper, we present and compare two different methods of interactive presentation of an oven touchscreen control panel. The panel was precisely recreated as an interactive 3D model, reflecting both its appearance and functionality. The system allows users to interact with the control panel using a mobile device touchscreen or hand movements in a virtual reality environment. We describe an experiment aimed at evaluating and comparing different forms of interaction to determine the benefits and barriers to the presentation of complex products in a virtual form for marketing purposes.},
booktitle = {Proceedings of the 28th International ACM Conference on 3D Web Technology},
articleno = {3},
numpages = {7},
keywords = {virtual user interfaces, mobile applications, marketing, VR, 3D presentation},
location = {San Sebastian, Spain},
series = {Web3D '23}
}

@proceedings{10.1145/3582437,
title = {FDG '23: Proceedings of the 18th International Conference on the Foundations of Digital Games},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3569219.3569382,
author = {Fallows, Emma and White, David and Brownsword, Neil},
title = {Design and Development Approach for an Interactive Virtual Museum with Haptic Glove Technology},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569219.3569382},
doi = {10.1145/3569219.3569382},
abstract = {With a rise in the use Virtual Reality (VR) applications in museums and exhibition displays, digital heritage has still shown limitations in what a visitor can experience from the intersection of technology and history. Traditionally, interpretative narrative within the museum has been communicated through text panels offering limited context from a largely connoisseurly perspective. In addition to these interactive digital resources are often data base orientated via touch screen technologies with no multisensory immersion. This paper addresses the digital changes in these educational landscapes and the way it is being handled to co-create digital tools for exhibitions to educate and entertain museum visitors. This paper explores the use of haptic technology in conjunction with virtual reality to facilitate multi-faceted modes of interpretation, that offer novel access to an artefact's history from a range of perspectives. It also provides evidence of increased visitor engagement with a ceramic display through these immersive methods to communicate a narrative. This research bridges the gap between history and technology to offer an immersive experience of visiting a museum virtually and providing an intimate one-one experience to interact with artefacts and learn about history. As its focus, this research digitally reconstructs a collection of East-Asian ceramics bequeathed by Ernest Thornhill in 1944 to North Staffordshire Technical College (Now known as Staffordshire University). The digital prototype was developed to replicate the museum environment without the restrictions to access artefacts and handle them. This experience offers visual insights to contextualise the history of a ceramic to be utlised as an education tool to enhance learning within a museum setting. Evidence showed a significantly positive response to this prototype in museum and gallery settings, responses revealed these methods of interaction did assist in learning about ceramics, with a distinct majority of participants confirming these installations would encourage future visits, shaping the possibilities of how history can be combined with technology to create new and innovative solutions to learn about an artefact.},
booktitle = {Proceedings of the 25th International Academic Mindtrek Conference},
pages = {242–255},
numpages = {14},
location = {Tampere, Finland},
series = {Academic Mindtrek '22}
}

@proceedings{10.1145/3623264,
title = {MIG '23: Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games},
year = {2023},
isbn = {9798400703935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rennes, France}
}

@inproceedings{10.1145/3655532.3655546,
author = {Chen, Hanxiao},
title = {Motion Control of Interactive Robotic Arms Based on Mixed Reality Development},
year = {2024},
isbn = {9798400708039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3655532.3655546},
doi = {10.1145/3655532.3655546},
abstract = {Mixed Reality (MR) is constantly evolving to inspire new patterns of robot manipulation for more advanced Human-Robot Interaction under the 4th Industrial Revolution Paradigm. Consider that Mixed Reality aims to connect physical and digital worlds to provide special immersive experiences, it is necessary to establish the information exchange platform and robot control systems within the developed MR scenarios. In this work, we mainly present multiple effective motion control methods applied on different interactive robotic arms (e.g., UR5, UR5e, myCobot) for the Unity-based development of MR applications, including GUI control panel, text input control panel, end-effector object dynamic tracking and ROS-Unity digital-twin connection.},
booktitle = {Proceedings of the 2023 6th International Conference on Robot Systems and Applications},
pages = {86–93},
numpages = {8},
keywords = {Mixed Reality, Motion Control, Robotic Arm},
location = {Wuhan, China},
series = {ICRSA '23}
}

@inproceedings{10.1145/3544548.3580668,
author = {Kocur, Martin and Jackermeier, Lukas and Schwind, Valentin and Henze, Niels},
title = {The Effects of Avatar and Environment on Thermal Perception and Skin Temperature in Virtual Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580668},
doi = {10.1145/3544548.3580668},
abstract = {Humans’ thermal regulation and subjective perception of temperature is highly plastic and depends on the visual appearance of the surrounding environment. Previous work shows that an environment’s color temperature affects the experienced temperature. As virtual reality (VR) enables visual immersion, recent work suggests that a VR scene’s color temperature also affects experienced temperature. It is, however, unclear if an avatar’s appearance also affects users’ thermal perception and if a change in thermal perception even influences the body temperature. Therefore, we conducted a study with 32 participants performing a task in an ice or fire world while having ice or fire hands. We show that being in a fire world or having fire hands increases the perceived temperature. We even show that having fire hands decreases the hand temperature compared to having ice hands. We discuss the implications for the design of VR systems and future research directions.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {231},
numpages = {15},
keywords = {embodiment, skin temperature, thermal perception, virtual reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3623809.3623830,
author = {Yadollahi, Elmira and Monteiro, Miguel Alexandre and Paiva, Ana},
title = {Learning Spatial Reasoning in Virtual vs. Physical Games with Robots},
year = {2023},
isbn = {9798400708244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623809.3623830},
doi = {10.1145/3623809.3623830},
abstract = {Spatial reasoning is one of the malleable skills well-suited to be developed using robotics that not only benefits children in their pursuit of STEM-related topics but also fosters their perspective-taking skills on dimensions beyond spatial skills. In a study involving elementary school children aged 7-10 years old, we investigated the impact of playing a game with physical robots in a physical environment versus playing the same game with virtually embodied robots in a virtual environment. The game focused on developing spatial perspective-taking skills, requiring children to make moves based on the robots’ point of view. We examined how the two environments influenced their experience of fun and learning spatial reasoning skills. We conducted a between-subject user study with 59 participants from 3rd and 4th grades, where they either played with the physical or virtual version of the game. Children in both conditions showed significant improvement in their perspective-taking and spatial orientation test scores. Furthermore, they rated the physical game as more fun compared to the virtual version.},
booktitle = {Proceedings of the 11th International Conference on Human-Agent Interaction},
pages = {162–170},
numpages = {9},
keywords = {child-robot interaction, perspective-taking, physical embodiment, spatial reasoning, virtual embodiment},
location = {Gothenburg, Sweden},
series = {HAI '23}
}

@inproceedings{10.1145/3665026.3665041,
author = {Kurniawardhani, Arrie and Defanra, Geri and Mahardhika, Galang Prihadi},
title = {Temple Relief Storytelling using Augmented Reality for Enhanced Engagement with Cultural Heritage Sites},
year = {2024},
isbn = {9798400716164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665026.3665041},
doi = {10.1145/3665026.3665041},
abstract = {There are a number of stories in Borobudur temple, but not all visitors who return from visiting Borobudur know what the stories told on the temple panels. One of the reasons that condition happened was the difficulty in accessing information. To overcome that problem, this study attempts to make an application that can facilitate tourists accessing the relief story directly when they see the panel. The AR-Relief application utilizes Augmented Reality (AR) technology to provide the relief story on the user's smartphone when their smartphone scans a panel in Borobudur. A three-dimensional (3D) object will be generated on the user's smartphone while they can still see the relief panel through their camera smartphone. That 3D object can be a 3D animation video playing a corresponding relief story when the user clicks the “Play” button. AR-Relief application is tested using Usability Testing to determine their feasibility. Aspects tested in Usability Testing are Learnability, Memorability, Efficiency, and Satisfaction. The test result shows that AR-Relief applications get an average score of 4.07 on a scale of 5 for all aspects. It means the AR-Relief application is feasible to implement, and respondents can use that application easily to provide information about relief stories when visiting Borobudur.},
booktitle = {Proceedings of the 2024 9th International Conference on Multimedia and Image Processing},
pages = {100–105},
numpages = {6},
keywords = {Augmented Reality, Borobudur Temple, Jataka Relief, Panel, Usability Testing},
location = {Osaka, Japan},
series = {ICMIP '24}
}

@article{10.1145/3571074,
author = {Choudhary, Zubin and Erickson, Austin and Norouzi, Nahal and Kim, Kangsoo and Bruder, Gerd and Welch, Gregory},
title = {Virtual Big Heads in Extended Reality: Estimation of Ideal Head Scales and Perceptual Thresholds for Comfort and Facial Cues},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1544-3558},
url = {https://doi.org/10.1145/3571074},
doi = {10.1145/3571074},
abstract = {Extended reality (XR) technologies, such as virtual reality (VR) and augmented reality (AR), provide users, their avatars, and embodied agents a shared platform to collaborate in a spatial context. Although traditional face-to-face communication is limited by users’ proximity, meaning that another human’s non-verbal embodied cues become more difficult to perceive the farther one is away from that person, researchers and practitioners have started to look into ways to accentuate or amplify such embodied cues and signals to counteract the effects of distance with XR technologies. In this article, we describe and evaluate the Big Head technique, in which a human’s head in VR/AR is scaled up relative to their distance from the observer as a mechanism for enhancing the visibility of non-verbal facial cues, such as facial expressions or eye gaze. To better understand and explore this technique, we present two complimentary human-subject experiments in this article. In our first experiment, we conducted a VR study with a head-mounted display to understand the impact of increased or decreased head scales on participants’ ability to perceive facial expressions as well as their sense of comfort and feeling of “uncannniness” over distances of up to 10 m. We explored two different scaling methods and compared perceptual thresholds and user preferences. Our second experiment was performed in an outdoor AR environment with an optical see-through head-mounted display. Participants were asked to estimate facial expressions and eye gaze, and identify a virtual human over large distances of 30, 60, and 90 m. In both experiments, our results show significant differences in minimum, maximum, and ideal head scales for different distances and tasks related to perceiving faces, facial expressions, and eye gaze, and we also found that participants were more comfortable with slightly bigger heads at larger distances. We discuss our findings with respect to the technologies used, and we discuss implications and guidelines for practical applications that aim to leverage XR-enhanced facial cues.},
journal = {ACM Trans. Appl. Percept.},
month = {jan},
articleno = {4},
numpages = {31},
keywords = {non verbal communication, outdoor augmented reality, social virtual reality, Virtual environments}
}

@inproceedings{10.1145/3594738.3611356,
author = {Yassien, Amal and Abdennadher, Slim},
title = {"\"Hello I am here\": Proximal Nonverbal Cues Role in Initiating Social Interactions in VR"},
year = {2023},
isbn = {9798400701993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594738.3611356},
doi = {10.1145/3594738.3611356},
abstract = {Virtual Reality (VR) has revolutionized social interactions, but limited field of view (FoV) remains a significant obstacle. Users often fail to notice others within the virtual environment, hindering social engagement. To facilitate initiating social interactions, we developed a novel social signaling technique that utilizes proximal nonverbal cues to indicate users’ location, name, and interests within a social distance. In a 2 \texttimes{} 2 mixed user study, we found that this technique greatly enhanced social presence and interaction quality among users with prior social ties. Our signaling technique has tremendous potential to facilitate social interactions across various social virtual events, such as staff meetings and reunions.},
booktitle = {Proceedings of the 2023 ACM International Symposium on Wearable Computers},
pages = {11–16},
numpages = {6},
keywords = {Visual Cues, Social VR, Social Signaling, Social Presence, Proxemics},
location = {Cancun, Quintana Roo, Mexico},
series = {ISWC '23}
}

@inproceedings{10.1145/3587281.3587959,
author = {Gay, Greg and Ralston, Matthew},
title = {Amaze3D: Making 3D Worlds Accessible to Blind Gamers},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587281.3587959},
doi = {10.1145/3587281.3587959},
abstract = {Serious games have become more common as learning tools in recent years. Like other learning materials, it is important that these games be accessible to learners with disabilities.The focus of our recent research was to look at the feasibility of using 3D games, or 3D worlds more specifically, as learning content, and whether they could be made accessible, thus complying with accessibility laws in Ontario, Canada.With the help of the Unity Accessibility Plugin (UAP), and the creation of a series of game scripts to aid with navigating a 3D world, it was possible to create a Unity based 3D world that could be played independently by blind players.},
booktitle = {Proceedings of the 20th International Web for All Conference},
pages = {164–165},
numpages = {2},
keywords = {3D world accessibility, Unity game accessibility, accessible game design, educational games, games for blind players},
location = {Austin, TX, USA},
series = {W4A '23}
}

@inproceedings{10.1145/3639474.3640067,
author = {Moster, Makayla and Kokinda, Ella and Boyer, D. Matthew and Rodeghero, Paige},
title = {Experiences with Summer Camp Communication via Discord},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640067},
doi = {10.1145/3639474.3640067},
abstract = {Teamwork and communication skills are essential for those entering the workforce, especially for software development positions. For remote development positions, the ability to work with a team and communicate remotely through a communication tool are important skills that are generally not taught in standard university courses. In this experience report, we discuss our experience using Discord for communication and collaboration during our virtual summer camp focused on teaching teamwork and game design to 27 autistic high school students. Overall, we found using Discord beneficial in many ways that we did not anticipate, including quicker instructor coordination, improved socialization, and more. Additionally, we provide recommendations for those who may want to use Discord in a similar virtual environment.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {56–65},
numpages = {10},
keywords = {remote, autism, game coding camp, discord},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3613904.3642359,
author = {Mello, Beatriz and Welsch, Robin and Verbokkem, Marissa Christien and Knierim, Pascal and Dechant, Martin Johannes},
title = {Navigating the Virtual Gaze: Social Anxiety's Role in VR Proxemics},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642359},
doi = {10.1145/3613904.3642359},
abstract = {For individuals with Social Anxiety (SA), interacting with others can be a challenging experience, a concern that extends into the virtual world. While technology has made significant strides in creating more realistic virtual human agents (VHA), the interplay of gaze and interpersonal distance when interacting with VHAs is often neglected. This paper investigates the effect of dynamic and static Gaze animations in VHAs on interpersonal distance and their relation to SA. A Bayesian analysis shows that static centered and dynamic centering gaze led participants to stand closer to VHAs than static averted and dynamic averting gaze, respectively. In the static gaze conditions, this pattern was found to be reversed in SA: participants with higher SA kept larger distances for static-centered gaze than for averted gaze VHAs. These findings update theory, elucidate how nuanced interactions with VHAs must be designed, and offer renewed guidelines for pleasant VHA interaction design.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {598},
numpages = {15},
keywords = {Proxemics, Virtual Human Agents, Virtual Reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3548659.3561305,
author = {Prasetya, I. S. W. B. and Pastor Ric\'{o}s, Fernando and Kifetew, Fitsum Meshesha and Prandi, Davide and Shirzadehhajimahmood, Samira and Vos, Tanja E. J. and Paska, Premysl and Hovorka, Karel and Ferdous, Raihana and Susi, Angelo and Davidson, Joseph},
title = {An agent-based approach to automated game testing: an experience report},
year = {2022},
isbn = {9781450394529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548659.3561305},
doi = {10.1145/3548659.3561305},
abstract = {Computer games are very challenging to handle for traditional automated testing algorithms. In this paper we will look at intelligent agents as a solution. Agents are suitable for testing games, since they are reactive and able to reason about their environment to decide the action they want to take.  
This paper presents the experience of using an agent-based automated testing framework called iv4xr to test computer games. Three games will be discussed, including a sophisticated 3D game called Space Engineers. We will show how the framework can be used in different ways, either directly to drive a test agent, or as an intelligent functionality that can be driven by a traditional automated testing algorithm such as a random algorithm or a model based testing algorithm.},
booktitle = {Proceedings of the 13th International Workshop on Automating Test Case Design, Selection and Evaluation},
pages = {1–8},
numpages = {8},
keywords = {model-based game testing, automated game testing, agent-based testing, 3D game testing},
location = {Singapore, Singapore},
series = {A-TEST 2022}
}

@inproceedings{10.1145/3623264.3624449,
author = {Remizova, Vera and Sand, Antti and \v{S}pakov, Oleg and Lylykangas, Jani and Qin, Moshi and Helminen, Terhi and Takio, Fiia and Rantanen, Kati and Kylli\"{a}inen, Anneli and Surakka, Veikko and Gizatdinova, Yulia},
title = {Exploring Mid-air Gestural Interfaces for Children with ADHD},
year = {2023},
isbn = {9798400703935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623264.3624449},
doi = {10.1145/3623264.3624449},
abstract = {This study examined the potential of mid-air gestural interfaces specifically designed for children, in the context of full-body motion control for confirmation actions while interacting with large conventional displays. The study aimed to investigate gestures for confirmation actions that were feasible for children, including children with challenges in behavior control and impulse inhibition. Two scenarios were explored, namely, active full-body gesturing (i.e., jumps and jumps with hands up) and freezing (i.e., motionless posture) to identify their usability for confirmation actions. Experiment I involved 11 typically developing children playing the game to check the system’s general functionality and robustness. Experiment II involved 18 children with attention deficit/hyperactivity disorder (ADHD) to evaluate a prolonged usage of the confirmation gestures in six game sessions. The results showed that the children were able to interact with the large conventional displays with consistent accuracy and responsiveness in both active gesturing and freezing scenarios. Moreover, jumps with rising hands were significantly preferred as an interaction input method. Further, there was no significant difference in ratings of the difficulty of freezing gesture, and children successfully repeated the predefined postures. The findings provided insights into the functionality and suitability of mid-air gestural interfaces for confirmation actions in interactive applications targeted to children.},
booktitle = {Proceedings of the 16th ACM SIGGRAPH Conference on Motion, Interaction and Games},
articleno = {7},
numpages = {10},
keywords = {Motion-controlled games, Mid-air gestural interaction, Confirmation gestures, Child-computer interaction},
location = {Rennes, France},
series = {MIG '23}
}

@inproceedings{10.1145/3543758.3549982,
author = {Wei\ss{}, Sebastian and Kimmel, Simon and With\"{o}ft, Ani and Jung, Frederike and Boll, Susanne and Heuten, Wilko},
title = {Elevating Stress Levels - Exploring Multimodality for Stress Induction in VR},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543758.3549982},
doi = {10.1145/3543758.3549982},
abstract = {Simulation training in Virtual Reality (VR) has gained attraction in recent years. With its broad application possibilities and implicit safety for users, simulation-based training may be imagined for safety-critical situations and exposure therapy. Beyond visual and auditory representation of the environment and stressors, upcoming hardware supports olfactory and haptic feedback. To examine the benefits of these technological advances in stress training, we present a Wizard of Oz pilot study (N=12). Therein, a bimodal presentation of the scenario ‘being stuck in an elevator’ was compared to a multimodal one. For the comparison, we measured qualitative feedback, the iGroup presence questionnaire scores, and physiological stress reactions by recording changes in cardiac and pulmonary activity. Results show trends for moderately more pronounced stress levels and perceived presence for the multimodal presentation. Thus, we argue that multimodal stress induction may better simulate hazardous situations in stress training.},
booktitle = {Proceedings of Mensch Und Computer 2022},
pages = {338–342},
numpages = {5},
keywords = {Virtual Reality, Stress Training, Multimodality},
location = {Darmstadt, Germany},
series = {MuC '22}
}

@inbook{10.1145/3563659.3563662,
author = {Vilhj\'{a}lmsson, Hannes H\"{o}gni},
title = {Interaction in Social Space},
year = {2022},
isbn = {9781450398961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3563659.3563662},
booktitle = {The Handbook on Socially Interactive Agents: 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 2: Interactivity, Platforms, Application},
pages = {3–44},
numpages = {42}
}

@inproceedings{10.1145/3538641.3561507,
author = {Kudry, Peter and Cohen, Michael},
title = {Prototype of a wearable force-feedback mechanism for free-range immersive experience},
year = {2022},
isbn = {9781450393980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538641.3561507},
doi = {10.1145/3538641.3561507},
abstract = {The recent rise in popularity of head-mounted displays (HMDs) for immersion into virtual reality has resulted in demand for new ways to interact with virtual objects. Most solutions utilize generic controllers for interaction within virtual environments and provide limited haptic feedback. We describe a prototype of an ambulatory (allowing walking) haptic feedback stylus with primary use in computer-aided design. Our stylus is a modified 3D Systems Touch force-feedback arm mounted on a wearable platform carried in front of a user. The wearable harness also holds a full-sized laptop which drives a Meta Quest 2 HMD, also worn by the user. Such design provides six degrees-of-freedom without tethered limitations, while ensuring high precision of force-feedback from virtual interaction. Our solution also provides an experience wherein a mobile user can explore different haptic feedback simulations as well as create, arrange, and deform general shapes.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {178–184},
numpages = {7},
keywords = {wearable computing, virtual reality, tactile display, perceptual overlay, kinesthetic awareness, human-computer interaction, haptic interface, force-feedback, embodied interaction, ambulatory},
location = {Virtual Event, Japan},
series = {RACS '22}
}

@inproceedings{10.1145/3594806.3596549,
author = {Hu, Jiayuan and Lykourentzou, Ioanna},
title = {TONIC: A teamwork simulator and digital twin for organizational innovation challenges},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596549},
doi = {10.1145/3594806.3596549},
abstract = {Efficient collaboration is the cornerstone of most successful projects and organizations are increasingly seeking tools to optimize their team formation processes. In this paper, we introduce an agent-based simulator designed for team formation in open collaboration settings, specifically innovation hackathons. The tool models and simulates three distinct team formation approaches: bottom-up team formation, where users self-organize with minimal algorithm intervention, top-down team formation, and hybrid team formation. With a broad range of customizable parameters, our tool can serve as a digital twin for collaboration within organizations, permitting organizational decision-makers to design tailored scenarios according to their team formation requirements, utilize existing models or develop their own, and evaluate the effectiveness of their team building approaches prior to real-world implementation.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {584–588},
numpages = {5},
keywords = {teamwork modeling, human modeling, agent-based simulations},
location = {Corfu, Greece},
series = {PETRA '23}
}

@article{10.1145/3659062,
author = {Calvo-Barajas, Natalia and Akkuzu, Anastasia and Castellano, Ginevra},
title = {Balancing Human Likeness in Social Robots: Impact on Children’s Lexical Alignment and Self-disclosure for Trust Assessment},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659062},
doi = {10.1145/3659062},
abstract = {While there is evidence that human-like characteristics in robots could benefit child-robot interaction in many ways, open questions remain about the appropriate degree of human likeness that should be implemented in robots to avoid adverse effects on acceptance and trust. This study investigates how human likeness, appearance and behavior, influence children’s social and competency trust in a robot. We first designed two versions of the Furhat robot with visual and auditory human-like and machine-like cues validated in two online studies. Secondly, we created verbal behaviors where human likeness was manipulated as responsiveness regarding the robot’s lexical matching. Then, 52 children (7-10 years old) played a storytelling game in a between-subjects experimental design. Results show that the conditions did not affect subjective trust measures. However, objective measures showed that human likeness affects trust differently. While low human-like appearance enhanced social trust, high human-like behavior improved children’s acceptance of the robot’s task-related suggestions. This work provides empirical evidence on manipulating facial features and behavior to control human likeness in a robot with a highly human-like morphology. We discuss the implications and importance of balancing human likeness in robot design and its impacts on task performance, as it directly impacts trust-building with children.},
note = {Just Accepted},
journal = {J. Hum.-Robot Interact.},
month = {may},
keywords = {child-robot interaction, social robots, human likeness, trust}
}

@inproceedings{10.1145/3544549.3585601,
author = {Pascher, Max and Franzen, Til and Kronhardt, Kirill and Gruenefeld, Uwe and Schneegass, Stefan and Gerken, Jens},
title = {HaptiX: Vibrotactile Haptic Feedback for Communication of 3D Directional Cues},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585601},
doi = {10.1145/3544549.3585601},
abstract = {In Human-Computer-Interaction, vibrotactile haptic feedback offers the advantage of being independent of any visual perception of the environment. Most importantly, the user’s field of view is not obscured by user interface elements, and the visual sense is not unnecessarily strained. This is especially advantageous when the visual channel is already busy, or the visual sense is limited. We developed three design variants based on different vibrotactile illusions to communicate 3D directional cues. In particular, we explored two variants based on the vibrotactile illusion of the cutaneous rabbit and one based on apparent vibrotactile motion. To communicate gradient information, we combined these with pulse-based and intensity-based mapping. A subsequent study showed that the pulse-based variants based on the vibrotactile illusion of the cutaneous rabbit are suitable for communicating both directional and gradient characteristics. The results further show that a representation of 3D directions via vibrations can be effective and beneficial.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {154},
numpages = {7},
keywords = {directional cues, haptic feedback, vibrotactile feedback},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3575879.3575960,
author = {Drampalou, Georgia and Kourniatis, Nikolaos and Voyiatzis, Ioannis},
title = {Customized toolbox in VR Design},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575960},
doi = {10.1145/3575879.3575960},
abstract = {Designing in an environment of Virtual Reality is a field that gains ground. Although more and more applications are constantly released, the majority of designers have not yet been introduced into Virtual Reality. A widely used tool for designing is Rhino which has been released by McNeel and Rhino Developer encourages the users to create plug-ins for custom use of the software. In parallel, Meta's developing tools offer their users the chance to customize their own applications. Towards the direction of Virtual Reality integration, both have already released new products for their users. RhinoVR is an open source plug-in for Rhino, while at the same time Meta provides Oculus hardware and platform solutions for users to turn their concept into reality. What is yet to be developed is a model that encourages the customization of a VR design platform, focusing mainly on developing a personalized toolbox for designing.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {14–20},
numpages = {7},
keywords = {cross-platform.NET plug-in SDK, VR Design, Rhino Developer, OpenVR API, Oculus Quest 2},
location = {Athens, Greece},
series = {PCI '22}
}

@inproceedings{10.1145/3573381.3597230,
author = {Peter, Kaulyaalalwa and Auala, Selma and Winschiers-Theophilus, Heike},
title = {An AR Game for Primary Learners to Safeguard Intangible Cultural Heritage of the Ovahimba Tribe},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3597230},
doi = {10.1145/3573381.3597230},
abstract = {Augmented Reality (AR) is a new technology that enhances the actual world by superimposing computer-generated or extracted real-world sensory data such as images or sound onto it. Incorporating Augmented Reality into Cultural Heritage has a slew of benefits. Safeguarding of Cultural Heritage is vital because it serves as a link between the past and the present. Museums do not engage their audiences, particularly primary school-aged children, and as a result, youngsters show little interest in their cultural history, and museums cannot compete with more technologically advanced and modern kinds of entertainment. The purpose of this study was to spark children’s interest in Cultural Heritage as they engage with it. Under the project, a Cultural Heritage game using eight Ovahimba items was developed and tested at a local primary school. Research-by-Design was the primary methodology of this research project.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {359–361},
numpages = {3},
keywords = {Research-by-Design, Ovahimba, Namibia, Embodied Interaction, Cultural heritage, Augmented Reality},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3573381.3596161,
author = {Sheremetieva, Anna and Romanovych, Ihor and Frish, Sam and Maksymenko, Mykola and Georgiou, Orestis},
title = {What’s my future: a Multisensory and Multimodal Digital Human Agent Interactive Experience},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596161},
doi = {10.1145/3573381.3596161},
abstract = {This paper describes an interactive multimodal and multisensory fortune-telling experience for digital signage applications that combines digital human agents along with touchless haptic technology and gesture recognition. For the first time, human-to-digital human interaction is mediated through hand gesture input and mid-air haptic feedback, motivating further research into multimodal and multisensory location-based experiences using these and related technologies. We take a phenomenological approach and present our design process, the system architecture, and discuss our gained insights, along with some of the challenges and opportunities we have encountered during this exercise. Finally, we use our singular implementation as a paradigm as a proxy for discussing complex aspects such as privacy, consent, gender neutrality, and the use of digital non-fungible tokens at the phygital border of the metaverse.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {40–46},
numpages = {7},
keywords = {NFT, Multisensory experience, Haptics, Digital Signage, Digital Humans},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3544549.3585838,
author = {Labrou, Katerina and Zaman, Cagri Hakan and Turkyasar, Arda and Davis, Randall},
title = {Following the Master’s Hands: Capturing Piano Performances for Mixed Reality Piano Learning Applications},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585838},
doi = {10.1145/3544549.3585838},
abstract = {Piano learning applications in Mixed Reality (MR) are a promising substitute for physical instruction when a piano teacher is absent. Existing piano learning applications that use visual indicators to highlight the notes to be played on the keyboard or employ video projections of a pianist provide minimal guidance on how the learner should execute hand movements to develop their technique in performance and prevent injuries. To address this gap, we developed an immersive first-person piano learning experience that uses a library of targeted visualizations of the teacher’s hands and 3D traces of hand movements in MR. Seeing the piano teacher’s hands while hearing the music is central to developing the novice’s musical intuition. We introduced an end-to-end workflow to accurately capture the pianist’s technical gestures and align them with the musical score. We recorded pianists playing technical exercises and music pieces. We developed a multimodal performance dataset (MPD) comprising virtual hand models, keyboard (MIDI) recordings and the corresponding music scores, and different visualizations of hand traces capturing movement. Finally, we developed Pianoverse, an MR application to assist piano learning, and performed exploratory user testing with novice piano players to understand the impact of multimodal representations of movement on skill learning. Our initial observations suggest that apprehending the movement traces of a recorded performance over a physical keyboard increases the learner’s ability to position their body and hands correctly and to replicate hand gestures while playing from written music. Further research will focus on automating performance data collection and a comprehensive evaluation of the use of leading movement traces in piano learning.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {141},
numpages = {8},
keywords = {Embodied Computing, Mixed Reality, Motion Capture, Piano training},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3581783.3612416,
author = {Guo, Peini and Liu, Hong and Wu, Jianbing and Wang, Guoquan and Wang, Tao},
title = {Semantic-aware Consistency Network for Cloth-changing Person Re-Identification},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612416},
doi = {10.1145/3581783.3612416},
abstract = {Cloth-changing Person Re-Identification (CC-ReID) is a challenging task that aims to retrieve the target person across multiple surveillance cameras when clothing changes might happen. Despite recent progress in CC-ReID, existing approaches are still hindered by the interference of clothing variations since they lack effective constraints to keep the model consistently focused on clothing-irrelevant regions. To address this issue, we present a Semantic-aware Consistency Network (SCNet) to learn identity-related semantic features by proposing effective consistency constraints. Specifically, we generate the black-clothing image by erasing pixels in the clothing area, which explicitly mitigates the interference from clothing variations. In addition, to fully exploit the fine-grained identity information, a head-enhanced attention module is introduced, which learns soft attention maps by utilizing the proposed part-based matching loss to highlight head information. We further design a semantic consistency loss to facilitate the learning of high-level identity-related semantic features, forcing the model to focus on semantically consistent cloth-irrelevant regions. By using the consistency constraint, our model does not require any extra auxiliary segmentation module to generate the black-clothing image or locate the head region during the inference stage. Extensive experiments on four cloth-changing person Re-ID datasets (LTCC, PRCC, Vc-Clothes, and DeepChange) demonstrate that our proposed SCNet makes significant improvements over prior state-of-the-art approaches. Our code is available at: https://github.com/Gpn-star/SCNet.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {8730–8739},
numpages = {10},
keywords = {semantic consistency, part-based matching, head enhancement, cloth-changing person re-identification},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3649921.3650020,
author = {Zomerplaag, Samira and Bakkes, Sander},
title = {An Exploratory Study on Gender Dysphoria &amp; Character Customisation},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3650020},
doi = {10.1145/3649921.3650020},
abstract = {In the present paper, we set out to understand in what manner character customisation of a video game could have an influence on the mental state of transgender players. Current literature shows that a player’s character can indeed be used to experiment with gender-related struggles in a safe environment. To this end, studies suggest that the character choice of transgender players is based on an alignment with their actual, self-identified gender identity; as a means of both experimentation and wishful identification. However, only limited research has been conducted to find how the portrayal of a video game character’s gender specifically influences the player’s gender dysphoria (i.e., the distress a person feels due to a mismatch between their gender identity and their sex assigned at birth). As such, the present paper contributes (1) an overview of selected background works on the interplay of gender and character customisation, and (2) a qualitative study that includes 37 transgender adolescents between the ages of 16 and 23. Findings of the study extensively discuss to what extent – and to what effect – transgender adolescents use the possibility to customise their character as a means to experiment and/or come to terms with their gender identity.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {36},
numpages = {12},
keywords = {Gender dysphoria, character customisation, video games},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@book{10.1145/3563659,
editor = {Lugrin, Birgit and Pelachaud, Catherine and Traum, David},
title = {The Handbook on Socially Interactive Agents: 20 years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 2: Interactivity, Platforms, Application},
year = {2022},
isbn = {9781450398961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {48},
abstract = {The Handbook on Socially Interactive Agents provides a comprehensive overview of the research fields of Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics. Socially Interactive Agents (SIAs), whether virtually or physically embodied, are autonomous agents that are able to perceive an environment including people or other agents, reason, and decide how to interact, and express attitudes such as emotions, engagement, or empathy. They are capable of interacting with people and each other in a socially intelligent manner using multimodal communicative behaviors with the goal to support humans in various domains.Written by international experts in their respective fields, the book summarizes research in the many important research communities pertinent for SIAs, while discussing current challenges and future directions. The handbook provides easy access to modeling and studying SIAs for researchers and students and aims at further bridging the gap between the research communities involved.In two volumes, the book clearly structures the vast body of research. The first volume starts by introducing what is involved in SIAs research, in particular research methodologies and ethical implications of developing SIAs. It further examines research on appearance and behavior, focusing on multimodality. Finally, social cognition for SIAs is investigated by different theoretical models and phenomena such as theory of mind or pro-sociality. The second volume starts with perspectives on interaction, examined from different angles such as interaction in social space, group interaction, or long-term interaction. It also includes an extensive overview summarizing research and systems of human-agent platforms and of some of the major application areas of SIAs such as education, aging support, autism or games.}
}

@inproceedings{10.1145/3550356.3559088,
author = {Yigitbas, Enes and Schmidt, Maximilian and Bucchiarone, Antonio and Gottschalk, Sebastian and Engels, Gregor},
title = {Gamification-based UML learning environment in virtual reality},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3559088},
doi = {10.1145/3550356.3559088},
abstract = {Modeling is a key activity in conceptual design and system design which makes learning and understanding modeling languages like the Unified Modeling Language (UML) important. Many interactive UML learning applications exist and also gamification-based alternatives have been proposed in recent years to promote the engagement of learners. However, none of the existing approaches provide an immersive learning environment that can be used to increase interactivity while learning. As a consequence, existing UML learning applications cannot create a feeling of presence that can positively influence learning outcomes, and the potential of gamification is not fully exploited. To overcome this problem, in this demo paper, we present an immersive gamification-based UML learning environment in Virtual Reality (VR) to practice modeling class diagrams in an interactive way. The VR environment provides minigames and multi-viewpoint modeling features to learn creating class diagrams. The multi-viewpoint component highlights correspondences between a class diagram and a 3D model of an example system. The goal of this approach is to improve the learners' motivation, make the learning process an enjoyable experience, and boost learning outcomes.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {27–31},
numpages = {5},
keywords = {virtual reality, immersive learning, gamification, UML},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3613905.3650852,
author = {Zhang, Mingyue and Li, Jialong and Li, Nianyu and Kang, Eunsuk and Tei, Kenji},
title = {User-Driven Adaptation: Tailoring Autonomous Driving Systems with Dynamic Preferences},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650852},
doi = {10.1145/3613905.3650852},
abstract = {In the realm of autonomous vehicles, dynamic user preferences are critical yet challenging to accommodate. Existing methods often misrepresent these preferences, either by overlooking their dynamism or overburdening users as humans often find it challenging to express their objectives mathematically. The previously introduced framework, which interprets dynamic preferences as inherent uncertainty and includes a “human-on-the-loop” mechanism enabling users to give feedback when dissatisfied with system behaviors, addresses this gap. In this study, we further enhance the approach with a user study of 20 participants, focusing on aligning system behavior with user expectations through feedback-driven adaptation. The findings affirm the approach’s ability to effectively merge algorithm-driven adjustments with user complaints, leading to improved participants’ subjective satisfaction in autonomous systems.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {361},
numpages = {8},
keywords = {Autonomous Driving, Human on the Loop, Preference Adaptation},
location = {
},
series = {CHI EA '24}
}

@inproceedings{10.1145/3549865.3549906,
author = {Ortegon-Sarmiento, Tatiana and Paderewski, Patricia and Gutierrez-Vela, Francisco and Kelouwani, Sousso and Uribe-Quevedo, Alvaro},
title = {Case study on technological acceptance of autonomous vehicles and the influence of situational awareness: Lane detection in winter conditions},
year = {2022},
isbn = {9781450397025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549865.3549906},
doi = {10.1145/3549865.3549906},
abstract = {The new technological era and digital transformation have brought different advances to our lives, among which autonomous vehicles (AVs) stand out. The increasing visibility of these has given way to several investigations on the acceptance and perception of this technology, however, in many of these studies the participants are based solely on a preconceived belief since most have not driven or had contact with these types of vehicles. This issue needs to be explored further, however, test studies with real AVs are expensive, dangerous and unethical. In response to this, virtual reality emerges as a possible solution as it provides a realistic, safe and controlled environment, where different road and weather situations can be implemented. This paper presents a preliminary research design, based on the Technology Acceptance Model (TAM), which seeks to assess and infer the level of acceptance of AVs and user behaviour when using and interacting with them, based on an immersive virtual simulation of an AV, where the user is exposed to a risky situation where the vehicle is unable to recognize the lane due to weather difficulties.},
booktitle = {Proceedings of the XXII International Conference on Human Computer Interaction},
articleno = {15},
numpages = {4},
keywords = {virtual reality, user experience, technological acceptance, autonomous vehicles},
location = {Teruel, Spain},
series = {Interacci\'{o}n '22}
}

@inproceedings{10.1145/3568444.3568446,
author = {Kobeisse, Suzanne and Holmquist, Lars Erik},
title = {“I Can Feel It in My Hand”: Exploring Design Opportunities for Tangible Interfaces to Manipulate Artefacts in AR},
year = {2022},
isbn = {9781450398206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568444.3568446},
doi = {10.1145/3568444.3568446},
abstract = {This paper shows how generic proxies as tangible interfaces can provide an intuitive and realistic interaction method to manipulate artefacts in AR. The domain we selected was cultural heritage, specifically the interaction with historical artefacts. We explored four interfaces: 1) a standard touch screen using a web 3D viewer, 2) a traditional flat AR marker, 3) a generic wooden cylinder, and 4) a 3D-printed replica of the digital artefact. Our results show that a 3D-printed replica represents the most realistic interaction method to manipulate artefacts in AR. However, we further found that using a generic object such as the cylinder can deliver a more immersive experience, offering an advantage over the standard touch screen and flat AR marker. As high-fidelity physical objects can be costly to produce. We argue that a more generic physical proxy is a feasible middle ground when 3D-printed replicas are not viable. We then discuss design opportunities for using generic objects to explore different artefacts in AR.},
booktitle = {Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia},
pages = {28–36},
numpages = {9},
keywords = {User evaluation, Tangible interfaces, Human-computer interaction, Cultural heritage, Augmented reality},
location = {Lisbon, Portugal},
series = {MUM '22}
}

@inproceedings{10.1145/3547522.3547721,
author = {Eriksson, Eva and Petersen, Jonas Oxenb\o{}ll and Bagge, Rolf and Kristensen, Janus Bager and Lervig, Morten and Torgersson, Olof and Baykal, G\"{o}k\c{c}e Elif},
title = {Quadropong - Conditions for Mediating Collaborative Interaction in a Co-located Collaborative Digital Game using Multi-Display Composition},
year = {2022},
isbn = {9781450394482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3547522.3547721},
doi = {10.1145/3547522.3547721},
abstract = {In this demo, we present Quadropong, a co-located collaborative digital game using multi-display composition. It consists of conditions in the form of ten game instances designed to mediate various levels of collaborative interaction in players. The conditions were designed based on variations in interdependence and in symmetry and asymmetry in skills and resources. The preliminary results from playtests indicate that it is possible to design conditions to mediate various levels of collaborative interaction.},
booktitle = {Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference},
articleno = {25},
numpages = {2},
keywords = {interaction design, game design, collaborative interaction, HCI, Collaboration},
location = {Aarhus, Denmark},
series = {NordiCHI '22}
}

@inproceedings{10.1145/3544793.3561317,
author = {Tsai, Wan-Lun and Pan, Tse-Yu and Hu, Min-Chun},
title = {Improve Immersion in Virtual Reality-Based Basketball Training By Haptic Feedback},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3561317},
doi = {10.1145/3544793.3561317},
abstract = {Many studies have investigated the feasibility of using real sport equipment in VR-based sport training. However, providing haptic feedback to mimic real-world ball interaction (passing and catching) is still less explored. In this paper, a VR-based basketball training system comprising vibrotactile gloves is proposed. With the aid of haptic feedback, the immersive feelings related to interaction with a basketball in a virtual court are improved. A within-subject experiment which compared with/without haptic feedback conditions was conducted. The improvement of presence was objectively measured by an electroencephalography (EEG) approach using a portable, low-cost EEG headset in a virtual catching scenario. Furthermore, the passing reaction time was assessed in a VR-based tactic training scenario to explore the effect of haptic feedback on the performance during training. The results demonstrated the possibility of using low-cost EEG headset to assess the presence. Because the perception of catching was mainly affected by visual, haptic feedback did not make significant effect on the passing reaction time. Nevertheless, the proposed gloves was commented to improve the experience of passing and catching.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {524–528},
numpages = {5},
keywords = {virtual reality, vibrotactile gloves, electroencephalography, basketball training},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3613904.3642316,
author = {Yamanaka, Shota and Stuerzlinger, Wolfgang},
title = {The Effect of Latency on Movement Time in Path-steering},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642316},
doi = {10.1145/3613904.3642316},
abstract = {In current graphical user interfaces, there exists a (typically unavoidable) end-to-end latency from each pointing-device movement to its corresponding cursor response on the screen, which is known to affect user performance in target selection, e.g., in terms of movement time (MT). Previous work also reported that a long latency increases MTs in path-steering tasks, but the quantitative relationship between latency and MT had not been previously investigated for path-steering. In this work, we derive models to predict MTs for path-steering and evaluate them with five tasks: goal crossing as a preliminary task for model derivation, linear-path steering, circular-path steering, narrowing-path steering, and steering with target pointing. The results show that the proposed models yielded an adjusted R2 &gt; 0.94, with lower AICs and smaller cross-validation RMSEs than the baseline models, enabling more accurate prediction of MTs.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {622},
numpages = {19},
keywords = {Human motor performance, graphical user interface, operational time prediction},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3678429.3678431,
author = {Ratajczyk, Dawid Jaroslaw},
title = {Dominant or Submissive? Exploring Social Perceptions Across the Human-Robot Spectrum},
year = {2024},
isbn = {9798400716812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678429.3678431},
doi = {10.1145/3678429.3678431},
abstract = {The study investigates the perception of humanlike robots, exploring the influence of behavior, particularly dominance or submissiveness, on social perceptions. The focus is on the Uncanny Valley Hypothesis (UVH), which suggests that as robots become more humanlike, they initially elicit positive responses but eventually lead to discomfort and negative reactions. The study analyzed data from 330 participants and utilized two sets of 11 stimuli representing humanlike-robot 3D morphs with either dominant or submissive behavior to examine emotions. The method involved an online setup where participants viewed movie clips featuring the morphs and completed questionnaires assessing dominance, categorization (human or robot), eeriness, likability, and perceived threat. The results indicated that humanlike robots, aligning with the UVH, evoked more negative emotions, although likability differences were not significant. Contrary to expectations based on previous studies, the results reveal that the negative perception of dominant robots is not solely attributable to their categorization as robots. Furthermore, the study explored the influence of the Uncanny Valley on social perception. While the results lean towards considering the Uncanny Valley as a potential side effect of perceptual processing, the evidence is inconclusive, pointing to the need for further exploration. The outcomes also underscore the challenges in categorizing humanlike morphs, emphasizing the ambiguity in the concept of a robot.},
booktitle = {Proceedings of the 2024 4th International Conference on Human-Machine Interaction},
pages = {8–14},
numpages = {7},
keywords = {Dominance perception, Human-robot interaction, Humanlike robots, Social perceptions, Uncanny Valley Hypothesis},
location = {Xi'an, China},
series = {ICHMI '24}
}

@inproceedings{10.1145/3495243.3517019,
author = {Zhang, Xiao and Guo, Hanqing and Mariani, James and Xiao, Li},
title = {U-star: an underwater navigation system based on passive 3D optical identification tags},
year = {2022},
isbn = {9781450391818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495243.3517019},
doi = {10.1145/3495243.3517019},
abstract = {Underwater optical wireless communication techniques are promising due to a broad bandwidth with a long communication range compared with existing expensive acoustic and RF-based underwater communication techniques. For underwater navigation assistance during dive and rescue, it is more practical to adopt passive optical tags for objects/human identification and location-based services. However, existing optical tags (bar/QR codes) employ one/two dimensional designs, which lack significant element/symbol distance for robust decoding and full-directional localization capabilities for underwater navigation tasks. This paper investigates opportunities to increase the element distance in passive low-order optical tags by exploiting 3D spatial diversity. Specifically, we design U-Star, a system that consists of Underwater Optical Identification (UOID) tags and commercial camera-based tag readers for underwater navigation. Our UOID tags embed rich location and guidance information. Additionally, because our UOID tags employ a three-dimensional design, they can also determine the relative location of a user in real-time based on the perspective principles. We design AI based mobile algorithms for underwater denoising, relative positioning, and robust data parsing for tag readers. Finally, we evaluate U-Star on real UOID tag prototypes under different underwater scenarios. Results show that our 3-order UOID tag can embed 21 bits with a BER of 0.003 at 1m and less than 0.05 at up to 3m, which is sufficient for underwater navigation guidance with backup database.},
booktitle = {Proceedings of the 28th Annual International Conference on Mobile Computing And Networking},
pages = {648–660},
numpages = {13},
keywords = {underwater optical wireless communication, underwater navigation system, passive 3D optical tag},
location = {Sydney, NSW, Australia},
series = {MobiCom '22}
}

@inproceedings{10.1145/3579375.3579422,
author = {Ashok, Akash and Darwish, Mazen and W\"{u}nsche, Burkhard Claus},
title = {Investigating Changes in Hand-Eye Coordination and Reaction Time Using a VR Squash Simulation},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579422},
doi = {10.1145/3579375.3579422},
abstract = {Hand-eye coordination and reaction time are essential for many sports and are associated with cognitive health. Training hand-eye coordination is hence important but can be limited by lack of equipment, suitable facilities, instructors, or training partners. In this research, we have designed a VR squash game for training hand-eye coordination at home, and we investigate users’ perceptions of the tool and how it compares with traditional training. A user study with 30 participants shows that our application is significantly more enjoyable (P = 0.031) than the control group activity and that users perceive the tool as similarly effective in improving hand-eye coordination as non-game training activities. While we didn’t perform a longitudinal study investigating the effectiveness of the training, our results are important since enjoyment and perceived usefulness are essential for long-term usage.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {257–260},
numpages = {4},
keywords = {virtual reality, squash, reaction time, hand-eye coordination, game, exergame, enjoyment},
location = {Melbourne, VIC, Australia},
series = {ACSW '23}
}

@inproceedings{10.1145/3551349.3560514,
author = {Karre, Sai Anirudh and Pareek, Vivek and Mittal, Raghav and Reddy, Raghu},
title = {A Role Based Model Template for Specifying Virtual Reality Software},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560514},
doi = {10.1145/3551349.3560514},
abstract = {Research in hardware and software support for Virtual Reality (VR) has significantly increased over the last decade. Given the software platform fragmentation and hardware volatility, there is an apparent disconnect among practitioners while building applications in the VR domain. This paper proposes a role-based model template as a meta-model to specify the bare minimum VR software system. We conducted a grounded-theory-based qualitative study on prevailing and phased-out VR SDKs and standards to propose this meta-model. This model template can help VR practitioners build open-source tools to develop, design, and test VR software systems.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {225},
numpages = {5},
keywords = {Virtual Reality, VR SDK, VR Model Template, Meta model, Grounded-theory},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3544549.3585897,
author = {Wald, Iddo Yehoshua and Maimon, Amber and Keniger De Andrade Gensas, Lucas and Guiot, Noemi and Ben Oz, Meshi and Corn, Benjamin W. and Amedi, Amir},
title = {Breathing based immersive interactions for enhanced agency and body awareness: a claustrophobia motivated study},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585897},
doi = {10.1145/3544549.3585897},
abstract = {This work explores utilizing representations of one’s physiological breath (embreathment) in immersive experiences, for enhancing presence and body awareness. Particularly, embreathment is proposed for reducing claustrophobia and associated negative cognitions such as feelings of restriction, loss of agency, and sense of suffocation, by enhancing agency and interoception in circumstances where one’s ability to act is restricted. The informed design process of an experience designed for this purpose is presented, alongside an experiment employing the experience, evaluating embodiment, presence, and interoception. The results indicate that embreathment leads to significantly greater levels of embodiment and presence than either an entrainment or control condition. In addition, a modest trend was observed in a heartbeat detection task implying better interoception in the intervention conditions than the control. These findings support the initial assumptions regarding presence and body awareness, paving the way for further evaluation with individuals and situations related to the claustrophobia use case.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {59},
numpages = {7},
keywords = {agency, breathing, claustrophobia, embodiment, embreathment, negative cognitions, presence, respiation, sense of control},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3544548.3581302,
author = {Nair, Vishnu and Zhu, Hanxiu 'Hazel' and Smith, Brian A.},
title = {ImageAssist: Tools for Enhancing Touchscreen-Based Image Exploration Systems for Blind and Low Vision Users},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581302},
doi = {10.1145/3544548.3581302},
abstract = {Blind and low vision (BLV) users often rely on alt text to understand what a digital image is showing. However, recent research has investigated how touch-based image exploration on touchscreens can supplement alt text. Touchscreen-based image exploration systems allow BLV users to deeply understand images while granting a strong sense of agency. Yet, prior work has found that these systems require a lot of effort to use, and little work has been done to explore these systems’ bottlenecks on a deeper level and propose solutions to these issues. To address this, we present ImageAssist, a set of three tools that assist BLV users through the process of exploring images by touch — scaffolding the exploration process. We perform a series of studies with BLV users to design and evaluate ImageAssist, and our findings reveal several implications for image exploration tools for BLV users.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {76},
numpages = {17},
keywords = {alt text, smartphone-based accessibility tools, touchscreen-based image exploration tools, visual impairments},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3562939.3565627,
author = {Gottsacker, Matt and Norouzi, Nahal and Schubert, Ryan and Guido-Sanz, Frank and Bruder, Gerd and Welch, Gregory},
title = {Effects of Environmental Noise Levels on Patient Handoff Communication in a Mixed Reality Simulation},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565627},
doi = {10.1145/3562939.3565627},
abstract = {When medical caregivers transfer patients to another person’s care (a patient handoff), it is essential they effectively communicate the patient’s condition to ensure the best possible health outcomes. Emergency situations caused by mass casualty events (e.g., natural disasters) introduce additional difficulties to handoff procedures such as environmental noise. We created a projected mixed reality simulation of a handoff scenario involving a medical evacuation by air and tested how low, medium, and high levels of helicopter noise affected participants’ handoff experience, handoff performance, and behaviors. Through a human-subjects experimental design study (N = 21), we found that the addition of noise increased participants’ subjective stress and task load, decreased their self-assessed and actual performance, and caused participants to speak louder. Participants also stood closer to the virtual human sending the handoff information when listening to the handoff than they stood to the receiver when relaying the handoff information. We discuss implications for the design of handoff training simulations and avenues for future handoff communication research.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {20},
numpages = {10},
keywords = {virtual and mixed reality, human-subject research, environmental noise, Patient handoffs},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@proceedings{10.1145/3626485,
title = {ISS Companion '23: Companion Proceedings of the 2023 Conference on Interactive Surfaces and Spaces},
year = {2023},
isbn = {9798400704253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Pittsburgh, PA, USA}
}

@inproceedings{10.1145/3611314.3616067,
author = {D\"{o}llner, J\"{u}rgen and de Amicis, Raffaele and Burmeister, Josafat-Mattias and Richter, Rico},
title = {Forests in the Digital Age: Concepts and Technologies for Designing and Deploying Forest Digital Twins},
year = {2023},
isbn = {9798400703249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611314.3616067},
doi = {10.1145/3611314.3616067},
abstract = {Forests are among the most widespread and diverse ecosystems on Earth, providing essential ecosystem services at local and global scales. However, they are facing major challenges due to climate change, economic pressures and human population growth. Digital twins of forests could help address these challenges by enabling comprehensive forest monitoring and supporting management decisions. In this publication, we describe how digital twins differ from other digital tools in the forest domain and explore concepts and technologies that can serve as the basis for implementing forest digital twins. We outline the underlying data model of the digital twins, which includes trees as the core forest elements, as well as their environment. We explain how a wide range of data collection approaches can be combined for comprehensive data collection and how the data can be integrated into a spatio-temporal forest data space. We describe data processing approaches to enrich raw data with semantic information and address how digital twins can support decision making through modeling and simulation. We explain the role of web-based visualization in interacting with forest digital twins. Overall, our concept lays the foundation for the technical implementation of forest digital twins that integrate, process, analyze and visualize forest data from a variety of sources. The implementation of forest digital twins in practice would enrich our understanding of forest ecosystems and enable targeted management of forests and their ecosystem services.},
booktitle = {Proceedings of the 28th International ACM Conference on 3D Web Technology},
articleno = {30},
numpages = {12},
keywords = {Sensor Data, Remote Sensing, Forests, Environmental Monitoring, Digital Twins, Data Visualization, Data Analysis},
location = {San Sebastian, Spain},
series = {Web3D '23}
}

@inproceedings{10.1145/3575879.3576002,
author = {Maragkoudaki, Sofia Niovi and Kalloniatis, Christos},
title = {Virtual Reality as a mean for increasing privacy awareness: The escape room example},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3576002},
doi = {10.1145/3575879.3576002},
abstract = {Abstract: The universal daily use of the Internet and social media raises the need of privacy awareness. Using innovative technologies for educational purposes seems to be the solution to this issue. More specifically, game-based learning effectively educates users in technological fields, while offering adequate learning opportunities for a wide range of target groups. Although there are several categories of educational games and tools, virtual reality games are efficient for training on privacy issues. The present study aims to validate the adding value virtual reality games can bring to in privacy awareness status quo. In more detail, the advantages of game-based learning using cut-edge technologies, such as virtual reality, are analyzed. In order to evaluate this theory, a virtual reality escape room was designed, implemented, and navigated by 13 users that provided important feedback regarding the effectiveness of game-based learning and virtual reality in privacy Awareness.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {261–266},
numpages = {6},
location = {Athens, Greece},
series = {PCI '22}
}

@proceedings{10.1145/3550340,
title = {SA '22: SIGGRAPH Asia 2022 Technical Communications},
year = {2022},
isbn = {9781450394659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Daegu, Republic of Korea}
}

@proceedings{10.1145/3652024,
title = {ISMM 2024: Proceedings of the 2024 ACM SIGPLAN International Symposium on Memory Management},
year = {2024},
isbn = {9798400706158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great pleasure that we welcome you to the 2024 ACM SIGPLAN International Symposium on Memory Management (ISMM '24)! This is the 23rd event in the ISMM series. We expanded the scope of ISMM this year and encouraged submissions and participation from related fields such as computer architecture and operating systems in addition to the programming languages community. The call for papers motivated submissions of work in the following areas.},
location = {Copenhagen, Denmark}
}

@article{10.1145/3594534,
author = {Squire, Kurt and Wells, Garrison and Anderson-Coto, Maria J. and Steinkuehler, Constance},
title = {Casual Games, Cognition, and Play across the Lifespan: A Critical Synthesis},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3594534},
doi = {10.1145/3594534},
abstract = {Games, including video games have long been associated with both rhetorics of progress and frivolity, simultaneously recruiting efforts to employ games toward furthering cognitive skills, while also eliciting concerns about the decadence of players. Casual games, defined as games with a low barrier to entry and quick play sessions often focus on cognitively-oriented challenges and are perceived by many players to promote cognitive, social, and emotional benefits. Research on the cognitive, social, and emotional impact of casual games now spans games marketed as entertainment, “brain games,” and digital therapeutics; despite these games sharing similar qualities, the bodies of research literature on them remains largely distinct. This review finds little support for the cognitive benefits of playing casual games, with exception of the elderly or those with dementia. This research synthesis finds evidence for the social and emotional benefits of casual games when they are sought for these purposes, played mindfully, and within robust social contexts. However, the same games, when played in different contexts can have negative consequences, consistent with findings from the mindset literature more broadly. Researchers thus should take seriously the context of game play, perhaps treating the emergent phenomena of play as the unit of analysis, rather than the media artifact.},
journal = {ACM Games},
month = {jun},
articleno = {14},
numpages = {25},
keywords = {well being, Casual games}
}

@inproceedings{10.1145/3524494.3527624,
author = {Paduraru, Ciprian and Paduraru, Miruna},
title = {Pedestrian motion in simulation applications using deep learning},
year = {2022},
isbn = {9781450392938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524494.3527624},
doi = {10.1145/3524494.3527624},
abstract = {The goal of this paper is to provide a framework for simulating pedestrian motion in simulation applications by using real-world examples of human motion. This process has two implications. The first one refers to the reduction of the development time, since a deep learning model can replace the classical pedestrian behavior development process for the targeted applications. The second relates to improving the quality of pedestrian movements, as manual development of behavior using classical methods can result in movements that appear too robotic or predictable. We propose a new deep learning model based on an encoder-decoder strategy and Graph Attention Networks, able to take into account both the semantics of the scene and the correlations between the simulated pedestrian movements. The evaluation shows that the methods are suitable for real-time simulations, even for applications with performance constraints such as video games.},
booktitle = {Proceedings of the 6th International ICSE Workshop on Games and Software Engineering: Engineering Fun, Inspiration, and Motivation},
pages = {1–8},
numpages = {8},
keywords = {video games, simulation software, pedestrians, motion forecasting, deep learning},
location = {Pittsburgh, Pennsylvania},
series = {GAS '22}
}

@inproceedings{10.1145/3539618.3592057,
author = {Ward, Austin and Avula, Sandeep and Cheng, Hao-Fei and Sarwar, Sheikh Muhammad and Murdock, Vanessa and Agichtein, Eugene},
title = {Searching for Products in Virtual Reality: Understanding the Impact of Context and Result Presentation on User Experience},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3592057},
doi = {10.1145/3539618.3592057},
abstract = {Immersive technologies such as virtual reality (VR) and head-mounted displays (HMD) have seen increased adoption in recent years. In this work, we study two factors that influence users' experience when shopping in VR through voice queries: (1) context alignment of the search environment and (2) the level of detail on the Search Engine Results Page (SERP). To this end, we developed a search system for VR and conducted a within-subject exploratory study (N=18) to understand the impact of the two experimental conditions. Our results suggest that both context alignment and SERP are important factors for information-seeking in VR, which present unique opportunities and challenges. More specifically, based on our findings, we suggest that search systems for VR must be able to: (1) provide cues for information-seeking in both the VR environment and SERP, (2) distribute attention between the VR environment and the search interface, (3) reduce distractions in the VR environment and (4) provide a ''sense of control'' to search in the VR environment.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2359–2363},
numpages = {5},
keywords = {information retrieval, user study, virtual reality},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1109/ICSE48619.2023.00140,
author = {Ryan, Ita and Roedig, Utz and Stol, Klaas-Jan},
title = {Measuring Secure Coding Practice and Culture: A Finger Pointing at the Moon is Not the Moon},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00140},
doi = {10.1109/ICSE48619.2023.00140},
abstract = {Software security research has a core problem: it is impossible to prove the security of complex software. A low number of known defects may simply indicate that the software has not been attacked yet, or that successful attacks have not been detected. A high defect count may be the result of white-hat hacker targeting, or of a successful bug bounty program which prevented insecurities from persisting in the wild. This makes it difficult to measure the security of non-trivial software. Researchers instead usually measure effort directed towards ensuring software security. However, different researchers use their own tailored measures, usually devised from industry secure coding guidelines. Not only is there no agreed way to measure effort, there is also no agreement on what effort entails. Qualitative studies emphasise the importance of security culture in an organisation. Where software security practices are introduced solely to ensure compliance with legislative or industry standards, a box-ticking attitude to security may result. The security culture may be weak or non-existent, making it likely that precautions not explicitly mentioned in the standards will be missed. Thus, researchers need both a way to assess software security practice and a way to measure software security culture. To assess security practice, we converted the empirically-established 12 most common software security activities into questions. To assess security culture, we devised a number of questions grounded in prior literature. We ran a secure development survey with both sets of questions, obtaining organic responses from 1,100 software coders in 59 countries. We used proven common activities to assess security practice, and made a first attempt to quantitatively assess aspects of security culture in the broad developer population. Our results show that some coders still work in environments where there is little to no attempt to ensure code security. Security practice and culture do not always correlate, and some organisations with strong secure coding practice have weak secure coding culture. This may lead to problems in defect prevention and sustained software security effort.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1622–1634},
numpages = {13},
keywords = {security compliance, secure coding},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3570945.3607314,
author = {Hurstak, Emily E. and Olafsson, Stefan and O'Leary, Teresa K. and Cabral, Howard J. and Paasche-Orlow, Michael and Bickmore, Timothy},
title = {Conversational Assessment of Mild Cognitive Impairment with Virtual Agents},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570945.3607314},
doi = {10.1145/3570945.3607314},
abstract = {Over 55 million adults worldwide have dementia, a syndrome characterized by deterioration in cognitive functioning. Screening for mild cognitive impairment is important to identify dementia early to facilitate diagnosis and initiate treatment that may modify the disease trajectory. However, standard cognitive screening tools are time-consuming, require expert administration, and make people feel as if they are being tested and are thus potentially stigmatizing. Consequently, standard cognitive screening tools are underutilized, and dementia is often identified much later in the disease process leading to greater health-related and societal harms. We explored cognitive ability assessments using virtual agents, in which assessments are made during conversational dialogues. We evaluated four strategies for the assessment of cognitive ability in a sample of 41 individuals with varying cognitive ability, comparing each strategy to scores on the Mini Mental Status Exam (MMSE), the gold standard clinical screening tool for cognitive impairment. One of the four conversational strategies demonstrated significant agreement with the MMSE, with a sensitivity of 92.3% and a specificity of 48.3% for identification of impairment compared to the MMSE. Participants reported moderate levels of satisfaction with the agent-based conversational assessment. This approach could be used to identify individuals warranting more evaluation for possible cognitive impairment.},
booktitle = {Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
articleno = {8},
numpages = {8},
keywords = {Cognitive Impairment, Embodied Conversational Agent, Virtual Agent},
location = {W\"{u}rzburg, Germany},
series = {IVA '23}
}

@inproceedings{10.1145/3556223.3556234,
author = {Putra, Hanif Fermanda and Ogata, Kohichi},
title = {Seamless Gaze Gesture Interactions With Various Devices Without Moving Objects as Visual Stimulation},
year = {2022},
isbn = {9781450396349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3556223.3556234},
doi = {10.1145/3556223.3556234},
abstract = {Although the current pandemic is expected to eventually end, similar outbreaks of viral disease may occur in the future. Such outbreaks can spread through direct social contact. To avoid such risks, we propose contactless interaction using an eye-tracking device. The proposed approach uses a conventional visible-light camera to detect a user’s iris. The camera is attached to a wearable frame. We believe that our proposed method is safer than exposing a user’s eyes to an infrared sensor at a close distance. Furthermore, we considered seamless multi-device control using the proposed device. The system utilizes QR codes to switch between controlled devices. Once the eye-tracking device detects a QR code, it connects to a target device and sends commands to control it. These commands are triggered by eye movement gestures performed by the user. We defined these control gestures as a simple straight eye movement in a given direction, to avoid the potential burden of requiring complex gestures. We experimentally implemented the proposed device, and the results show that it achieved an overall accuracy of 81.66%. Thus, it may prove useful for multitasking applications or for contactless interaction.},
booktitle = {Proceedings of the 10th International Conference on Computer and Communications Management},
pages = {73–78},
numpages = {6},
keywords = {seamless control, human-computer interaction, gaze detection, gaze control},
location = {Okayama, Japan},
series = {ICCCM '22}
}

@inproceedings{10.1145/3544549.3577037,
author = {Ruiz-Rodriguez, Aurora and Hermens, Hermie and van Asseldonk, Edwin},
title = {HEROES, Design of an Exergame for Balance Recovery of Stroke Patients for a Home Environment},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3577037},
doi = {10.1145/3544549.3577037},
abstract = {Stroke survivors suffer from balance impairment, causing an increased risk of falling. To recover balance, adequate stepping responses are key and these are practised by perturbing the patients during rehabilitation. Serious videogames, such as exergames, that train voluntary stepping can be found, but they don’t allow to practice fast recovery steps in people with stroke. In this paper, we propose the design of a serious exergame (HEROES) to train stepping responses of stroke patients in a home environment. For this, we employed recent findings of action observation and motor imagery. We followed an iterative user-centred methodology to design the HEROES exergame. Stroke patients, physiotherapists and game designers were involved in every stage. The design of the HEROES exergame complies with the stroke accessibility guidelines, providing clear instructions and feedback. Therapeutic goals are defined by the progression of the level, ensuring to train paretic and non-paretic legs in a safe but challenging set-up.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {499},
numpages = {4},
keywords = {Stroke patients, exergames, rehabilitation, serious videogames},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3543712.3543729,
author = {Holly, Fabian and Zigart, Tanja and Maurer, Martina and Wolfartsberger, Josef and Brunnhofer, Magdalena and Sorko, Sabrina Romina and Moser, Thomas and Schlager, Alexander},
title = {Gaining Impact with Mixed Reality in Industry – A Sustainable Approach},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543729},
doi = {10.1145/3543712.3543729},
abstract = {Mixed reality (augmented and virtual reality) technology continues to find its way into the industry. Nevertheless, prototypes and island solutions are developed in companies, which are not embedded in the corporate strategy. This paper presents a systematic approach for finding and developing use cases accompanied by a strategic implementation and evaluation process to integrate mixed reality in the industry. Sustainability aspects like energy and resource efficiency, possible reduction of the ecological footprint, and sustainable solutions for lasting and vision-based use in companies are considered. Within several use cases with 20 industry partners in the following areas are developed: 1) novel forms of space-independent collaboration (e.g., collaborative work by integrating real-time 3D depth information of the real environment and visualization of and interaction with real-time production data) and 2) XR-supported training and learning methods (e.g., parameterizable and adaptive training scenarios, roll-out of training content for several participants and integration of gamification mechanisms). Additionally, the methodology for a sustainability assessment, technology acceptance, and a multi-criteria evaluation are shown, and first results are discussed.},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {128–134},
numpages = {7},
keywords = {Sustainability, Mixed Reality, Industry Use Cases},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@inproceedings{10.1145/3564982.3564983,
author = {Sinpan, Nitinun and Sasithong, Pruk and Chaudhary, Sushank and Poomrittigul, Suvit and Leelawat, Natt and Wuttisittikulkij, Lunchakorn},
title = {Simulative Investigations of Crowd Evacuation by Incorporating Reinforcement Learning Scheme},
year = {2023},
isbn = {9781450397407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564982.3564983},
doi = {10.1145/3564982.3564983},
abstract = {Safe and effective evacuation is very essential to decrease casualties in an emergency event. The social force model is used to simulate the movement of evacuee and leader crowd evacuation with leader in an experimental area. The a-star algorithm is used to find the evacuation routes of each evacuee from the initial location to the exit. In this paper, we proposed the application of reinforcement learning to train the agent to become an evacuation leader via the Unity ML-Agents Toolkit. This model is tested in three scenarios: an evacuation without a leader, an evacuation with a leader, and an evacuation with randomly located agent. They are considered for comparison purposes to show the impact of an agent that tries to tell an exit to all evacuees in the experimental area. The experimental results show that the proposed crowd evacuation agent could effectively evacuate all evacuees in the experimental area.},
booktitle = {Proceedings of the 6th International Conference on Algorithms, Computing and Systems},
articleno = {1},
numpages = {5},
keywords = {Social Force Model, Reinforcement Learning, Evacuation Leader, Crowd Evacuation},
location = {Larissa, Greece},
series = {ICACS '22}
}

@inproceedings{10.1145/3554364.3559121,
author = {Batista, Bruno G. and Rodrigues, Ana F. D. and Miranda, D\'{e}bora M. and Ishitani, Lucila and Nobre, Cristiane N.},
title = {Developing an edutainment game, taboo!, for children with ADHD based on socially aware design and VCIA model},
year = {2022},
isbn = {9781450395069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3554364.3559121},
doi = {10.1145/3554364.3559121},
abstract = {This paper analyzes how the Socially Aware Design (SAwD) and Value-oriented and Culturally Approach (VCIA) design model can be used to develop an edutainment game for children with Attention Deficit Hyperactivity Disorder (ADHD). The SAwD approach seeks a design that considers new dimensions in human-computer interaction, such as cultural, emotional, and social aspects of the user's everyday experience. From this perspective, the game was based on the VCIA model, which includes the stakeholders in the design process, through participatory methodologies, considering their behavioral patterns, culture, and values, once they influence how technology is understood and used, and the way it impacts people's lives. Serious games have been used as part of the treatment of ADHD in children to improve focus and attention, stimulate concentration, and be a tool for enhancing learning in areas such as math, combining education and entertainment (edutainment). In this sense, the main objective of the research was to identify the stakeholder's values and the impact of these values on game design. The first step was to conduct a literature review followed by interviews with stakeholders to raise and compare their values, as required by the VCIA model, so that a game prototype could be developed, followed by an iterative validation process with the target audience.},
booktitle = {Proceedings of the 21st Brazilian Symposium on Human Factors in Computing Systems},
articleno = {13},
numpages = {11},
keywords = {taboo!, socially aware design, edutainment, VCIA model, ADHD},
location = {Diamantina, Brazil},
series = {IHC '22}
}

@inproceedings{10.1145/3658852.3659077,
author = {Vincs, Kim and Mccormick, John and Mardamootoo, Pajani},
title = {Volumetric Interaction: a new approach to expanding embodied experience in XR},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658852.3659077},
doi = {10.1145/3658852.3659077},
abstract = {This paper presents a prototype for a new form of interaction termed ‘volumetric interaction’ (VI), developed by the authors, along with a specialist development team of programmers, 3D artists and dancers, in the Embodied Movement Design Studio within the Centre for Transformative Media Technologies in Melbourne, Australia. Current XR interaction is based on tracking users’ motion as a series of coordinates in space, which are mapped to virtual space to enable interaction within a virtual environment. VI proposes a new logic based on contemporary dance movement concepts that understand body movement as a series of shifting volumes rather than as a series of joint actions in space. This paper describes the creation and workshop testing of the first VI prototypes and reflects on the potential of VI to provide a new capacity to tailor interaction systems to differently abled people for whom gesture-based systems may not be suitable.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {18},
numpages = {8},
keywords = {Dance Knowledge, Dance Technology, Human Mesh Reconstruction, Motion Capture, SMPL},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@proceedings{10.1145/3623053,
title = {SA '23: SIGGRAPH Asia 2023 Doctoral Consortium},
year = {2023},
isbn = {9798400703928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3613347.3613360,
author = {Wen, Like and Zhuang, Mansheng and Kwok, Alex Pak Ki and Yan, Mian},
title = {Beyond Charts: A Mixed-Reality System for Visualizing and Experiencing Data},
year = {2023},
isbn = {9798400700187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613347.3613360},
doi = {10.1145/3613347.3613360},
abstract = {Data visualization plays a crucial role in understanding complex datasets. However, traditional data visualization methods often fall short of engaging non-professionals and providing immersive experiences. This drawback can make it challenging and less attractive for students to understand the information and story hidden in the data. This research developed a mixed-reality-based data experience system to enhance student engagement during lectures. The system takes inspiration from car driving games and leverages mixed reality (MR) technology to create an interactive and visually captivating experience for data exploration. The system seamlessly integrates entertainment and data visualization to bring historical data to life by processing and to visualize the relevant data using a mountain driving track. Additionally, it allows multiple students to participate simultaneously, fostering collaborative learning. This novel approach to data experience holds significant potential to revolutionize student engagement and historical data presentations within educational settings.},
booktitle = {Proceedings of the 2023 6th International Conference on Mathematics and Statistics},
pages = {85–90},
numpages = {6},
keywords = {data visualization, driving simulator, education and training, mixed reality, student engagement},
location = {Leipzig, Germany},
series = {ICoMS '23}
}

@inproceedings{10.1145/3573381.3596150,
author = {Robert, Florent and Wu, Hui-Yin and Sassatelli, Lucile and Ramano\"{e}l, Stephen and Gros, Auriane and Winckler, Marco},
title = {An Integrated Framework for Understanding Multimodal Embodied Experiences in Interactive Virtual Reality},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596150},
doi = {10.1145/3573381.3596150},
abstract = {Virtual Reality (VR) technology enables “embodied interactions” in realistic environments where users can freely move and interact, with deep physical and emotional states. However, a comprehensive understanding of the embodied user experience is currently limited by the extent to which one can make relevant observations, and the accuracy at which observations can be interpreted. Paul Dourish proposed a way forward through the characterisation of embodied interactions in three senses: ontology, intersubjectivity, and intentionality. In a joint effort between computer and neuro-scientists, we built a framework to design studies that investigate multimodal embodied experiences in VR, and apply it to study the impact of simulated low-vision on user navigation. Our methodology involves the design of 3D scenarios annotated with an ontology, modelling intersubjective tasks, and correlating multimodal metrics such as gaze and physiology to derive intentions. We show how this framework enables a more fine-grained understanding of embodied interactions in behavioural research.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {14–26},
numpages = {13},
keywords = {user experience analysis, task modeling, scene ontology, navigation, interaction, immersion, Embodied experiences, 3D environments},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3582515.3609525,
author = {Franceschini, Andrea and Rod\`{a}, Antonio},
title = {Play to Learn: from Serious Games to just Games},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582515.3609525},
doi = {10.1145/3582515.3609525},
abstract = {Games have been used for a few decades in research, formal education, and training of children and adults, and digital games are no strangers to educational uses. After all, everyone likes to play games, so it should stand to reason that educational digital games are going to be a hit. Unfortunately, this is not the case. In fact, educational digital games are often criticized for being too focused on educational content and not enough on engaging, challenging, and entertaining players. Making games for entertainment is difficult and requires multidisciplinary expertise. Making educational games that are engaging and entertaining is also difficult and requires additional input from educators and domain experts, and rigorous evaluation methodologies, all of which must revolve around the players. In this position article, we introduce the early stage “EduGames: Play to Learn” research project aimed at supporting the public in acquiring Critical and Computational Thinking skills to tackle the problem of detecting misinformation, and supporting the game development and research communities in creating and evaluating games that are entertaining and educational. As part of this project, we call for more, and more structured, synergy between academia, educators, and the game development industry.},
booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
pages = {117–127},
numpages = {11},
keywords = {video games, serious games, entertainment, digital games},
location = {Lisbon, Portugal},
series = {GoodIT '23}
}

@proceedings{10.1145/3586182,
title = {UIST '23 Adjunct: Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3544549.3585730,
author = {McDade, Jeremy and Jing, Allison and Stanton, Tasha and Smith, Ross},
title = {Assessing Superhuman Speed as a Gamified Reward in a Virtual Reality Bike Exergame},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585730},
doi = {10.1145/3544549.3585730},
abstract = {This research presents a Virtual Reality (VR) bike exergame system consisting of a stationed road bike, four novel virtual steering methods, two in-game superhuman speed reward ideas, and a pedalling system simulating real-life resistance. A study is conducted to understand how virtual speed can be used as a reward to encourage enjoyment in physical activities. The result suggests that adding reward-based superhuman speed positively impacts user enjoyment. Adding steering control and increasing speed potentially do not inhibit greater motion sickness symptoms using well-aligned physical-to-virtual input and output representations.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {41},
numpages = {8},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@proceedings{10.1145/3574131,
title = {VRCAI '22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry},
year = {2022},
isbn = {9798400700316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@inproceedings{10.1145/3544549.3585639,
author = {Cai, Jinghe and Li, Xiaohan and Chen, Bohan and Wang, Zhigang and Jia, Jia},
title = {CatHill: Emotion-Based Interactive Storytelling Game as a Digital Mental Health Intervention},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585639},
doi = {10.1145/3544549.3585639},
abstract = {In this paper, we introduce CatHill, an emotion-based interactive storytelling game leveraging Cognitive Behavioral Therapy (CBT) to help college students with chronic mental health conditions. The game utilizes evidence-based stories to integrate three CBT techniques: exposure therapy, cognitive restructuring, and relaxation training. We propose a novel interface only controlled by players’ mouths to engage players better. The game allows players to chat with non-player characters (NPCs) and their speech emotions will deeply influence NPCs’ actions and story progression. Besides, players can also conduct mindful breathing exercises by breath control. Such fun and impressive interaction modes teach young people to understand the correlation among thoughts, emotions, and behaviors and change irrational automatic thoughts, and overcome anxiety or distress. Through our practice, we show that popular game elements and new interaction technologies have the potential to expand the impact of digital mental health interventions.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {64},
numpages = {7},
keywords = {CBT, emotion-based game, mental health},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3658852.3659089,
author = {Tokida, Satomi and Ishiguro, Yoshio},
title = {Dance with Rhythmic Frames: Improving Dancing Skills by Frame-by-Frame Presentation},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658852.3659089},
doi = {10.1145/3658852.3659089},
abstract = {Fitness video observation is a common approach in sports practice. However, in videos, important frames and others are presented at a constant speed, there is a substantial cognitive load in accurately capturing key movements and timing, especially in the quick-paced videos of complex activities such as dance. We hypothesize that extracting keyframes from dance videos and replaying them in sync with rhythm (frame-by-frame presentation) can reduce this cognitive load and improve the dance technique execution. Our first study using a 2D display suggests that frame-by-frame presentation is not only as preferred as conventional videos, but also enables more accurate learning of movements. Based on that, we developed DRF, a VR application that combines frame-by-frame presentation with motion trajectory visualization. User study results indicate that with DRF, users could significantly improve both choreographic dance technique and rhythm accuracy compared to video-based VR systems. Qualitative user evaluations from beginners, experienced dancers, and professionals expressed the benefits and potential use of the frame-by-frame presentation method.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {30},
numpages = {6},
keywords = {Education/Learning, Games/Play, Virtual/Augmented Reality},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3610977.3634923,
author = {Crosato, Luca and Wei, Chongfeng and Ho, Edmond S. L. and Shum, Hubert P. H. and Sun, Yuzhu},
title = {A Virtual Reality Framework for Human-Driver Interaction Research: Safe and Cost-Effective Data Collection},
year = {2024},
isbn = {9798400703225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610977.3634923},
doi = {10.1145/3610977.3634923},
abstract = {The advancement of automated driving technology has led to new challenges in the interaction between automated vehicles and human road users. However, there is currently no complete theory that explains how human road users interact with vehicles, and studying them in real-world settings is often unsafe and time-consuming. This study proposes a 3D Virtual Reality (VR) framework for studying how pedestrians interact with human-driven vehicles. The framework uses VR technology to collect data in a safe and cost-effective way, and deep learning methods are used to predict pedestrian trajectories. Specifically, graph neural networks have been used to model pedestrian future trajectories and the probability of crossing the road. The results of this study show that the proposed framework can be for collecting high-quality data on pedestrian-vehicle interactions in a safe and efficient manner. The data can then be used to develop new theories of human-vehicle interaction and aid the Autonomous Vehicles research.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {167–174},
numpages = {8},
keywords = {autonomous driving, human-robot interaction, unreal engine, virtual reality},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/3628096.3629055,
author = {Arendttorp, Emilie and Auala, Selma and Winschiers-Theophilus, Heike and Rodil, Kasper and Magot, Samkao},
title = {A Community-Based Exploration Into Gesture-driven Locomotion Control in Virtual Reality},
year = {2024},
isbn = {9798400708879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628096.3629055},
doi = {10.1145/3628096.3629055},
abstract = {Gesture-based control in virtual reality applications provides a more natural interaction but at the same time brings about new challenges in regards to implementing actions such as locomotion. In an effort to maintain a user-driven approach to the design of more intuitive and natural control gestures, we have run an exploratory study with a rural partner community in Southern Africa. We conducted one gesture elicitation session, which informed the implementation of a locomotion control gesture, which was evaluated after a second elicitation session. The findings will guide application specific implementations and further research to confirm user approval and minimized cybersickness.},
booktitle = {Proceedings of the 4th African Human Computer Interaction Conference},
pages = {180–189},
numpages = {10},
keywords = {Namibia, Virtual Reality, community-based co-design, diversity, gestures, locomotion, participatory design},
location = {East London, South Africa},
series = {AfriCHI '23}
}

@inproceedings{10.1145/3626705.3627769,
author = {Kocur, Martin and Mayer, Manuel and Karber, Amelie and Witte, Miriam and Henze, Niels and Bogon, Johanna},
title = {The Absence of Athletic Avatars' Effects on Physiological and Perceptual Responses while Cycling in Virtual Reality},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626705.3627769},
doi = {10.1145/3626705.3627769},
abstract = {Virtual reality (VR) allows to embody avatars—the digital self-representation of the user. An avatar’s appearance can change users’ perception and behavior—a phenomenon known as the Proteus effect. Previous work found that athletic avatars can reduce heart rate and perceived exertion during physical effort. Although these findings are promising to create more effective VR exercises, they have not been replicated yet. Hence, the reliability and consistency of such effects is unknown. Therefore, we conducted a study with 32 participants to investigate physiological and perceptual effects of athletic avatars while cycling in VR following a standardized exercise protocol. We could not find effects of the avatars’ athletic appearance on heart rate, perceived exertion, and imagined velocity. Our findings indicate that athletic avatars do not necessarily affect users during physical exertion. We discuss potential factors that can cause the Proteus effect fail to occur.},
booktitle = {Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
pages = {366–376},
numpages = {11},
keywords = {Proteus effect, avatars, body ownership, cycling, physical performance, virtual reality},
location = {Vienna, Austria},
series = {MUM '23}
}

@inproceedings{10.1145/3573382.3616060,
author = {Saxena, Roshni and Gaydos, Zachary and Saaty, Morva and Haqq, Derek and Nair, Priyanka and Grutzik, Gary and Wang, Wei Lu and Patel, Jaitun},
title = {Fit to Draw: An Elevation of Location-Based Exergames},
year = {2023},
isbn = {9798400700293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573382.3616060},
doi = {10.1145/3573382.3616060},
abstract = {Many location-based games have a multiplayer aspect; however, this is typically inconsequential to the actual gameplay, which is usually geared toward a single-player experience. Thus, we present Fit to Draw, a multiplayer location-based exergame that combines simple picture-guessing gameplay with physical movement. While other location-based games have the gameplay elements tangentially related to physical movement, Fit to Draw requires players to walk outdoors to draw a picture based on a given word. Companion players then guess what other players drew to earn points, providing a multiplayer and social experience that many other location-based games do not have. The goals of Fit to Draw are to motivate users to exercise, enjoy the outdoors, socialize, and have an opportunity to be creative.},
booktitle = {Companion Proceedings of the Annual Symposium on Computer-Human Interaction in Play},
pages = {312–317},
numpages = {6},
keywords = {Unity, Pictionary, Mobile Applications, Location-Based Games, GPS, Exergames, Arts},
location = {Stratford, ON, Canada},
series = {CHI PLAY Companion '23}
}

@inproceedings{10.1145/3544548.3580665,
author = {Liu, Shengmei and Kuwahara, Atsuo and Scovell, James J and Claypool, Mark},
title = {The Effects of Frame Rate Variation on Game Player Quality of Experience},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580665},
doi = {10.1145/3544548.3580665},
abstract = {For gamers, high frame rates are important for a smooth visual display and good quality of experience (QoE). However, high frame rates alone are not enough as variations in the frame display times can degrade QoE even as the average frame rate remains high. While the impact of steady frame rates on player QoE is fairly well-studied, the effects of frame rate variation is not. This paper presents a 33-person user study that evaluates the impact of frame rate variation on users playing three different computer games. Analysis of the results shows average frame rate alone is a poor predictor of QoE, and frame rate variation has a significant impact on player QoE. While the standard deviation of frame times is promising as a general predictor for QoE, frame time standard deviation may not be accurate for all individual games. However, 95% frame rate floor -– the bottom 5% of frame rates the player experiences –- appears to be an effective predictor of both QoE overall and for the individual games tested.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {573},
numpages = {10},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3543758.3547565,
author = {Liu, Shi and Toreini, Peyman and Maedche, Alexander},
title = {Designing Gaze-Aware Attention Feedback for Learning in Mixed Reality},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543758.3547565},
doi = {10.1145/3543758.3547565},
abstract = {Mixed Reality (MR) has demonstrated its potential in the application field of education. In particular, in contrast to traditional learning, students using MR get the possibility of learning and exploring the content in a self-directed way. Meanwhile, research in learning technology has revealed the significance of supporting learning activities with feedback. Since such feedback is often missing in MR-based learning environments, we propose a solution of using eye-tracking in MR to provide gaze-aware attention feedback to students and evaluate it with potential users in a preliminary user study.},
booktitle = {Proceedings of Mensch Und Computer 2022},
pages = {503–508},
numpages = {6},
keywords = {mixed reality, feedback for learning, eye-tracking},
location = {Darmstadt, Germany},
series = {MuC '22}
}

@inproceedings{10.1145/3570945.3607294,
author = {Steenstra, Ian and Murali, Prasanth and Perkins, Rebecca and Joseph, Natalie and Paasche-Orlow, Michael and Bickmore, Timothy},
title = {Changing Parent Attitudes Towards HPV Vaccination by Including Adolescents in Multiparty Counseling using Virtual Agents},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570945.3607294},
doi = {10.1145/3570945.3607294},
abstract = {Parental permission is required for medical care for children, and decisions may be made without incorporating children's views, even for adolescents. We explore the impact of including adolescents in virtual agent-based multiparty health counseling to promote Human Papillomavirus (HPV) vaccination. The agent is designed to encourage HPV vaccination for children aged 9-12 by engaging co-present parent/adolescent dyads in an online interaction. Several techniques are incorporated, including HPV education, motivational interviewing, persuasion, modeling, and enablement, to address parents' intent to vaccinate their children. We conduct a between-subjects randomized study comparing a version of the agent exclusively for the parent, to one that includes the adolescent in the conversation and incorporates the child's views in counseling strategies. We measure pre- and post-intervention changes in intent to vaccinate, vaccination hesitancy, and knowledge in both the parent and the adolescent, hypothesizing greater improvements in these measures when the adolescent is included in the conversation. We also examine the satisfaction, engagement, and comfort of the parent/child interactions with the virtual agent. We found significant pre-post increases in parent intent to vaccinate their adolescent for both versions of the agent. Our work provides insights into the effectiveness of the virtual agent in promoting HPV vaccination, the impact of child participation on healthcare decision-making, and the user experience of multi-party interaction with the virtual agent.},
booktitle = {Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
articleno = {7},
numpages = {8},
keywords = {Embodied Conversational Agents, Multi-Party Interactions, Persuasive Technology, Vaccination Promotion, Virtual Agents},
location = {W\"{u}rzburg, Germany},
series = {IVA '23}
}

@inproceedings{10.1145/3587819.3590977,
author = {Liu, Shengmei and Claypool, Mark},
title = {The Impact of Latency on Target Selection in First-Person Shooter Games},
year = {2023},
isbn = {9798400701481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587819.3590977},
doi = {10.1145/3587819.3590977},
abstract = {While target selection in a 2D space is fairly well-studied, target selection in a 3D space, such as shooting in first-person shooter (FPS) games, is not, nor are the benefits to players for many latency compensation techniques. This paper presents results from a user study that evaluates the impact of latency and latency compensation techniques on 3D target selection via a bespoke FPS shooter. Analysis of the results shows latency degrades player performance (time to select/shoot a target), with subjective opinions on Quality of Experience (QOE) following suit. Individual latency compensation techniques cannot fully overcome the effects of latency but combined techniques can, letting players perform and feel as if there is no network latency. We derive a basic analytic model for the distribution of the player selection times which can be part of a simulation of a full-range of FPS games.},
booktitle = {Proceedings of the 14th ACM Multimedia Systems Conference},
pages = {51–61},
numpages = {11},
keywords = {shooting, lag, user study, FPS, gamer},
location = {Vancouver, BC, Canada},
series = {MMSys '23}
}

@proceedings{10.1145/3590837,
title = {ICIMMI '22: Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
year = {2022},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jaipur, India}
}

@inproceedings{10.1145/3544549.3585881,
author = {Kruse, Lucie and Wittig, Joel and Finnern, Sebastian and Gundlach, Melvin and Iserlohe, Niclas and Ariza, Oscar and Steinicke, Frank},
title = {Blended Collaboration: Communication and Cooperation Between Two Users Across the Reality-Virtuality Continuum},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585881},
doi = {10.1145/3544549.3585881},
abstract = {Mixed reality (MR) technologies provide enormous potential for collaboration between multiple users across the Reality–Virtuality continuum. We evaluate communication in a MR-based two-user collaboration task, in which the users have to move an object through an obstacle without collision. We used a blended reality environment, in which one user is immersed in virtual reality, whereas the other uses mobile augmented reality. Both users have different abilities and information and mutually depend on each other for successful completion of the task. Communication consensus can either be achieved by using speech, visual widgets, or a combination of both. The results indicate that speech plays a fundamental role. The usage of widgets served as an extension rather than a replacement of language. However, the combination of speech and widgets improved the clearness of communication with less miscommunication. These results provide important indications about how to design blended collaboration across the Reality–Virtuality continuum.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {54},
numpages = {8},
keywords = {3D interaction, augmented reality, collaboration, communication, cooperation, multi-user XR, virtual reality},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3544549.3583824,
author = {Lee, Jaejun and Cho, Hyeyoon and Kim, Yonghyun},
title = {Bean Academy: A Music Composition Game for Beginners with Vocal Query Transcription},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583824},
doi = {10.1145/3544549.3583824},
abstract = {Bean Academy is a music composition game designed for musically-unskilled learners to lower entry barriers to music composition learning such as music theory comprehension, literacy and proficiency in utilizing music composition software. As a solution, Bean Academy’s Studio Mode was designed with the adaptation of an auditory-based ‘Vocal Query Transcription(VQT)’ model to enhance learners’ satisfaction and enjoyment towards music composition learning. Through the VQT model, players can experience a simple and efficient music composition process by experiencing their recorded voice input being transcripted into an actual musical piece. Based on our playtest, thematic analysis was conducted in two separate experiment groups. Here, we noticed that although Bean Academy does not outperform the current-level Digital Audio Workstation(DAW) in terms of performance or functionality, it can be highly considered as suitable learning material for musically-unskilled learners.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {591},
numpages = {6},
keywords = {Education game, Music composition, Serious game, Vocal Query Transcription},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3565066.3608687,
author = {Marques, Bernardo and Ferreira, Carlos and Silva, Samuel and Dias, Paulo and Santos, Beatriz Sousa},
title = {How to Notify Team Members during Asynchronous Remote Collaboration supported by Mixed Reality: Comparing Visual, Audio and Tactile Notifications},
year = {2023},
isbn = {9781450399241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565066.3608687},
doi = {10.1145/3565066.3608687},
abstract = {Solutions exploring Mixed Reality (MR) have been applied to remote scenarios, allowing to create a common ground. Literature reports that most studies focus on synchronous scenarios (team members interact in real-time). Therefore, an opportunity exists for asynchronous situations, in which collaborative actions occur at different moments and in which content is created/shared for later consumption. Hence, team members require notification methods to call their attention when new content is available. This work explored three notification cues: visual; audio; tactile, to comprehend which one captures the attention of team members easily. A user study was conducted with 26 participants to evaluate these conditions during asynchronous remote maintenance. The study was divided into two parts, focusing on the remote and on-site roles, given that both require methods to enhance their awareness. We report the results obtained, suggesting tactile cues represent the predominant condition for both roles.},
booktitle = {Proceedings of the 25th International Conference on Mobile Human-Computer Interaction},
articleno = {10},
numpages = {8},
keywords = {Visual, User Study, Remote Collaboration, Notification Cues, Mixed Reality, Audio and Tactile Cues},
location = {Athens, Greece},
series = {MobileHCI '23 Companion}
}

@article{10.1145/3546827,
author = {Pal, Amitangshu and Campagnaro, Filippo and Ashraf, Khadija and Rahman, Md Rashed and Ashok, Ashwin and Guo, Hongzhi},
title = {Communication for Underwater Sensor Networks: A Comprehensive Summary},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/3546827},
doi = {10.1145/3546827},
abstract = {Sensing and communication technology has been used successfully in various event monitoring applications over the last two decades, especially in places where long-term manual monitoring is infeasible. However, the major applicability of this technology was mostly limited to terrestrial environments. On the other hand, underwater wireless sensor networks (UWSNs) opens a new space for the remote monitoring of underwater species and faunas, along with communicating with underwater vehicles, submarines, and so on. However, as opposed to terrestrial radio communication, underwater environment brings new challenges for reliable communication due to the high conductivity of the aqueous medium which leads to major signal absorption. In this paper, we provide a detailed technical overview of different underwater communication technologies, namely acoustic, magnetic, and visual light, along with their potentials and challenges in submarine environments. Detailed comparison among these technologies have also been laid out along with their pros and cons using real experimental results.},
journal = {ACM Trans. Sen. Netw.},
month = {dec},
articleno = {22},
numpages = {44},
keywords = {visual light communication, magnetic induction communications, acoustic communications, RF communications, channel modeling, Underwater communication}
}

@inproceedings{10.1145/3638067.3638128,
author = {Carneiro, Nayana and Nunes, Caio and Raulino, Nat\~{a} and Gomes, Igor and Andrade, Rossana and Darin, Ticianne},
title = {IoT Games and Gamified Systems: Summertime Sadness or Lust for Hype?},
year = {2024},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638067.3638128},
doi = {10.1145/3638067.3638128},
abstract = {The Internet of Things (IoT) has gained much space in people’s everyday lives in the last few years, and it is already possible to see IoT applications widely used in various domains, such as healthcare and industry. The convergence of IoT and digital games also creates opportunities for HCI &amp; Digital Games research, supposedly opening countless possibilities for innovative games and challenging research. However, research on this topic appears to suffer from a generalized lack of clarity in its basics. Following this thread, we conducted a systematic mapping study and analyzed 22 papers that proposed or evaluated IoT games and gamified systems (IoTGGS), aiming to identify characteristics, patterns, and research opportunities related to HCI design and evaluation. However, our own frustrations in this endeavor also led us to identify shortcomings in commonly-used approaches. Thus, this paper contributes to the debate not only by focusing on the convergence between games and IoT with a systematic approach but also discussing the apparently dead-end we have found when closely analyzing this research topic. We present a brief panorama of the field and the identified gaps and raise possible new directions that could help take IoTGGS research to the next level.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
articleno = {23},
numpages = {12},
keywords = {Digital games, Gamified systems, Internet of Things, IoT games},
location = {Macei\'{o}, Brazil},
series = {IHC '23}
}

@inproceedings{10.1145/3577190.3614203,
author = {Pinitas, Kosmas and Renaudie, David and Thomsen, Mike and Barthet, Matthew and Makantasis, Konstantinos and Liapis, Antonios and Yannakakis, Georgios N.},
title = {Predicting Player Engagement in Tom Clancy's The Division 2: A Multimodal Approach via Pixels and Gamepad Actions},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614203},
doi = {10.1145/3577190.3614203},
abstract = {This paper introduces a large scale multimodal corpus collected for the purpose of analysing and predicting player engagement in commercial-standard games. The corpus is solicited from 25 players of the action role-playing game Tom Clancy’s The Division 2, who annotated their level of engagement using a time-continuous annotation tool. The cleaned and processed corpus presented in this paper consists of nearly 20 hours of annotated gameplay videos accompanied by logged gamepad actions. We report preliminary results on predicting long-term player engagement based on in-game footage and game controller actions using Convolutional Neural Network architectures. Results obtained suggest we can predict the player engagement with up to accuracy on average ( at best) when we fuse information from the game footage and the player’s controller input. Our findings validate the hypothesis that long-term (i.e. 1 hour of play) engagement can be predicted efficiently solely from pixels and gamepad actions.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {488–497},
numpages = {10},
keywords = {engagement modelling, digital games, datasets, convolutional neural networks, affect modelling},
location = {Paris, France},
series = {ICMI '23}
}

@proceedings{10.1145/3609025,
title = {FUNARCH 2023: Proceedings of the 1st ACM SIGPLAN International Workshop on Functional Software Architecture},
year = {2023},
isbn = {9798400702976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This workshop was born out of the observation that the functional 
programming community has developed a great body of knowledge on how 
to develop software. Increasingly, industry is applying functional 
programming to great effect in large-scale projects. Unfortunately, 
very little is written up on how to do this in comprehensive form. 
Thus, adopters of functional programming in the large must rely on 
folklore and experience (or wading through decades of ICFP papers). 
This makes functional programming effectively inaccessible to many 
architects, developers, and projects. One goal of this workshop is to 
be part of a long-term effort to address this problem. 

Furthermore, the software architecture community has developed a large 
body of useful knowledge, literature and pedagogy, largely untouched 
by functional programming. The two communities have much to learn 
from each other. Facilitating this cross-pollination is another goal 
of this workshop.},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/3562939.3565611,
author = {Wu, Fei and Suma Rosenberg, Evan},
title = {Adaptive Field-of-view Restriction: Limiting Optical Flow to Mitigate Cybersickness in Virtual Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565611},
doi = {10.1145/3562939.3565611},
abstract = {Dynamic field-of-view (FOV) restriction is a widely used software technique to mitigate cybersickness in commercial virtual reality (VR) applications. The classical FOV restrictor is implemented using a symmetric mask that occludes the periphery in response to translational and/or angular velocity. In this paper, we introduce adaptive field-of-view restriction, a novel technique that responds dynamically based on real-time assessment of optical flow generated by movement through a virtual environment. The adaptive restrictor utilizes an asymmetric mask to obscure regions of the periphery with higher optical flow during virtual locomotion while leaving regions with lower optical flow visible. To evaluate the proposed technique, we conducted a gender-balanced user study (N = 38) in which participants completed in a navigation task in two different types of virtual scenes using controller-based locomotion. Participants were instructed to navigate through either close-quarter or open virtual environments using adaptive restriction, traditional symmetric restriction, or an unrestricted control condition in three VR sessions separated by at least 24 hours. The results showed that the adaptive restrictor was effective in mitigating cybersickness and reducing subjective discomfort, while simultaneously enabling participants to remain immersed for a longer amount of time compared to the control condition. Additionally, presence ratings were significantly higher when using the adaptive restrictor compared to symmetric restriction. In general, these results suggest that adaptive field-of-view restriction based on real-time measurement of optical flow is a promising approach for virtual reality applications that seek to provide a better cost-benefit tradeoff between comfort and a high-fidelity experience.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {28},
numpages = {11},
keywords = {optical flow, field-of-view, cybersickness, Virtual reality},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@inproceedings{10.1145/3544549.3585816,
author = {Doerr, Nina and Angerbauer, Katrin and Reinelt, Melissa and Sedlmair, Michael},
title = {Bees, Birds and Butterflies: Investigating the Influence of Distractors on Visual Attention Guidance Techniques},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585816},
doi = {10.1145/3544549.3585816},
abstract = {Visual attention guidance methods direct the viewer’s gaze in immersive environments by visually highlighting elements of interest. The highlighting can be done, for instance, by adding a colored circle around elements, adding animated swarms (HiveFive), or removing objects from one eye in a stereoscopic display (Deadeye). We contribute a controlled user experiment (N=30) comparing these three techniques under the influence of visual distractors, such as bees flying by. Our results show that Circle and HiveFive performed best in terms of task performance and qualitative feedback, and were largely robust against different levels of distractions. Furthermore, we discovered a high mental demand for Deadeye.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {51},
numpages = {7},
keywords = {attention guidance, perception, virtual reality, visual attention},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3544548.3580888,
author = {Zenner, Andr\'{e} and Ullmann, Kristin and Ariza, Oscar and Steinicke, Frank and Kr\"{u}ger, Antonio},
title = {Induce a Blink of the Eye: Evaluating Techniques for Triggering Eye Blinks in Virtual Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580888},
doi = {10.1145/3544548.3580888},
abstract = {As more and more virtual reality (VR) headsets support eye tracking, recent techniques started to use eye blinks to induce unnoticeable manipulations to the virtual environment, e.g., to redirect users’ actions. However, to exploit their full potential, more control over users’ blinking behavior in VR is required. To this end, we propose a set of reflex-based blink triggers that are suited specifically for VR. In accordance with blink-based techniques for redirection, we formulate (i) effectiveness, (ii) efficiency, (iii) reliability, and (iv) unobtrusiveness as central requirements for successful triggers. We implement the soft- and hardware-based methods and compare the four most promising approaches in a user study. Our results highlight the pros and cons of the tested triggers, and show those based on the menace, corneal, and dazzle reflexes to perform best. From these results, we derive recommendations that help choosing suitable blink triggers for VR applications.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {256},
numpages = {12},
keywords = {blink triggers, change blindness, eye blinks, virtual reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3613904.3642626,
author = {Alabood, Lorans and Dow, Travis and Feeley, Kaylyn B and Jaswal, Vikram K. and Krishnamurthy, Diwakar},
title = {From Letterboards to Holograms: Advancing Assistive Technology for Nonspeaking Autistic Individuals with the HoloBoard},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642626},
doi = {10.1145/3613904.3642626},
abstract = {About one-third of autistic individuals are nonspeaking, i.e., they cannot use speech to convey their thoughts reliably. Many in this population communicate via spelling, a process in which they point to letters on a letterboard held upright in their field of view by a trained Communication and Regulation Partner (CRP). This paper focuses on transitioning such individuals to more independent, digital spelling that requires less support from the CRP, a goal most nonspeakers we consulted with desire. To enable this transition, we followed an approach that mimics an environment familiar to the nonspeaker and that harnesses the skills they already possess from physical letterboard training. Using this approach, we developed HoloBoard, a system that allows a nonspeaker, their CRP, and others, e.g., researchers, to share a common Augmented Reality (AR) environment containing a virtual letterboard. We configured the system to offer a brief (less than 10 minutes, on average) training module with graduated spelling tasks on the virtual letterboard. In a study involving 23 participants, 16 completed the entire module. These participants were able to spell words on the virtual letterboard without the CRP holding that board, an outcome we had not expected. When offered the opportunity to continue interacting with the virtual letterboard after the training module, 14 performed more complicated tasks than we had anticipated, spelling full sentences, or even offering feedback on the HoloBoard using solely the virtual board. Furthermore, five of these participants used the system solo, i.e., with the CRP and researchers absent from the virtual environment. These results suggest that training with the HoloBoard can lay the foundation for more independent communication, providing new social and educational opportunities for this marginalized population.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {71},
numpages = {18},
keywords = {Cross-reality, accessibility, assistive technology, extended reality, nonspeaking autistic people},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@proceedings{10.1145/3626203,
title = {PEARC '24: Practice and Experience in Advanced Research Computing 2024: Human Powered Computing},
year = {2024},
isbn = {9798400704192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Providence, RI, USA}
}

@inproceedings{10.1145/3544549.3585716,
author = {Lingelbach, Katharina and Diers, Daniel and Vukeli\'{c}, Mathias},
title = {Towards User-Aware VR Learning Environments: Combining Brain-Computer Interfaces with Virtual Reality for Mental State Decoding},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585716},
doi = {10.1145/3544549.3585716},
abstract = {User-aware adaptive systems can greatly benefit from brain-computer interface (BCI) technologies. BCIs allow continuous monitoring of users’ mental states and tailoring of the system to their individual skills and needs. We conducted a feasibility study integrating a BCI using functional near-infrared spectroscopy (fNIRS) into a virtual reality (VR) environment for realistic industrial learning scenarios. Using a fNIRS-based BCI allowed us to a) identify learning progress of individuals based on their working memory load across multiple learning sessions and b) investigate the underlying brain patterns. Our results showed a non-linear relationship between task difficulty and brain responses in the prefrontal cortex (PFC). Finally, we were able to draw four major conclusions regarding architecture components and vital research perspectives, to progress towards a vision of user-aware adaptive system design.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {292},
numpages = {8},
keywords = {Brain-Computer Interface (BCI), Learning, Mental State Monitoring, Working Memory, Workload},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3561212.3561234,
author = {McIntosh, Tyler Howard and Weinel, Jonathan and Cunningham, Stuart},
title = {Lundheim: Exploring Affective Audio Techniques in an Action-Adventure Video Game},
year = {2022},
isbn = {9781450397018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561212.3561234},
doi = {10.1145/3561212.3561234},
abstract = {This paper discusses Lundheim, a video game prototype made in Unity that incorporates interactive mechanisms based on affective computing techniques, which are used to control audio-visual aspects of the game. The project is based on a fictitious Old Norse realm named ’Lundheim’, a place where emotions are woven into the fabric of reality. The game utilises Russell’s circumplex model of affect, providing four runes which correspond with different sections of the circumplex model. The player must activate each rune by entering the corresponding emotion state, which is captured using a consumer-grade Interaxon Muse electroencephalograph (EEG) headband. Activating each emotion triggers particle effects and corresponding sonic materials including interactive music, which are implemented with the Wwise video game audio middleware software. The project thereby demonstrates a novel implementation of affective technologies and sound in a video game, contributing towards discourses in this area of research.},
booktitle = {Proceedings of the 17th International Audio Mostly Conference},
pages = {155–158},
numpages = {4},
keywords = {video games, sound design, interactive music, game mechanics, biofeedback, affective computing, action-adventure games},
location = {St. P\"{o}lten, Austria},
series = {AM '22}
}

@inproceedings{10.1145/3613904.3642292,
author = {Dufresne, Florian and Nilsson, Tommy and Gorisse, Geoffrey and Guerra, Enrico and Zenner, Andr\'{e} and Christmann, Olivier and Bensch, Leonie and Callus, Nikolai Anton and Cowley, Aidan},
title = {Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence for Operational Assessments in Virtual Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642292},
doi = {10.1145/3613904.3642292},
abstract = {Space agencies are in the process of drawing up carefully thought-out Concepts of Operations (ConOps) for future human missions on the Moon. These are typically assessed and validated through costly and logistically demanding analogue field studies. While interactive simulations in Virtual Reality (VR) offer a comparatively cost-effective alternative, they have faced criticism for lacking the fidelity of real-world deployments. This paper explores the applicability of passive haptic interfaces in bridging the gap between simulated and real-world ConOps assessments. Leveraging passive haptic props (equipment mockup and astronaut gloves), we virtually recreated the Apollo 12 mission procedure and assessed it with experienced astronauts and other space experts. Quantitative and qualitative findings indicate that haptics increased presence and embodiment, thus improving perceived simulation fidelity and validity of user reflections. We conclude by discussing the potential role of passive haptic modalities in facilitating early-stage ConOps assessments for human endeavours on the Moon and beyond.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {413},
numpages = {18},
keywords = {concepts of operations, embodiment, passive haptic feedback, presence, scenario assessment, space exploration, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@proceedings{10.1145/3669947,
title = {ICEDS '24: Proceedings of the 2024 5th International Conference on Education Development and Studies},
year = {2024},
isbn = {9798400718083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cambridge, United Kingdom}
}

@inproceedings{10.1145/3562939.3565628,
author = {Kocur, Martin and Bogon, Johanna and Mayer, Manuel and Witte, Miriam and Karber, Amelie and Henze, Niels and Schwind, Valentin},
title = {Sweating Avatars Decrease Perceived Exertion and Increase Perceived Endurance while Cycling in Virtual Reality},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565628},
doi = {10.1145/3562939.3565628},
abstract = {Avatars are used to represent users in virtual reality (VR) and create embodied experiences. Previous work showed that avatars’ stereotypical appearance can affect users’ physical performance and perceived exertion while exercising in VR. Although sweating is a natural human response to physical effort, surprisingly little is known about the effects of sweating avatars on users. Therefore, we conducted a study with 24 participants to explore the effects of sweating avatars while cycling in VR. We found that visualizing sweat decreases the perceived exertion and increases perceived endurance. Thus, users feel less exerted while embodying sweating avatars. We conclude that sweating avatars contribute to more effective exergames and fitness applications.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {29},
numpages = {12},
keywords = {virtual reality, sweating, perception of effort, exergames, body ownership, avatars, Proteus effect},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@article{10.1145/3660525,
author = {Wu, Ke and Dong, Dezun and Xu, Weixia},
title = {COER: A Network Interface Offloading Architecture for RDMA and Congestion Control Protocol Codesign},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3566},
url = {https://doi.org/10.1145/3660525},
doi = {10.1145/3660525},
abstract = {RDMA (Remote Direct Memory Access) networks require efficient congestion control to maintain their high throughput and low latency characteristics. However, congestion control protocols deployed at the software layer suffer from slow response times due to the communication overhead between host hardware and software. This limitation has hindered their ability to meet the demands of high-speed networks and applications. Harnessing the capabilities of rapidly advancing Network Interface Card (NIC) can drive progress in congestion control. Some simple congestion control protocols have been offloaded to RDMA NIC to enable faster detection and processing of congestion. However, offloading congestion control to the RDMA NIC faces a significant challenge in integrating the RDMA transport protocol with advanced congestion control protocols that involve complex mechanisms. We have observed that reservation-based proactive congestion control protocols share strong similarities with RDMA transport protocols, allowing them to integrate seamlessly and combine the functionalities of the transport layer and network layer. In this paper, we present COER, an RDMA NIC architecture that leverages the functional components of RDMA to perform reservations and completes the scheduling of congestion control during the scheduling process of the RDMA protocol. COER facilitates the streamlined development of offload strategies for congestion control techniques, specifically proactive congestion control, on RDMA NIC. We use COER to design offloading schemes for eleven congestion control protocols, which we implement and evaluate using a network emulator with a cycle-accurate RDMA NIC model that can load MPI programs. The evaluation results demonstrate that the architecture of COER does not compromise the original characteristics of the congestion control protocols. Compared to a layered protocol stack approach, COER enables the performance of RDMA networks to reach new heights.},
note = {Just Accepted},
journal = {ACM Trans. Archit. Code Optim.},
month = {apr},
keywords = {congestion control, RDMA, NIC, offloading}
}

@inproceedings{10.1145/3544549.3583827,
author = {Jeong, Sihyun and Yun, Hyun Ho and Lee, Yoonji and Han, Yeeun},
title = {Glow the Buzz: a VR Puzzle Adventure Game Mainly Played Through Haptic Feedback},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583827},
doi = {10.1145/3544549.3583827},
abstract = {Virtual Reality (VR) has become a popular tool, leading to increased demands for various immersive VR games for players. In addition, haptic technology is gaining attention as it adds a sense of touch to the visual and auditory dominant Human-Computer Interface (HCI) in terms of providing more extended VR experiences. However, most games, including VR, use haptics as a supplement while mostly depending on the visual elements as their main mode of transferring information. It is because the complexity of haptic in accurately capturing and replicating touch is still in its infancy. To further investigate the potential of haptics, we propose to Glow the Buzz, a VR game in which haptic feedback serves as a core element using wearable haptic devices. Our research explores whether haptic stimuli can be a primary form of interaction by conceiving iterative playtests for three puzzle designs - rhythm, texture, and direction. By proposing a VR haptic puzzle game that cannot be played without haptics, the study concludes that haptic technology in VR has the potential extendability. The study also suggests elements that enhance discriminability of haptic stimuli in each puzzle.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {593},
numpages = {6},
keywords = {Haptic Direction, Haptic Rhythm, Haptic Texturing, Haptic-central Gameplay, VR games, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@proceedings{10.1145/3659994,
title = {FRAME '24: Proceedings of the 4th Workshop on Flexible Resource and Application Management on the Edge},
year = {2024},
isbn = {9798400706417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Pisa, Italy}
}

@inproceedings{10.1145/3652920.3652928,
author = {Teo, Theophilus and Sakurada, Kuniharu and Sugimoto, Maki and Lee, Gun and Billinghurst, Mark},
title = {Evaluations of Parallel Views for Sequential VR Search Tasks},
year = {2024},
isbn = {9798400709807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652920.3652928},
doi = {10.1145/3652920.3652928},
abstract = {In collaborative virtual environments, sharing a mutual first-person view can lead to different problem-solving strategies among users. What if all views are controlled and seen by the same user? Could this impact how visual search tasks are performed? This paper explores the effects of a user receiving different numbers of parallel views while solving object search tasks in a virtual environment. We developed a prototype and conducted a pilot study, comparing two (2Heads), four (4Heads), and eight (8Heads) additional views. The results suggested that participants’ behaviors could be influenced by the number of parallel views used to solve tasks. Participants found the experiences with 2Heads and 8Heads unpleasant. In 2Heads, the ability to see additional but partial perspectives discouraged participants from using parallel views. Conversely, in 8Heads, the view’s full and overlapping nature forced participants to use parallel views regardless of their preferences. 4Heads received the least complaints as it provided users with the freedom and flexibility to choose their task-solving strategies. These results were translated into design implications for future development and research involving parallel views.},
booktitle = {Proceedings of the Augmented Humans International Conference 2024},
pages = {148–156},
numpages = {9},
keywords = {Eye Gaze, Multiple Perspectives, Parallel Experience, Virtual Reality},
location = {Melbourne, VIC, Australia},
series = {AHs '24}
}

@inproceedings{10.1145/3626705.3627801,
author = {L\"{o}chtefeld, Markus and M\o{}ller, Frederik and Kyster, Lukas and Petersen, Anton Nygaard and Jaqu\'{e}, Sture Nicolai and Larsen, Anita and Ziadeh, Hamzah},
title = {FridgeSort - Improving Fridge Sorting behaviour to reduce Food Waste through a Mobile Serious Game},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626705.3627801},
doi = {10.1145/3626705.3627801},
abstract = {Food waste in private households in industrialized countries is a growing concern. Inadequate food storage practices can lead to storing items in a way that leads to forgetting them until they become spoiled, increasing food waste. In this paper, we present FridgeSort, a serious mobile game that aims at teaching users how to sort their refrigerators with the underlying goal of reducing household food waste. The game was evaluated with 14 participants, and the results show that the participants were able to quickly learn the sorting method and recall it after an extended period of time. Overall, the results suggest that FridgeSort succeeded in teaching users how to sort a fridge that can be applied to the real world and lead to a reduction in food waste.},
booktitle = {Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
pages = {420–427},
numpages = {8},
keywords = {Food Waste, Fridge Organization, Serious Games, Sustainability},
location = {Vienna, Austria},
series = {MUM '23}
}

@inproceedings{10.1145/3544548.3580751,
author = {Tang, Kymeng and Gerling, Kathrin and Vanden Abeele, Vero and Geurts, Luc and Aufheimer, Maria},
title = {Playful Reflection: Impact of Gamification on a Virtual Reality Simulation of Breastfeeding},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580751},
doi = {10.1145/3544548.3580751},
abstract = {Gamification is a popular technique to improve task engagement, and has broadly been deployed in health and education to a point where many users now expect gameful experiences in these settings. However, gamification has been criticised for being a potential obstacle to the experience of reflection. Motivated by this tension, our work examines how the addition of gamification to a Virtual Reality simulation of breastfeeding impacts player experience and reflection. Using a within-subjects design, we invited 34 participants to take part in a mixed-methods evaluation of a gamified and non-gamified variant of the simulation that included questionnaires and semi-structured interviews. Results show that gamification improved player experience and encouraged players to reflect on goal achievement and performance. However, it also diverted players’ attention from nuances within the act of nursing. Drawing on our findings, we contribute considerations for the application of gamification in personal and sensitive settings such as breastfeeding.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {787},
numpages = {13},
keywords = {Breastfeeding, Gamification, Reflection, Simulation, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3587819,
title = {MMSys '23: Proceedings of the 14th ACM Multimedia Systems Conference},
year = {2023},
isbn = {9798400701481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1145/3573382.3616056,
author = {Lam, Juan Francisco and Henry, Spencer and Melilli, Kalli and Garrett, Ryan and Kang, Hyo Jeong},
title = {The Sea Is a Sky: Towards a Poetry of Motion},
year = {2023},
isbn = {9798400700293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573382.3616056},
doi = {10.1145/3573382.3616056},
abstract = {This paper presents "The Sea is a Sky," an immersive virtual reality (VR) poetry experience that takes inspiration from the Cuban poem "Del Otro Lado Del Mar," which translates to "on the other side of the sea." In this VR game, players are transported into the journey of a Cuban immigrant as they leave their homeland in search of a better future in America. Through physical actions such as crawling, swimming, and rowing within the VR environment, players embody the struggles and challenges faced by the immigrant protagonist. Throughout the experience, they encounter visionary hallucinations and hear messages of hope for the future as they navigate the vast ocean. However, the game also highlights the tragic reality of many Cuban immigrants who have lost their lives while attempting to cross the treacherous sea. The theme of drowning serves as a poignant symbol of their plight. The development of "The Sea is a Sky" involved collaboration among students from diverse disciplines, including English Literature, Computer Science, and Digital Arts. The project aims to achieve two primary objectives: raising awareness about social issues related to refugees and immigrants, and pushing the boundaries of game design by creating an engaging experience for younger generations to connect with poetry through the medium of virtual reality. While poetry has the inherent power to evoke deep reflection, it often struggles to resonate with younger individuals in its traditional forms. By harnessing the potential of virtual reality, our project seeks to expand the horizons of traditional poetry and explore innovative ways of experiencing it in the digital realm. The design and implementation of our VR experience not only offer a unique and engaging approach to poetry but also have the potential to inform future applications of VR in digital poetry and the cultivation of empathy among audiences.},
booktitle = {Companion Proceedings of the Annual Symposium on Computer-Human Interaction in Play},
pages = {286–291},
numpages = {6},
keywords = {virtual reality, serious game, embodiment},
location = {Stratford, ON, Canada},
series = {CHI PLAY Companion '23}
}

@inproceedings{10.1145/3623462.3623474,
author = {Bertocci, Stefano and Cioli, Federico and Cottini, Anastasia},
title = {Unlocking cultural heritage: leveraging georeferenced tools and open data for enhanced cultural tourism experiences.},
year = {2023},
isbn = {9798400708367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623462.3623474},
doi = {10.1145/3623462.3623474},
abstract = {This article underscores the importance of digitally preserving and accessing Cultural Heritage through techniques like 3D modelling, WebGIS, and cloud storage. It explores collaborative efforts using crowdsourcing and Linked Data initiatives. The second part focuses on integrated digitisation strategies, advocating an integrated approach involving computer and human sciences, addressing tangible and intangible heritage, globalisation's impact, and UNESCO guidelines. The article shares insights from the research group's experiences in digitising Cultural Heritage, encompassing projects at territorial and urban scales. These initiatives encompass diverse documentation methods, 3D modeling, and innovative technologies for disseminating data and creating virtual experiences. Ultimately, the article concludes by underlining how digital technologies have the potential to enrich Cultural Heritage, for example fostering sustainable tourism.},
booktitle = {Proceedings of the 20th International Conference on Culture and Computer Science: Code and Materiality},
articleno = {6},
numpages = {9},
location = {Lisbon, Portugal},
series = {KUI '23}
}

@inproceedings{10.1145/3503161.3548220,
author = {Subramanyam, Shishir and Viola, Irene and Jansen, Jack and Alexiou, Evangelos and Hanjalic, Alan and Cesar, Pablo},
title = {Evaluating the Impact of Tiled User-Adaptive Real-Time Point Cloud Streaming on VR Remote Communication},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548220},
doi = {10.1145/3503161.3548220},
abstract = {Remote communication has rapidly become a part of everyday life in both professional and personal contexts. However, popular video conferencing applications present limitations in terms of quality of communication, immersion and social meaning. VR remote communication applications offer a greater sense of co-presence and mutual sensing of emotions between remote users. Previous research on these applications has shown that realistic point cloud user reconstructions offer better immersion and communication as compared to synthetic user avatars. However, photorealistic point clouds require a large volume of data per frame and are challenging to transmit over bandwidth-limited networks. Recent research has demonstrated significant improvements to perceived quality by optimizing the usage of bandwidth based on the position and orientation of the user's viewport with user-adaptive streaming. In this work, we developed a real-time VR communication application with an adaptation engine that features tiled user-adaptive streaming based on user behaviour. The application also supports traditional network adaptive streaming. The contribution of this work is to evaluate the impact of tiled user-adaptive streaming on quality of communication, visual quality, system performance and task completion in a functional live VR remote communication system. We performed a subjective evaluation with 33 users to compare the different streaming conditions with a neck exercise training task. As a baseline, we use uncompressed streaming requiring approximately 300 megabits per second and our solution achieves similar visual quality with tiled adaptive streaming at 14 megabits per second. We also demonstrate statistically significant gains in the quality of interaction and improvements to system performance and CPU consumption with tiled adaptive streaming as compared to the more traditional network adaptive streaming.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3094–3103},
numpages = {10},
keywords = {virtual reality, remote communication, adaptive streaming, 3D point clouds},
location = {Lisboa, Portugal},
series = {MM '22}
}

@proceedings{10.1145/3569951,
title = {PEARC '23: Practice and Experience in Advanced Research Computing 2023: Computing for the Common Good},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Portland, OR, USA}
}

@inproceedings{10.1145/3517428.3544802,
author = {Nair, Vishnu and Ma, Shao-en and Gonzalez Penuela, Ricardo E. and He, Yicheng and Lin, Karen and Hayes, Mason and Huddleston, Hannah and Donnelly, Matthew and Smith, Brian A.},
title = {Uncovering Visually Impaired Gamers’ Preferences for Spatial Awareness Tools Within Video Games},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517428.3544802},
doi = {10.1145/3517428.3544802},
abstract = {Sighted players gain spatial awareness within video games through sight and spatial awareness tools (SATs) such as minimaps. Visually impaired players (VIPs), however, must often rely heavily on SATs to gain spatial awareness, especially in complex environments where using rich ambient sound design alone may be insufficient. Researchers have developed many SATs for facilitating spatial awareness within VIPs. Yet this abundance disguises a gap in our understanding about how exactly these approaches assist VIPs in gaining spatial awareness and what their relative merits and limitations are. To address this, we investigate four leading approaches to facilitating spatial awareness for VIPs within a 3D video game context. Our findings uncover new insights into SATs for VIPs within video games, including that VIPs value position and orientation information the most from an SAT; that none of the approaches we investigated convey position and orientation effectively; and that VIPs highly value the ability to customize SATs.},
booktitle = {Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {6},
numpages = {16},
keywords = {Audio navigation tools, blind-accessible games, spatial awareness tools, visual impairments},
location = {Athens, Greece},
series = {ASSETS '22}
}

@inproceedings{10.1145/3543507.3587432,
author = {Yang, Simin and Gao, Ze and Hadi Mogavi, Reza and Hui, Pan and Braud, Tristan},
title = {Tangible Web: An Interactive Immersion Virtual Reality Creativity System that Travels Across Reality},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3587432},
doi = {10.1145/3543507.3587432},
abstract = {With the advancement of virtual reality (VR) technology, virtual displays have become integral to how museums, galleries, and other tourist destinations present their collections to the public. However, the current lack of immersion in virtual reality displays limits the user’s ability to experience and appreciate its aesthetics. This paper presents a case study of a creative approach taken by a tourist attraction venue in developing a physical network system that allows visitors to enhance VR’s aesthetic aspects based on environmental parameters gathered by external sensors. Our system was collaboratively developed through interviews and sessions with twelve stakeholder groups interested in art and exhibitions. This paper demonstrates how our technological advancements in interaction, immersion and visual attractiveness surpass those of earlier virtual display generations. Through multimodal interaction, we aim to encourage innovation on the Web and create more visually appealing and engaging virtual displays. It is hoped that the greater online art community will gain fresh insight into how people interact with virtual worlds as a result of this work.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3915–3922},
numpages = {8},
keywords = {Creativity, Digital Storytelling, Human-Machine Interaction, Human-centred Design, Immersion, User Engagement, User Experience Design, Web VR, Web-based Application},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3563657.3596114,
author = {Tsenova, Violeta and Wood, Gavin and Kirk, David},
title = {Loci Stories: Exploring Design for Polyvocality&nbsp;},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596114},
doi = {10.1145/3563657.3596114},
abstract = {We describe the design of Loci Stories – a networked storytelling cabinet which uses voice recognition to tell colourful and unique stories from volunteers working at an English country hall. Loci Stories was created as part of a wider co-design research within heritage venues that sought to improve interpretation and designs approaches engaging with polyvocality. The work aimed to nuance existing authorised heritage discourse interpretations by championing the stories of volunteers who have a unique perspective on the place. We contribute an account of our design and its evaluation findings to propose four “textures” of polyvocality inspired by music theory – monophony, homophony, polyphony, and cacophony. We discuss Loci Stories through the lens of these textures and situate our work in furthering the design space of polyvocal heritage-based interaction design.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {15–30},
numpages = {16},
keywords = {Cultural Heritage, Polyvocal Design Textures, Polyvocal Interface, Polyvocality},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/3631085.3631286,
author = {Bicalho, Lu\'{\i}s Fernando and Baffa, Augusto and Feij\'{o}, Bruno},
title = {A Dynamic Difficulty Adjustment Algorithm With Generic Player Behavior Classification Unity Plugin In Single Player Games},
year = {2024},
isbn = {9798400716270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631085.3631286},
doi = {10.1145/3631085.3631286},
abstract = {With the game market growing year by year, game developers find themselves in an extremely competitive scenario. To draw players attention towards their game and to engage them even more during gameplay, one alternative is to apply a Dynamic Difficulty Adjustment algorithm. But the problem of the DDA approach is usually not the algorithm itself, but the player classification step. Therefore, we created a generic Unity Plugin that, allied with a Python API, will be able to classify players, using unsupervised and supervised Machine Learning techniques, based on game telemetry. We also implemented our own simple DDA algorithm, to test how it would work allied with the online classification process. The results show that the DDA version outperforms the standard one in the Video-Game category (CEGE Framework). The resultant classification was 63% completely accurate and 100% partially accurate. Moreover, no other work was able to create a generic plugin that simplified the use of ML in the game development context, allowing to test 28 different algorithm combinations.},
booktitle = {Proceedings of the 22nd Brazilian Symposium on Games and Digital Entertainment},
pages = {76–85},
numpages = {10},
keywords = {adaptivity;, dynamic difficulty adjustment, machine learning, player classification},
location = {Rio Grande (RS), Brazil},
series = {SBGames '23}
}

@inproceedings{10.1145/3544548.3581064,
author = {Herbst, Yair and Wolf, Alon and Zelnik-Manor, Lihi},
title = {HUGO, a High-Resolution Tactile Emulator for Complex Surfaces},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581064},
doi = {10.1145/3544548.3581064},
abstract = {Many of our activities rely on tactile feedback perceived through mechanoreceptors in our skin. While visual and auditory devices provide immersive experiences, cutaneous feedback devices are typically limited in the range of sensations they provide and are hence usually used and tested on relatively simple synthetic surfaces. We present a device designed in a human-centered process, triggering the mechanoreceptors sensitive to pressure, low-frequency vibrations, and high-frequency vibrations, enabling one to experience touch of complex real-world surfaces. The device is based on a parallel manipulator and a pin-array, that operate simultaneously at 200Hz and emulate coarse and fine geometrical features, respectively. The decomposition into coarse and fine features, alongside the high operation frequency, enable simulation of virtual surfaces. This was corroborated via experiments on complex real-world surfaces via both a quantitative recognition test and a usability questionnaire. We believe that this design can be incorporated in numerous applications.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {746},
numpages = {15},
keywords = {Haptic Textures, Haptics, High-Resolution Haptics, Human Computer Interface, User Study},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3597638.3608398,
author = {May, Lloyd and Park, So Yeon and Berger, Jonathan},
title = {Enhancing Non-Speech Information Communicated in Closed Captioning Through Critical Design},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608398},
doi = {10.1145/3597638.3608398},
abstract = {The communication of non-speech information (NSI) in closed captioning is essential in providing full access and increased enjoyment of video content, particularly for d/Deaf or Hard-of-Hearing (DHH) viewers. We identified the limitations and frustrations of current NSI captioning through needfinding interviews and then employed a critical design framework to develop a medium-fidelity audio-reactive animated overlay prototype to explore the opinions and values of DHH users regarding NSI communication through surveys and interviews. The results show that current NSI captioning strategies lack consistency across platforms, adequate temporal information, clarity in emotional conveyance, and lack customization options. The study suggests that novel sound communication technologies show promise in enhancing certain aspects NSI communication and that there is a strong desire to move beyond the current, inflexible format of single-track closed captions for NSI communication.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {16},
numpages = {14},
keywords = {Accessibility, Captioning, Non-Speech Sounds},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3544548.3581162,
author = {Schlagowski, Ruben and Nazarenko, Dariia and Can, Yekta and Gupta, Kunal and Mertes, Silvan and Billinghurst, Mark and Andr\'{e}, Elisabeth},
title = {Wish You Were Here: Mental and Physiological Effects of Remote Music Collaboration in Mixed Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581162},
doi = {10.1145/3544548.3581162},
abstract = {With face-to-face music collaboration being severely limited during the recent pandemic, mixed reality technologies and their potential to provide musicians a feeling of "being there" with their musical partner can offer tremendous opportunities. In order to assess this potential, we conducted a laboratory study in which musicians made music together in real-time while simultaneously seeing their jamming partner’s mixed reality point cloud via a head-mounted display and compared mental effects such as flow, affect, and co-presence to an audio-only baseline. In addition, we tracked the musicians’ physiological signals and evaluated their features during times of self-reported flow. For users jamming in mixed reality, we observed a significant increase in co-presence. Regardless of the condition (mixed reality or audio-only), we observed an increase in positive affect after jamming remotely. Furthermore, we identified heart rate and HF/LF as promising features for classifying the flow state musicians experienced while making music together.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {102},
numpages = {16},
keywords = {Augmented Reality, Co-Presence, Head-mounted Displays, Mixed Reality, Networked Music Performance, Physiological Signal Processing, Psychophysiology, Remote Collaboration, Social Presence},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3527188.3561934,
author = {Guinot, Lena and Ando, Kozo and Iwata, Hiroyasu},
title = {Study on the Perception of Implicit Indication When Collaborating with an Artificial Agent: Example with the Japanese Rice Cake “Mochi” Making},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527188.3561934},
doi = {10.1145/3527188.3561934},
abstract = {Body language is an essential component of communication. The amount of unspoken information it transmits during interpersonal interactions is an invaluable complement to simple speech and makes the process smoother and more sustainable. On the contrary, existing approaches to human-robot collaboration and communication are not as intuitive, especially when it comes to robots transmitting information to their users. This is an issue that needs to be addressed if we aim to continue using artificial intelligence and machines to increase our capabilities, or as collaboration partners. The purpose of this study was to analyze the concept of a machine communicating with its user in a manner that closely relates to how humans communicate during collaborative work, using the Japanese rice cake making process as an example. First, the elements necessary to achieve the synchrony of individuals participating in the rice cake making process were clarified. Then, the same elements were reproduced in a simulated environment and applied to a rice cake making situation, this time requiring a collaboration between an individual and a robot. In this seccond step, the robot was made to mimick some key inter-personnal implicit communication elements discovered during the first stage of the study. Results showed an improvement in performance, along with a better capacity to predict and adapt motion according to the other party’s, suggesting that the system was capable of assisting in realizing a certain level of understanding between biological and artificial agents for collaborative work without any form of explicit communication.},
booktitle = {Proceedings of the 10th International Conference on Human-Agent Interaction},
pages = {140–147},
numpages = {8},
keywords = {inplicit indications, Virtual Reality, Human-Robot collaboration},
location = {Christchurch, New Zealand},
series = {HAI '22}
}

@article{10.1145/3640334,
author = {Song, Qunying and Engstr\"{o}m, Emelie and Runeson, Per},
title = {Industry Practices for Challenging Autonomous Driving Systems with Critical Scenarios},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3640334},
doi = {10.1145/3640334},
abstract = {Testing autonomous driving systems for safety and reliability is essential, yet complex. A primary challenge is identifying relevant test scenarios, especially the critical ones that may expose hazards or harm to autonomous vehicles and other road users. Although numerous approaches and tools for critical scenario identification are proposed, the industry practices for selection, implementation, and evaluation of approaches, are not well understood. Therefore, we aim at exploring practical aspects of how autonomous driving systems are tested, particularly the identification and use of critical scenarios. We interviewed 13 practitioners from 7 companies in autonomous driving in Sweden. We used thematic modeling to analyse and synthesize the interview data. As a result, we present 9 themes of practices and 4 themes of challenges related to critical scenarios. Our analysis indicates there is little joint effort in the industry, despite every approach has its own limitations, and tools and platforms are lacking. To that end, we recommend the industry and academia combine different approaches, collaborate among different stakeholders, and continuously learn the field. The contributions of our study are exploration and synthesis of industry practices and related challenges for critical scenario identification and testing, and potential increase of industry relevance for future studies.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {99},
numpages = {35},
keywords = {Critical scenario identification, testing, autonomous driving systems, industry practices, challenges, interview}
}

@inproceedings{10.1145/3544548.3581362,
author = {Cao, Jiaxun and He, Qingyang and Wang, Zhuo and LC, RAY and Tong, Xin},
title = {DreamVR: Curating an Interactive Exhibition in Social VR Through an Autobiographical Design Study},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581362},
doi = {10.1145/3544548.3581362},
abstract = {Virtual exhibitions have long been regarded as an extension of information delivery for physical exhibitions. However, what virtual exhibitions can offer audiences as a novel experience independently from physical exhibitions has been largely unexplored. In this study, we aim to understand the promises and challenges of experiencing and curating exhibitions in VR by interviewing nine expert curators. Drawing from expert insights, we summarized a set of design guidelines to inform what we can learn and adapt from physical exhibitions when curating in VR. Then, using an autobiographical design approach, we curated an interactive exhibition in VRChat to explore novel interaction techniques. We also hosted an open tour guide in the user study to validate our design guidelines with thirty participants. Results show that our approach of curating an exhibition in VRChat provided the participants with engaging and novel experiences interacting with the exhibits and other audiences.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {511},
numpages = {18},
keywords = {Virtual exhibition, autobiographical design study, interview, social virtual reality, virtual museum},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3659626,
author = {Kova\v{c}evi\'{c}, Nikola and Holz, Christian and Gross, Markus and Wampfler, Rafael},
title = {The Personality Dimensions GPT-3 Expresses During Human-Chatbot Interactions},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659626},
doi = {10.1145/3659626},
abstract = {Large language models such as GPT-3 and ChatGPT can mimic human-to-human conversation with unprecedented fidelity, which enables many applications such as conversational agents for education and non-player characters in video games. In this work, we investigate the underlying personality structure that a GPT-3-based chatbot expresses during conversations with a human. We conducted a user study to collect 147 chatbot personality descriptors from 86 participants while they interacted with the GPT-3-based chatbot for three weeks. Then, 425 new participants rated the 147 personality descriptors in an online survey. We conducted an exploratory factor analysis on the collected descriptors and show that, though overlapping, human personality models do not fully transfer to the chatbot's personality as perceived by humans. We also show that the perceived personality is significantly different from that of virtual personal assistants, where users focus rather on serviceability and functionality. We discuss the implications of ever-evolving large language models and the change they affect in users' perception of agent personalities.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {may},
articleno = {61},
numpages = {36},
keywords = {conversational agents, human-chatbot interaction, personality traits}
}

@inproceedings{10.1145/3579856.3590340,
author = {Braun, Lennart and Huppert, Moritz and Khayata, Nora and Schneider, Thomas and Tkachenko, Oleksandr},
title = {FUSE – Flexible File Format and Intermediate Representation for Secure Multi-Party Computation},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579856.3590340},
doi = {10.1145/3579856.3590340},
abstract = {Secure Multi-Party Computation (MPC) is continuously becoming more and more practical. Many optimizations have been introduced, making MPC protocols more suitable for solving real-world problems. However, the MPC protocols and optimizations are usually implemented as a standalone proof of concept or in an MPC framework and are tightly coupled with special-purpose circuit formats, such as Bristol Format. This makes it very hard and time-consuming to re-use algorithmic advances and implemented applications in a different context. Developing generic algorithmic optimizations is exceptionally hard because the available MPC tools and formats are not generic and do not provide the necessary infrastructure. In this paper, we present FUSE: A Framework for Unifying and Optimizing Secure Multi-Party Computation Implementations with Efficient Circuit Storage. FUSE provides a flexible intermediate representation (FUSE IR) that can be used across different platforms and in different programming languages, including C/C++, Java, Rust, and Python. We aim at making MPC tools more interoperable, removing the tight coupling between high-level compilers for MPC and specific MPC protocol engines, thus driving knowledge transfer. Our framework is inspired by the widely known LLVM compiler framework. FUSE is portable, extensible, and it provides implementation-agnostic optimizations. As frontends, we implement HyCC (CCS’18), the Bristol circuit format, and MOTION (TOPS’22), s.t. these can be automatically converted to FUSE IR. We implement several generic optimization passes, such as automatic subgraph replacement and vectorization, to showcase the utility and efficiency of our framework. Finally, we implement as backends MOTION and MP-SPDZ (CCS’20), so that FUSE IR can be run by these frameworks in an MPC protocol, as well as other useful backends for JSON output and the DOT language for graph visualization. With FUSE, it is possible to use any implemented frontend with any implemented backend and vice-versa. FUSE IR is not only efficient to work on and much more generic than any other format so far – supporting, e.g., function calls, hybrid MPC protocols as well as user-defined building blocks, and annotations – while maintaining backwards-compatibility, but also compact, with smaller storage size than even minimalistic formats such as Bristol already for a few hundred operations.},
booktitle = {Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
pages = {649–663},
numpages = {15},
location = {Melbourne, VIC, Australia},
series = {ASIA CCS '23}
}

@inproceedings{10.1145/3603555.3603577,
author = {Radiah, Rivu and Prodan, Pia and M\"{a}kel\"{a}, Ville and Knierim, Pascal and Alt, Florian},
title = {How Are Your Participants Feeling Today? Accounting For and Assessing Emotions in Virtual Reality},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603555.3603577},
doi = {10.1145/3603555.3603577},
abstract = {Emotions affect our perception, attention, and behavior. Hereby, the emotional state is greatly affected by the surrounding environment that can seamlessly be designed in Virtual Reality (VR). However, research typically does not account for the influence of the environment on participants’ emotions, even if this influence might alter acquired data. To mitigate the impact, we formulated a design space that explains how the creation of virtual environments influences emotions. Furthermore, we present EmotionEditor, a toolbox that assists researchers in rapidly developing virtual environments that influence and asses the users’ emotional state. We evaluated the capability of EmotionEditor to elicit emotions in a lab study (n=30). Based on interviews with VR experts (n=13), we investigate how they consider the effect of emotions in their research, how the EmotionEditor can prospectively support them, and analyze prevalent challenges in the design as well as development of VR user studies.},
booktitle = {Proceedings of Mensch Und Computer 2023},
pages = {37–48},
numpages = {12},
keywords = {emotions, user studies, virtual environments, virtual reality},
location = {Rapperswil, Switzerland},
series = {MuC '23}
}

@inproceedings{10.1145/3563359.3596991,
author = {Matsangidou, Maria and Solomou, Theodoros and H\o{}egh Langvad, Cecilie and Xynari, Katerina and Papayianni, Ersi and Pattichis, Constantinos S.},
title = {Virtual Reality Health Education to Prevent Musculoskeletal Disorders and Chronic Low Back Pain in Formal and Informal Caregivers.},
year = {2023},
isbn = {9781450398916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563359.3596991},
doi = {10.1145/3563359.3596991},
abstract = {Formal and informal caregivers are suffering from musculoskeletal disorders and low back pain which develops in practice. Previous research has suggested that health education can help to prevent musculoskeletal disorders and low back pain in caregivers. With the ability to simulate real-world scenarios, healthcare education is experiencing rapid growth in the use of immersive technologies such as Virtual Reality, aiming to enhance lifelong learning for caregivers, with promising results. However, the creation of such technologies has not been well documented, which forces educators to face the same setbacks during this development process. Using a user-centred design approach, which involved 14 medical experts and computer scientists, we designed a mobile Virtual Reality application to enhance learning and reduce work attributions for caregivers. Then we evaluated the system with a total of 30 formal and informal caregivers, documenting that Virtual Reality can be an effective solution for lifelong learning. Through this paper, we explain the process and analysis we run to identify how to create an effective Virtual Reality learning system.},
booktitle = {Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
pages = {343–351},
numpages = {9},
location = {Limassol, Cyprus},
series = {UMAP '23 Adjunct}
}

@inproceedings{10.1145/3582700.3582705,
author = {Kato, Ryoto and Kikuchi, Yusuke and Yem, Vibol and Ikei, Yasushi},
title = {Generation of realistic facial animation of a CG avatar speaking a moraic language},
year = {2023},
isbn = {9781450399845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582700.3582705},
doi = {10.1145/3582700.3582705},
abstract = {We propose a new method for generating real-time realistic facial animation using face mesh data corresponding to the fifty-six C+V (Consonant and Vowel) type morae that form the basis of Japanese speech. This method produces facial expressions by weighted addition of fifty-three face meshes based on the mapping of voice streaming to registered morae in real-time. Both photogrammetric models and existing off-the-shelf head models can be used as face meshes. Natural facial expressions of speech can be synthesized from a modeling to live animation in less than two hours. The results of a user study of our method showed that the facial expression during Japanese speech was more natural than popular real-time methods to generate facial animation, English-base Oculus Lipsync and volume intensity based facial animations.},
booktitle = {Proceedings of the Augmented Humans International Conference 2023},
pages = {77–85},
numpages = {9},
keywords = {Virtual human, Speech expression, Realistic avatar, Real-time facial animation},
location = {Glasgow, United Kingdom},
series = {AHs '23}
}

@article{10.1145/3550454.3555503,
author = {Nuvoli, Stefano and Pietroni, Nico and Cignoni, Paolo and Scateni, Riccardo and Tarini, Marco},
title = {SkinMixer: Blending 3D Animated Models},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3550454.3555503},
doi = {10.1145/3550454.3555503},
abstract = {We propose a novel technique to compose new 3D animated models, such as videogame characters, by combining pieces from existing ones. Our method works on production-ready rigged, skinned, and animated 3D models to reassemble new ones. We exploit mix-and-match operations on the skeletons to trigger the automatic creation of a new mesh, linked to the new skeleton by a set of skinning weights and complete with a set of animations. The resulting model preserves the quality of the input meshings (which can be quad-dominant and semi-regular), skinning weights (inducing believable deformation), and animations, featuring coherent movements of the new skeleton.Our method enables content creators to reuse valuable, carefully designed assets by assembling new ready-to-use characters while preserving most of the hand-crafted subtleties of models authored by digital artists. As shown in the accompanying video, it allows for drastically cutting the time needed to obtain the final result.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {250},
numpages = {15},
keywords = {model composition, skinning weights}
}

@inproceedings{10.1145/3613904.3642187,
author = {Du, Qiuxin and Song, Zhen and Jiang, Haiyan and Wei, Xiaoying and Weng, Dongdong and Fan, Mingming},
title = {LightSword: A Customized Virtual Reality Exergame for Long-Term Cognitive Inhibition Training in Older Adults},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642187},
doi = {10.1145/3613904.3642187},
abstract = {The decline of cognitive inhibition significantly impacts older adults’ quality of life and well-being, making it a vital public health problem in today’s aging society. Previous research has demonstrated that Virtual reality (VR) exergames have great potential to enhance cognitive inhibition among older adults. However, existing commercial VR exergames were unsuitable for older adults’ long-term cognitive training due to the inappropriate cognitive activation paradigm, unnecessary complexity, and unbefitting difficulty levels. To bridge these gaps, we developed a customized VR cognitive training exergame (LightSword) based on Dual-task and Stroop paradigms for long-term cognitive inhibition training among healthy older adults. Subsequently, we conducted an eight-month longitudinal user study with 12 older adults aged 60 years and above to demonstrate the effectiveness of LightSword in improving cognitive inhibition. After the training, the cognitive inhibition abilities of older adults were significantly enhanced, with benefits persisting for 6 months. This result indicated that LightSword has both short-term and long-term effects in enhancing cognitive inhibition. Furthermore, qualitative feedback revealed that older adults exhibited a positive attitude toward long-term training with LightSword, which enhanced their motivation and compliance.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {462},
numpages = {17},
keywords = {Health, Older Adults, Virtual Reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3647984,
author = {Capelli, Beatrice and Santos, Mar\'{\i}a and Sabattini, Lorenzo},
title = {Towards the Legibility of Multi-robot Systems},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
url = {https://doi.org/10.1145/3647984},
doi = {10.1145/3647984},
abstract = {Communication is crucial for human-robot collaborative tasks. In this context, legibility studies movement as the means of implicit communication between robotic systems and a human observer. This concept has been explored mostly for manipulators and humanoid robots. In contrast, little information is available in the literature about legibility of multi-robot systems or swarms, where simplicity and non-anthropomorphism of robots, along with the complexity of their interactions and aggregated behavior impose different challenges that are not encountered in single-robot scenarios. This article investigates legibility of multi-robot systems. Hence, we extend the definition of legibility, incorporating information about high-level goals in terms of the coordination objective of the group of robots, to previous results that focused solely on the legibility of spatial goals. A set of standard multi-robot algorithms corresponding to different coordination objectives are implemented and their legibility is evaluated in a user study, where participants observe the behavior of the multi-robot system in a virtual reality setup and are asked to identify the system’s spatial goal and coordination objective. The results of the study confirmed that coordination objectives are discernible by the users, hence multi-robot systems can be controlled to be legible, in terms of spatial goal and coordination objective.},
journal = {J. Hum.-Robot Interact.},
month = {jun},
articleno = {21},
numpages = {32},
keywords = {Multi-robot systems, human-centered robotics, swarms, human-aware multi-robot systems}
}

@proceedings{10.1145/3649405,
title = {ITiCSE 2024: Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 29th annual conference on Innovation and Technology in Computer Science Education (ITiCSE 2024), hosted by Universita degli Studi di Milano in Milan, Italy.ITiCSE 2024 will take place from Friday July 5 to Wednesday July 10. The conference program includes a keynote address, paper sessions, a panel, tips, techniques &amp; courseware demonstrations, posters, a doctoral consortium, and working group presentations. Working groups meet July 5-7 and will submit draft reports before the conference begins on July 8.The submissions to ITiCSE 2024 were reviewed by 446 researchers and practitioners from computing education and related fields, including 44 program committee members and 402 reviewers. Thanks to their outstanding effort and commitment, every submission received a metareview and most received at least three reviews, providing authors of all submissions with constructive feedback. Although no review process is flawless, we are confident that this effort led to a vibrant conference program, capturing multiple voices and perspectives in the field.},
location = {Milan, Italy}
}

@proceedings{10.1145/3573382,
title = {CHI PLAY Companion '23: Companion Proceedings of the Annual Symposium on Computer-Human Interaction in Play},
year = {2023},
isbn = {9798400700293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stratford, ON, Canada}
}

@inproceedings{10.1145/3581783.3611715,
author = {Wang, Yubin and Yu, Huimin and Yan, Yuming and Song, Shuyi and Liu, Biyang and Lu, Yichong},
title = {Exploring Shape Embedding for Cloth-Changing Person Re-Identification via 2D-3D Correspondences},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611715},
doi = {10.1145/3581783.3611715},
abstract = {Cloth-Changing Person Re-Identification (CC-ReID) is a common and realistic problem since fashion constantly changes over time and people's aesthetic preferences are not set in stone. While most existing cloth-changing ReID methods focus on learning cloth-agnostic identity representations from coarse semantic cues (e.g. silhouettes and part segmentation maps), they neglect the continuous shape distributions at the pixel level. In this paper, we propose Continuous Surface Correspondence Learning (CSCL), a new shape embedding paradigm for cloth-changing ReID. CSCL establishes continuous correspondences between a 2D image plane and a canonical 3D body surface via pixel-to-vertex classification, which naturally aligns a person image to the surface of a 3D human model and simultaneously obtains pixel-wise surface embeddings. We further extract fine-grained shape features from the learned surface embeddings and then integrate them with global RGB features via a carefully designed cross-modality fusion module. The shape embedding paradigm based on 2D-3D correspondences remarkably enhances the model's global understanding of human body shape. To promote the study of ReID under clothing change, we construct 3D Dense Persons (DP3D), which is the first large-scale cloth-changing ReID dataset that provides densely annotated 2D-3D correspondences and a precise 3D mesh for each person image, while containing diverse cloth-changing cases over all four seasons. Experiments on both cloth-changing and cloth-consistent ReID benchmarks validate the effectiveness of our method.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {7121–7130},
numpages = {10},
keywords = {shape embedding, large-scale dataset, cross-modality fusion, cloth-changing person re-identification, 2d-3d correspondences},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3582437.3582484,
author = {Rabii, Youn\`{e}s and Cook, Michael},
title = {Why Oatmeal is Cheap: Kolmogorov Complexity and Procedural Generation},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582484},
doi = {10.1145/3582437.3582484},
abstract = {Although procedural generation is popular among game developers, academic research on the topic has primarily focused on new applications, with some research into empirical analysis. In this paper we relate theoretical work in information theory to the generation of content for games. We prove that there is a relationship between the Kolomogorov complexity of the most complex artifact a generator can produce, and the size of that generator’s possibility space. In doing so, we identify the limiting relationship between the knowledge encoded in a generator, the density of its output space, and the intricacy of the artifacts it produces. We relate our result to the experience of expert procedural generator designers, and illustrate it with some examples.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {25},
numpages = {7},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3505270.3558326,
author = {Pedersen, Thomas R. and Ivanova, Iveta and Hj\o{}rringgaard, Michael and Fredsg\r{a}rd, Julie and Cornelius, Freya K. B. and Pust, Oliver Esman and Pohl, Henning},
title = {Eggventures: Strengthening Connectedness through Coop-Play},
year = {2022},
isbn = {9781450392112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505270.3558326},
doi = {10.1145/3505270.3558326},
abstract = {Cooperative play has been shown to increase connectedness and enjoyment. We developed Eggventures, a platforming game for two players, to investigate how movement mechanics influence connectedness. In one version of the game, the two players are tied together with a rubber band and have to make use of it to catapult each other through the levels. In a between-subjects study, we compare this to a version with only jumping. We find that both versions offer an enjoyable experience, but that the rubber band version increased player communication. A likely contributor to this is the increased level of challenge and frustration that forced players to engage more with each other to succeed.},
booktitle = {Extended Abstracts of the 2022 Annual Symposium on Computer-Human Interaction in Play},
pages = {70–74},
numpages = {5},
keywords = {platformer, cooperative game, connectedness},
location = {Bremen, Germany},
series = {CHI PLAY '22}
}

@inproceedings{10.1145/3514197.3549679,
author = {Murali, Prasanth and Nouraei, Farnaz and Fallah, Mina and Kearns, Aisling and Rebello, Keith and O'Leary, Teresa and Perkins, Rebecca and Joseph, Natalie Pierre and Dedier, Julien and Paasche-Orlow, Michael and Bickmore, Timothy},
title = {Training lay counselors with virtual agents to promote vaccination},
year = {2022},
isbn = {9781450392488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514197.3549679},
doi = {10.1145/3514197.3549679},
abstract = {Training laypeople to promote vaccination among their friends and family may be an effective way to boost the reach of vaccination interventions. We describe a virtual agent system that teaches laypeople communication and counseling skills using a combination of a pedagogical agent as well as a role-playing agent that takes on the persona of someone resistant to vaccination. We conducted a preliminary evaluation of the prototype, in which trainees first interacted with the prototype and then had a recorded conversation with a second person who was unvaccinated. Firstly, we found that trainees were mostly adherent to the skills taught by the agent. Secondly, there was a positive correlation of change in unvaccinated individuals' intent to get vaccinated with objective scores of display of empathy by the trainee during their conversation. Thirdly, unvaccinated partners rated the trainees high on relationship quality and use of empathic listening skills.},
booktitle = {Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents},
articleno = {20},
numpages = {8},
keywords = {virtual agents, vaccination promotion, role-playing, lay counselor training},
location = {Faro, Portugal},
series = {IVA '22}
}

@proceedings{10.1145/3568160,
title = {CSCS '22: Proceedings of the 6th ACM Computer Science in Cars Symposium},
year = {2022},
isbn = {9781450397865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ingolstadt, Germany}
}

@inproceedings{10.1145/3577530.3577546,
author = {Wang, Guan and Li, Yundong},
title = {Monocular depth estimation using synthetic data with domain-separated feature alignment},
year = {2023},
isbn = {9781450397773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577530.3577546},
doi = {10.1145/3577530.3577546},
abstract = {Depth estimation is a research hotspot in today's computer vision tasks, and its depth information is of great significance for many applications such as autonomous driving, 3D reconstruction, and object tracking. Although supervised depth estimation based on deep learning has high prediction accuracy, it is difficult to obtain a large amount of labeled data. Therefore, self-supervised monocular depth estimation methods that do not require labeled data have become the mainstream of research. Self-supervised learning utilizes the constraints of objects on spatial geometry, reducing the need for labeled data, but the problems of dynamic objects, shooting angles, and visibility weaken the effect of self-supervision. Using the accurate depth information of synthetic datasets to assist the self-supervised training of real datasets can improve the accuracy of self-supervised depth estimation, but most methods do not consider the distribution difference between synthetic data and real data, which affects the estimation effect. Aiming at this problem, this paper proposes a domain-separated Monocular Depth Estimation (DsMDE) algorithm based on domain separation network, which uses orthogonal loss to separate the public and private features of each domain, and then uses the maximum mean difference to The common features are aligned to reduce the difference between the synthetic domain and the real domain. The experimental results show that the DsMDE method proposed in this paper improves the depth estimation effect, and the depth estimation accuracy is better than that of the mainstream algorithms.},
booktitle = {Proceedings of the 2022 6th International Conference on Computer Science and Artificial Intelligence},
pages = {100–105},
numpages = {6},
keywords = {synthetic data, monocular depth estimation, migration learning, domain adaptation},
location = {Beijing, China},
series = {CSAI '22}
}

@proceedings{10.1145/3505270,
title = {CHI PLAY '22: Extended Abstracts of the 2022 Annual Symposium on Computer-Human Interaction in Play},
year = {2022},
isbn = {9781450392112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bremen, Germany}
}

@inproceedings{10.1145/3526113.3545701,
author = {Zhang, Ting and Hu, Zhenhong and Gupta, Aakar and Wu, Chi-Hao and Benko, Hrvoje and Jonker, Tanya R.},
title = {RIDS: Implicit Detection of a Selection Gesture Using Hand Motion Dynamics During Freehand Pointing in Virtual Reality},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545701},
doi = {10.1145/3526113.3545701},
abstract = {Freehand interactions with augmented and virtual reality are growing in popularity, but they lack reliability and robustness. Implicit behavior from users, such as hand or gaze movements, might provide additional signals to improve the reliability of input. In this paper, the primary goal is to improve the detection of a selection gesture in VR during point-and-click interaction. Thus, we propose and investigate the use of information contained within the hand motion dynamics that precede a selection gesture. We built two models that classified if a user is likely to perform a selection gesture at the current moment in time. We collected data during a pointing-and-selection task from 15 participants and trained two models with different architectures, i.e., a logistic regression classifier was trained using predefined hand motion features and a temporal convolutional network (TCN) classifier was trained using raw hand motion data. Leave-one-subject-out cross-validation PR-AUCs of 0.36 and 0.90 were obtained for each model respectively, demonstrating that the models performed well above chance (=0.13). The TCN model was found to improve the precision of a noisy selection gesture by 11.2% without sacrificing recall performance. An initial analysis of the generalizability of the models demonstrated above-chance performance, suggesting that this approach could be scaled to other interaction tasks in the future.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {4},
numpages = {12},
keywords = {virtual reality, temporal convolutional network, selection, pointing, interaction, hand motion dynamics, gesture detection},
location = {Bend, OR, USA},
series = {UIST '22}
}

@proceedings{10.1145/3610602,
title = {ICGJ '23: Proceedings of the 7th International Conference on Game Jams, Hackathons and Game Creation Events},
year = {2023},
isbn = {9798400708794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Ukraine}
}

@inproceedings{10.1145/3577190.3614120,
author = {Gemicioglu, Tan and Winters, R. Michael and Wang, Yu-Te and Gable, Thomas M. and Tashev, Ivan J.},
title = {TongueTap: Multimodal Tongue Gesture Recognition with Head-Worn Devices},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614120},
doi = {10.1145/3577190.3614120},
abstract = {Mouth-based interfaces are a promising new approach enabling silent, hands-free and eyes-free interaction with wearable devices. However, interfaces sensing mouth movements are traditionally custom-designed and placed near or within the mouth. TongueTap synchronizes multimodal EEG, PPG, IMU, eye tracking and head tracking data from two commercial headsets to facilitate tongue gesture recognition using only off-the-shelf devices on the upper face. We classified eight closed-mouth tongue gestures with 94% accuracy, offering an invisible and inaudible method for discreet control of head-worn devices. Moreover, we found that the IMU alone differentiates eight gestures with 80% accuracy and a subset of four gestures with 92% accuracy. We built a dataset of 48,000 gesture trials across 16 participants, allowing TongueTap to perform user-independent classification. Our findings suggest tongue gestures can be a viable interaction technique for VR/AR headsets and earables without requiring novel hardware.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {564–573},
numpages = {10},
keywords = {tongue interface, tongue gestures, non-intrusive, hands-free, BCI},
location = {Paris, France},
series = {ICMI '23}
}

@inproceedings{10.1145/3511808.3557292,
author = {Jiang, Hao and Liu, Yuntao and Li, Shengze and Zhang, Jieyuan and Xu, Xinhai and Liu, Donghong},
title = {Diverse Effective Relationship Exploration for Cooperative Multi-Agent Reinforcement Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557292},
doi = {10.1145/3511808.3557292},
abstract = {In some complex multi-agent environments, the types of relationships between agents are diverse and their intensity changes during the policy learning process. Theoretically, some of these relationships can facilitate cooperative policy learning. However, acquiring these relationships is an intractable problem. To tackle the problem, we propose a diverse effective relationship exploration based multi-agent reinforcement learning (DERE) method. Specifically, a potential fields model is firstly designed to represent relationships between agents. Then to encourage the exploration of effective relationships, we define an information-theoretic objective function. Finally, an intrinsic reward function is designed to optimize the information-theoretic objective, meanwhile, guide agents to learn more effective collaborative policies. Experimental results show that our method outperforms state-of-the-art methods on both super hard StarCraft II micromanagement tasks (SMAC) and Google Research Football (GRF).},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {842–851},
numpages = {10},
keywords = {potential field, multi-agent reinforcement learning, information-theoretic, effective relationship},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3544548.3581038,
author = {Ganesh, Anirudh and Ndulue, Chinenye and Orji, Rita},
title = {Tailoring a Persuasive Game to Promote Secure Smartphone Behaviour},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581038},
doi = {10.1145/3544548.3581038},
abstract = {The use of smartphones has become an integral part of everyone's lives. Due to the ubiquitous nature and multiple functionalities of smartphones, the data handled by these devices are sensitive in nature. Despite the measures companies take to protect users’ data, research has shown that people do not take the necessary actions to stay safe from security and privacy threats. Persuasive games have been implemented across various domains to motivate people towards a positive behaviour change. Even though persuasive games could be effective, research has shown that the one-size-fits-all approach to designing persuasive games might not be as effective as the tailored versions of the game. This paper presents the design and evaluation of a persuasive game to improve user awareness about smartphone security and privacy tailored to the user's motivational orientation using Regulatory Focus Theory. From the results of our mixed-methods in-the-wild study of 102 people followed by a one-on-one interview of 25 people, it is evident that the tailored version of the persuasive game performed better than the non-tailored version of the game towards improving users’ secure smartphone behaviour. We contribute to the broader HCI community by offering design suggestions and the benefits of tailoring persuasive games.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {618},
numpages = {18},
keywords = {Protection Motivation Theory, Regulatory Focus Theory, Secure Smartphone Behaviour, Smartphone Security Game, Tailoring Persuasive Game},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3546155.3546695,
author = {Jung, Frederike and M\"{u}ller, Heiko and Boll, Susanne CJ},
title = {It’s Not Warm But That’s Okay: About Robots That Avoid Human Stereotypes},
year = {2022},
isbn = {9781450396998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546155.3546695},
doi = {10.1145/3546155.3546695},
abstract = {Robot designs for social applications, such as care, often imitate human appearance and behavior. However, anthropomorphizing technology may reinforce connections between stereotypical characteristics and a robot’s assumed social category such as gender. Several problems are associated with stereotyping. We explore how to design for important social dimensions in care, such as perceived warmth or competence, without reverting to stereotypical images of human caretakers. Aiming towards “de-stereotyping” robotics, we provide a systematic exploration of multimodal behavioral cues for non-anthropomorphic care robots. In an online video study with a non-humanoid performing typical care tasks, all presented behavioral cues resulted in low RoSAS (Robot Social Attribute Scale) warmth, yet high task performance ratings. This emphasizes perceived competence (over warmth) as an essential criterion for non-humanoids. Our findings indicate that a “thing-like” robot appearance may not elicit the same social expectations as humanoid forms, shedding new light on the design space of these robots.},
booktitle = {Nordic Human-Computer Interaction Conference},
articleno = {48},
numpages = {15},
keywords = {stereotypes, robotics, non-humanoid, non-anthropomorphic, care},
location = {Aarhus, Denmark},
series = {NordiCHI '22}
}

@article{10.1145/3676557,
author = {Gilberto, Lucas G. and Bermejo, Fernando and Tommasini, Fabi\'{a}n C. and Garc\'{\i}a Bauza, Cristian},
title = {Virtual Reality Audio Game for Entertainment &amp; Sound Localization Training},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3558},
url = {https://doi.org/10.1145/3676557},
doi = {10.1145/3676557},
abstract = {Within the gaming and electronics industry, there is a continuous evolution of alternative applications. Nevertheless, accessibility to video games remains a persistent hurdle for individuals with disabilities, especially those with visual impairments due to the inherent visual-oriented design of games. Audio games (AGs) are electronic games that rely primarily on auditory cues instead of visual interfaces. This study focuses on the creation of a virtual reality AG for cell phones that integrates natural head and torso movements involved in spatial hearing. Its assessment encompasses user experience, interface usability, and sound localization performance. The study engaged eighteen sighted participants in a pre-post test with a control group. The experimental group underwent 7 training sessions with the AG. Via interviews, facets of the gaming experience were explored, while horizontal plane sound source localization was also tested before and after the training. The results enabled the characterization of sensations related to the use of the game and the interaction with the interfaces. Sound localization tests demonstrated distinct enhancements in performance among trained participants, varying with assessed stimuli. These promising results show advances for future virtual AGs, presenting prospects for auditory training. These innovations hold potential for skill development, entertainment, and the integration of visually impaired individuals.},
note = {Just Accepted},
journal = {ACM Trans. Appl. Percept.},
month = {jul},
keywords = {audio games, spatial hearing training, natural interfaces}
}

@inproceedings{10.1145/3503161.3548246,
author = {Gong, Kaixiong and Li, Shuang and Li, Shugang and Zhang, Rui and Liu, Chi Harold and Chen, Qiang},
title = {Improving Transferability for Domain Adaptive Detection Transformers},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548246},
doi = {10.1145/3503161.3548246},
abstract = {DETR-style detectors stand out amongst in-domain scenarios, but their properties in domain shift settings are under-explored. This paper aims to build a simple but effective baseline with a DETR-style detector on domain shift settings based on two findings. For one, mitigating the domain shift on the backbone and the decoder output features excels in getting favorable results. For another, advanced domain alignment methods in both parts further enhance the performance. Thus, we propose the Object-Aware Alignment (OAA) module and the Optimal Transport based Alignment (OTA) module to achieve comprehensive domain alignment on the outputs of the backbone and the detector. The OAA module aligns the foreground regions identified by pseudo-labels in the backbone outputs, leading to domain-invariant base features. The OTA module utilizes sliced Wasserstein distance to maximize the retention of location information while minimizing the domain gap in the decoder outputs. We implement the findings and the alignment modules into our adaptation method, and it benchmarks the DETR-style detector on the domain shift settings. Experiments on various domain adaptive scenarios validate the effectiveness of our method.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {1543–1551},
numpages = {9},
keywords = {object detection, feature alignment, domain adaptation, detection transformer},
location = {Lisboa, Portugal},
series = {MM '22}
}

@proceedings{10.1145/3610548,
title = {SA '23: SIGGRAPH Asia 2023 Conference Papers},
year = {2023},
isbn = {9798400703157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3524458,
title = {GoodIT '22: Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@inproceedings{10.1145/3585088.3589384,
author = {Qiao, Hannah and Suriyaarachchi, Hussel and Cooray, Sankha and Nanayakkara, Suranga},
title = {Snatch and Hatch: Improving Receptivity Towards a Nature of Science with a Playful Mobile Application},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585088.3589384},
doi = {10.1145/3585088.3589384},
abstract = {Science literacy is an increasingly important skill in the 21st century. With engagement and motivation as vital precursors to learning science, we believe introducing children to playful interactions with scientific phenomena would improve their motivations and attitudes towards science. To investigate this, we developed a tablet application where children journey in a story-driven game to capture virtual creatures by manipulating the sound measured using the built-in microphone. This game was designed with feedback from 16 children and 10 parents. In this paper, we describe the iterative design process and findings in a multi-day study with 11 more children aged between 8 and 12. Children were motivated by the game, demonstrated a strong association between sound and its behaviour in the physical world, and expressed enthusiasm to learn more in the classroom.},
booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
pages = {278–288},
numpages = {11},
keywords = {Children, Intrinsic Motivation, Mobile Game, Nature of Science, Science Education, Technology Enhanced Learning},
location = {Chicago, IL, USA},
series = {IDC '23}
}

@inproceedings{10.1145/3555858.3555905,
author = {Healy, John and Cullen, Charlie},
title = {Navigating Complexity: Investigating How Students Move Through the Game Design Process},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555905},
doi = {10.1145/3555858.3555905},
abstract = {Game design has become a common feature at many higher education institutions since the first programmes emerged in the early 90s and a growing body of research around games education has emerged in recent years. This paper is part of an ongoing research project investigating how game design takes place and how designers navigate the complexities of the discipline. Student development logs (DevLogs) were produced throughout a six-week game design project and were analysed using a grounded theory approach. The study took a naturalistic approach and attempted to capture the process by which students moved from project brief to final deliverable. Through this analysis, four specific design approaches applied by students were identified: Open-Ended Design, Player Experience Design, World Design and Technical Design. In addition to this, we observed an overarching process of design moving from initiation to outcome and specifically we discuss the complexity of design approaches and their interrelated nature. Finally, we ask game design educators to consider if this complexity is necessary and if so how can we design learning to better support their academic development.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {61},
numpages = {4},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3638380.3638448,
author = {Ott, Claudia and Park, Noel and Varaine, Kerian and Regenbrecht, Holger},
title = {Experiencing Reconstructed Reality: The Perception of Visual-Acoustic Properties},
year = {2024},
isbn = {9798400717079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638380.3638448},
doi = {10.1145/3638380.3638448},
abstract = {Modern Virtual Reality technology allows for an affordable, immersive experience of three-dimensionally reconstructed real, built environments. Increasingly, not only the visual aspects of buildings can be captured and reconstructed in much detail, but also the acoustic properties of them, for instance with convolution reverb recordings. In an exploratory, empirical study with 27 lay people we investigated to which degree the captured and rendered acoustic properties of a building have to match the visual properties for a coherent virtual reality experience, mainly addressing two questions: (1) can we as non-acoustics experts, using conventional hardware, produce a realistic experience and (2) to which degree can participants distinguish different sound conditions? To do so we recorded Room Impulse Response files of three different built environments using consumer-grade equipment. We found that more than half of the participants chose the technically most accurate sound condition as the best matching for the environment. Furthermore, participants reported high levels of confidence and indicated that they could distinguish the different sound conditions to a high degree. Our study and findings are embedded into the cultural context of the indigenous people and architecture of Aotearoa/New Zealand.},
booktitle = {Proceedings of the 35th Australian Computer-Human Interaction Conference},
pages = {573–584},
numpages = {12},
keywords = {3D Reconstruction, Indigenous UI, Spatial Audio, Speaking, Storytelling, Virtual Reality},
location = {Wellington, New Zealand},
series = {OzCHI '23}
}

@article{10.1145/3569499,
author = {Tang, Xiao and Li, Ruihui and Fu, Chi-Wing},
title = {CAFI-AR: Contact-aware Freehand Interaction with AR Objects},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
url = {https://doi.org/10.1145/3569499},
doi = {10.1145/3569499},
abstract = {Freehand interaction enhances user experience, allowing one to use bare hands to manipulate virtual objects in AR. Yet, it remains challenging to accurately and efficiently detect contacts between real hand and virtual object, due to the imprecise captured/estimated hand geometry. This paper presents CAFI-AR, a new approach for Contact-Aware Freehand Interaction with virtual AR objects, enabling us to automatically detect hand-object contacts in real-time with low latency. Specifically, we formulate a compact deep architecture to efficiently learn to predict hand action and contact moment from sequences of captured RGB images relative to the 3D virtual object. To train the architecture for detecting contacts on AR objects, we build a new dataset with 4,008 frame sequences, each with annotated hand-object interaction information. Further, we integrate CAFI-AR into our prototyping AR system and develop various interactive scenarios, demonstrating fine-grained contact-aware interactions on a rich variety of virtual AR objects, which cannot be achieved by existing AR interaction approaches. Lastly, we also evaluate CAFI-AR, quantitatively and qualitatively, through two user studies to demonstrate its effectiveness in terms of accurately detecting the hand-object contacts and promoting fluid freehand interactions},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jan},
articleno = {183},
numpages = {23},
keywords = {virtual object manipulation, freehand interaction, contact-aware, augmented reality}
}

@inproceedings{10.1145/3544548.3581050,
author = {Matviienko, Andrii and Hoxha, Hajris and M\"{u}hlh\"{a}user, Max},
title = {What does it mean to cycle in Virtual Reality? Exploring Cycling Fidelity and Control of VR Bicycle Simulators},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581050},
doi = {10.1145/3544548.3581050},
abstract = {Creating highly realistic Virtual Reality (VR) bicycle experiences can be time-consuming and expensive. Moreover, it is unclear what hardware parts are necessary to design a bicycle simulator and whether a bicycle is needed at all. In this paper, we investigated cycling fidelity and control of VR bicycle simulators. For this, we developed and evaluated three cycling simulators: (1) cycling without a bicycle (bikeless), (2) cycling on a fixed (stationary) and (3) moving bicycle (tandem) with four levels of control (no control, steering, pedaling, and steering + pedaling). To evaluate all combinations of fidelity and control, we conducted a controlled experiment (N = 24) in indoor and outdoor settings. We found that the bikeless setup provides the highest feeling of safety, while the tandem leads to the highest realism without increasing motion sickness. Moreover, we discovered that bicycles are not essential for cycling in VR.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {879},
numpages = {15},
keywords = {bicycle simulators, cycling, locomotion, virtual reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3613904.3641909,
author = {Colley, Mark and Wanner, Beate and R\"{a}dler, Max and R\"{o}tzer, Marcel and Frommel, Julian and Hirzle, Teresa and Jansen, Pascal and Rukzio, Enrico},
title = {Effects of a Gaze-Based 2D Platform Game on User Enjoyment, Perceived Competence, and Digital Eye Strain},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641909},
doi = {10.1145/3613904.3641909},
abstract = {Gaze interaction is a promising interaction method to increase variety, challenge, and fun in games.We present “Shed Some Fear”, a 2D platform game including numerous eye-gaze-based interactions. “Shed Some Fear” includes control with eye-gaze and traditional keyboard input. The eye-gaze interactions are partially based on eye exercises reducing digital eye strain but also on employing peripheral vision. By employing eye-gaze as a necessary input mechanism, we explore the effects on and tradeoffs between user enjoyment and digital eye strain in a five-day longitudinal between-subject study (N=17) compared to interaction with a traditional mouse. We found that perceived competence was significantly higher with eye gaze interaction and significantly higher internal eye strain. With this work, we contribute to the not straightforward inclusion of eye tracking as a useful and fun input method for games.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {369},
numpages = {14},
keywords = {2D platform, digital eye strain, eye-gaze, gamification},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3606936,
author = {Egan, D\'{o}nal and Cosker, Darren and McDonnell, Rachel},
title = {NeuroDog: Quadruped Embodiment using Neural Networks},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
url = {https://doi.org/10.1145/3606936},
doi = {10.1145/3606936},
abstract = {Virtual reality (VR) allows us to immerse ourselves in alternative worlds in which we can embody avatars to take on new identities. Usually, these avatars are humanoid or possess very strong anthropomorphic qualities. Allowing users of VR to embody non-humanoid virtual characters or animals presents additional challenges. Extreme morphological differences and the complexities of different characters' motions can make the construction of a real-time mapping between input human motion and target character motion a difficult challenge. Previous animal embodiment work has focused on direct mapping of human motion to the target animal via inverse kinematics. This can lead to the target animal moving in a way which is inappropriate or unnatural for the animal type. We present a novel real-time method, incorporating two neural networks, for mapping human motion to realistic quadruped motion. Crucially, the output quadruped motions are realistic, while also being faithful to the input user motions. We incorporate our mapping into a VR embodiment system in which users can embody a virtual quadruped from a first person perspective. Further, we evaluate our system via a perceptual experiment in which we investigate the quality of the synthesised motion, the system's response to user input and the sense of embodiment experienced by users. The main findings of the study are that the system responds as well as traditional embodiment systems to user input, produces higher quality motion and users experience a higher sense of body ownership when compared to a baseline method in which the human to quadruped motion mapping relies solely on inverse kinematics. Finally, our embodiment system relies solely on consumer-grade hardware, thus making it appropriate for use in applications such as VR gaming or VR social platforms.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {aug},
articleno = {38},
numpages = {19},
keywords = {Deep Learning, Motion Capture, Motion synthesis, Perception, Quadruped embodiment, VR embodiment}
}

@proceedings{10.1145/3649902,
title = {ETRA '24: Proceedings of the 2024 Symposium on Eye Tracking Research and Applications},
year = {2024},
isbn = {9798400706073},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3573381.3596162,
author = {Li, Liangding and Carnell, Stephanie and Harris, Katherine and Walters, Linda and Reiners, Dirk and Cruz-Neira, Carolina},
title = {LIFT - A System to Create Mixed 360° Video and 3D Content for Live Immersive Virtual Field Trip},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596162},
doi = {10.1145/3573381.3596162},
abstract = {Our paper presents LIFT, a system that enables educators to create immersive virtual field trip experiences for their students. LIFT overcomes the challenges of enabling non-technical educators to create their own content and allows educators to act as guides during the immersive experience. The system combines live-streamed 360° video, 3D models, and live instruction to create collaborative virtual field trips. To evaluate LIFT, we developed a field trip with biology educators from the University of Central Florida(UCF) and showcased it at a science festival. Our results suggest that LIFT can help educators create immersive educational content while out in the field. However, our pilot observational study at the museum highlighted the need for further research to explore the instructional design of mixed immersive content created with LIFT. Overall, our work provides an application development framework for educators to create immersive, hands-on field trip experiences.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {83–93},
numpages = {11},
keywords = {virtual reality, telepresence, liveevents, interactive system and tools},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3582437.3582449,
author = {Goodman, James and Perez-Liebana, Diego and Lucas, Simon},
title = {Following the Leader in Multiplayer Tabletop Games},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582449},
doi = {10.1145/3582437.3582449},
abstract = {In a two-player zero-sum game, players classically want to maximise their chance of winning. When a game has more than two players, using the binary win rate as an objective is no longer such an obvious choice. A player might instead have the objective of doing as well as possible in terms of ranked order, or in maximising their score. We investigate the impact of different game-agnostic objectives in several popular tabletop games, and whether it can be better to use the game score as a proxy for winning. We find that the games considered largely fall into two groups. In one it is helpful to focus just on one’s own score during the game, and then shift to beating opponents only in the end-game. In the other, larger, group it is better to ‘Follow the Leader’ and constantly track one’s relative position to the opponents throughout the game. Using the score as a proxy enables rollout simulations in Monte Carlo (Tree) Search to be terminated early, and we investigate the optimal length of these. Games in which long-term planning is required, or when the full score is only known at the end of the game benefit from a full rollout, while games with adversarial counter-moves benefit from a short rollout length.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {22},
numpages = {11},
keywords = {MCTS, heuristic, multi-player, tabletop games},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3510456.3514154,
author = {Moster, Makayla and Kokinda, Ella and Re, Matthew and Dominic, James and Lehmann, Jason and Begel, Andrew and Rodeghero, Paige},
title = {"Can you help me?": an experience report of teamwork in a game coding camp for autistic high school students},
year = {2022},
isbn = {9781450392259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510456.3514154},
doi = {10.1145/3510456.3514154},
abstract = {Teamwork skills are increasingly important for students to have as they enter the workforce, especially in software development positions. However, autistic students do not get to practice teamwork since much of their education is focused on learning social skills. The hybrid mode of education comes with challenges, including communication and collaboration issues and teaming difficulties, however, this method of teaching and learning can be difficult for students with autism. In this experience report paper, we discuss our experience planning and running a hybrid camp to teach teamwork and programming to 14 autistic high school students. Overall, our camp was successful in teaching students software development skills with open source software, and, from our experience, we detail our lessons learned and provide recommendations for educators and researchers working with autistic students in a hybrid setting.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {50–61},
numpages = {12},
keywords = {hybrid, game coding camp, autism},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEET '22}
}

@article{10.1145/3603623,
author = {Muresan, Andreea and Mcintosh, Jess and Hornb\ae{}k, Kasper},
title = {Using Feedforward to Reveal Interaction Possibilities in Virtual Reality},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3603623},
doi = {10.1145/3603623},
abstract = {In virtual reality (VR), interactions may fail when users encounter new, unknown, or unexpected objects. We propose using feedforward in VR to help users interact with objects by revealing how such objects work. Feedforward lets users know what to do and how to do it by showing the available actions and outcomes before an interaction. In this article, we first chart the design space of feedforward in VR and illustrate how to design feedforward for specific VR interactions. We discuss starting the feedforward, previewing actions and outcomes, and returning the virtual world to its state before the feedforward. Second, we implement three real-world VR applications to show how feedforward can be applied to multistep interactions, perceived interactivity, and discoverability. Third, we conduct an evaluation of the design space with 14 VR experts to understand its usefulness. Finally, we summarize the findings of our work on VR feedforward in 15 guidelines.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {sep},
articleno = {82},
numpages = {47},
keywords = {feedforward, Virtual reality}
}

@inproceedings{10.1145/3565970.3567684,
author = {Kruse, Lucie and Karaosmanoglu, Sukran and Rings, Sebastian and Steinicke, Frank},
title = {Evaluating Difficulty Adjustments in a VR Exergame for Younger and Older Adults: Transferabilities and Differences},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565970.3567684},
doi = {10.1145/3565970.3567684},
abstract = {Virtual reality (VR) exergames have the potential to train cognitive and physical abilities. However, most of these spatial games are developed for younger users and do not consider older adults with different design requirements. Yet, to be entertaining and efficient, the difficulty of games has to match the needs of players with different abilities. In this paper, we explore the effects of individually calibrating a starting difficulty and adjusting it: i) exactly as calibrated before, ii) 50% more difficult, and iii) 50% less difficult. In a user study, we compare the effects of using these adjustments on reaction times and subjective measures on younger (n=30) and older adults (n=9). The results show that most of the users prefer a faster-paced VR game in terms of enjoyment, but this also resulted in a higher perceived workload. Compared to the younger adults, the older adults rated the game more positive in terms of higher enjoyment and eagerness to play the game again, as well as lower perceived workload. This emphasizes the need for games to be designed for the user group they are intended for; both in terms of cognitive-physical difficulty and game content. Furthermore, we reflect on the transferability of the results obtained from testing with the younger adults and highlight their potential, especially for identifying suggestions and issues with the gameplay.},
booktitle = {Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
articleno = {7},
numpages = {11},
keywords = {VR, cognitive-physical training, dynamic difficulty, exergames, older adults, serious games, virtual reality},
location = {Online, CA, USA},
series = {SUI '22}
}

@inproceedings{10.1145/3544548.3580768,
author = {Pooryousef, Vahid and Cordeil, Maxime and Besan\c{c}on, Lonni and Hurter, Christophe and Dwyer, Tim and Bassed, Richard},
title = {Working with Forensic Practitioners to Understand the Opportunities and Challenges for Mixed-Reality Digital Autopsy},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580768},
doi = {10.1145/3544548.3580768},
abstract = {Forensic practitioners analyse intrinsic 3D data daily on 2D screens. We explore novel immersive visualisation techniques that enable digital autopsy through analysis of 3D imagery. We employ a user-centred design process involving four rounds of user feedback: (1) formative interviews eliciting opportunities and requirements for mixed-reality digital autopsies; (2) a larger workshop identifying our prototype’s limitations and further use-cases and interaction ideas; (3+4) two rounds of qualitative user validation of successive prototypes of novel interaction techniques for pathologist sensemaking. Overall, we find MR holds great potential to enable digital autopsy, initially to supplement physical autopsy, but ultimately to replace it. We found that experts were able to use our tool to perform basic virtual autopsy tasks, MR setup promotes exploration and sense making of cause of death, and subject to limitations of current MR technology, the proposed system is a valid option for digital autopsies, according to experts’ feedback. – Warning: This paper contains sensitive images which are 3D visualisation of deceased people.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {843},
numpages = {15},
keywords = {autopsy, forensics, mixed reality, pathology, user-centred design},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3638380.3638432,
author = {Zsolczay, Rodney G and Perrin, Dimitri and Maire, Frederic and T\"{u}rkay, Selen},
title = {Touch-less Remedial Game Prototype for Hand Rehabilitation - Feedback Results},
year = {2024},
isbn = {9798400717079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638380.3638432},
doi = {10.1145/3638380.3638432},
abstract = {Remedial hand therapy can be required for many reasons, so burn injuries were used as a specific case for development of the remedial prototype of this paper. With burns, 80% of injuries involve the hands. Infection is also a constant concern with burns patients. In this paper, we sought feedback on our remedial prototype, from both professional hand therapists and past patients. Both groups gave very positive responses. With regard to burn injuries, the prototype had been developed with the following three considerations: 1) Touch-less hand control to reduce chances of infection. 2) Computer vision datasets for damaged/bandaged hands, as most existing datasets target healthy hands. 3) Ensure that the remedial game is engaging to help distract from pain and provide motivation, thereby encouraging patients to perform their required remedial exercises. Our contribution is the continued development of an application for hand rehabilitation, where hand detection and classification were specifically developed for a minority group of damaged/bandaged hands. The feedback obtained here helps to focus further iterative development.},
booktitle = {Proceedings of the 35th Australian Computer-Human Interaction Conference},
pages = {423–431},
numpages = {9},
keywords = {Game artifact, computer vision., datasets, hand rehabilitation, remedial therapy},
location = {Wellington, New Zealand},
series = {OzCHI '23}
}

@inproceedings{10.1145/3643916.3644427,
author = {Hoff, Adrian and Lungu, Mircea and Seidl, Christoph and Lanza, Michele},
title = {Collaborative Software Exploration with Multimedia Note Taking in Virtual Reality},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644427},
doi = {10.1145/3643916.3644427},
abstract = {Exploring and comprehending a software system, e.g., as preparation for its re-engineering, is a relevant, yet challenging endeavour often conducted by teams of engineers. Collaborative exploration tools aim to ease the process, e.g., via interactive visualizations in virtual reality (VR). However, these neglect to provide engineers with capabilities for persisting their thoughts and findings.We present an interactive VR visualization method that enables (distributed) teams of engineers to collaboratively (1) explore a subject system, while (2) persisting insights via free-hand diagrams, audio recordings, and in-visualization VR screenshots.We invited pairs of software engineering practitioners to use our method to collaboratively explore a software system. We observed how they used our method and collected their feedback and impressions before replaying their findings to the original developers of the subject system for assessment.Video Demonstration---youtube.com/watch?v=32EIpf4V3b4},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {346–357},
numpages = {12},
keywords = {software visualization, software comprehension, collaborative software engineering, virtual reality},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3544548.3580724,
author = {Park, Chaeyong and Kim, Jeongwoo and Choi, Seungmoon},
title = {Visuo-haptic Crossmodal Shape Perception Model for Shape-Changing Handheld Controllers Bridged by Inertial Tensor},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580724},
doi = {10.1145/3544548.3580724},
abstract = {We present a visuo-haptic crossmodal model of shape perception designed for shape-changing handheld controllers. The model uses the inertia tensor of an object to bridge the two senses. The model was constructed from the results of three perceptual experiments. In the first two experiments, we validate that the primary moment and product of inertia (MOI and POI) in the inertia tensor have critical effects on the haptic perception of object length and asymmetry. Then, we estimate a haptic-to-visual shape matching model using MOI and POI as two link variables from the results of the third experiment for crossmodal magnitude production. Finally, we validate in a summative user study that the inverse of the shape matching model is effective for pairing a perceptually-congruent haptic object from a virtual object—the functionality we need for shape-changing handheld interfaces to afford perceptually-fulfilling sensory experiences in virtual reality.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {699},
numpages = {18},
keywords = {Dynamic Touch, Handheld Controller, Perception Model, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3526113.3545705,
author = {Zhu, Junyi and Lei, Yuxuan and Shah, Aashini and Schein, Gila and Ghaednia, Hamid and Schwab, Joseph and Harteveld, Casper and Mueller, Stefanie},
title = {MuscleRehab: Improving Unsupervised Physical Rehabilitation by Monitoring and Visualizing Muscle Engagement},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545705},
doi = {10.1145/3526113.3545705},
abstract = {Unsupervised physical rehabilitation traditionally has used motion tracking to determine correct exercise execution. However, motion tracking is not representative of the assessment of physical therapists, which focus on muscle engagement. In this paper, we investigate if monitoring and visualizing muscle engagement during unsupervised physical rehabilitation improves the execution accuracy of therapeutic exercises by showing users whether they target the right muscle groups. To accomplish this, we use wearable electrical impedance tomography (EIT) to monitor muscle engagement and visualize the current state on a virtual muscle-skeleton avatar. We use additional optical motion tracking to also monitor the user’s movement. We conducted a user study with 10&nbsp;participants that compares exercise execution while seeing muscle + motion data vs. motion data only, and also presented the recorded data to a group of physical therapists for post-rehabilitation analysis. The results indicate that monitoring and visualizing muscle engagement can improve both the therapeutic exercise accuracy during rehabilitation, and post-rehabilitation evaluation for physical therapists.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {33},
numpages = {14},
keywords = {physical rehabilitation, muscle engagement., health sensing, EIT},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{10.1145/3576914.3589563,
author = {Liu, Frank and Narsipur, Anish and Kemeklis, Andrew and Song, Lucy and Likamwa, Robert},
title = {Spatial Audio Empowered Smart speakers with Xblock - A Pose-Adaptive Crosstalk Cancellation Algorithm for Free-moving Users},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576914.3589563},
doi = {10.1145/3576914.3589563},
abstract = {Smart IoT Speakers, while connected over a network, currently only produce sounds that come directly from the individual devices. We envision a future where smart speakers collaboratively produce a fabric of spatial audio, capable of perceptually placing sound in a range of locations in physical space. This could provide audio cues in homes, offices and public spaces that are flexibly linked to various positions. The perception of spatialized audio relies on binaural cues, especially the time difference and the level difference of incident sound at a user’s left and right ears. Traditional stereo speakers cannot create the spatialization perception for a user when playing binaural audio due to auditory crosstalk, as each ear hears a combination of both speaker outputs. We present Xblock, a novel time-domain pose-adaptive crosstalk cancellation technique that creates a spatial audio perception over a pair of speakers using knowledge of the user’s head pose and speaker positions. We build a prototype smart speaker IoT system empowered by Xblock, explore the effectiveness of Xblock through signal analysis, and discuss future perceptual user studies and future work.},
booktitle = {Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
pages = {285–291},
numpages = {7},
keywords = {algorithm, crosstalk cancellation, internet of things, spatial audio},
location = {San Antonio, TX, USA},
series = {CPS-IoT Week '23}
}

@inproceedings{10.1145/3582700.3582717,
author = {Omarali, Bukeikhan and Javaid, Shafiq and Valle, Maurizio and Farkhatdinov, Ildar},
title = {Workspace Scaling in Virtual Reality based Robot Teleoperation},
year = {2023},
isbn = {9781450399845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582700.3582717},
doi = {10.1145/3582700.3582717},
abstract = {We explore how human-operators perform manual scaling of virtual reality (VR) scenes used to represent remote physical environments in robotic telemanipulation tasks. In our experiment, 15 human-participants were asked to use manual gesture-based navigation in a VR scene that allowed them to change the virtual-to-real scale of the virtual world and perform a simple pick-and-place task in supervised robot control mode. We have compared the virtual world scale of participants at the beginning of a 3-day experiment when they were considered to be novice teleoperators and at the end of it when they were considered to be expert teleoperators. Expert teleoperators as a group used a smaller virtual world scale that allowed them to perform the experimental task faster than novices, although this behaviour was not exhibited by every teleoperator individually. Our study also demonstrated that participants’ prior video gaming experience affects the virtual world scale as participants with video gaming experience used smaller virtual world scales and performed the experimental task faster.},
booktitle = {Proceedings of the Augmented Humans International Conference 2023},
pages = {98–104},
numpages = {7},
keywords = {virtual reality, robot teleoperation, human-operator’s perception},
location = {Glasgow, United Kingdom},
series = {AHs '23}
}

@inproceedings{10.1145/3544548.3581189,
author = {Wei\ss{}, Sebastian and Heuten, Wilko},
title = {Don’t Panic! - Influence of Virtual Stressor Representations from the ICU Context on Perceived Stress Levels},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581189},
doi = {10.1145/3544548.3581189},
abstract = {Intensive care nurses are prone to suffering from chronic stress due to constant exposure to two main profession-related stressors: interruption and time pressure. These stressors have detrimental effects on the well-being of the nursing staff and, by proxy, the patients. To alleviate stress, increase safety, and support the training of stressful scenarios, we investigate the impact these stressors have on subjective and objective stress levels in a virtual environment. We designed an intensive care unit in which participants (n=26, 18 healthcare professionals) perform common tasks, e.g. refilling an infusion pump, whilst being exposed to interruptions and time pressure. Results from our between-subjects study provide data indicating stress increase in both stressor conditions, suggesting that artificially evoking work-related stressors for stress inoculation training (SIT) is a possible extension to simulation training during nursing education. This knowledge is helpful for designing training scenarios of safety-critical situations early in the professional apprenticeship.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {642},
numpages = {15},
keywords = {Nursing, Stress, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3550082,
title = {SA '22: SIGGRAPH Asia 2022 Posters},
year = {2022},
isbn = {9781450394628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Daegu, Republic of Korea}
}

@inproceedings{10.1145/3544549.3585859,
author = {Alabood, Lorans and Dow, Travis and Kaufman, Kate M. and Jaswal, Vikram K. and Krishnamurthy, Diwakar},
title = {Can Cross-Reality Help Nonspeaking Autistic People Transition to AR Typing?},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585859},
doi = {10.1145/3544549.3585859},
abstract = {About 30% of autistic people are nonspeakers; they cannot use speech to communicate effectively. Pointing to letters on a letterboard held by a Communication and Regulation Partner (CRP) is one alternative method of expressive communication that some members of this population use. In the training of this method, a CRP delivers engaging and customized lessons. Additionally, the CRP provides regulatory, sensory, and attentional support and also works to strengthen the subject’s pointing skills. The goal of this training is to equip individuals with the required skills to be able to type independently. Recent studies have proposed using AR to provide opportunities for nonspeakers to practice the motor skills involved in typing. To use such systems, however, there needs to be a transition phase where a CRP teaches their subject how to interact with a virtual letterboard. In this paper, we explore the feasibility of using cross-reality, in which a CRP and nonspeaker can interact with the same virtual objects simultaneously, as a possible means of fostering this transition. We report a study involving 5 nonspeaking autistic subjects with diverse motor skills interacting using a virtual HoloLens 2 letterboard system we developed called HoloBoard. All subjects succeeded in pointing to letters correctly or spelling on the virtual board. We report process and design recommendations based on feedback obtained from subjects and their CRPs.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {62},
numpages = {7},
keywords = {Nonspeaking autistic people, assistive technology, augmented reality, cross-reality},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@proceedings{10.1145/3549555,
title = {CBMI '22: Proceedings of the 19th International Conference on Content-based Multimedia Indexing},
year = {2022},
isbn = {9781450397209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Graz, Austria}
}

@inproceedings{10.1145/3613904.3642222,
author = {Zhao, Yichun and Nacenta, Miguel A and Sukhai, Mahadeo A. and Somanath, Sowmya},
title = {TADA: Making Node-link Diagrams Accessible to Blind and Low-Vision People},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642222},
doi = {10.1145/3613904.3642222},
abstract = {Diagrams often appear as node-link representations in contexts such as taxonomies, mind maps and networks in textbooks. Despite their pervasiveness, they present accessibility challenges for blind and low-vision people. To address this challenge, we introduce Touch-and-Audio-based Diagram Access (TADA), a tablet-based interactive system that makes diagram exploration accessible through musical tones and speech. We designed TADA informed by an interview study with 15 participants who shared their challenges and strategies with diagrams. TADA enables people to access a diagram by: i) engaging in open-ended touch-based explorations, ii) searching for nodes, iii) navigating between nodes and iv) filtering information. We evaluated TADA with 25 participants and found it useful for gaining different perspectives on diagrammatic information.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {45},
numpages = {20},
keywords = {Accessibility, Artifact or System, Assistive Technologies, Gestures, Haptics, Individuals with Disabilities, Pointing, Touch},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3555858.3563272,
author = {Acevedo, Pedro and Choi, Minsoo and Liu, Huimin and Kao, Dominic and Mousas, Christos},
title = {Procedural Game Level Design to Trigger Spatial Exploration},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3563272},
doi = {10.1145/3555858.3563272},
abstract = {Synthesizing game levels that evoke players’ curiosity, driving them to explore different level parts, is time-consuming and tedious. Typically, game level designers manually perform this synthesis using trial and error. In this paper, we propose a method with which to replace this manual, time-consuming process. We benefited from recent work that had proposed game level design patterns to evoke curiosity, and we propose an approach to automatically synthesizing game levels in order to encourage players to pursue designer-specified exploration goals. We started by creating a dataset of level assets, based on the four design patterns that evoke curiosity-driven exploration in games (reaching extreme points, resolving visual obstructions, out-of-place objects, and understanding spatial connections). We annotated the assets in our dataset with spatial exploration measurements (the time players took to explore an asset over their total time spent in the game level). We then formulated game level design as an optimization problem, encoding both spatial exploration (mean spatial exploration, spatial exploration variance, and spatial exploration distribution) and game level design (occupied area, adjacent penalty, and height distribution) decisions. Then, we solved this problem by implementing a reversible-jump Markov chain Monte Carlo method. We demonstrate our method’s ability to synthesize game level variations with different spatial exploration and level design decisions. Finally, a user study showed that our approach can automatically synthesize game levels, encouraging a certain amount of spatial exploration by players.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {70},
numpages = {11},
keywords = {spatial exploration, procedural content generation, level design, game level, curiosity},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3562939.3565614,
author = {Kocur, Martin and Kalus, Alexander and Bogon, Johanna and Henze, Niels and Wolff, Christian and Schwind, Valentin},
title = {The Rubber Hand Illusion in Virtual Reality and the Real World - Comparable but Different},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562939.3565614},
doi = {10.1145/3562939.3565614},
abstract = {Feeling ownership of a virtual body is crucial for immersive experiences in VR. Knowledge about body ownership is mainly based on rubber hand illusion (RHI) experiments in the real world. Watching a rubber hand being stroked while one’s own hidden hand is synchronously stroked, humans experience the rubber hand as their own hand and underestimate the distance between the rubber hand and the real hand (proprioceptive drift). There is also evidence for a decrease in hand temperature. Although the RHI has been induced in VR, it is unknown whether effects in VR and the real world differ. We conducted a RHI experiment with 24 participants in the real world and in VR and found comparable effects in both environments. However, irrespective of the RHI, proprioceptive drift and temperature differences varied between settings. Our findings validate the utilization of the RHI in VR to increase our understanding of embodying virtual avatars.},
booktitle = {Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
articleno = {31},
numpages = {12},
keywords = {virtual reality, rubber hand illusion, proprioceptive drift, disownership, body ownership illusion, avatars},
location = {Tsukuba, Japan},
series = {VRST '22}
}

@proceedings{10.1145/3555858,
title = {FDG '22: Proceedings of the 17th International Conference on the Foundations of Digital Games},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3550472,
title = {SA '22: SIGGRAPH Asia 2022 XR},
year = {2022},
isbn = {9781450394734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Daegu, Republic of Korea}
}

@article{10.1145/3611023,
author = {Cmentowski, Sebastian and Karaosmanoglu, Sukran and Kievelitz, Fabian and Steinicke, Frank and Kr\"{u}ger, Jens},
title = {A Matter of Perspective: Designing Immersive Character Transitions for Virtual Reality Games},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3611023},
doi = {10.1145/3611023},
abstract = {Virtual reality (VR) games intensify storytelling experiences by letting players take the role of a character. However, in contrast to films, novels, or games, VR experiences often remain centered around one single character without using the potential of complex multiprotagonist plots. Our work engages in this critical topic by investigating the design of immersive and natural transitions between different characters. First, we conducted a scoping review to identify existing multiprotagonist VR games (N=18) and grouped their used transition techniques into four categories. Based on these findings and prior research, we designed two transition techniques (Static Map vs. Rebodying) and conducted a between-participants (N=36) study to explore their effect on user experience. Our results show that Rebodying outperforms Static Map regarding the perceived realism, acceptance, and spatial understanding of the character transitions. Both conditions do not differ significantly in terms of cybersickness. Finally, we provide future directions for developing, improving, and exploring of multiprotagonist transition techniques in VR games.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {377},
numpages = {31},
keywords = {virtual reality games, virtual avatars, transitions, perspectives, game characters, embodiment}
}

@inproceedings{10.1145/3561212.3561243,
author = {Larrieux, Eric and Speziali, Stella},
title = {Augmented Objects as Portals into Virtual Worlds:Using Audio to Create Immersive Experiences in Extended Realities},
year = {2022},
isbn = {9781450397018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561212.3561243},
doi = {10.1145/3561212.3561243},
abstract = {Building on a long history of the integration of new technology into the arts, in this research we set out to develop and evaluate new methods for creating immersive, audio-centric artworks in a Mixed Reality (MR) context. We employ multiple levels of spatial audio, both within the room as well as the umbrellas that participants navigate the space with, in order to provide a 6 degree of freedom immersive experience. Furthermore, we use dynamic projection mapping, again both throughout the global environment, and on the umbrellas, to create immersive experiences. This allows the umbrellas to function as so-called Augmented Objects that facilitate the composition and navigation of Responsive Sonic Environments, thus encouraging participants to examine their own perceptions of reality while incorporating them into the larger artistic installation. Additionally, we explore the creation of Spatial Metacompositions, navigable environments that function as performance spaces for the participants who interact with them. Finally, we present the technical foundations necessary to create such systems and discuss techniques to effectively employ them in one’s artistic practice.},
booktitle = {Proceedings of the 17th International Audio Mostly Conference},
pages = {44–51},
numpages = {8},
keywords = {spatial metacompositions, spatial augmented reality, spatial audio, responsive sonic environments, projection mapping, mixed reality, interactive sonic art, immersion, extended reality, embedded systems., digital augmentation, ambisonics, Augmented objects},
location = {St. P\"{o}lten, Austria},
series = {AM '22}
}

@inproceedings{10.1145/3582515.3609542,
author = {Gaggi, Ombretta and Grosset, Luca and Pante, Giulio},
title = {A Virtual Reality Application to Make Mathematical Functions Accessible},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582515.3609542},
doi = {10.1145/3582515.3609542},
abstract = {This paper focusses on creating accessible alternatives for the graphical representation of mathematical functions, with particular attention to students with visual impairments and children with Attention Deficit/Hyperactivity Disorder (ADHD). We design a virtual reality (VR) application which can stimulate different senses by immersing students in a virtual reality environment where they can explore the graphical representation of a function and hear how it is played. The purpose of the application is to improve immersion and inclusion in mathematics. The results of the tests show that this type of tool is useful both for learning through play and for students with different learning needs. The use of audio is an effective way to make graphs accessible, but the problem is that there are no universal design principles that apply to various graphs that encode different data types. Additionally, users with ADHD could appreciate the flexibility, speed, cost effectiveness and greater measure of independence provided by the system; however, more research is needed to justify this statement.},
booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
pages = {257–264},
numpages = {8},
keywords = {teaching mathematics, math graph, accessibility},
location = {Lisbon, Portugal},
series = {GoodIT '23}
}

@proceedings{10.1145/3658852,
title = {MOCO '24: Proceedings of the 9th International Conference on Movement and Computing},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Utrecht, Netherlands}
}

@inproceedings{10.1145/3548771.3561408,
author = {Garaccione, Giacomo and Fulcini, Tommaso and Torchiano, Marco},
title = {GERRY: a gamified browser tool for GUI testing},
year = {2022},
isbn = {9781450394543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548771.3561408},
doi = {10.1145/3548771.3561408},
abstract = {Graphical User Interface (GUI) testing is a relevant step of the  
software development process which is not often performed  
thoroughly due to its unappealing nature, to the inherent  
fragility of test cases, and to the fact that test cases – composed  
of long and complicated sequences of operations –  
have to be manually written by testers.  
We propose GERRY , a Capture &amp; Replay GUI testing tool  
which implements an approach based on Gamification, i.e.,  
the application of gaming elements to non-ludic activities.  
The purpose of the tool is to increase the engagement of the  
testers when performing GUI test case definition tasks.  
The tool makes use of mechanics typical of games such as  
progress indicators, leaderboards, and unlockable rewards,  
to increase user interest and involvement. GERRY also generates  
reports (i.e., traces of all actions and milestones reached  
during a session), written logs of the performed testing sessions,  
and scripts compatible with existing GUI testing tools  
(SikuliX and Selenium) for replay purposes.},
booktitle = {Proceedings of the 1st International Workshop on Gamification of Software Development, Verification, and Validation},
pages = {2–9},
numpages = {8},
keywords = {Software Testing, Software Engineering, Gamification, GUI Testing},
location = {Singapore, Singapore},
series = {Gamify 2022}
}

@inproceedings{10.1145/3505270.3558344,
author = {Chisalita, Raluca and Murtinger, Markus and Kriglstein, Simone},
title = {Grow Your Plant: A Plant-Based Game For Creating Awareness About Sustainability Behaviour by Using Renewable Energy},
year = {2022},
isbn = {9781450392112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505270.3558344},
doi = {10.1145/3505270.3558344},
abstract = {Environmental sustainability has become more and more a very important topic that will also change our way of life in the near future. A question is how people can be influenced into becoming more aware of environmental sustainability. Since technology is considered a means to simplify people’s lives, it can also be used as a tool to support people’s awareness of these aspects. This paper investigates how we can use a mobile phone in combination with a solar panel as a wearable in a playful way to support people’s awareness of plant-based environments. For this purpose, the first version of a mobile game was developed with the goal that the players can not only learn what a plant needs to grow healthy but also learn about the sun as a renewable energy source.},
booktitle = {Extended Abstracts of the 2022 Annual Symposium on Computer-Human Interaction in Play},
pages = {177–182},
numpages = {6},
keywords = {Wearable, Sustainability, Solar Panel, Renewable Energy, Mobile Game},
location = {Bremen, Germany},
series = {CHI PLAY '22}
}

@inproceedings{10.1145/3613904.3642041,
author = {Cheng, Alan Y. and Guo, Meng and Ran, Melissa and Ranasaria, Arpit and Sharma, Arjun and Xie, Anthony and Le, Khuyen N. and Vinaithirthan, Bala and Luan, Shihe (Tracy) and Wright, David Thomas Henry and Cuadra, Andrea and Pea, Roy and Landay, James A.},
title = {Scientific and Fantastical: Creating Immersive, Culturally Relevant Learning Experiences with Augmented Reality and Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642041},
doi = {10.1145/3613904.3642041},
abstract = {Motivating children to learn is a major challenge in education. One way to inspire motivation to learn is through immersion. We combine the immersive potential of augmented reality (AR), narrative, and large language models (LLMs) to bridge fantasy with reality in a mobile application, Moon Story, that teaches elementary schoolers astronomy and environmental science. Our system also builds upon learning theories such as culturally-relevant pedagogy. Using our application, a child embarks on a journey inspired by Chinese mythology, engages in real-world AR activities, and converses with a fictional character powered by an LLM. We conducted a controlled experiment (N = 50) with two conditions: one using an LLM and one that was hard-coded. Both conditions resulted in learning gains, high engagement levels, and increased science learning motivation. Participants in the LLM condition also wrote more relevant answers. Finally, participants of both Chinese and non-Chinese heritage found the culturally-based narrative compelling.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {275},
numpages = {23},
keywords = {Artifact or System, Children/Parents, Education/Learning},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.5555/3586210.3586312,
author = {Anagnostou, Anastasia and Groen, Derek and Taylor, Simon J. E. and Suleimenova, Diana and Abubakar, Nura and Saha, Arindam and Mintram, Kate and Ghorbani, Maziar and Daroge, Habiba and Islam, Tasin and Xue, Yani and Okine, Edward and Anokye, Nana},
title = {Facs-Charm: A Hybrid Agent-Based and Discrete-Event Simulation Approach for Covid-19 Management at Regional Level},
year = {2023},
publisher = {IEEE Press},
abstract = {Pandemics have huge impact on all aspect of people's lives. As we have experienced during the Coronavirus pandemic, healthcare, education and the economy have been put under extreme strain. It is important therefore to be able to respond to such events fast in order to limit the damage to the society. Decision-makers typically are advised by experts in order to inform their response strategies. One of the tools that is widely used to support evidence-based decisions is modeling and simulation. In this paper, we present a hybrid agent-based and discrete-event simulation for the Coronavirus pandemic management at regional level. Our model considers disease dynamics, population interactions and dynamic ICU bed capacity management and predicts the impact of various public health preventive measures on the population and the healthcare service.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1223–1234},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3613904.3642147,
author = {Kang, Sei and Jeong, Jaejoon and Lee, Gun A. and Kim, Soo-Hyung and Yang, Hyung-Jeong and Kim, Seungwon},
title = {The RayHand Navigation: A Virtual Navigation Method with Relative Position between Hand and Gaze-Ray},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642147},
doi = {10.1145/3613904.3642147},
abstract = {In this paper, we introduce a novel Virtual Reality (VR) navigation method using gaze ray and hand, named RayHand navigation. It supports controlling navigation speed and direction by quickly indicating the initial direction using gaze and then using dexterous hand movement for controlling the speed and direction based on the relative position between the gaze ray and user's hand. We conducted a user study comparing our approach to the head-hand and torso-leaning-based navigation methods, and also evaluated their learning effect. The results showed that the RayHand and head-hand navigations were less physically demanding than the torso-leaning navigation, and the RayHand supported rich navigation experience with high hedonic quality and solved the issue of the user unintentionally stepping out from the designated interaction area. In addition, our approach showed a significant improvement over time with a learning effect.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {634},
numpages = {15},
keywords = {gaze-ray, navigation, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3616961.3616971,
author = {Paananen, Ville and Kiarostami, Sina and Lee, Lik-Hang and Visuri, Aku and Kheirinejad, Saba and Hosio, Simo},
title = {Exploring Situated Empathy through a Metaverse Campus},
year = {2023},
isbn = {9798400708749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616961.3616971},
doi = {10.1145/3616961.3616971},
abstract = {Virtual Reality (VR) is promising in communicating people’s hardship experiences in simulated situations. This can help foster empathy among people. In this paper, we present a VR experience designed to showcase the hardship experiences of an international higher education community concerning their studies and lives in an unfamiliar neighborhood. We collected hardship stories and data from 40 members of the community through an online questionnaire. The questionnaire analysis led to understanding critical issues, such as social problems, language barriers, issues with bureaucracy, and racism. We then turned the issues into interactive stories in VR. We recruited 18 participants to experience the hardship stories through interactions with avatars in a VR version of the campus where the community is located. Our preliminary results from the questionnaires suggest that the participants’ knowledge and tendency to willingness to discuss the hardships improved due to participating in the experience. Further, our semi-structured interviews reflect positively on the VR experience’s memorability, the stories’ plausibility and participants’ increased situated empathy and awareness regarding the hardships of the local international community. This early exploration informs future studies focusing on situated empathy.},
booktitle = {Proceedings of the 26th International Academic Mindtrek Conference},
pages = {1–12},
numpages = {12},
keywords = {virtual reality, social issues, empathy, community engagement},
location = {Tampere, Finland},
series = {Mindtrek '23}
}

@inproceedings{10.1145/3551708.3551770,
author = {Riantini, Rona and Hariadi, Mochamad and Nugroho, Supeno Mardi Susiki and Wulandari, Diah Puspito},
title = {Serious Game for Learning Ship Electrical Earth Fault Troubleshooting based on User and Expert Involvement},
year = {2022},
isbn = {9781450396455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551708.3551770},
doi = {10.1145/3551708.3551770},
abstract = {Instructing troubleshoot competence in ship electrical learning subject remains a challenging process. A fault condition is something we cannot find at any time, including earth fault, which is very critical onboard the ship. Moreover, not all educational institutions have easy access to an actual ship. Utilizing a serious game in the learning process gives an advantage in providing a virtual environment and experience in simulating troubleshooting. Incorporating game design elements and gamification features will also create a positive learning environment, enhancing excitement, motivation, and engagement. However, designing educational games is a challenging endeavor. Including feedback from both experts and users in developing a game will provide information that can clarify gameplay, instruction, and in-game objective. Several methods in designing the games offer different stages and different stakeholders being involved. This paper proposed a new approach involving users and experts in the earliest stage of design which is applied in designing a ship's electrical earth fault troubleshoot serious game. A better notion in the initial design is considerably more advantageous than modifying the software prototype based on feedback from users and experts in a later design stage.},
booktitle = {Proceedings of the 6th International Conference on Education and Multimedia Technology},
pages = {304–310},
numpages = {7},
keywords = {User Expert Involvement, Troubleshooting, Ship, Serious Game, Game Design, Electrical},
location = {Guangzhou, China},
series = {ICEMT '22}
}

@article{10.1145/3648685,
author = {Taheri, Atieh and Gomez-Monroy, Carlos Gilberto and Borja, Vicente and Sra, Misha},
title = {MouseClicker: Exploring Tactile Feedback and Physical Agency for People with Hand Motor Impairments},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3648685},
doi = {10.1145/3648685},
abstract = {Assistive technology (AT) design is critical in enabling functionality for people with disabilities, blending essential elements of both practical utility and user experience. Traditionally, AT has successfully addressed core functional needs, such as enabling cursor movement and clicking actions with devices like computer mice. However, a comprehensive approach to AT design also necessitates a thorough consideration of sensory feedback, including tactile sensations, ergonomics, and auditory cues like button click sounds. These aspects are not merely supplementary but are integral to the device’s functionality, enhancing user interaction and long-term comfort, especially for individuals with motor impairments. In this work, we present MouseClicker, a mechatronic AT to surrogate physical agency over a computer mouse and to foster the haptic sensory experience of clicking on it tailored specifically for an individual with Spinal Muscular Atrophy (SMA) who faces challenges in using a standard mouse due to severe hand motor impairments. Our design aims at replicating the holistic experience of clicking a mouse, from its functional mechanical actions to its nuanced tactile and auditory feedback. This work details the MouseClicker’s design and reports on an exploratory user study aimed at identifying optimal vibrotactile feedback parameters – such as location, and intensity – that represent mouse button clicks. MouseClicker presents a step forward in AT design by integrating the functionality, sensory feedback, and the overall experience of taking control over non-AT devices.},
journal = {ACM Trans. Access. Comput.},
month = {mar},
articleno = {5},
numpages = {31},
keywords = {Hand motor impairments, holistic inclusion, haptic feedback, facial expressions, hands-free input, computer mouse}
}

@inproceedings{10.1145/3626705.3627970,
author = {Halbhuber, David and Thomaschke, Roland and Henze, Niels and Wolff, Christian and Probst, Kilian and Bogon, Johanna},
title = {Play with my Expectations: Players Implicitly Anticipate Game Events Based on In-Game Time-Event Correlations},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626705.3627970},
doi = {10.1145/3626705.3627970},
abstract = {Temporal regularities and the timing of events and actions such as anticipating enemy movements or planning one’s next move are essential components of almost every video game. Thus, to succeed in video games, it is advantageous to anticipate events and prepare relevant actions before they occur. This work explores whether elapsed time can be used as a predictive cue for implicitly anticipating events in video games. Inspired by findings from psychology, we implemented multiple time-event correlations in a custom video game by pairing specific delays with specific game events. Participants had to shoot targets that appeared at different locations. After a certain delay (e.g., 0.8&nbsp;s), the targets appeared more frequently (80&nbsp;% of all appearances) at a specific location (e.g., left up). Our analysis of 25 participants provides evidence that players implicitly learned the implemented time-event correlations and used them to anticipate the location of upcoming targets. This led to improved game performance. Although no participant realised the implemented temporal regularities, targets were shot faster when preceded by the frequently paired delay. Our findings pave the way for game developers and researchers alike to more creatively combine human temporal processing with temporal aspects of video games.},
booktitle = {Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
pages = {386–397},
numpages = {12},
keywords = {time and timing in video games, time perception in video games, time-based event expectancy, video games},
location = {Vienna, Austria},
series = {MUM '23}
}

@article{10.1145/3555564,
author = {Jing, Allison and May, Kieran and Matthews, Brandon and Lee, Gun and Billinghurst, Mark},
title = {The Impact of Sharing Gaze Behaviours in Collaborative Mixed Reality},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555564},
doi = {10.1145/3555564},
abstract = {In a remote collaboration involving a physical task, visualising gaze behaviours may compensate for other unavailable communication channels. In this paper, we report on a 360° panoramic Mixed Reality (MR) remote collaboration system that shares gaze behaviour visualisations between a local user in Augmented Reality and a remote collaborator in Virtual Reality. We conducted two user studies to evaluate the design of MR gaze interfaces and the effect of gaze behaviour (on/off) and gaze style (bi-/uni-directional). The results indicate that gaze visualisations amplify meaningful joint attention and improve co-presence compared to a no gaze condition. Gaze behaviour visualisations enable communication to be less verbally complex therefore lowering collaborators' cognitive load while improving mutual understanding. Users felt that bi-directional behaviour visualisation, showing both collaborator's gaze state, was the preferred condition since it enabled easy identification of shared interests and task progress.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {463},
numpages = {27},
keywords = {mixed reality remote collaboration, human-computer interaction, gaze visualization}
}

@proceedings{10.1145/3573381,
title = {IMX '23: Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nantes, France}
}

@proceedings{10.1145/3610541,
title = {SA '23: SIGGRAPH Asia 2023 Emerging Technologies},
year = {2023},
isbn = {9798400703126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.5555/3545946.3598707,
author = {Obremski, David and Akuffo, Ohenewa Bediako and L\"{u}cke, Leonie and Semineth, Miriam and Tomiczek, Sarah and Weichert, Hanna-Finja and Lugrin, Birgit},
title = {Reducing Racial Bias by Interacting with Virtual Agents: An Intervention in Virtual Reality},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Racial bias, implicit or explicit, is still a widespread phenomenon in our modern society, negatively affecting the way we interact with foreigners. These biases can also lead to the general tendency to avoid encounters with foreigners, which has a critical impact on society as a whole. This paper presents an approach to reduce implicit and explicit racial bias using two Intelligent Virtual Agents (IVAs) in Virtual Reality (VR). Based on previous research from social psychology and the field of enculturated IVAs, a sympathetic East African-German mixed-cultural IVA, and an antipathetic German mono-cultural IVA were implemented to interact with the participants in a virtual pub quiz. Pre- and post-intervention measures of participants' implicit and explicit bias showed a significant decrease in both scores. This work demonstrates the capability of enculturated IVAs to reduce real-world racial biases and sets the base for cultural interventions with IVAs.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {747–755},
numpages = {9},
keywords = {culture, intelligent virtual agents, mixed-cultural, racial bias},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3597638.3608386,
author = {Collins, Jazmin and Jung, Crescentia and Jang, Yeonju and Montour, Danielle and Won, Andrea Stevenson and Azenkot, Shiri},
title = {“The Guide Has Your Back”: Exploring How Sighted Guides Can Enhance Accessibility in Social Virtual Reality for Blind and Low Vision People},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608386},
doi = {10.1145/3597638.3608386},
abstract = {As social VR applications grow in popularity, blind and low vision users encounter continued accessibility barriers. Yet social VR, which enables multiple people to engage in the same virtual space, presents a unique opportunity to allow other people to support a user’s access needs. To explore this opportunity, we designed a framework based on physical sighted guidance that enables a guide to support a blind or low vision user with navigation and visual interpretation. A user can virtually hold on to their guide and move with them, while the guide can describe the environment. We studied the use of our framework with 16 blind and low vision participants and found that they had a wide range of preferences. For example, we found that participants wanted to use their guide to support social interactions and establish a human connection with a human-appearing guide. We also highlight opportunities for novel guidance abilities in VR, such as dynamically altering an inaccessible environment. Through this work, we open a novel design space for a versatile approach for making VR fully accessible.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {38},
numpages = {14},
keywords = {blind and low vision, sighted guide, social virtual reality},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3586183.3606754,
author = {Nair, Vivek C and Munilla-Garrido, Gonzalo and Song, Dawn},
title = {Going Incognito in the Metaverse: Achieving Theoretically Optimal Privacy-Usability Tradeoffs in VR},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606754},
doi = {10.1145/3586183.3606754},
abstract = {Virtual reality (VR) telepresence applications and the so-called “metaverse” promise to be the next major medium of human-computer interaction. However, with recent studies demonstrating the ease at which VR users can be profiled and deanonymized, metaverse platforms carry many of the privacy risks of the conventional internet (and more) while at present offering few of the defensive utilities that users are accustomed to having access to. To remedy this, we present the first known method of implementing an “incognito mode” for VR. Our technique leverages local ε-differential privacy to quantifiably obscure sensitive user data attributes, with a focus on intelligently adding noise when and where it is needed most to maximize privacy while minimizing usability impact. Our system is capable of flexibly adapting to the unique needs of each VR application to further optimize this trade-off. We implement our solution as a universal Unity (C#) plugin that we then evaluate using several popular VR applications. Upon faithfully replicating the most well-known VR privacy attack studies, we show a significant degradation of attacker capabilities when using our solution.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {61},
numpages = {16},
keywords = {data harvesting, differential privacy, identification, incognito mode, private browsing, profiling, usable security, virtual reality},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@proceedings{10.1145/3564982,
title = {ICACS '22: Proceedings of the 6th International Conference on Algorithms, Computing and Systems},
year = {2022},
isbn = {9781450397407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Larissa, Greece}
}

@inproceedings{10.1145/3582437.3587182,
author = {Paller, Christian},
title = {A Framework for Analogue Game-modification Learning: Guidelines to Lower Barriers for Games in Education},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587182},
doi = {10.1145/3582437.3587182},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {59},
numpages = {9},
keywords = {Learning games, game modification, game-based learning, participatory game design},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@inproceedings{10.1145/3536221.3556593,
author = {Hedeshy, Ramin and Kumar, Chandan and Lauer, Mike and Staab, Steffen},
title = {All Birds Must Fly: The Experience of Multimodal Hands-free Gaming with Gaze and Nonverbal Voice Synchronization},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3556593},
doi = {10.1145/3536221.3556593},
abstract = {Eye tracking has evolved as a promising hands-free interaction mechanism to support people with disabilities. However, its adoption as a control mechanism in the gaming environment is constrained due to erroneous recognition of user intention and commands. Previous studies have suggested combining eye gaze with other modalities like voice input for improved interaction experience. However, speech recognition latency and accuracy is a major bottleneck, and the use of dictated verbal commands can disrupt the flow in gaming environment. Furthermore, several people with physical disabilities also suffer from speech impairments to utter precise verbal voice commands. In this work, we introduce nonverbal voice interaction (NVVI) to synchronize with gaze for an intuitive hands-free gaming experience. We propose gaze and NVVI (e.g., humming) for a spatio-temporal interaction applicable to several modern gaming apps, and developed ‘All Birds Must Fly’ as a representative app. In the experiment, we first compared the gameplay experience of gaze and NVVI (GV) with the conventional mouse and keyboard (MK) in a study with 15 non-disabled participants. The participants could effectively control the game environment with GV (expectedly a bit slower than MK). More importantly, they found GV more engaging, fun, and enjoyable. In a second study with 10 participants, we successfully validated the feasibility of GV with a target user group of people with disabilities.},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {278–287},
numpages = {10},
keywords = {nonverbal voice inputs, humming, game interaction, eye tracking},
location = {Bengaluru, India},
series = {ICMI '22}
}

@proceedings{10.1145/3610549,
title = {SA '23: SIGGRAPH Asia 2023 XR},
year = {2023},
isbn = {9798400703164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3586183,
title = {UIST '23: Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3527188.3561922,
author = {Phaijit, Ornnalin and Sammut, Claude and Johal, Wafa},
title = {Let’s Compete! The Influence of Human-Agent Competition and Collaboration on Agent Learning and Human Perception},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527188.3561922},
doi = {10.1145/3527188.3561922},
abstract = {In interactive agent learning, the human may teach in a collaborative or adversarial manner. Past research has been focusing on collaborative teaching styles as these are common in human education settings, while overlooking adversarial ones despite promising results in recent research. Moreover, agent performance has been the main focal point while neglecting the perspective of the human teacher, who is crucial to the instructional process. In this work, we examine the impact of competitive and collaborative teaching styles on agent learning and human perception. We conducted a study (N=40) for participants to demonstrate a task in different interaction modes for teaching a computer agent: collaboratively, competitively, or without interacting with the agent. Most participants reported that they preferred competing against the computer agent to the other two modes. Despite smaller numbers of demonstrations given from the user, the agent performance from the interactive modes (collaborative and competitive) was comparable to the non-interactive mode (solo). The agent was perceived as being more competent in the competitive mode than the collaborative mode despite the marginally worse in-task performance. These preliminary findings suggest that competitive types of interaction, when agents or robots learn from humans, lead to better human perception of the agent’s learning when compared to collaborative, and better user engagement when compared to non-interactive learning from demonstrations.},
booktitle = {Proceedings of the 10th International Conference on Human-Agent Interaction},
pages = {86–94},
numpages = {9},
keywords = {learning from demonstration, interactive learning, human-computer interaction, curriculum learning, agent learning},
location = {Christchurch, New Zealand},
series = {HAI '22}
}

@inproceedings{10.1145/3568162.3578627,
author = {Weber, Daniel and Fuhl, Wolfgang and Kasneci, Enkelejda and Zell, Andreas},
title = {Multiperspective Teaching of Unknown Objects via Shared-gaze-based Multimodal Human-Robot Interaction},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3578627},
doi = {10.1145/3568162.3578627},
abstract = {For successful deployment of robots in multifaceted situations, an understanding of the robot for its environment is indispensable. With advancing performance of state-of-the-art object detectors, the capability of robots to detect objects within their interaction domain is also enhancing. However, it binds the robot to a few trained classes and prevents it from adapting to unfamiliar surroundings beyond predefined scenarios. In such scenarios, humans could assist robots amidst the overwhelming number of interaction entities and impart the requisite expertise by acting as teachers. We propose a novel pipeline that effectively harnesses human gaze and augmented reality in a human-robot collaboration context to teach a robot novel objects in its surrounding environment. By intertwining gaze (to guide the robot's attention to an object of interest) with augmented reality (to convey the respective class information) we enable the robot to quickly acquire a significant amount of automatically labeled training data on its own. Training in a transfer learning fashion, we demonstrate the robot's capability to detect recently learned objects and evaluate the influence of different machine learning models and learning procedures as well as the amount of training data involved. Our multimodal approach proves to be an efficient and natural way to teach the robot novel objects based on a few instances and allows it to detect classes for which no training dataset is available. In addition, we make our dataset publicly available to the research community, which consists of RGB and depth data, intrinsic and extrinsic camera parameters, along with regions of interest.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {544–553},
numpages = {10},
keywords = {augmented reality, dataset, eye tracking, gaze, human-robot interaction, multimodal interaction, shared attention, teaching, unknown object detection},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@proceedings{10.1145/3652212,
title = {MMVE '24: Proceedings of the 16th International Workshop on Immersive Mixed and Virtual Environment Systems},
year = {2024},
isbn = {9798400706189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to present the technical program of the 16th ACM International Workshop on Immersive Mixed and Virtual Environment systems (MMVE) 2024. This workshop has always embraced a multidisciplinary approach, exploring not only the evolution of immersive experiences but also the crossroads where immersive technology intersects with diverse domains. Co-located with ACM Multimedia Systems Conference (MMSys) 2024, MMVE allows the gathering and interaction of researchers in the field of immersive technology, from both academia and industry, with multimedia system researchers.},
location = {Bari, Italy}
}

@proceedings{10.1145/3605495,
title = {SAP '23: ACM Symposium on Applied Perception 2023},
year = {2023},
isbn = {9798400702525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Los Angeles, CA, USA}
}

@inproceedings{10.1145/3555776.3577642,
author = {Yang, Qin and Parasuraman, Ramviyas},
title = {A Hierarchical Game-Theoretic Decision-Making for Cooperative Multiagent Systems Under the Presence of Adversarial Agents},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577642},
doi = {10.1145/3555776.3577642},
abstract = {Underlying relationships among Multi-Agent Systems (MAS) in hazardous scenarios can be represented as Game-theoretic models. This paper proposes a new hierarchical network-based model called Game-theoretic Utility Tree (GUT), which decomposes high-level strategies into executable low-level actions for cooperative MAS decisions. It combines with a new payoff measure based on agent needs for real-time strategy games. We present an Explore game domain, where we measure the performance of MAS achieving tasks from the perspective of balancing the success probability and system costs. We evaluate the GUT approach against state-of-the-art methods that greedily rely on rewards of the composite actions. Conclusive results on extensive numerical simulations indicate that GUT can organize more complex relationships among MAS cooperation, helping the group achieve challenging tasks with lower costs and higher winning rates. Furthermore, we demonstrated the applicability of the GUT using the simulator-hardware testbed - Robotarium. The performances verified the effectiveness of the GUT in the real robot application and validated that the GUT could effectively organize MAS cooperation strategies, helping the group with fewer advantages achieve higher performance.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {773–782},
numpages = {10},
keywords = {adversaries, cooperative, agent needs, hierarchical decomposition, game-theoretic, multi-agent systems},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@proceedings{10.1145/3582177,
title = {IPMV '23: Proceedings of the 2023 5th International Conference on Image Processing and Machine Vision},
year = {2023},
isbn = {9781450397926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@proceedings{10.1145/3565066,
title = {MobileHCI '23 Companion: Proceedings of the 25th International Conference on Mobile Human-Computer Interaction},
year = {2023},
isbn = {9781450399241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@article{10.1145/3592459,
author = {Charalambous, Panayiotis and Pettre, Julien and Vassiliades, Vassilis and Chrysanthou, Yiorgos and Pelechano, Nuria},
title = {GREIL-Crowds: Crowd Simulation with Deep Reinforcement Learning and Examples},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592459},
doi = {10.1145/3592459},
abstract = {Simulating crowds with realistic behaviors is a difficult but very important task for a variety of applications. Quantifying how a person balances between different conflicting criteria such as goal seeking, collision avoidance and moving within a group is not intuitive, especially if we consider that behaviors differ largely between people. Inspired by recent advances in Deep Reinforcement Learning, we propose Guided REinforcement Learning (GREIL) Crowds, a method that learns a model for pedestrian behaviors which is guided by reference crowd data. The model successfully captures behaviors such as goal seeking, being part of consistent groups without the need to define explicit relationships and wandering around seemingly without a specific purpose. Two fundamental concepts are important in achieving these results: (a) the per agent state representation and (b) the reward function. The agent state is a temporal representation of the situation around each agent. The reward function is based on the idea that people try to move in situations/states in which they feel comfortable in. Therefore, in order for agents to stay in a comfortable state space, we first obtain a distribution of states extracted from real crowd data; then we evaluate states based on how much of an outlier they are compared to such a distribution. We demonstrate that our system can capture and simulate many complex and subtle crowd interactions in varied scenarios. Additionally, the proposed method generalizes to unseen situations, generates consistent behaviors and does not suffer from the limitations of other data-driven and reinforcement learning approaches.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {137},
numpages = {15},
keywords = {crowd simulation, data-driven methods, user control, crowd authoring, reinforcement learning}
}

@inproceedings{10.1145/3551349.3560504,
author = {Paduraru, Ciprian and Cristea, Rares and Stefanescu, Alin},
title = {Enhancing the security of gaming transactions using blockchain technology},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560504},
doi = {10.1145/3551349.3560504},
abstract = {In this paper, we propose GameBlockchain, an open-source blockchain framework designed to support secure transactions of NFTs in modern computer games. Its purpose is to enable game industry stakeholders such as game developers, content creators, and regular gamers to create and exchange game assets in a more secure and trusted environment. The security of traditional databases and potential data tampering or dangerous user behavior is improved, as outlined in the paper, by blockchain technology, which is used to record critical operations in a ledger, preserving the identity of the user at all times. From a technical perspective, the main goal is to provide an architecture that is easy to use, flexible, understandable, and has an extensible SDK. Using the framework, game developers and regular users should be able to create and trade assets without third-party providers, and use all related services directly in the game interface itself, without having to switch between applications or pay additional transfer fees to providers. We also encourage the development of games with shared marketplaces and wallets on both the developer and user sides, making it easier to monetize assets and services.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {209},
numpages = {8},
keywords = {transaction, games, framework, blockchain, NFT},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@proceedings{10.1145/3674746,
title = {RobCE '24: Proceedings of the 2024 4th International Conference on Robotics and Control Engineering},
year = {2024},
isbn = {9798400716782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Edinburgh, United Kingdom}
}

@inbook{10.1145/3563659.3563674,
author = {Aylett, Ruth},
title = {Interactive Narrative and Story-telling},
year = {2022},
isbn = {9781450398961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3563659.3563674},
booktitle = {The Handbook on Socially Interactive Agents: 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 2: Interactivity, Platforms, Application},
pages = {463–492},
numpages = {30}
}

@article{10.1145/3596235,
author = {Vijay, Harsh and Pushp, Saumay and Mittal, Amish and Gupta, Praveen and Gupta, Meghna and Gambhira, Sirish and Chopra, Shivang and Baranwal, Mayank and Arya, Arshia and Manchepalli, Ajay and Padmanabhan, Venkata N.},
title = {HyWay: Enabling Mingling in the Hybrid World},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3596235},
doi = {10.1145/3596235},
abstract = {We present HyWay, short for "Hybrid Hallway", to enable mingling and informal interactions among physical and virtual users, in casual spaces and settings, such as office water cooler areas, conference hallways, trade show floors, and more. We call out how the hybrid and unstructured (or semi-structured) nature of such settings set these apart from the all-virtual and/or structured settings considered in prior work. Key to the design of HyWay is bridging the awareness gap between physical and virtual users, and providing the virtual users the same agency as physical users.To this end, we have designed HyWay to incorporate reciprocity (users can see and hear others only if they can be seen and heard), porosity (conversations in physical space are porous and not within airtight compartments), and agency (the ability for users to seamlessly move between conversations). We present our implementation of HyWay and the user survey findings from multiple deployments in unstructured settings (e.g., social gatherings), and semi-structured ones (e.g., a poster event). Results from these deployments show that HyWay enables effective mingling between physical and virtual users.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jun},
articleno = {77},
numpages = {33},
keywords = {Hybrid mingling, agency, awareness, porosity, reciprocity, unstructured and semi-structured conversations}
}

@proceedings{10.1145/3611659,
title = {VRST '23: Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},
year = {2023},
isbn = {9798400703287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Christchurch, New Zealand}
}

@inproceedings{10.1145/3543174.3546089,
author = {Detjen, Henrik and Faltaous, Sarah and Keppel, Jonas and Prochazka, Marvin and Gruenefeld, Uwe and Sadeghian, Shadan and Schneegass, Stefan},
title = {Investigating the Influence of Gaze- and Context-Adaptive Head-up Displays on Take-Over Requests},
year = {2022},
isbn = {9781450394154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543174.3546089},
doi = {10.1145/3543174.3546089},
abstract = {In Level 3 automated vehicles, preparing drivers for take-over requests (TORs) on the head-up display (HUD) requires their repeated attention. Visually salient HUD elements can distract attention from potentially critical parts in a driving scene during a TOR. Further, attention is (a) meanwhile needed for non-driving-related activities and can (b) be over-requested. In this paper, we conduct a driving simulator study (N=12), varying required attention by HUD warning presence (absent vs. constant vs. TOR-only) across gaze-adaptivity (with vs. without) to fit warnings to the situation. We found that (1) drivers value visual support during TORs, (2) gaze-adaptive scene complexity reduction works but creates a benefit-neutralizing distraction for some, and (3) drivers perceive constant HUD warnings as annoying and distracting over time. Our findings highlight the need for (a) HUD adaptation based on user activities and potential TORs and (b) sparse use of warning cues in future HUD designs.},
booktitle = {Proceedings of the 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {108–118},
numpages = {11},
keywords = {Warning Cue Design, Warning Continuity, Take-Over Requests, SAE Level 3, Head-up Displays, Gaze-Interaction, Automated Vehicles},
location = {Seoul, Republic of Korea},
series = {AutomotiveUI '22}
}

@proceedings{10.1145/3674912,
title = {CompSysTech '24: Proceedings of the International Conference on Computer Systems and Technologies 2024},
year = {2024},
isbn = {9798400716843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ruse, Bulgaria}
}

@inproceedings{10.1145/3569009.3572745,
author = {Sagheb, Shahabedin and Liu, Frank Wencheng and Vuong, Alex and Dai, Shiling and Wirjadi, Ryan and Bao, Yueming and Likamwa, Robert},
title = {Geppetteau: Enabling haptic perceptions of virtual fluids in various vessel profiles using a string-driven haptic interface},
year = {2023},
isbn = {9781450399777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569009.3572745},
doi = {10.1145/3569009.3572745},
abstract = {What we feel from handling liquids in vessels produces unmistakably fluid tactile sensations. These stimulate essential perceptions in home, laboratory, or industrial contexts. Feeling fluid interactions from virtual fluids would similarly enrich experiences in virtual reality. We introduce Geppetteau, a novel string-driven weight shifting mechanism capable of providing perceivable tactile sensations of handling virtual liquids within a variety of vessel shapes. These mechanisms widen the range of augmentable shapes beyond the state-of-the-art of existing mechanical systems. In this work, Geppetteau is integrated into conical, spherical, cylindrical, and cuboid shaped vessels. Variations of these shapes are often used for fluid containers in our day-to-day. We studied the effectiveness of Geppetteau in simulating fine and coarse-grained tactile sensations of virtual liquids across three user studies. Participants found Geppetteau successful in providing congruent physical sensations of handling virtual liquids in a variety of physical vessel shapes and virtual liquid volumes and viscosities.},
booktitle = {Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {14},
numpages = {14},
keywords = {Ungrounded haptic feedback, fluid dynamics, string-driven actuation, virtual reality},
location = {Warsaw, Poland},
series = {TEI '23}
}

@article{10.1145/3549505,
author = {Mittmann, Gloria and Barnard, Adam and Krammer, Ina and Martins, Diogo and Dias, Jo\~{a}o},
title = {LINA - A Social Augmented Reality Game around Mental Health, Supporting Real-world Connection and Sense of Belonging for Early Adolescents},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549505},
doi = {10.1145/3549505},
abstract = {Early adolescence is a time of major social change; a strong sense of belonging (SB) and peer connectedness is an essential protective factor in mental health (MH) during that period. In this paper we introduce LINA, an augmented reality (AR) smartphone-based serious game played in school by an entire class (age 10+) together with their teacher, which aims to facilitate and improve peer interaction, SB and class climate, while creating a safe space to reflect on MH and external stressors related to family circumstance. LINA was developed through an interdisciplinary collaboration involving a playwright, software developers, psychologists and artists, via an iterative co-development process with young people. A prototype has been evaluated quantitatively for usability and qualitatively for efficacy in a study with 91 early adolescents (agemean=11.41). Results from the Game User Experience Satisfaction Scale (GUESS-18) and data from qualitative focus groups showed high acceptability and preliminary efficacy of the game. Using AR, a shared immersive narrative and collaborative gameplay in a shared physical space offers an opportunity to harness adolescent affinity for digital technology towards improving real-world social connection and SB.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {242},
numpages = {21},
keywords = {social connectedness, serious games, mixed methods, immersive storytelling, digital intervention, collaborative gameplay, co-development, belonging, augmented reality}
}

@proceedings{10.1145/3675249,
title = {ICCMT '24: Proceedings of the 2024 International Conference on Computer and Multimedia Technology},
year = {2024},
isbn = {9798400718267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanming, China}
}

@article{10.1145/3611021,
author = {Terkildsen, Thomas and Engelst, Lene and Clasen, Mathias},
title = {Work Hard, Scare Hard? An Investigation of How Mental Workload Impacts Jump Scare Intensity},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3611021},
doi = {10.1145/3611021},
abstract = {Despite the prevalence and relevance of jump scares in horror video games, there is little empirical research on them. While HCI research commonly uses horror games as experimental stimuli, even less scientific research exists on what makes a jump scare in a game more or less scary. The present between-subject study (n=60) addresses this by investigating whether jump scare intensity—measured physiologically and subjectively—scales with task difficulty. We triggered in-game jump scares at increasing levels of mental workload across four counterbalanced conditions, manipulated using N-back tasks of varying difficulty. Results demonstrate a significant linear relationship between mental workload and physiological arousal. However, this is not the case for subjective perception of arousal elicited by the jump scare. These findings have design implications for horror games. They show that the level of physiological arousal caused by a jump scare can be controlled by changing the difficulty of an in-game task that necessitates a substantial amount of mental work at the same time.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {375},
numpages = {18},
keywords = {Player Experience, Mental Workload, Jump Scare, Horror, Game Design, Electrodermal Activity}
}

@inproceedings{10.1145/3544548.3580944,
author = {Fitton, Isabel and Clarke, Christopher and Dalton, Jeremy and Proulx, Michael J and Lutteroth, Christof},
title = {Dancing with the Avatars: Minimal Avatar Customisation Enhances Learning in a Psychomotor Task},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580944},
doi = {10.1145/3544548.3580944},
abstract = {Virtual environments can support psychomotor learning by allowing learners to observe instructor avatars. Instructor avatars that look like the learner hold promise in enhancing learning; however, it is unclear whether this works for psychomotor tasks and how similar avatars need to be. We investigated ‘minimal’ customisation of instructor avatars, approximating a learner’s appearance by matching only key visual features: gender, skin-tone, and hair colour. These avatars can be created easily and avoid problems of highly similar avatars. Using modern dancing as a skill to learn, we compared the effects of visually similar and dissimilar avatars, considering both learning on a screen (n=59) and in VR (n=38). Our results indicate that minimal avatar customisation leads to significantly more vivid visual imagery of the dance moves than dissimilar avatars. We analyse variables affecting interindividual differences, discuss the results in relation to theory, and derive design implications for psychomotor training in virtual environments.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {714},
numpages = {16},
keywords = {Avatar Customisation, Psychomotor, Skills Training, Virtual Environments, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3610879,
author = {Strecker, Jannis and Akhunov, Khakim and Carbone, Federico and Garc\'{\i}a, Kimberly and Bekta\c{s}, Kenan and Gomez, Andres and Mayer, Simon and Yildirim, Kasim Sinan},
title = {MR Object Identification and Interaction: Fusing Object Situation Information from Heterogeneous Sources},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610879},
doi = {10.1145/3610879},
abstract = {The increasing number of objects in ubiquitous computing environments creates a need for effective object detection and identification mechanisms that permit users to intuitively initiate interactions with these objects. While multiple approaches to such object detection -- including through visual object detection, fiducial markers, relative localization, or absolute spatial referencing -- are available, each of these suffers from drawbacks that limit their applicability. In this paper, we propose ODIF, an architecture that permits the fusion of object situation information from such heterogeneous sources and that remains vertically and horizontally modular to allow extending and upgrading systems that are constructed accordingly. We furthermore present BLEARVIS, a prototype system that builds on the proposed architecture and integrates computer-vision (CV) based object detection with radio-frequency (RF) angle of arrival (AoA) estimation to identify BLE-tagged objects. In our system, the front camera of a Mixed Reality (MR) head-mounted display (HMD) provides a live image stream to a vision-based object detection module, while an antenna array that is mounted on the HMD collects AoA information from ambient devices. In this way, BLEARVIS is able to differentiate between visually identical objects in the same environment and can provide an MR overlay of information (data and controls) that relates to them. We include experimental evaluations of both, the CV-based object detection and the RF-based AoA estimation, and discuss the applicability of the combined RF and CV pipelines in different ubiquitous computing scenarios. This research can form a starting point to spawn the integration of diverse object detection, identification, and interaction approaches that function across the electromagnetic spectrum, and beyond.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {124},
numpages = {26},
keywords = {computer vision, detection, identification, mixed reality}
}

@inproceedings{10.1145/3613904.3642351,
author = {Bhatia, Arpit and Pohl, Henning and Hirzle, Teresa and Seifi, Hasti and Hornb\ae{}k, Kasper},
title = {Using the Visual Language of Comics to Alter Sensations in Augmented Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642351},
doi = {10.1145/3613904.3642351},
abstract = {Augmented Reality (AR) excels at altering what we see but non-visual sensations are difficult to augment. To augment non-visual sensations in AR, we draw on the visual language of comic books. Synthesizing comic studies, we create a design space describing how to use comic elements (e.g., onomatopoeia) to depict non-visual sensations (e.g., hearing). To demonstrate this design space, we built eight demos, such as speed lines to make a user think they are faster and smell lines to make a scent seem stronger. We evaluate these elements in a qualitative user study (N=20) where participants performed everyday tasks with comic elements added as augmentations. All participants stated feeling a change in perception for at least one sensation, with perceived changes detected by between four participants (touch) and 15 participants (hearing). The elements also had positive effects on emotion and user experience, even when participants did not feel changes in perception.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {603},
numpages = {17},
keywords = {augmented reality, comics, sensory augmentation},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3582437.3587205,
author = {Cooper, Seth},
title = {Sturgeon-MKIII: Simultaneous Level and Example Playthrough Generation via Constraint Satisfaction with Tile Rewrite Rules},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3587205},
doi = {10.1145/3582437.3587205},
abstract = {Completability is a key aspect of procedural level generation. In this work, we present a constraint-based approach to level generation for 2D tile-based games that simultaneously generates a level and an example playthrough of the level demonstrating its completability. The approach represents game mechanics as tile rewrite rules, which allows a variety of games and mechanics (beyond simple pathfinding) to be incorporated. The mechanics are represented as constraints in the same problem along with the constraints used to generate the level itself. Thus, the solution to the constraint problem contains both a level and a playthrough of the level. We demonstrate the flexibilty of the system and of tile rewrite rules in several applications, including lock-and-key dungeons, platformers, puzzles, and match-three style games.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {64},
numpages = {9},
keywords = {constraints, procedural content generation, tile rewrite rules},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@proceedings{10.1145/3639701,
title = {IMX '24: Proceedings of the 2024 ACM International Conference on Interactive Media Experiences},
year = {2024},
isbn = {9798400705038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stockholm, Sweden}
}

@proceedings{10.1145/3597063,
title = {MetaSys '23: Proceedings of the First Workshop on Metaverse Systems and Applications},
year = {2023},
isbn = {9798400702136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Helsinki, Finland}
}

@proceedings{10.1145/3562939,
title = {VRST '22: Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology},
year = {2022},
isbn = {9781450398893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tsukuba, Japan}
}

@inproceedings{10.1145/3613904.3642118,
author = {Tran, Tram Thi Minh and Parker, Callum and Hoggenm\"{u}ller, Marius and Wang, Yiyuan and Tomitsch, Martin},
title = {Exploring the Impact of Interconnected External Interfaces in Autonomous Vehicles on Pedestrian Safety and Experience},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642118},
doi = {10.1145/3613904.3642118},
abstract = {Policymakers advocate for the use of external Human-Machine Interfaces (eHMIs) to allow autonomous vehicles (AVs) to communicate their intentions or status. Nonetheless, scalability concerns in complex traffic scenarios arise, such as potentially increasing pedestrian cognitive load or conveying contradictory signals. Building upon precursory works, our study explores ‘interconnected eHMIs,’ where multiple AV interfaces are interconnected to provide pedestrians with clear and unified information. In a virtual reality study (N=32), we assessed the effectiveness of this concept in improving pedestrian safety and their crossing experience. We compared these results against two conditions: no eHMIs and unconnected eHMIs. Results indicated interconnected eHMIs enhanced safety feelings and encouraged cautious crossings. However, certain design elements, such as the use of the colour red, led to confusion and discomfort. Prior knowledge slightly influenced perceptions of interconnected eHMIs, underscoring the need for refined user education. We conclude with practical implications and future eHMI design research directions.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {89},
numpages = {17},
keywords = {autonomous vehicles, eHMIs, external communication, scalability, vehicle-pedestrian interaction, vulnerable road users},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3565970.3567700,
author = {Li, Yuan and Lee, Sang Won and Bowman, Doug A. and Hicks, David and Lages, Wallace santos and Sharma, Akshay},
title = {ARCritique: Supporting Remote Design Critique of Physical Artifacts through Collaborative Augmented Reality},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565970.3567700},
doi = {10.1145/3565970.3567700},
abstract = {Critique sessions are an essential educational activity at the center of many design disciplines, especially those involving the creation of physical mockups. Conventional approaches often require the students and the instructor to be in the same space to jointly view and discuss physical artifacts. However, in remote learning contexts, available tools (such as videoconferencing) are insufficient due to ineffective, inefficient spatial referencing. This paper presents ARCritique, a mobile Augmented Reality application that allows users to 1) scan physical artifacts, generate corresponding 3D models, and share them with distant instructors; 2) view the model simultaneously in a synchronized virtual environment with remote collaborators; and 3) point to and draw on the model synchronously to aid communication. We evaluated ARCritique with seven Industrial Design students and three faculty to use the app in a remote critique setting. The results suggest that direct support for spatial communication improves collaborative experiences.},
booktitle = {Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
articleno = {10},
numpages = {12},
keywords = {Augmented Reality, Collaboration, Remote Learning},
location = {Online, CA, USA},
series = {SUI '22}
}

@inproceedings{10.1145/3579375.3579396,
author = {Draper, Corban and Cheung, Joe Ee and Wuensche, Burkhard and Sanders, Philip J.},
title = {Development of a Virtual Reality Treatment for Tinnitus - A User Study},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579396},
doi = {10.1145/3579375.3579396},
abstract = {Tinnitus is the perception of sound when no external sound source exists. There is currently no cure for tinnitus. Existing treatments aim to mitigate tinnitus through Cognitive Behavioural Therapy and Sound Therapy. Virtual Reality (VR) has been adopted in certain medical fields due to its ability to combine visual and audial stimuli, and create immersive, controlled environments. The purpose of this research was to develop a novel tinnitus treatment, combining VR with existing tinnitus sound therapy, and assess its usability. A user study with 18 healthy participants was conducted. Quantitative and qualitative analyses garnered largely positive results. Participants found the application enjoyable, relaxing, and not stressful. The combination of audial and visual elements inside the virtual environment delivered a unified audio-visual stimulus and a sense of control of the location of the auditory stimulus that would serve as a masker sound in future tinnitus masking paradigms. More elaborate environments (beach and forest) were perceived as more enjoyable and realistic than a scene with minimal content for functionality. The beach scene was perceived as the most relaxing set-up, but the forest scene was preferred overall. Our results suggest the VR application can be developed into a multisensory tinnitus treatment and that future testing of the application in a sample of patients with bothersome tinnitus is justified. We provide guidelines for future researchers looking to create their own VR tinnitus tools.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {160–169},
numpages = {10},
keywords = {virtual reality, ventriloquist effect, tinnitus, multisensory integration, masking, digital therapeutic},
location = {Melbourne, VIC, Australia},
series = {ACSW '23}
}

@inproceedings{10.1145/3564625.3564636,
author = {Gong, Chen and Yang, Zhou and Bai, Yunpeng and Shi, Jieke and Sinha, Arunesh and Xu, Bowen and Lo, David and Hou, Xinwen and Fan, Guoliang},
title = {Curiosity-Driven and Victim-Aware Adversarial Policies},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564625.3564636},
doi = {10.1145/3564625.3564636},
abstract = {Recent years have witnessed great potential in applying Deep Reinforcement Learning (DRL) in various challenging applications, such as autonomous driving, nuclear fusion control, complex game playing, etc. However, recently researchers have revealed that deep reinforcement learning models are vulnerable to adversarial attacks: malicious attackers can train adversarial policies to tamper with the observations of a well-trained victim agent, the latter of which fails dramatically when faced with such an attack. Understanding and improving the adversarial robustness of deep reinforcement learning is of great importance in enhancing the quality and reliability of a wide range of DRL-enabled systems. In this paper, we develop curiosity-driven and victim-aware adversarial policy training, a novel method that can more effectively exploit the defects of victim agents. To be victim-aware, we build a surrogate network that can approximate the state-value function of a black-box victim to collect the victim’s information. Then we propose a curiosity-driven approach, which encourages an adversarial policy to utilize the information from the hidden layer of the surrogate network to exploit the vulnerability of victims efficiently. Extensive experiments demonstrate that our proposed method outperforms or achieves a similar level of performance as the current state-of-the-art across multiple environments. We perform an ablation study to emphasize the benefits of utilizing the approximated victim information. Further analysis suggests that our method is harder to defend against a commonly used defensive strategy, which calls attention to more effective protection on the systems using DRL.},
booktitle = {Proceedings of the 38th Annual Computer Security Applications Conference},
pages = {186–200},
numpages = {15},
keywords = {Reinforcement Learning, Curiosity Mechanism, Adversarial Attack},
location = {Austin, TX, USA},
series = {ACSAC '22}
}

@inproceedings{10.1145/3544548.3581182,
author = {Salagean, Anca and Crellin, Eleanor and Parsons, Martin and Cosker, Darren and Stanton Fraser, Dana\"{e}},
title = {Meeting Your Virtual Twin: Effects of Photorealism and Personalization on Embodiment, Self-Identification and Perception of Self-Avatars in Virtual Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581182},
doi = {10.1145/3544548.3581182},
abstract = {Embodying virtual twins – photorealistic and personalized avatars – will soon be easily achievable in consumer-grade VR. For the first time, we explored how photorealism and personalization impact self-identification, as well as embodiment, avatar perception and presence. Twenty participants were individually scanned and, in a two-hour session, embodied four avatars (high photorealism personalized, low photorealism personalized, high photorealism generic, low photorealism generic). Questionnaire responses revealed stronger mid-immersion body ownership for the high photorealism personalized avatars compared to all other avatar types, and stronger embodiment for high photorealism compared to low photorealism avatars and for personalized compared to generic avatars. In a self-other face distinction task, participants took significantly longer to pause the face morphing videos of high photorealism personalized avatars, suggesting a stronger self-identification bias with these avatars. Photorealism and personalization were perceptually positive features; how employing these avatars in VR applications impacts users over time requires longitudinal investigation.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {499},
numpages = {16},
keywords = {personalization, photorealism, self-identification, virtual reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3565970,
title = {SUI '22: Proceedings of the 2022 ACM Symposium on Spatial User Interaction},
year = {2022},
isbn = {9781450399487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Online, CA, USA}
}

@inproceedings{10.1145/3544548.3581005,
author = {Sabie, Dina and Sheta, Hala and Ferdous, Hasan Shahid and Kopalakrishnan, Vannie and Ahmed, Syed Ishtiaque},
title = {Be Our Guest: Intercultural Heritage Exchange through Augmented Reality (AR)},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581005},
doi = {10.1145/3544548.3581005},
abstract = {This paper explores how interactive applications can help mitigate the adversity of facing cultural differences between migrants and the host community, and between migrants of diverse backgrounds to foster intercultural exchange. Based on literature about situated cognition, immersive theater, and affordance, we designed and built Be Our Guest: an augmented reality application where a user is invited to the houses of people from different cultures and is asked to help with one of their cultural rituals around simple everyday objects. We detail the various phases we took to collect the cultural stories and construct the application. We then report the results of a user study with the developed application. Our findings show that participants were easily immersed in the augmented space due to the app’s narrative, visuals, and interactive nature. Moreover, they enjoyed exploring cultural rituals, including their own, and felt more confident connecting with people from other cultures.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {510},
numpages = {15},
keywords = {AR, Augmented Reality, HCI, communication, culture, exploration, heritage, host community, immersive theater, immigrant, migrant},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3609987,
title = {CHIGREECE '23: Proceedings of the 2nd International Conference of the ACM Greek SIGCHI Chapter},
year = {2023},
isbn = {9798400708886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@inproceedings{10.1145/3568162.3576955,
author = {Hu, Yuhan and Ryu, Jin and Gundana, David and Petersen, Kirstin H. and Kress-Gazit, Hadas and Hoffman, Guy},
title = {Nudging or Waiting? Automatically Synthesized Robot Strategies for Evacuating Noncompliant Users in an Emergency Situation},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3576955},
doi = {10.1145/3568162.3576955},
abstract = {Robots have the potential to assist in emergency evacuation tasks, but it is not clear how robots should behave to evacuate people who are not fully compliant, perhaps due to panic or other priorities in an emergency. In this paper, we compare two robot strategies: an actively nudging robot that initiates evacuation and pulls toward the exit and a passively waiting robot that stays around users and waits for instruction. Both strategies were automatically synthesized from a description of the desired behavior. We conduct a within participant study ( = 20) in a simulated environment to compare the evacuation effectiveness between the two robot strategies. Our results indicate an advantage of the nudging robot for effective evacuation when being exposed to the evacuation scenario for the first time. The waiting robot results in lower efficiency, higher mental load, and more physical conflicts. However, participants like the waiting robots equally or slightly more when they repeat the evacuation scenario and are more familiar with the situation. Our qualitative analysis of the participants' feedback suggests several design implications for future emergency evacuation robots.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {603–611},
numpages = {9},
keywords = {automatic synthesis, evacuation robot, human-robot interaction, noncompliant user, nudging robot, waiting robot},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@inproceedings{10.1145/3544548.3581444,
author = {Johnson, Janet G and Sharkey, Tommy and Butarbutar, Iramuali Cynthia and Xiong, Danica and Huang, Ruijie and Sy, Lauren and Weibel, Nadir},
title = {UnMapped: Leveraging Experts’ Situated Experiences to Ease Remote Guidance in Collaborative Mixed Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581444},
doi = {10.1145/3544548.3581444},
abstract = {Collaborative Mixed Reality (MR) systems that help extend expertise for physical tasks to remote environments often situate experts in an immersive view of the task environment to bring the collaboration closer to collocated settings. In this paper, we design UnMapped, an alternative interface for remote experts that combines a live 3D view of the active space within the novice’s environment with a static 3D recreation of the expert’s own workspace to leverage their existing spatial memories within it. We evaluate the impact of this approach on single and repeated use of collaborative MR systems for remote guidance through a comparative study. Our results indicate that despite having a limited understanding of the novice’s environment, using an UnMapped interface increased performance and communication efficiency while reducing experts’ task load. We also outline the various affordances of providing remote experts with a familiar and spatially-stable environment to assist novices.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {878},
numpages = {20},
keywords = {Augmented Reality, Mixed Reality, Physical Tasks, Remote Collaboration, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3549509,
author = {Cmentowski, Sebastian and Kievelitz, Fabian and Krueger, Jens Harald},
title = {Outpace Reality: A Novel Augmented-Walking Technique for Virtual Reality Games},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549509},
doi = {10.1145/3549509},
abstract = {The size of most virtual environments exceeds the tracking space available for physical walking. One solution to this disparity is to extend the available walking range by augmenting users' actual movements. However, the resulting increase in visual flow can easily cause cybersickness. Therefore, we present a novel augmented-walking approach for virtual reality games. Our core concept is a virtual tunnel that spans the entire travel distance when viewed from the outside. However, its interior is only a fraction as long, allowing users to cover the distance by real walking. Whereas the tunnel hides the visual flow from the applied movement acceleration, windows on the tunnel's walls still reveal the actual expedited motion. Our evaluation reveals that our approach avoids cybersickness while enhancing physical activity and preserving presence. We finish our paper with a discussion of the design considerations and limitations of our proposed locomotion technique.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {246},
numpages = {24},
keywords = {virtual reality games, tunnel, travel, portal, physical walking, navigation, movement, locomotion technique, cybersickness}
}

@inproceedings{10.1145/3503161.3548209,
author = {Su, Siwei and Wang, Haijian and Yang, Meng},
title = {Consistency Learning based on Class-Aware Style Variation for Domain Generalizable Semantic Segmentation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548209},
doi = {10.1145/3503161.3548209},
abstract = {Domain generalizable (DG) semantic segmentation, i.e., a semantic segmentation model pretrained from a source domain performs well in previously unseen target domains without any fine-tuning, remains an open question. A promising solution is learning style-agnostic and domain-invariant features with stylized augmented data. However, existing methods mainly focused on performing stylization on coarse-grained image-level features, while ignoring to explore fine-grained semantic style clues and high-order semantic context correlation, which are essential in enhancing the generalization. Motivated by this, we propose a novel framework termed Consistent Learning based on Class-Aware Style Variation (CL-CASV) for DG semantic segmentation. Specifically, with the guidance of class-level semantic information, our proposed Class-Aware Style Variation (CASV) module simulates imaging object and imaging condition style variation that can appear in complex real-world scenarios, thus generating fine-grained class-aware stylized images with rich style variation. Then the similarities between augmentations and original images are exploited via our Self-Correlation Consistency Learning (SCCL) that mines global context consistency from the views of channel correlation and spatial correlation in the feature and prediction spaces. Extensive experiments on mainstream benchmarks, including Cityscapes, GTAV, BDD100K, SYNTHIA, and Mapillary, demonstrate the effectiveness of our method as it surpasses the state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {6029–6038},
numpages = {10},
keywords = {semantic segmentation, domain generalization, consistency learning, class-aware style transfer},
location = {Lisboa, Portugal},
series = {MM '22}
}

@proceedings{10.1145/3607822,
title = {SUI '23: Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3617695,
title = {BDIOT '23: Proceedings of the 2023 7th International Conference on Big Data and Internet of Things},
year = {2023},
isbn = {9798400708015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.1145/3543174.3545252,
author = {Hock, Philipp and Colley, Mark and Askari, Ali and Wagner, Tobias and Baumann, Martin and Rukzio, Enrico},
title = {Introducing VAMPIRE – Using Kinaesthetic Feedback in Virtual Reality for Automated Driving Experiments},
year = {2022},
isbn = {9781450394154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543174.3545252},
doi = {10.1145/3543174.3545252},
abstract = {Investigating trust, acceptance, and attitudes towards automated driving is often investigated in simulator experiments. Therefore, behavioral validity is a crucial aspect of automated driving studies. However, static simulators have reduced behavioral validity because of their inherent safe environment. We propose VAMPIRE (VR automated movement platform for immersive realistic experiences), a movement platform designed to increase the sensation of realism in automated driving simulator studies using an automated wheelchair. In this work, we provide a detailed description to build the prototype (including software components and assembly instructions), a proposal for safety precautions, an analysis of possible movement patterns for overtaking scenarios, and practical implications for designers and practitioners. We provide all project-related files as auxiliary materials.},
booktitle = {Proceedings of the 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {204–214},
numpages = {11},
keywords = {user studies, on-road simulation., driving simulator, Immersive technology, Automated vehicles},
location = {Seoul, Republic of Korea},
series = {AutomotiveUI '22}
}

@inbook{10.1145/3563659.3563673,
author = {Nadel, Jacqueline and Grynszpan, Ouriel and Martin, Jean-Claude},
title = {Autism and Socially Interactive Agents},
year = {2022},
isbn = {9781450398961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3563659.3563673},
booktitle = {The Handbook on Socially Interactive Agents: 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 2: Interactivity, Platforms, Application},
pages = {437–462},
numpages = {26}
}

@proceedings{10.1145/3625468,
title = {MMSys '24: Proceedings of the 15th ACM Multimedia Systems Conference},
year = {2024},
isbn = {9798400704123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Dear MMSys 2024 Participants,On behalf of the organizers, we are very pleased to welcome you to the 15th ACM Multimedia Systems Conference, taking place for the first time in Italy, in the city of Bari.MMSys is a premier conference dedicated to the exciting and multidisciplinary field of multimedia, with a specific focus on its systems and applications. The conference provides a platform for researchers from both academia and industry to share their latest findings in the multimedia systems research area. Many international researchers, practitioners, engineers, and students from academia, industry, standardization bodies, and government agencies join the MMSys conference each year.},
location = {Bari, Italy}
}

@inproceedings{10.1145/3544548.3581365,
author = {Cheng, Alan Y. and Ritchie, Jacob and Agrawal, Niki and Childs, Elizabeth and DeVeaux, Cyan and Jee, Yubin and Leon, Trevor and Maples, Bethanie and Cuadra, Andrea and Landay, James A.},
title = {Designing Immersive, Narrative-Based Interfaces to Guide Outdoor Learning},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581365},
doi = {10.1145/3544548.3581365},
abstract = {Outdoor learning experiences, such as field trips, can improve children’s science achievement and engagement, but these experiences are often difficult to deliver without extensive support. Narrative in educational experiences can provide needed structure, while also increasing engagement. We created a narrative-based, mobile application to investigate how to guide young learners in interacting with their local, outdoor environment. In a second variant, we added augmented reality and image classification to explore the value of these features. A study (n = 44) found that participants using our system demonstrated learning gains and found the experience engaging. Our findings identified several major themes, including participant excitement for hands-on interactions with nature, curiosity about the characters, and enthusiasm toward typing their thoughts and observations. We offer a set of design implications for supporting narrative-based, outdoor learning with immersive technology.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {445},
numpages = {22},
keywords = {augmented reality, computer vision, immersive technology, machine learning, mobile computing, outdoor learning, ubiquitous computing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3611069,
author = {Kontio, Reetu and Laattala, Markus and Welsch, Robin and H\"{a}m\"{a}l\"{a}inen, Perttu},
title = {“I Feel My Abs”: Exploring Non-standing VR Locomotion},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3611069},
doi = {10.1145/3611069},
abstract = {Virtual Reality (VR) games and experiences predominantly have the users interact while standing or seated. However, this only represents a fraction of the full diversity of human movement. In this paper, we explore a novel non-standing approach to VR locomotion where the user performs locomotion movements in the air or only slightly touching the ground with their feet. For instance, the user may lie supine on the ground, reminiscent of the Bicycle Crunch, a core training movement common in Pilates and other forms of bodyweight exercise. Although this cannot generally replace traditional VR locomotion, it provides two benefits that we believe can be of use for specific application domains such as VR exergames: First, the user's lower body movement is not impeded by a small real-life space, allowing versatile navigation of large virtual worlds using walking, running, strafing, and jumping. Second, we allow new ways to activate parts of the body that remain passive in most existing VR interactions. We describe and discuss four different variants of the approach, and investigate two prototypes further in a qualitative user study, to better understand their strengths, weaknesses, and application potential.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {423},
numpages = {26},
keywords = {VR, exergames, locomotion, virtual reality}
}

@inproceedings{10.5555/3545946.3598650,
author = {Porteous, Julie and Lindsay, Alan and Charles, Fred},
title = {Communicating Agent Intentions for Human-Agent Decision Making under Uncertainty},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent advances in visualisation technologies have opened up new possibilities for human-agent communication. For systems where agents use automated planning, visualisation of agent intentions, i.e., agent planned actions, can assist human understanding and decision making (e.g., deciding when human control is required or when it can be delegated to an agent). We are working in an application area, shipbuilding, where branched plans are often essential, due to the typical uncertainty experienced. Our focus is how best to communicate, using visualisation, the key information content of branched plans. It is important that such visualisations communicate the complexity and variety of the possible agent intentions i.e., executions, captured in a branched plan, whilst also connecting to the practitioner's understanding of the problem. Thus we utilise an approach to generate the complete branched plan, to be able to provide a full picture of its complexity, and a mechanism to select a subset of diverse traces that characterise the possible agent intentions. We have developed an interface which uses 3D visualisation to communicate details of these characterising execution traces. Using this interface, we conducted a study evaluating the impact of different modes of presentation on user understanding. Our results support our expectation that visualisation of branched plan characterising execution traces increases user understanding of agent intention and plan execution possibilities.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {290–298},
numpages = {9},
keywords = {explainable AI planning, human-agent decision making, virtual agents},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{10.1145/3610911,
author = {Ahmad, Fawad and Shin, Christina Suyong and Ghosh, Rajrup and D'Ambrosio, John and Chai, Eugene and Sundaresan, Karthikeyan and Govindan, Ramesh},
title = {AeroTraj: Trajectory Planning for Fast, and Accurate 3D Reconstruction Using a Drone-based LiDAR},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610911},
doi = {10.1145/3610911},
abstract = {This paper presents AeroTraj, a system that enables fast, accurate, and automated reconstruction of 3D models of large buildings using a drone-mounted LiDAR. LiDAR point clouds can be used directly to assemble 3D models if their positions are accurately determined. AeroTraj uses SLAM for this, but must ensure complete and accurate reconstruction while minimizing drone battery usage. Doing this requires balancing competing constraints: drone speed, height, and orientation. AeroTraj exploits building geometry in designing an optimal trajectory that incorporates these constraints. Even with an optimal trajectory, SLAM's position error can drift over time, so AeroTraj tracks drift in-flight by offloading computations to the cloud and invokes a re-calibration procedure to minimize error. AeroTraj can reconstruct large structures with centimeter-level accuracy and with an average end-to-end latency below 250 ms, significantly outperforming the state of the art.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {83},
numpages = {28},
keywords = {3D Reconstruction, Localization, Mapping, Trajectory Planning}
}

@proceedings{10.1145/3631204,
title = {CSCS '23: Proceedings of the 7th ACM Computer Science in Cars Symposium},
year = {2023},
isbn = {9798400704543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Darmstadt, Germany}
}

@proceedings{10.1145/3632776,
title = {ARTECH '23: Proceedings of the 11th International Conference on Digital and Interactive Arts},
year = {2023},
isbn = {9798400708725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Faro, Portugal}
}

@article{10.1145/3567727,
author = {Di Gioia, Francesco Riccardo and Brasier, Eugenie and Pietriga, Emmanuel and Appert, Caroline},
title = {Investigating the Use of AR Glasses for Content Annotation on Mobile Devices},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {ISS},
url = {https://doi.org/10.1145/3567727},
doi = {10.1145/3567727},
abstract = {Mobile devices such as smartphones and tablets have limited display size and input capabilities that make a variety of tasks challenging. Coupling the mobile device with Augmented Reality eyewear such as smartglasses can help address some of these challenges. In the specific context of digital content annotation tasks, this combination has the potential to enhance the user experience on two fronts. First, annotations can be offloaded into the air around the mobile device, freeing precious screen real-estate. Second, as smartglasses often come equipped with a variety of sensors including a camera, users can annotate documents with pictures or videos of their environment, captured on the spot, hands-free, and from the wearer's perspective. We present AnnotAR, a prototype that we use as a research probe to assess the viability of this approach to digital content annotation. We use AnnotAR to gather users' preliminary feedback in a laboratory setting, and to showcase how it could support real-world use cases.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {574},
numpages = {18},
keywords = {mobile device, augmented reality, annotation}
}

@proceedings{10.1145/3526114,
title = {UIST '22 Adjunct: Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bend, OR, USA}
}

@inproceedings{10.1145/3613904.3642346,
author = {Liu, Shuqi and Bu, Jia and Ye, Huayuan and Chen, Juntong and Jiang, Shiqi and Tao, Mingtian and Guo, Liping and Wang, Changbo and Li, Chenhui},
title = {DoodleTunes: Interactive Visual Analysis of Music-Inspired Children Doodles with Automated Feature Annotation},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642346},
doi = {10.1145/3613904.3642346},
abstract = {Music and visual arts are essential in children’s arts education, and their integration has garnered significant attention. Existing data analysis methods for exploring audio-visual correlations are limited. Yet, relevant research is necessary for innovating and promoting arts integration courses. In our work, we collected substantial volumes of music-inspired doodles created by children and interviewed education experts to comprehend the challenges they encountered in the relevant analysis. Based on the insights we obtained, we designed and constructed an interactive visualization system DoodleTunes. DoodleTunes integrates deep learning-driven methods for automatically annotating several types of data features. The visual designs of the system are based on a four-level analysis structure to construct a progressive workflow, facilitating data exploration and insight discovery between doodle images and corresponding music pieces. We evaluated the accuracy of our feature prediction results and collected usage feedback on DoodleTunes from five domain experts.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {889},
numpages = {19},
keywords = {Multimodal data visualization, audio data, data annotation, painting, user interface, visual analysis},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@proceedings{10.1145/3639477,
title = {ICSE-SEIP '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3638530,
title = {GECCO '24 Companion: Proceedings of the Genetic and Evolutionary Computation Conference Companion},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3544548.3580718,
author = {Nilsson, Tommy and Rometsch, Flavie and Becker, Leonie and Dufresne, Florian and Demedeiros, Paul and Guerra, Enrico and Casini, Andrea Emanuele Maria and Vock, Anna and Gaeremynck, Florian and Cowley, Aidan},
title = {Using Virtual Reality to Shape Humanity’s Return to the Moon: Key Takeaways from a Design Study},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580718},
doi = {10.1145/3544548.3580718},
abstract = {Revived interest in lunar exploration is heralding a new generation of design solutions in support of human operations on the Moon. While space system design has traditionally been guided by prototype deployments in analogue studies, the resource-intensive nature of this approach has largely precluded application of proficient user-centered design (UCD) methods from human-computer interaction (HCI). This paper explores possible use of Virtual Reality (VR) to simulate analogue studies in lab settings and thereby bring to bear UCD in this otherwise engineering-dominated field. Drawing on the ongoing development of the European Large Logistics Lander, we have recreated a prospective lunar operational scenario in VR and evaluated it with a group of astronauts and space experts (n=20). Our qualitative findings demonstrate the efficacy of VR in facilitating UCD, enabling efficient contextual inquiries and improving project team coordination. We conclude by proposing future directions to further exploit VR in lunar systems design.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {305},
numpages = {16},
keywords = {HCI research, Virtual Reality, ergonomics, human factors, lunar lander, space system engineering, user centered design},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3643832,
title = {MOBISYS '24: Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services},
year = {2024},
isbn = {9798400705816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Minato-ku, Tokyo, Japan}
}

@inproceedings{10.1145/3613904.3642915,
author = {Manakhov, Pavel and Sidenmark, Ludwig and Pfeuffer, Ken and Gellersen, Hans},
title = {Gaze on the Go: Effect of Spatial Reference Frame on Visual Target Acquisition During Physical Locomotion in Extended Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642915},
doi = {10.1145/3613904.3642915},
abstract = {Spatial interaction relies on fast and accurate visual acquisition. In this work, we analyse how visual acquisition and tracking of targets presented in a head-mounted display is affected by the user moving linearly at walking and jogging paces. We study four reference frames in which targets can be presented: Head and World where targets are affixed relative to the head and environment, respectively; HeadDelay where targets are presented in the head coordinate system but follow head movement with a delay, and novel Path where targets remain at fixed distance in front of the user, in the direction of their movement. Results of our study in virtual reality demonstrate that the more stable the target is relative to the environment, the faster and more precise it can be fixated. The results have practical significance as head-mounted displays enable interaction during mobility, and in particular when eye tracking is considered as input.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {373},
numpages = {16},
keywords = {UI placement, extended reality, eye tracking, gaze interaction, physical locomotion, reference frames, spatial UIs},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@proceedings{10.1145/3670653,
title = {MuC '24: Proceedings of Mensch und Computer 2024},
year = {2024},
isbn = {9798400709982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Karlsruhe, Germany}
}

@inproceedings{10.1145/3544548.3580723,
author = {Matthews, Brandon J and Thomas, Bruce H and Von Itzstein, G Stewart and Smith, Ross T},
title = {Towards Applied Remapped Physical-Virtual Interfaces: Synchronization Methods for Resolving Control State Conflicts},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580723},
doi = {10.1145/3544548.3580723},
abstract = {User interfaces in virtual reality enable diverse interactions within the virtual world, though they typically lack the haptic cues provided by physical interface controls. Haptic retargeting enables flexible mapping between dynamic virtual interfaces and physical controls to provide real haptic feedback. This investigation aims to extend these remapped interfaces to support more diverse control types. Many interfaces incorporate sliders, switches, and knobs. These controls hold fixed states between interactions creating potential conflicts where a virtual control has a different state from the physical control. This paper presents two methods, “manual” and “automatic”, for synchronizing physical and virtual control states and explores the effects of these methods on the usability of remapped interfaces. Results showed that interfaces without retargeting were the ideal configuration, but they lack the flexibility that remapped interfaces provide. Automatic synchronization was faster and more usable; however, manual synchronization is suitable for a broader range of physical interfaces.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {698},
numpages = {18},
keywords = {haptic retargeting, interaction, remapped interfaces, user interfaces, virtual reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3660043,
title = {ICIEAI '23: Proceedings of the 2023 International Conference on Information Education and Artificial Intelligence},
year = {2023},
isbn = {9798400716157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3672406,
title = {IMXw '24: Proceedings of the 2024 ACM International Conference on Interactive Media Experiences Workshops},
year = {2024},
isbn = {9798400717949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Stockholm, Sweden}
}

@article{10.1145/3555548,
author = {Chen, Bo-Han and Wong, Sai-Keung and Chang, Wei-Che and Fan, Roy Ping-Hao},
title = {LAGH: Towards Asymmetrical Collaborative Bodily Play between 1st and 2nd Person Perspectives},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555548},
doi = {10.1145/3555548},
abstract = {This paper investigated to what extent social interactions and empathy of users could be induced when different control mechanisms were used in an asymmetric collaboration. We conducted a user study to explore the user experience under one decentralized and two centralized control conditions via using the proposed two-player asymmetric collaborative bodily play, LAGH, which supports perspective-taking through the integration with the first- and second-perspectives and shared objects. The two players have complementary views and controls to each other in an immersive environment. The results indicate that participant pairs were encouraged by the asymmetric collaboration interface to share their emotional and physiological perspectives with each other. When their control abilities were balanced, they were more motivated to perform information sharing and interact with each other, thereby enhancing closeness and stimulating empathy. Furthermore, users could improve the collaboration efficiency.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {447},
numpages = {26},
keywords = {social interaction, person perspectives, empathy, collaborative exertion game, bodily play, asymmetrical virtual reality}
}

@proceedings{10.5555/3606013,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3510456.3514157,
author = {Stegh\"{o}fer, Jan-Philipp and Burden, H\r{a}kan},
title = {One block on top of the other: using minetest to teach Scrum},
year = {2022},
isbn = {9781450392259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510456.3514157},
doi = {10.1145/3510456.3514157},
abstract = {Teaching Scrum using Lego has been an established teaching technique for years. However, the COVID-19 pandemic forced teachers all over the globe to rethink this valuable teaching tool. In this experience report, we show how we transferred our version of a Lego Scrum workshop into the world of Minetest, an open-source variant of Minecraft. We detail our reasoning, the concrete technical and pedagogical challenges, as well as experiences and reflections from the students, us as teachers, our peers, and theoretical frameworks. Finally, we share our materials to enable other teachers to use this new tool.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {176–186},
numpages = {11},
keywords = {software engineering education, serious games, online education, Scrum},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEET '22}
}

@inproceedings{10.1145/3613904.3641925,
author = {Reddy, G S Rajshekar and Proulx, Michael J and Hirshfield, Leanne and Ries, Anthony},
title = {Towards an Eye-Brain-Computer Interface: Combining Gaze with the Stimulus-Preceding Negativity for Target Selections in XR},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641925},
doi = {10.1145/3613904.3641925},
abstract = {Gaze-assisted interaction techniques enable intuitive selections without requiring manual pointing but can result in unintended selections, known as Midas touch. A confirmation trigger eliminates this issue but requires additional physical and conscious user effort. Brain-computer interfaces (BCIs), particularly passive BCIs harnessing anticipatory potentials such as the Stimulus-Preceding Negativity (SPN) - evoked when users anticipate a forthcoming stimulus - present an effortless implicit solution for selection confirmation. Within a VR context, our research uniquely demonstrates that SPN has the potential to decode intent towards the visually focused target. We reinforce the scientific understanding of its mechanism by addressing a confounding factor - we demonstrate that the SPN is driven by the user’s intent to select the target, not by the stimulus feedback itself. Furthermore, we examine the effect of familiarly placed targets, finding that SPN may be evoked quicker as users acclimatize to target locations; a key insight for everyday BCIs.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {376},
numpages = {17},
keywords = {Assistive technology, Brain-Computer Interfaces, EEG, Eye-tracking, Gaze interaction, Menu selection, Midas touch, Pointing, Spatial Computing},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3549500,
author = {Lankes, Michael and Ramirez Gomez, Argenis},
title = {GazeCues: Exploring the Effects of Gaze-based Visual Cues in Virtual Reality Exploration Games},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549500},
doi = {10.1145/3549500},
abstract = {This paper examines different gaze-based cue visualization techniques in a Virtual Reality (VR) exploration game. It is argued that gaze could be a valuable design tool for providing cues to players. However, little is known about how these elements could be visually presented and integrated to inform players and fit into the game world. An exploratory study was carried out to investigate four different design approaches (subtle, overlaid-virtual, integrated, emphasized) and their effects on aspects such as clarity, usefulness, and curiosity. In general, the visualizations led to different player impressions on all scales. While players perceived the subtle and emphasized variants as more aesthetically appealing, integrated and virtual-overlaid solutions received more positive ratings regarding tool-related aspects such as ease of extraction and accurateness. We anticipate future work could employ gaze-based cues in various application contexts based on their utility as eye input in VR games continues developing.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {237},
numpages = {25},
keywords = {virtual reality, gaze-based cues, feedback visualization}
}

@proceedings{10.1145/3615979,
title = {SIGSIM-PADS '24: Proceedings of the 38th ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
year = {2024},
isbn = {9798400703638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Atlanta, GA, USA}
}

@article{10.1145/3611056,
author = {Montoya, Maria F. and Ji, YuYang and Wee, Ryan and Overdevest, Nathalie and Patibanda, Rakesh and Saini, Aryan and Pell, Sarah Jane and Mueller, Florian ‘Floyd’},
title = {Fluito: Towards Understanding the Design of Playful Water Experiences through an Extended Reality Floatation Tank System},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3611056},
doi = {10.1145/3611056},
abstract = {Water's pleasant nature and associated health benefits have captivated the interest of HCI researchers. Prior WaterHCI work mainly focused on advancing instrumental applications, such as improving swimming performance, and less on designing systems that support interacting with technology in water in more playful contexts. In this regard, we propose floatation tanks as research vehicles to investigate the design of playful interactive water experiences. Employing somaesthetic design, we developed a playful extended reality floatation tank experience: "Fluito". We conducted a 13-participant study to understand how specific design features amplified participants' water experiences. We used a postphenomenological lens to articulate eight strategies useful for designers aiming to develop digital playful experiences in water, such as designing to call attention to the water and designing to encourage breathing and body awareness in water experiences. Ultimately, we hope that our work supports people to be playful and benefit from the many advantages of being in water.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {410},
numpages = {28},
keywords = {water activities, water, somaesthetic, postphenomenology, playful experience, flotation pod, floatation tank, extended reality, WaterHCI}
}

@proceedings{10.1145/3588015,
title = {ETRA '23: Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},
year = {2023},
isbn = {9798400701504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tubingen, Germany}
}

@article{10.1145/3549518,
author = {Sin, Zackary P. T. and Chen, Peter Q. and Ng, Peter H. F. and Leong, Hong Va},
title = {Tracking Stuffed Toy for Naturally Mapped Interactive Play via a Soft-Pose Estimator},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549518},
doi = {10.1145/3549518},
abstract = {Have you ever picked up a stuffed toy and pretended to play with it in your childhood? We are motivated by the novel use of stuffed toys in enhancing extended reality interaction. A key goal of extended reality is to induce the feeling of presence in its users. Naturally mapped control interface has been shown to enhance presence. The literature also indicates that a high degree of freedom tracking is important to extended reality. Based on these observations, we show that a free-form naturally mapped control interface is well-motivated via a theoretical contextualization. We explore the possibility of building such a controller in the form of stuffed toys. To realize stuffed toys as controllers, a novel soft-pose estimator empowered by cage-based deformation is proposed. It is shown to be effective in tracking the poses and deformations of real soft objects even by training with synthetic data only. Three gameplay prototypes are developed to demonstrate that interactive play can be enabled by the soft-pose estimator. They also form the basis for two user studies that validate the success of tracking stuffed toys with the soft-pose estimator for interactive play.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {255},
numpages = {25},
keywords = {tracking stuffed toy, soft-pose estimator, novel controls for games and play, naturally mapping control interface, interactive XR play}
}

@proceedings{10.1145/3576914,
title = {CPS-IoT Week '23: Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Antonio, TX, USA}
}

@proceedings{10.1145/3672758,
title = {CAICE '24: Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering},
year = {2024},
isbn = {9798400716942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi' an, China}
}

@article{10.1145/3659588,
author = {Wicaksono, Irmandy and Maheshwari, Aditi and Haddad, Don Derek and Paradiso, Joseph and Danielescu, Andreea},
title = {Design and Fabrication of Multifunctional E-Textiles by Upcycling Waste Cotton Fabrics through Carbonization},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659588},
doi = {10.1145/3659588},
abstract = {The merging of electronic materials and textiles has triggered the proliferation of wearables and interactive surfaces in the ubiquitous computing era. However, this leads to e-textile waste that is difficult to recycle and decompose. Instead, we demonstrate an eco-design approach to upcycle waste cotton fabrics into functional textile elements through carbonization without the need for additional materials. We identify optimal parameters for the carbonization process and develop encapsulation techniques to improve the response, durability, and washability of the carbonized textiles. We then configure these e-textiles into various 'design primitives' including sensors, interconnects, and heating elements, and evaluate their electromechanical properties against commercially available e-textiles. Using these primitives, we demonstrate several applications, including a haptic-transfer fabric, a joint-sensing wearable, and an intelligent sailcloth. Finally, we highlight how the sensors can be composted, re-carbonized and coated onto other fabrics, or repurposed into different sensors towards their end-of-life to promote a circular manufacturing process.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {may},
articleno = {45},
numpages = {31},
keywords = {carbonization, circular manufacturing, electronic textiles, functionalization, human-computer interaction, multimodal sensing, proxemic and tactile sensing, smart fabrics, sustainability}
}

@proceedings{10.1145/3635636,
title = {C&amp;C '24: Proceedings of the 16th Conference on Creativity &amp; Cognition},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@article{10.1145/3524023,
author = {Kumar, Abhishek and Kumar, Ankit and Raja, Linesh and Singh, Kamred Udham},
title = {Rediscovering the Traditional UNESCO World Heritage Hawamahal through 3D Animation and Immersive Technology},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3524023},
doi = {10.1145/3524023},
abstract = {Nowadays, humans are searching for alternative energy resources due to the high demand in energy consumption by several means like automobiles, electronic devices, household items, and many more. Nature has given us limited resources, but population and pollution levels increase daily. This article aims at improving the thermal environment, which is becoming increasingly important due to the energy-saving effects on occupants and health concerns. The focus of the research would be a change in the procedures to follow nature and draw attention and inspiration from the ancient architecture of the Hawamahal (a palace in the city of Jaipur, India). This article examines Hawamahal’s ancient architecture regarding temperature and passive cooling insulation. It makes recommendations for combining traditional ideas with a method of integrated, immersive technology to demonstrate environmental challenges and preserve the ethnic heritage digitally. The simulation results represent the temperature difference between Hawamahal’s inner and outer spaces during summer days. The study’s findings include using a digital twin system to discover the immersive experience of Jaipur’s Hawamahal without requiring physical participation. For the culture and tourism industries, this has the potential to be a digital revolution. The findings also show the wealth of ancient Indian architecture compared to modern twenty-first-century infrastructure and provide a virtual reality experience of Hawamahal.},
journal = {J. Comput. Cult. Herit.},
month = {feb},
articleno = {61},
numpages = {34},
keywords = {immersive technology, virtual reality, heritage preservation, design, 3D reconstruction, Animation}
}

@proceedings{10.1145/3582700,
title = {AHs '23: Proceedings of the Augmented Humans International Conference 2023},
year = {2023},
isbn = {9781450399845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Glasgow, United Kingdom}
}

@inproceedings{10.1145/3536221.3556608,
author = {Obremski, David and Hering, Helena Babette and Friedrich, Paula and Lugrin, Birgit},
title = {Exploratory Study on the Perception of Intelligent Virtual Agents With Non-Native Accents Using Synthetic and Natural Speech in German},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3556608},
doi = {10.1145/3536221.3556608},
abstract = {This paper presents an exploratory study which investigates the impact of different non-native accents and the naturalness of speech on the correct assignment of an Intelligent Virtual Agent’s (IVA) mother tongue, as well as its perceived warmth, competence and intelligibility. An online-experiment with a between subjects design was conducted, in which the participants, who were native speakers of German, watched a video of an IVA that spoke German with a non-native accent. The IVA’s speech was either synthetically generated or pre-recorded using non-native speakers. The participants experienced an IVA with either a Turkish, Italian or Polish accent, based on the most frequent accents in the German-speaking area. The results revealed that the IVA’s accent impacted its perceived warmth, but not its perceived competence and intelligibility. The IVA’s naturalness of speech played no role in its classification as a non-native speaker of German but on the correctness of the assigned mother tongue within the Polish accent condition. These results give valuable insight in the perception of non-native speaking IVAs and constitute helpful implications for future research with mixed-cultural IVAs.},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {15–24},
numpages = {10},
keywords = {synthetic speech, non-native accent, intelligent virtual agents},
location = {Bengaluru, India},
series = {ICMI '22}
}

@proceedings{10.1145/3663529,
title = {FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to FSE 2024, the ACM International Conference on the Foundations of Software Engineering (FSE) 2024. The conference now has a shorter name! FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {Porto de Galinhas, Brazil}
}

@proceedings{10.1145/3652628,
title = {ICAICE '23: Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering},
year = {2023},
isbn = {9798400708831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dalian, China}
}

@inproceedings{10.1145/3563657.3596045,
author = {Asha, Ashratuz Zavin and Sharlin, Ehud},
title = {Designing Inclusive Interaction with Autonomous Vehicles for Older Passengers},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596045},
doi = {10.1145/3563657.3596045},
abstract = {With the introduction of autonomous vehicles (AVs) in real-world traffic, there are several human-computer interactions (HCI) challenges—especially how a passenger might interact with an AV where the human driver is absent. Riding inside an AV could be more challenging for older adults who are expected to be one of the largest user cohorts of AV. In this work, we conducted an interview session with senior participants (N=10) which allowed us to identify specific design needs for elderly passengers to have reliable communication with AVs. Based on the findings, we implemented five proof-of-concept prototypes in virtual reality (VR) and evaluated them through a user study with another group of elderly participants (N=15). Informed by the outcome, we discuss design insights for implementing inclusive interfaces inside the AVs that will be supportive of the functional abilities and cognitive needs of older adults to increase psychological driving comfort and perceived safety.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {2138–2154},
numpages = {17},
keywords = {Older adults, autonomous vehicles, passengers},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/3569219.3569311,
author = {Low, Amari and Turner, Jane and Foth, Marcus},
title = {Pla(y)cemaking With Care: Locative Mobile Games as Agents of Place Cultivation},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569219.3569311},
doi = {10.1145/3569219.3569311},
abstract = {There is growing academic interest in how people use ubiquitous computing devices—particularly smartphones—for cultivating liveable, enjoyable places, replete with layers of living meaning, memory, heritage and social connections. This is a trajectory embraced by two related paradigms of urban studies: slow cities and playable cities. In this paper we investigate how locative mobile games—alternately known as location-based games and pervasive games—can be vehicles of placemaking through play, enhancing the careful processes that form places out of layered networks of affection, habit, and social bonds in the playable city. We highlight carefulness as a latent theme not previously given a close treatment in locative game scholarship, applying it as a lens in our study of locative mobile games. Drawing on autoethnography, we investigate three locative mobile games—Niantic's Ingress and Pikmin Bloom, and Meyran Games’ Plant the World—conducting a qualitative analysis of the findings using Dena's elements-behaviour-experiences (EBE) framework. We distil our findings into four design implications for developing locative mobile games that support pla(y)cemaking with care: physical anchorage, slow mechanics, ownership, and co-construction.},
booktitle = {Proceedings of the 25th International Academic Mindtrek Conference},
pages = {135–146},
numpages = {12},
keywords = {Urban Informatics, Space and Place, Slow Cities, Locative Mobile Games, Location-Based Media, Digital Placemaking},
location = {Tampere, Finland},
series = {Academic Mindtrek '22}
}

@proceedings{10.1145/3660515,
title = {EICS '24 Companion: Companion Proceedings of the 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
year = {2024},
isbn = {9798400706516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cagliari, Italy}
}

@proceedings{10.1145/3638209,
title = {CIIS '23: Proceedings of the 2023 6th International Conference on Computational Intelligence and Intelligent Systems},
year = {2023},
isbn = {9798400709067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3678429,
title = {ICHMI '24: Proceedings of the 2024 4th International Conference on Human-Machine Interaction},
year = {2024},
isbn = {9798400716812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xi'an, China}
}

@proceedings{10.1145/3610977,
title = {HRI '24: Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
year = {2024},
isbn = {9798400703225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome one and all to the 19th Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI)!We are so pleased to re-welcome the HRI community to Boulder, Colorado, where HRI 2021 would have been held, had the COVID pandemic not interfered. Following up on the successful in-person conference held last year in Sweden, this year's theme is "HRI in the Real World," and focuses on advances that aim to bring human-robot interaction out of the lab and into everyday life.One aspect of this that we are very excited about is the introduction of a robot challenge to the conference activities, where teams from around the world will showcase their research and development via actual, interactive robots in the "real world" of an academic conference. It is our hope that this feature will grow and develop over the coming years into a staple of the HRI conference.This year's HRI conference saw an impressive surge in global interest, with 352 full paper submissions from around the world, marking a significant 40% increase compared to the previous year. These papers were categorized under relevant thematic subcommittees and underwent a double-blind review process, a rebuttal phase, and selective shepherding by the HRI program committee. From this process, 87 outstanding papers (24.7%) were chosen for full presentation at the conference. Reflecting our joint sponsorship with IEEE and ACM, all accepted papers will be accessible in the ACM Digital Library and IEEE Xplore.},
location = {Boulder, CO, USA}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3570945,
title = {IVA '23: Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the papers presented at the 23nd International Conference on Intelligent Virtual Agents (IVA 2023) located in W\"{u}rzburg, Germany, from 19. to 22.09.2023.},
location = {W\"{u}rzburg, Germany}
}

@proceedings{10.1145/3628228,
title = {i-CREATe '23: Proceedings of the 16th International Convention on Rehabilitation Engineering and Assistive Technology},
year = {2023},
isbn = {9798400709159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Pathum Thani, Thailand}
}

@proceedings{10.1145/3613905,
title = {CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3561833,
title = {COMPUTE '22: Proceedings of the 15th Annual ACM India Compute Conference},
year = {2022},
isbn = {9781450397759},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jaipur, India}
}

@proceedings{10.1145/3561212,
title = {AM '22: Proceedings of the 17th International Audio Mostly Conference},
year = {2022},
isbn = {9781450397018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. P\"{o}lten, Austria}
}

@proceedings{10.1145/3625704,
title = {ICEMT '23: Proceedings of the 7th International Conference on Education and Multimedia Technology},
year = {2023},
isbn = {9798400709142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3652920,
title = {AHs '24: Proceedings of the Augmented Humans International Conference 2024},
year = {2024},
isbn = {9798400709807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3549865,
title = {Interacci\'{o}n '22: Proceedings of the XXII International Conference on Human Computer Interaction},
year = {2022},
isbn = {9781450397025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Teruel, Spain}
}

@proceedings{10.1145/3638837,
title = {ICNCC '23: Proceedings of the 2023 12th International Conference on Networks, Communication and Computing},
year = {2023},
isbn = {9798400709265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Osaka, Japan}
}

@proceedings{10.1145/3617733,
title = {ICCCM '23: Proceedings of the 2023 11th International Conference on Computer and Communications Management},
year = {2023},
isbn = {9798400707735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nagoya, Japan}
}

@inproceedings{10.1145/3540250.3549083,
author = {Li, Cong and Jiang, Yanyan and Xu, Chang},
title = {Cross-device record and replay for Android apps},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549083},
doi = {10.1145/3540250.3549083},
abstract = {Cross-device replay for Android apps is challenging because apps have to adapt or even restructure their GUIs responsively upon screen-size or orientation change across devices. As a first exploratory work, this paper demonstrates that cross-device record and replay can be made simple and practical by a one-pass, greedy algorithm by the Rx framework leveraging the least surprise principle in the GUI design. The experimental results of over 1,000 replay settings encouragingly show that our implemented Rx prototype tool effectively solved non-trivial cross-device replay cases beyond any known non-search-based work's scope, and had still competitive capabilities on same-device replay with start-of-the-art techniques.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {395–407},
numpages = {13},
keywords = {record and replay, Android app testing},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@proceedings{10.1145/3610978,
title = {HRI '24: Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome one and all to the 19th Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI)!We are so pleased to re-welcome the HRI community to Boulder, Colorado, where HRI 2021 would have been held, had the COVID pandemic not interfered. Following up on the successful in-person conference held last year in Sweden, this year's theme is "HRI in the Real World," and focuses on advances that aim to bring human-robot interaction out of the lab and into everyday life.One aspect of this that we are very excited about is the introduction of a robot challenge to the conference activities, where teams from around the world will showcase their research and development via actual, interactive robots in the "real world" of an academic conference. It is our hope that this feature will grow and develop over the coming years into a staple of the HRI conference.This year's HRI conference saw an impressive surge in global interest, with 352 full paper submissions from around the world, marking a significant 40% increase compared to the previous year. These papers were categorized under relevant thematic subcommittees and underwent a double-blind review process, a rebuttal phase, and selective shepherding by the HRI program committee. From this process, 87 outstanding papers (24.7%) were chosen for full presentation at the conference. Reflecting our joint sponsorship with IEEE and ACM, all accepted papers will be accessible in the ACM Digital Library and IEEE Xplore.},
location = {Boulder, CO, USA}
}

@proceedings{10.1145/3581754,
title = {IUI '23 Companion: Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3617553,
title = {Gamify 2023: Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation},
year = {2023},
isbn = {9798400703737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the Program Committee, we are pleased to present the proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation (Gamify 2023). The workshop is virtually co-located with the 2023 edition of the ESEC/FSE conference, held in San Francisco (CA, USA). The workshop will be held online only the 4th of December 2023.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3544548.3580973,
author = {Cmentowski, Sebastian and Karaosmanoglu, Sukran and Nacke, Lennart E. and Steinicke, Frank and Kr\"{u}ger, Jens Harald},
title = {Never Skip Leg Day Again: Training the Lower Body with Vertical Jumps in a Virtual Reality Exergame},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580973},
doi = {10.1145/3544548.3580973},
abstract = {Virtual Reality (VR) exergames can increase engagement in and motivation for physical activities. Most VR exergames focus on the upper body because many VR setups only track the users’ heads and hands. To become a serious alternative to existing exercise programs, VR exergames must provide a balanced workout and train the lower limbs, too. To address this issue, we built a VR exergame focused on vertical jump training to explore full-body exercise applications. To create a safe and effective training, nine domain experts participated in our prototype design. Our mixed-methods study confirms that the jump-centered exercises provided a worthy challenge and positive player experience, indicating long-term retention. Based on our findings, we present five design implications to guide future work: avoid an unintended forward drift, consider technical constraints, address safety concerns in full-body VR exergames, incorporate rhythmic elements with fluent movement patterns, adapt difficulty to players’ fitness progression status.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {786},
numpages = {18},
keywords = {VR, dynamic difficulty, exergame, health, serious games, sport, training, vertical jump, virtual reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3543758,
title = {MuC '22: Proceedings of Mensch und Computer 2022},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Darmstadt, Germany}
}

@proceedings{10.1145/3627050,
title = {IoT '23: Proceedings of the 13th International Conference on the Internet of Things},
year = {2023},
isbn = {9798400708541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nagoya, Japan}
}

@proceedings{10.5555/3643142,
title = {WSC '23: Proceedings of the Winter Simulation Conference},
year = {2023},
isbn = {9798350369663},
publisher = {IEEE Press},
location = {San Antonio, Texas, USA}
}

@proceedings{10.1145/3587281,
title = {W4A '23: Proceedings of the 20th International Web for All Conference},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3548608,
title = {ICCIR '22: Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics},
year = {2022},
isbn = {9781450397179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, China}
}

@proceedings{10.1145/3599640,
title = {ICETT '23: Proceedings of the 9th International Conference on Education and Training Technologies},
year = {2023},
isbn = {9781450399593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@proceedings{10.1145/3634713,
title = {VaMoS '24: Proceedings of the 18th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2024},
isbn = {9798400708770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bern, Switzerland}
}

@proceedings{10.1145/3652037,
title = {PETRA '24: Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Crete, Greece}
}

@proceedings{10.1145/3594806,
title = {PETRA '23: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Corfu, Greece}
}

@article{10.1145/3555604,
author = {Kumar, Kartikaeya and Poretski, Lev and Li, Jiannan and Tang, Anthony},
title = {Tourgether360: Collaborative Exploration of 360° Videos using Pseudo-Spatial Navigation},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555604},
doi = {10.1145/3555604},
abstract = {Collaborative exploration of 360 videos with contemporary interfaces is challenging because collaborators do not have awareness of one another's viewing activities. Tourgether360 enhances social exploration of 360° tour videos using a pseudo-spatial navigation technique that provides both an overhead "context" view of the environment as a minimap, as well as a shared pseudo-3D environment for exploring the video. Collaborators are embodied as avatars along a track depending on their position in the video timeline and can point and synchronize their playback. We evaluated the Tourgether360 concept through two studies: first, a comparative study with a simplified version of Tourgether360 with collaborator embodiments and a minimap versus a conventional interface; second, an exploratory study where we studied how collaborators used Tourgether360 to navigate and explore 360° environments together. We found that participants adopted the Tourgether360 approach with ease and enjoyed the shared social aspects of the experience. Participants reported finding the experience similar to an interactive social video game.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {546},
numpages = {27},
keywords = {video navigation, collaborative navigation, 360 video}
}

@proceedings{10.1145/3569173,
title = {CSERC '22: Proceedings of the 11th Computer Science Education Research Conference},
year = {2022},
isbn = {9781450397476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Leiden, Netherlands}
}

@proceedings{10.1145/3613904,
title = {CHI '24: Proceedings of the CHI Conference on Human Factors in Computing Systems},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Honolulu, HI, USA}
}

@proceedings{10.1145/3665026,
title = {ICMIP '24: Proceedings of the 2024 9th International Conference on Multimedia and Image Processing},
year = {2024},
isbn = {9798400716164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Osaka, Japan}
}

@proceedings{10.1145/3585059,
title = {SIGITE '23: Proceedings of the 24th Annual Conference on Information Technology Education},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Marietta, GA, USA}
}

@proceedings{10.1145/3643833,
title = {WiSec '24: Proceedings of the 17th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
year = {2024},
isbn = {9798400705823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2024 ACM Conference on Security and Privacy in Wireless and Mobile Networks (ACM WiSec)!Now in its 17th year, WiSec continues to be the premier venue for research on all aspects of security and privacy in wireless and mobile networks, their systems, and their applications. We are hosted by the Korea Institute of Information Security &amp; Cryptology, located in the city center of Seoul, Korea - a city known for its dynamic mix of 600-year-old palaces and the contemporary urban landscape characterized by towering skyscrapers.We begin our exciting three-day main conference program on May 27th with single-track technical paper sessions, a poster and demo session, two excellent keynotes from telecommunication security expert Prof. Jean-Pierre Seifert (TU Berlin) and wireless security expert Mathy Vanhoef (KU Leuven), and a panel on wireless security and AI. Three invited talks named "Vision Talk" discuss the future of wireless and mobile security issues. The WiseML Workshop follows the main program on May 30th. We invite participants to attend the exciting paper presentations and keynotes, interact with the presenters during the Q&amp;A sessions after each talk, network during the coffee breaks and lunches each day, and socialize during the banquet dinner.},
location = {Seoul, Republic of Korea}
}

@proceedings{10.1145/3606094,
title = {ICDEL '23: Proceedings of the 2023 8th International Conference on Distance Education and Learning},
year = {2023},
isbn = {9798400700422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3626705,
title = {MUM '23: Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@proceedings{10.1145/3643916,
title = {ICPC '24: Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICPC is the premier (CORE A) venue for research on program comprehension. Research on program comprehension encompasses both human activities for comprehending the software and technologies for supporting such comprehension.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3589806,
title = {ACM REP '23: Proceedings of the 2023 ACM Conference on Reproducibility and Replicability},
year = {2023},
isbn = {9798400701764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Santa Cruz, CA, USA}
}

@proceedings{10.1145/3597503,
title = {ICSE '24: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3583133,
title = {GECCO '23 Companion: Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GECCO is the largest peer-reviewed conference in the field of Evolutionary Computation, and the main conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO) of the Association for Computing Machinery (ACM).},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3578837,
title = {ICEEL '22: Proceedings of the 2022 6th International Conference on Education and E-Learning},
year = {2022},
isbn = {9781450398428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yamanashi, Japan}
}

@proceedings{10.1145/3577190,
title = {ICMI '23: Proceedings of the 25th International Conference on Multimodal Interaction},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

@proceedings{10.1145/3628516,
title = {IDC '24: Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Delft, Netherlands}
}

@proceedings{10.1145/3626252,
title = {SIGCSE 2024: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 55th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2024)! This year, we have returned to Portland, Oregon. We hope that, like us, you are looking forward to a highly productive and engaging symposium that provides ample opportunity to renew old relationships, build new connections, and learn about the latest advances in our field. While we are sure that there will be a few surprises along the way, we hope and expect that we won't experience anything nearly as disruptive as the opening days of the pandemic, which occurred when we last tried to gather here in 2020.Our theme for this year's symposium is "Blazing New Trails in CS Education." This broad theme captures the exceptional work being performed by this community to enhance our teaching, improve our assessments, attract diverse students, and all of the other laudable projects, initiatives, and undertakings that affect positive change. The breadth of the program is substantial - there truly should be something for everyone. In fact, your biggest challenge may be deciding which session to attend in each time slot because there is so much going on! We know that many of you want to attend as many sessions as possible while you are here in Portland, but we encourage you to also find a little bit of time for yourself so that you leave Portland refreshed, renewed and encouraged, rather than exhausted or burnt out.},
location = {Portland, OR, USA}
}

@proceedings{10.1145/3615522,
title = {VINCI '23: Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
year = {2023},
isbn = {9798400707513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@proceedings{10.1145/3560470,
title = {ICHMI '22: Proceedings of the 2022 International Conference on Human Machine Interaction},
year = {2022},
isbn = {9781450396615},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@article{10.1613/jair.1.13445,
author = {Ganapathi Subramanian, Sriram and Taylor, Matthew E. and Larson, Kate and Crowley, Mark},
title = {Multi-Agent Advisor Q-Learning},
year = {2022},
issue_date = {Sep 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {74},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13445},
doi = {10.1613/jair.1.13445},
abstract = {In the last decade, there have been significant advances in multi-agent reinforcement learning (MARL) but there are still numerous challenges, such as high sample complexity and slow convergence to stable policies, that need to be overcome before wide-spread deployment is possible. However, many real-world environments already, in practice, deploy sub-optimal or heuristic approaches for generating policies. An interesting question that arises is how to best use such approaches as advisors to help improve reinforcement learning in multi-agent domains. In this paper, we provide a principled framework for incorporating action recommendations from online suboptimal advisors in multi-agent settings. We describe the problem of ADvising Multiple Intelligent Reinforcement Agents (ADMIRAL) in nonrestrictive general-sum stochastic game environments and present two novel Q-learning based algorithms: ADMIRAL - Decision Making (ADMIRAL-DM) and ADMIRAL - Advisor Evaluation (ADMIRAL-AE), which allow us to improve learning by appropriately incorporating advice from an advisor (ADMIRAL-DM), and evaluate the effectiveness of an advisor (ADMIRAL-AE). We analyze the algorithms theoretically and provide fixed point guarantees regarding their learning in general-sum stochastic games. Furthermore, extensive experiments illustrate that these algorithms: can be used in a variety of environments, have performances that compare favourably to other related baselines, can scale to large state-action spaces, and are robust to poor advice from advisors.},
journal = {J. Artif. Int. Res.},
month = {sep},
numpages = {74}
}

@proceedings{10.1145/3628034,
title = {EuroPLoP '23: Proceedings of the 28th European Conference on Pattern Languages of Programs},
year = {2023},
isbn = {9798400700408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irsee, Germany}
}

@proceedings{10.1145/3616901,
title = {FAIML '23: Proceedings of the 2023 International Conference on Frontiers of Artificial Intelligence and Machine Learning},
year = {2023},
isbn = {9798400707544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3603555,
title = {MuC '23: Proceedings of Mensch und Computer 2023},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rapperswil, Switzerland}
}

@proceedings{10.1145/3517428,
title = {ASSETS '22: Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3604078,
title = {ICDIP '23: Proceedings of the 15th International Conference on Digital Image Processing},
year = {2023},
isbn = {9798400708237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, China}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3620678,
title = {SoCC '23: Proceedings of the 2023 ACM Symposium on Cloud Computing},
year = {2023},
isbn = {9798400703874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Santa Cruz, CA, USA}
}

@proceedings{10.1145/3595916,
title = {MMAsia '23: Proceedings of the 5th ACM International Conference on Multimedia in Asia},
year = {2023},
isbn = {9798400702051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tainan, Taiwan}
}

@proceedings{10.1145/3639474,
title = {ICSE-SEET '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3655755,
title = {IVSP '24: Proceedings of the 2024 6th International Conference on Image, Video and Signal Processing},
year = {2024},
isbn = {9798400716829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ikuta, Japan}
}

@proceedings{10.1145/3544549,
title = {CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3527188,
title = {HAI '22: Proceedings of the 10th International Conference on Human-Agent Interaction},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Christchurch, New Zealand}
}

@proceedings{10.1145/3581961,
title = {AutomotiveUI '23 Adjunct: Adjunct Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
year = {2023},
isbn = {9798400701122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ingolstadt, Germany}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@proceedings{10.1145/3629264,
title = {ICCDA '23: Proceedings of the 2023 7th International Conference on Computing and Data Analysis},
year = {2023},
isbn = {9798400700576},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guiyang, China}
}

@proceedings{10.1145/3575879,
title = {PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics},
year = {2022},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3637989,
title = {ICEEL '23: Proceedings of the 2023 7th International Conference on Education and E-Learning},
year = {2023},
isbn = {9798400708732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3623809,
title = {HAI '23: Proceedings of the 11th International Conference on Human-Agent Interaction},
year = {2023},
isbn = {9798400708244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Gothenburg, Sweden}
}

@proceedings{10.1145/3594441,
title = {ICIEI '23: Proceedings of the 2023 8th International Conference on Information and Education Innovations},
year = {2023},
isbn = {9798400700613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Manchester, United Kingdom}
}

@proceedings{10.1145/3549737,
title = {SETN '22: Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Corfu, Greece}
}

@proceedings{10.1145/3597638,
title = {ASSETS '23: Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3594738,
title = {ISWC '23: Proceedings of the 2023 ACM International Symposium on Wearable Computers},
year = {2023},
isbn = {9798400701993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cancun, Quintana Roo, Mexico}
}

@proceedings{10.1145/3582515,
title = {GoodIT '23: Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@proceedings{10.1145/3584931,
title = {CSCW '23 Companion: Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Minneapolis, MN, USA}
}

@proceedings{10.1145/3579375,
title = {ACSW '23: Proceedings of the 2023 Australasian Computer Science Week},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3543712,
title = {ICCTA '22: Proceedings of the 2022 8th International Conference on Computer Technology Applications},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@book{10.1145/3617448,
author = {Myers, Brad A.},
title = {Pick, Click, Flick! The Story of Interaction Techniques},
year = {2024},
isbn = {9798400709494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {57},
abstract = {This book provides a comprehensive study of the many ways to interact with computers and computerized devices. An “interaction technique” starts when the user performs an action that causes an electronic device to respond, and includes the direct feedback from the device to the user. Examples include physical buttons and switches, on-screen menus and scrollbars operated by a mouse, touchscreen widgets, gestures such as flick-to-scroll, text entry on computers and touchscreens, input for virtual reality systems, interactions with conversational agents such as Apple Siri, Google Assistant, Amazon Alexa, and Microsoft Cortana, and adaptations of all of these for people with disabilities. Pick, Click, Flick! is written for anyone interested in interaction techniques, including computer scientists and designers working on human-computer interaction, as well as implementers and consumers who want to understand and get the most out of their digital devices.REVIEWS“Pick, Click, Flick! is an impressive reference manual of the many years of interaction design development. It is a reference book, invaluable when questions arise, whether while you are busy designing something, or learning, or teaching, where assigning sections of the reference will be a valuable resource and learning tool for students. Brad Myers has provided a great service to the interaction community.” ‐ Don Norman, Distinguished Prof. Emeritus, Design Lab, University of California, San Diego“Every UX professional should immerse themselves in this book. Not only does it unravel the fascinating and complex history of GUI widgets that will captivate any user interface nerd, but it also stands as the definitive guide to an incredibly diverse array of interaction techniques. This is not just an engaging read; it’s an essential toolkit.” ‐ Jakob Nielsen, Principal, Nielsen Design Group}
}

@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3637907,
title = {ICETM '23: Proceedings of the 2023 6th International Conference on Educational Technology Management},
year = {2023},
isbn = {9798400716676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@proceedings{10.1145/3591196,
title = {C&amp;C '23: Proceedings of the 15th Conference on Creativity and Cognition},
year = {2023},
isbn = {9798400701801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, USA}
}

@proceedings{10.1145/3616961,
title = {Mindtrek '23: Proceedings of the 26th International Academic Mindtrek Conference},
year = {2023},
isbn = {9798400708749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3547522,
title = {NordiCHI '22: Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference},
year = {2022},
isbn = {9781450394482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@proceedings{10.1145/3630970,
title = {CLIHC '23: Proceedings of the XI Latin American Conference on Human Computer Interaction},
year = {2023},
isbn = {9798400716577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Puebla, Mexico}
}

@proceedings{10.1145/3572549,
title = {ICETC '22: Proceedings of the 14th International Conference on Education Technology and Computers},
year = {2022},
isbn = {9781450397766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@proceedings{10.1145/3616195,
title = {AM '23: Proceedings of the 18th International Audio Mostly Conference},
year = {2023},
isbn = {9798400708183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Edinburgh, United Kingdom}
}

@proceedings{10.5555/3623293,
title = {ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@proceedings{10.1145/3581641,
title = {IUI '23: Proceedings of the 28th International Conference on Intelligent User Interfaces},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3579654,
title = {ACAI '22: Proceedings of the 2022 5th International Conference on Algorithms, Computing and Artificial Intelligence},
year = {2022},
isbn = {9781450398336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@proceedings{10.5555/3635637,
title = {AAMAS '24: Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Welcome to AAMAS-2024, the 23th edition of the International Conference on Autonomous Agents and Multiagent Systems!AAMAS is the largest and most influential conference in the area of agents and multiagent systems, bringing together researchers and practitioners in all areas of agent technology and providing an internationally renowned high-profile forum for publishing and finding out about the latest developments in the field. AAMAS is the flagship conference of the non-profit International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS).After two attempts to hold AAMAS in New Zealand for the first time, which were forced online by the COVID19 pandemic, we are happy that the 2024 edition finally comes to Auckland, New Zealand. Previous editions were held in Bologna (2002), Melbourne (2003), New York (2004), Utrecht (2005), Hakodate (2006), Honolulu (2007), Estoril (2008), Budapest (2009), Toronto (2010), Taipei (2011), Valencia (2012), Saint Paul (2013), Paris (2014), Istanbul (2015), Singapore (2016), Sao Paulo (2017), Stockholm (2018), Montreal (2019), Auckland/online (2020), London/online (2021), Auckland/online (2022), and London (2023).},
location = {Auckland, New Zealand}
}

@proceedings{10.1145/3629606,
title = {CHCHI '23: Proceedings of the Eleventh International Symposium of Chinese CHI},
year = {2023},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denpasar, Bali, Indonesia}
}

@proceedings{10.1145/3638380,
title = {OzCHI '23: Proceedings of the 35th Australian Computer-Human Interaction Conference},
year = {2023},
isbn = {9798400717079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Wellington, New Zealand}
}

@proceedings{10.1145/3561613,
title = {ICCCV '22: Proceedings of the 5th International Conference on Control and Computer Vision},
year = {2022},
isbn = {9781450397315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3570361,
title = {ACM MobiCom '23: Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Madrid, Spain}
}

@proceedings{10.1145/3617184,
title = {ICCSIE '23: Proceedings of the 8th International Conference on Cyber Security and Information Engineering},
year = {2023},
isbn = {9798400708800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Putrajaya, Malaysia}
}

@proceedings{10.1145/3565698,
title = {Chinese CHI '22: Proceedings of the Tenth International Symposium of Chinese CHI},
year = {2022},
isbn = {9781450398695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China and Online, China}
}

@proceedings{10.1145/3677182,
title = {ASENS '24: Proceedings of the International Conference on Algorithms, Software Engineering, and Network Security},
year = {2024},
isbn = {9798400709784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanchang, China}
}

@proceedings{10.1145/3569219,
title = {Academic Mindtrek '22: Proceedings of the 25th International Academic Mindtrek Conference},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3607199,
title = {RAID '23: Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, China}
}

@proceedings{10.1145/3610661,
title = {ICMI '23 Companion: Companion Publication of the 25th International Conference on Multimodal Interaction},
year = {2023},
isbn = {9798400703218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

@proceedings{10.1145/3627611,
title = {MAB '23: Proceedings of the 6th Media Architecture Biennale Conference},
year = {2023},
isbn = {9798400716355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Toronto, ON, Canada}
}

@proceedings{10.1145/3556223,
title = {ICCCM '22: Proceedings of the 10th International Conference on Computer and Communications Management},
year = {2022},
isbn = {9781450396349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Okayama, Japan}
}

@proceedings{10.1145/3599609,
title = {ICEEG '23: Proceedings of the 2023 7th International Conference on E-Commerce, E-Business and E-Government},
year = {2023},
isbn = {9798400708398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Plymouth, United Kingdom}
}

@proceedings{10.1145/3544793,
title = {UbiComp/ISWC '22 Adjunct: Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
year = {2022},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cambridge, United Kingdom}
}

@proceedings{10.1145/3536220,
title = {ICMI '22 Companion: Companion Publication of the 2022 International Conference on Multimodal Interaction},
year = {2022},
isbn = {9781450393898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bengaluru, India}
}

@proceedings{10.1145/3585088,
title = {IDC '23: Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@proceedings{10.1145/3563359,
title = {UMAP '23 Adjunct: Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
year = {2023},
isbn = {9781450398916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@proceedings{10.1145/3579856,
title = {ASIA CCS '23: Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3628096,
title = {AfriCHI '23: Proceedings of the 4th African Human Computer Interaction Conference},
year = {2023},
isbn = {9798400708879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {East London, South Africa}
}

@proceedings{10.1145/3568444,
title = {MUM '22: Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia},
year = {2022},
isbn = {9781450398206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3573428,
title = {EITCE '22: Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
year = {2022},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3572921,
title = {OzCHI '22: Proceedings of the 34th Australian Conference on Human-Computer Interaction},
year = {2022},
isbn = {9798400700248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Canberra, ACT, Australia}
}

@proceedings{10.1145/3655532,
title = {ICRSA '23: Proceedings of the 2023 6th International Conference on Robot Systems and Applications},
year = {2023},
isbn = {9798400708039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Wuhan, China}
}

@proceedings{10.1145/3536221,
title = {ICMI '22: Proceedings of the 2022 International Conference on Multimodal Interaction},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bengaluru, India}
}

@proceedings{10.1145/3547578,
title = {ICCMS '22: Proceedings of the 14th International Conference on Computer Modeling and Simulation},
year = {2022},
isbn = {9781450396547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chongqing, China}
}

@proceedings{10.1145/3580252,
title = {CHASE '23: Proceedings of the 8th ACM/IEEE International Conference on Connected Health: Applications, Systems and Engineering Technologies},
year = {2023},
isbn = {9798400701023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the eighth edition of the IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE 2023). This is a leading international conference in the field of connected health, an interdisciplinary area with rich research problems and opportunities. CHASE aims to bring together researchers working in the smart and connected health area around the world to exchange ideas and foster collaborations. Its scope covers sensing, communications, and intelligent analytics in support of health-related applications, systems, and engineering technologies. The innovative works published at CHASE will revolutionize preventative health and personalized medicine, providing rich medical information never-before available to individuals while driving down healthcare costs.},
location = {Orlando, FL, USA}
}

@proceedings{10.1145/3564625,
title = {ACSAC '22: Proceedings of the 38th Annual Computer Security Applications Conference},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3546155,
title = {NordiCHI '22: Nordic Human-Computer Interaction Conference},
year = {2022},
isbn = {9781450396998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@proceedings{10.1145/3594739,
title = {UbiComp/ISWC '23 Adjunct: Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing},
year = {2023},
isbn = {9798400702006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cancun, Quintana Roo, Mexico}
}

@proceedings{10.1145/3593743,
title = {C&amp;T '23: Proceedings of the 11th International Conference on Communities and Technologies},
year = {2023},
isbn = {9798400707582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lahti, Finland}
}

@proceedings{10.1145/3613347,
title = {ICoMS '23: Proceedings of the 2023 6th International Conference on Mathematics and Statistics},
year = {2023},
isbn = {9798400700187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Leipzig, Germany}
}

@proceedings{10.1145/3638067,
title = {IHC '23: Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
year = {2023},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@proceedings{10.1145/3551708,
title = {ICEMT '22: Proceedings of the 6th International Conference on Education and Multimedia Technology},
year = {2022},
isbn = {9781450396455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@proceedings{10.1145/3577530,
title = {CSAI '22: Proceedings of the 2022 6th International Conference on Computer Science and Artificial Intelligence},
year = {2022},
isbn = {9781450397773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/3603269,
title = {ACM SIGCOMM '23: Proceedings of the ACM SIGCOMM 2023 Conference},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3592813,
title = {SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@proceedings{10.1145/3543174,
title = {AutomotiveUI '22: Proceedings of the 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
year = {2022},
isbn = {9781450394154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seoul, Republic of Korea}
}

@proceedings{10.1145/3569009,
title = {TEI '23: Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction},
year = {2023},
isbn = {9781450399777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Warsaw, Poland}
}

@proceedings{10.1145/3580585,
title = {AutomotiveUI '23: Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
year = {2023},
isbn = {9798400701054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ingolstadt, Germany}
}



@inproceedings{abdallah_potential_2020,
	address = {Cham},
	title = {The {Potential} of {Game} {Development} {Platforms} for {Digital} {Twins} and {Virtual} {Labs}},
	isbn = {978-3-030-57997-5},
	doi = {10.1007/978-3-030-57997-5_14},
	abstract = {In this paper, we present the first steps towards realizing a digital twin with integrated virtual laboratory possibilities, for a newly established Energy Analytics and Solution lab, using the Unity3D game development platform. The presented example is a case study that shows the possibilities of such development environment for creating a fully connectable digital twin of an energy analytics lab and other more complex industrial environments.},
	language = {en},
	booktitle = {Advances in {Production} {Management} {Systems}. {Towards} {Smart} and {Digital} {Manufacturing}},
	publisher = {Springer International Publishing},
	author = {Abdallah, Ali and Primas, Matthias and Turcin, Ioan and Traussnigg, Udo},
	editor = {Lalic, Bojan and Majstorovic, Vidosav and Marjanovic, Ugljesa and von Cieminski, Gregor and Romero, David},
	year = {2020},
	keywords = {Digital twin, Energy analytics lab, Unity 3D, Virtual lab, Virtual reality},
	pages = {117--121},
	file = {Full Text PDF:files/21/Abdallah et al. - 2020 - The Potential of Game Development Platforms for Digital Twins and Virtual Labs.pdf:application/pdf},
}

@inproceedings{oliveira_gvrf_2019,
	address = {Cham},
	title = {{GVRf} and {Blender}: {A} {Path} for {Android} {Apps} and {Games} {Development}},
	isbn = {978-3-030-21565-1},
	shorttitle = {{GVRf} and {Blender}},
	doi = {10.1007/978-3-030-21565-1_22},
	abstract = {Virtual reality on mobile devices has been used in many areas such as entertainment, health care, training and simulations, although it still has limitations in terms of graphics quality and application performance. The ease of portable devices that can be used for virtual experiences in other “worlds” provides its fascinations, however, this technology can not match the virtual reality for personal computers, with high levels of quality in its graphics, with multiple features and high superior processing power while offering simplicity in use and being cheaper than its competitors on other platforms, such as personal computers and video game consoles. Thus, the purpose of this essay is to present a solution that GVRf, together with Blender, the GVRf Exporter, that makes the process of developing applications for VR technologies more optimized.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Oliveira, Bruno and Azulay, Diego and Carvalho, Paulo},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {3D, Blender, Exporter, Gear VR, GVRf, Virtual reality},
	pages = {329--337},
	file = {Full Text PDF:files/22/Oliveira et al. - 2019 - GVRf and Blender A Path for Android Apps and Games Development.pdf:application/pdf},
}

@inproceedings{duran_teaching_2013,
	address = {Berlin, Heidelberg},
	title = {Teaching {3D} {Arts} {Using} {Game} {Engines} for {Engineering} and {Architecture}},
	isbn = {978-3-642-39420-1},
	doi = {10.1007/978-3-642-39420-1_13},
	abstract = {The main objective of this paper is to evaluate the application of 3D virtual worlds for teaching different subjects mainly oriented to architectural visualization and creating 3D models for multimedia. The use of 3D technologies, multi-user virtual environments and avatars are new methodologies for the student to have a much richer experience and therefore more motivating for a deeper understanding of the assessment and help understand more collaborative the projects. In this paper we work on the concept e-learning and blended learning technologies related to interactive 3D spaces such as OpenSim, Activeworlds, Secondlife, Unity and others. The students’ participation in these virtual 3D environments will help to understand the concept of an architectural project and 3D creation, improving collaboration between students and teacher, and dramatically increase in a greater understanding of the project and a high degree of their involvement with design develop. The paper describes the method of teaching 3D arts using Game Engines like Unity.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Systems} and {Applications}},
	publisher = {Springer},
	author = {Duran, Jaume and Villagrasa, Sergi},
	editor = {Shumaker, Randall},
	year = {2013},
	keywords = {Game engines, Virtual reality, Visual learning},
	pages = {113--121},
	file = {Full Text PDF:files/23/Duran e Villagrasa - 2013 - Teaching 3D Arts Using Game Engines for Engineering and Architecture.pdf:application/pdf},
}

@inproceedings{wu_application_2019,
	address = {Cham},
	title = {The {Application} of {Visual} {Image} and {Interactive} {Storytelling} to {Stage} {Performance}},
	isbn = {978-3-030-23541-3},
	doi = {10.1007/978-3-030-23541-3_36},
	abstract = {New media technologies bring to light new techniques in representation and storytelling. Particularly, in theatre performance that is delivered live to the audience, new techniques have the capacity to create more immersive experiences to audiences. In recent years, Chinese audience are witnessing an increasing number of new representation and storytelling techniques being applied on stage productions. These technological innovations on stage are rapidly developing and inherit tremendous creative potentials. This paper focuses on the application of game engine-powered technologies such as real-time rendering used in 3D mapping, interactive, Virtual Reality, Mixed Reality etc. On the frontstage that is visible to the audience, the way of storytelling and the visual narration of the creative concept appears to be the result of these technological innovations on stage. This paper focuses on the application of technologies such as real-time rendering, virtual reality, and mixed reality on stage performances powered by game engine to realize intermixing of storytelling. Throughout the paper, we provide examples of various stage productions, such as theatre, music performances, and dance productions. These examples demonstrate the wide range of applications that are possible in enriching ways of storytelling and engaging audience. Yet, despite all of the above benefits, we also call for is a reflection upon the relationship between technology and arts. We argue that in spite of the availabilities of these technologies, it is imperative to also slow down and reassess the content we are going to create and begin a series of creative conversations between artistic expressions and technological excursions.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {User} {Experience} in {Advanced} {Technological} {Environments}},
	publisher = {Springer International Publishing},
	author = {Wu, Zhen and Zhou, Ximin},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2019},
	keywords = {Montage, Stage performance, Visual image},
	pages = {502--516},
	file = {Full Text PDF:files/24/Wu e Zhou - 2019 - The Application of Visual Image and Interactive Storytelling to Stage Performance.pdf:application/pdf},
}

@inproceedings{mueller_benchmark_2016,
	address = {Cham},
	title = {A {Benchmark} and {Simulator} for {UAV} {Tracking}},
	isbn = {978-3-319-46448-0},
	doi = {10.1007/978-3-319-46448-0_27},
	abstract = {In this paper, we propose a new aerial video dataset and benchmark for low altitude UAV target tracking, as well as, a photo-realistic UAV simulator that can be coupled with tracking methods. Our benchmark provides the first evaluation of many state-of-the-art and popular trackers on 123 new and fully annotated HD video sequences captured from a low-altitude aerial perspective. Among the compared trackers, we determine which ones are the most suitable for UAV tracking both in terms of tracking accuracy and run-time. The simulator can be used to evaluate tracking algorithms in real-time scenarios before they are deployed on a UAV “in the field”, as well as, generate synthetic but photo-realistic tracking datasets with automatic ground truth annotations to easily extend existing real-world datasets. Both the benchmark and simulator are made publicly available to the vision community on our websiteto further research in the area of object tracking from UAVs. (https://ivul.kaust.edu.sa/Pages/pub-benchmark-simulator-uav.aspx.).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Mueller, Matthias and Smith, Neil and Ghanem, Bernard},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Aerial object tracking, UAV simulator, UAV tracking},
	pages = {445--461},
	file = {Full Text PDF:files/25/Mueller et al. - 2016 - A Benchmark and Simulator for UAV Tracking.pdf:application/pdf},
}

@inproceedings{wang_software_2015,
	address = {Cham},
	title = {Software {Architectures} and the {Creative} {Processes} in {Game} {Development}},
	isbn = {978-3-319-24589-8},
	doi = {10.1007/978-3-319-24589-8_21},
	abstract = {Game development is different from traditional software engineering in that there are no real functional requirements and the customers buy and use the software only because it is engaging and fun. This article investigates how game developers think about and use software architecture in the development of games. Further, it looks at how creative development processes are managed and supported. The results presented in this article come from responses to a questionnaire and a survey among thirteen game developers. The research questions answered in this study are: what role does the software architecture play in game development, how do game developers manage changes to the software architecture, how are creative development processes managed and supported, and how has game development evolved the last couple of years. Some of our findings are that software architectures play a central role in game development where the focus is mainly on achieving software with good performance and high modifiability, creative processes are supported through flexible game engines and tools, use of scripting and dynamic loading of assets, and feature-based teams with both creative and technical professions represented, and game developers are incrementally using more game-specific engines, tools and middleware in their development now compared to earlier.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Wang, Alf Inge and Nordmark, Njål},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {Creative software development, Game development, Software architecture},
	pages = {272--285},
	file = {Full Text PDF:files/26/Wang e Nordmark - 2015 - Software Architectures and the Creative Processes in Game Development.pdf:application/pdf},
}

@inproceedings{westhoven_towards_2015,
	address = {Cham},
	title = {Towards a {Structured} {Selection} of {Game} {Engines} for {Virtual} {Environments}},
	isbn = {978-3-319-21067-4},
	doi = {10.1007/978-3-319-21067-4_16},
	abstract = {Development and maintenance of virtual reality engines are coupled with large effort. It is therefore common today, to use existing solutions originating from the entertainment sector. This is often a compromise, since they fulfill individual requirements only in parts, due to their different background. The decision for a specific engine can have a large effect on the effort required to implement own functionality. The number of existing engines further complicates decision making. To enable a comprehensible and replicable decision making, we propose a structured selection process. In a multi-step approach, first the requirements and criteria for comparison are identified and analyzed. A pre-filtering is then used to select a feasible number of engines which are then compared in detail.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Westhoven, Martin and Alexander, Thomas},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2015},
	keywords = {Game engines, Serious gaming, Virtual environments},
	pages = {142--152},
	file = {Full Text PDF:files/27/Westhoven e Alexander - 2015 - Towards a Structured Selection of Game Engines for Virtual Environments.pdf:application/pdf},
}

@inproceedings{calvo_programming_2018,
	address = {Cham},
	title = {Programming {Virtual} {Interactions} for {Gamified} {Educational} {Proposes} of {Urban} {Spaces}},
	isbn = {978-3-319-91152-6},
	doi = {10.1007/978-3-319-91152-6_10},
	abstract = {The architectural students must acquire for their professional future the comprehension of the space and the usage of the urban projects. In this sense, the possibilities offered by interactive visualization platforms, typically used for the creation of computerized games, can be especially useful given their power and versatility (for example through the process of rendering a scene with lights, textures and materials in real time). The present article contextualizes three-dimensional interactive visualization systems in architectural education, and explains the development of the main interactions for students, with elements of urban furniture, under the programming in Unreal.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Learning} and {Teaching}},
	publisher = {Springer International Publishing},
	author = {Calvo, Xavier and Fonseca, David and Sánchez-Sepúlveda, Mónica and Amo, Daniel and Llorca, Josep and Redondo, Ernesto},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2018},
	keywords = {Architecture education, Render, Unreal, Urban design, Virtual interactions, Virtual reality},
	pages = {128--140},
	file = {Full Text PDF:files/28/Calvo et al. - 2018 - Programming Virtual Interactions for Gamified Educational Proposes of Urban Spaces.pdf:application/pdf},
}

@inproceedings{sengul_haptic_2014,
	address = {Cham},
	title = {Haptic {User} {Interface} {Integration} for {3D} {Game} {Engines}},
	isbn = {978-3-319-07227-2},
	doi = {10.1007/978-3-319-07227-2_62},
	abstract = {Touch and feel senses of human beings provide important information about the environment. When those senses are integrated with the eyesight, we may get all the necessary information about the environment. In terms of human-computer-interaction, the eyesight information is provided by visual displays. On the other hand, touch and feel senses are provided by means of special devices called “haptic” devices. Haptic devices are used in many fields such as computer-aided design, distance-surgery operations, medical simulation environments, training simulators for both military and medical applications, etc. Besides the touch and sense feelings haptic devices also provide force-feedbacks, which allows designing a realistic environment in virtual reality applications.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Applications} and {Services}},
	publisher = {Springer International Publishing},
	author = {Sengul, Gokhan and Çaǧıltay, Nergiz Ercil and Özçelik, Erol and Tuner, Emre and Erol, Batuhan},
	editor = {Kurosu, Masaaki},
	year = {2014},
	keywords = {game engines, haptic devices, interaction, Surgical simulation},
	pages = {654--662},
	file = {Full Text PDF:files/29/Sengul et al. - 2014 - Haptic User Interface Integration for 3D Game Engines.pdf:application/pdf},
}

@inproceedings{schild_integrating_2011,
	address = {Berlin, Heidelberg},
	title = {Integrating {Stereoscopic} {Video} in {3D} {Games}},
	isbn = {978-3-642-24500-8},
	doi = {10.1007/978-3-642-24500-8_13},
	abstract = {Recent advances in commercial technology increase the use of stereoscopy in games. While current applications display existing games in real-time rendered stereoscopic 3D, future games will also feature S3D video as part of the virtual game world, in interactive S3D movies, or for new interaction methods. Compared to the rendering of 2D video within a 3D game scene, displaying S3D video includes some technical challenges related to rendering and adaption of the depth range. Rendering is exclusively possible on professional hardware not appropriate for gaming. Our approach, Multi-pass Stereoscopic Video Rendering (MSVR), allows to present stereoscopic video streams within game engines on consumer graphics boards. We further discuss aspects of performance and occlusion of virtual objects. This allows developers and other researchers to easily apply S3D video with current game engines to explore new innovations in S3D gaming.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2011},
	publisher = {Springer},
	author = {Schild, Jonas and Seele, Sven and Masuch, Maic},
	editor = {Anacleto, Junia Coutinho and Fels, Sidney and Graham, Nicholas and Kapralos, Bill and Saif El-Nasr, Magy and Stanley, Kevin},
	year = {2011},
	keywords = {3D gaming, game engine, S3D video, Stereoscopic rendering},
	pages = {124--135},
	file = {Full Text PDF:files/30/Schild et al. - 2011 - Integrating Stereoscopic Video in 3D Games.pdf:application/pdf},
}

@inproceedings{zaratti_3d_2007,
	address = {Berlin, Heidelberg},
	title = {A {3D} {Simulator} of {Multiple} {Legged} {Robots} {Based} on {USARSim}},
	isbn = {978-3-540-74024-7},
	doi = {10.1007/978-3-540-74024-7_2},
	abstract = {This paper presents a flexible 3D simulator able to reproduce the appearance and the dynamics of generic legged robots and objects in the environment at full frame rate (30 frames per second). Such a simulator extends and improves USARSim (Urban Search and Rescue Simulator), a robot simulator in turn based on the game platform Unreal Engine. This latter provides facilities for good quality rendering, physics simulation, networking, highly versatile scripting language and a powerful visual editor. Our simulator extends USARSim features by allowing for the simulation and control of legged robots and it introduces a multi-view functionality for multi-robot support. We successfully tested the simulator capabilities by mimicking a virtual environment with up to five network-controlled legged robots, like AIBO ERS-7 and QRIO.},
	language = {en},
	booktitle = {{RoboCup} 2006: {Robot} {Soccer} {World} {Cup} {X}},
	publisher = {Springer},
	author = {Zaratti, Marco and Fratarcangeli, Marco and Iocchi, Luca},
	editor = {Lakemeyer, Gerhard and Sklar, Elizabeth and Sorrenti, Domenico G. and Takahashi, Tomoichi},
	year = {2007},
	keywords = {Controller Robot, Legged Robot, Multiple Robot, Real Robot, Virtual Environment},
	pages = {13--24},
	file = {Full Text PDF:files/51/Zaratti et al. - 2007 - A 3D Simulator of Multiple Legged Robots Based on USARSim.pdf:application/pdf},
}

@inproceedings{gardo_vclav_2011,
	address = {Berlin, Heidelberg},
	title = {{VClav} 2.0 – {System} for {Playing} {3D} {Virtual} {Copy} of a {Historical} {Clavichord}},
	isbn = {978-3-642-22024-1},
	doi = {10.1007/978-3-642-22024-1_16},
	abstract = {VClav 2.0 system presented in the paper enables user to interact with a digital 3D reconstruction of the historical clavichord in a manner similar to lifelike using Virtual Reality gloves to “play” music. The real clavichord was constructed by the famous maker in 18th century, Johann Adolph Hass from Hamburg, and is at the exposition in the Museum of Musical Instruments in Poznan, Poland (department of the National Museum). This is a system powered by the NeoAxis game engine and equipped with 5DT Data Glove 14 and Polhemus Patriot tracker. It is an exemplary solution for museums to actively present musical instruments.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality} - {Systems} and {Applications}},
	publisher = {Springer},
	author = {Gardo, Krzysztof and Lukasik, Ewa},
	editor = {Shumaker, Randall},
	year = {2011},
	keywords = {3D modeling, clavichord, cultural heritage, gesture driven HCI, Virtual Reality},
	pages = {141--150},
	file = {Full Text PDF:files/52/Gardo e Lukasik - 2011 - VClav 2.0 – System for Playing 3D Virtual Copy of a Historical Clavichord.pdf:application/pdf},
}

@inproceedings{majumdar_paracosm_2021,
	address = {Cham},
	title = {Paracosm: {A} {Test} {Framework} for {Autonomous} {Driving} {Simulations}},
	isbn = {978-3-030-71500-7},
	shorttitle = {Paracosm},
	doi = {10.1007/978-3-030-71500-7_9},
	abstract = {Systematic testing of autonomous vehicles operating in complex real-world scenarios is a difficult and expensive problem. We present Paracosm, a framework for writing systematic test scenarios for autonomous driving simulations. Paracosm allows users to programmatically describe complex driving situations with specific features, e.g., road layouts and environmental conditions, as well as reactive temporal behaviors of other cars and pedestrians. A systematic exploration of the state space, both for visual features and for reactive interactions with the environment is made possible. We define a notion of test coverage for parameter configurations based on combinatorial testing and low dispersion sequences. Using fuzzing on parameter configurations, our automatic test generator can maximize coverage of various behaviors and find problematic cases. Through empirical evaluations, we demonstrate the capabilities of Paracosm in programmatically modeling parameterized test environments, and in finding problematic scenarios.},
	language = {en},
	booktitle = {Fundamental {Approaches} to {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Majumdar, Rupak and Mathur, Aman and Pirron, Marcus and Stegner, Laura and Zufferey, Damien},
	editor = {Guerra, Esther and Stoelinga, Mariëlle},
	year = {2021},
	keywords = {Autonomous driving, Reactive programming, Testing},
	pages = {172--195},
	file = {Full Text PDF:files/54/Majumdar et al. - 2021 - Paracosm A Test Framework for Autonomous Driving Simulations.pdf:application/pdf},
}

@inproceedings{saari_emotionally_2009,
	address = {Berlin, Heidelberg},
	title = {Emotionally {Adapted} {Games} – {An} {Example} of a {First} {Person} {Shooter}},
	isbn = {978-3-642-02583-9},
	doi = {10.1007/978-3-642-02583-9_45},
	abstract = {This paper discusses a specific customization technology – Psychological Customization - which enables the customization of information presented on a computer-based system in real-time and its application to manipulating emotions when playing computer games. The possibilities of customizing different elements of games to manipulate emotions are presented and a definition of emotionally adaptive games is given. A psychophysiologically adaptive game is discussed as an example of emotionally adapted games.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Interacting} in {Various} {Application} {Domains}},
	publisher = {Springer},
	author = {Saari, Timo and Turpeinen, Marko and Kuikkaniemi, Kai and Kosunen, Ilkka and Ravaja, Niklas},
	editor = {Jacko, Julie A.},
	year = {2009},
	keywords = {adaptive systems, Customization, emotion, emotionally adapted games, games, Psychological Customization, psychological effects, psychophysiological measurement},
	pages = {406--415},
	file = {Full Text PDF:files/56/Saari et al. - 2009 - Emotionally Adapted Games – An Example of a First Person Shooter.pdf:application/pdf},
}

@inproceedings{stiegler_gamification_2014,
	address = {Cham},
	title = {Gamification in the {Development} of {Accessible} {Software}},
	isbn = {978-3-319-07437-5},
	doi = {10.1007/978-3-319-07437-5_17},
	abstract = {This paper describes a theoretical framework covering game design, game mechanics and game engines, linking examples from actual commercial games with a gamification application. The goal of the framework is to develop an online platform for software developers to aid them in designing accessible applications, finding help on the topic etc. A software stack will be derived taking a typical game development project as an example. We will further identify process requirements for implementing crucial game design rules, like immediate feedback. Finally, an outlook on the final project will be given and possible evaluation metrics will be described.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Design} and {Development} {Methods} for {Universal} {Access}},
	publisher = {Springer International Publishing},
	author = {Stiegler, Andreas and Zimmermann, Gottfried},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {Game Development, Gamification, Serious Games},
	pages = {171--180},
	file = {Full Text PDF:files/57/Stiegler e Zimmermann - 2014 - Gamification in the Development of Accessible Software.pdf:application/pdf},
}

@inproceedings{kim_collaborative_2014,
	address = {Cham},
	title = {Collaborative {Visualization} of a {Warfare} {Simulation} {Using} a {Commercial} {Game} {Engine}},
	isbn = {978-3-319-07464-1},
	doi = {10.1007/978-3-319-07464-1_36},
	abstract = {The requirement about reusable 3D visualization tool was continuously raised in various industries. Especially in the defense modeling and simulation field, there are abundant researches about reusable and interoperable visualization system, since it has a critical role to the efficient decision making by offering diverse validation and analyzing process. Also to facilitate the effectiveness, many current operating systems are applying VR(Virtual Reality) and AR(Augmented Reality) technologies aggressively. In this background, we conducted the research about the design for the collaborative visualization environment for the warfare simulation through commercial game engine. We define the requirements by analyzing advantages and disadvantages of existing tools or engines like SIMDIS or Vega, and propose the methods how to utilize the functionalities of commercial game engine to satisfy the requirements. The implemented prototype offers collaborative visualization environment inside the CAVE environment, which is the facility for immersive virtual environment, by cooperating with handheld devices.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} of {Virtual} and {Augmented} {Reality}},
	publisher = {Springer International Publishing},
	author = {Kim, Hyungki and Kang, Yuna and Shin, Suchul and Kim, Imkyu and Han, Soonhung},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2014},
	keywords = {3D Visualization, Collaborative Visualization Environment, Game Engine, Warfare Simulation},
	pages = {390--401},
	file = {Full Text PDF:files/59/Kim et al. - 2014 - Collaborative Visualization of a Warfare Simulation Using a Commercial Game Engine.pdf:application/pdf},
}

@inproceedings{vilhjalmsson_social_2007,
	address = {Berlin, Heidelberg},
	title = {Social {Puppets}: {Towards} {Modular} {Social} {Animation} for {Agents} and {Avatars}},
	isbn = {978-3-540-73257-0},
	shorttitle = {Social {Puppets}},
	doi = {10.1007/978-3-540-73257-0_22},
	abstract = {State-of-the-art computer graphics can give autonomous agents a compelling appearance as animated virtual characters. Typically the agents are directly responsible for controlling their graphical representation, but this places too much burden on the agents that already deal with difficult high-level tasks such as dialog planning. This paper presents work, done in the context of an interactive language and culture training system, on a new kind of engine that fits between the high level cognitive agent models and the animated graphics that represent them. This is a social engine that generates socially appropriate nonverbal behavior based on rules reflecting social norms. Similar to modular physics engines, the social engine introduces a re-usable component that can heighten believability of animated agents in games and simulations with relatively little effort.},
	language = {en},
	booktitle = {Online {Communities} and {Social} {Computing}},
	publisher = {Springer},
	author = {Vilhjalmsson, Hannes and Merchant, Chirag and Samtani, Prasan},
	editor = {Schuler, Douglas},
	year = {2007},
	pages = {192--201},
	file = {Full Text PDF:files/62/Vilhjalmsson et al. - 2007 - Social Puppets Towards Modular Social Animation for Agents and Avatars.pdf:application/pdf},
}

@inproceedings{best_searching_2009,
	address = {Berlin, Heidelberg},
	title = {Searching for {Concurrent} {Design} {Patterns} in {Video} {Games}},
	isbn = {978-3-642-03869-3},
	doi = {10.1007/978-3-642-03869-3_84},
	abstract = {The transition to multicore architectures has dramatically underscored the necessity for parallelism in software. In particular, while new gaming consoles are by and large multicore, most existing video game engines are essentially sequential and thus cannot easily take advantage of this hardware. In this paper we describe techniques derived from our experience parallelizing an open-source video game Cube 2. We analyze the structure and unique requirements of this complex application domain, drawing conclusions about parallelization tools and techniques applicable therein. Our experience and analysis convinced us that while existing tools and techniques can be used to solve parts of this problem, none of them constitutes a comprehensive solution. As a result we were inspired to design a new parallel programming environment (PPE) targeted specifically at video game engines and other complex soft real-time systems. The initial implementation of this PPE, Cascade, and its performance analysis are also presented.},
	language = {en},
	booktitle = {Euro-{Par} 2009 {Parallel} {Processing}},
	publisher = {Springer},
	author = {Best, Micah J. and Fedorova, Alexandra and Dickie, Ryan and Tagliasacchi, Andrea and Couture-Beil, Alex and Mustard, Craig and Mottishaw, Shane and Brown, Aron and Huang, Zhi Feng and Xu, Xiaoyuan and Ghazali, Nasser and Brownsword, Andrew},
	editor = {Sips, Henk and Epema, Dick and Lin, Hai-Xiang},
	year = {2009},
	keywords = {Batch Size, Dependency Graph, Logical Dependency, Task Graph, Video Game},
	pages = {912--923},
	file = {Full Text PDF:files/64/Best et al. - 2009 - Searching for Concurrent Design Patterns in Video Games.pdf:application/pdf},
}

@inproceedings{friese_using_2008,
	address = {Boston, MA},
	title = {Using {Game} {Engines} for {Visualization} in {Scientific} {Applications}},
	isbn = {978-0-387-09701-5},
	doi = {10.1007/978-0-387-09701-5_2},
	abstract = {In recent years, the computer gaming industry has become a large and important market and impressive amounts of money are spent on the development of new game engines. In contrast to their development costs, the price for the final product is very low compared to a professional 3D visualization/animation program. The idea to use this potential for other purposes than gaming seems obvious. This work gives a review on three Serious Gamingprojects, analyzes the encountered problems in a greater context and reflects the pros and cons of using game engines for scientific applications in general.},
	language = {en},
	booktitle = {New {Frontiers} for {Entertainment} {Computing}},
	publisher = {Springer US},
	author = {Friese, Karl-Ingo and Herrlich, Marc and Wolter, Franz-Erich},
	editor = {Ciancarini, Paolo and Nakatsu, Ryohei and Rauterberg, Matthias and Roccetti, Marco},
	year = {2008},
	keywords = {Commodity Hardware, Computer Game, Game Engine, Landscape Architect, Virtual Reality},
	pages = {11--22},
	file = {Full Text PDF:files/66/Friese et al. - 2008 - Using Game Engines for Visualization in Scientific Applications.pdf:application/pdf},
}

@inproceedings{gutbell_web-based_2018,
	address = {Cham},
	title = {Web-{Based} {Visualization} {Component} for {Geo}-{Information}},
	isbn = {978-3-319-92043-6},
	doi = {10.1007/978-3-319-92043-6_2},
	abstract = {Three-dimensional visualization of maps is becoming an increasingly important issue on the Internet. The growing computing power of consumer devices and the establishment of new technologies like HTML5 and WebGL allow a plug-in free display of 3D geo applications directly in the browser. Existing software solutions like Google Earth or Cesium either lack the necessary customizability or fail to deliver a realistic representation of the world. In this work a browser-based visualization component for geo-information is designed and a prototype is implemented in the gaming engine Unity3D. Unity3D allows translating the implementation to JavaScript and to embed it in the browser with WebGL. A comparison of the prototype with the open-source geo-visualization framework Cesium shows, that while maintaining an acceptable performance an improvement of the visual quality is achieved. Another reason to use a gaming engine as platform for our streaming algorithm is that they usually feature engines for physics, audio, traffic simulations and more, which we want to use in our future work.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Interaction}, {Visualization}, and {Analytics}},
	publisher = {Springer International Publishing},
	author = {Gutbell, Ralf and Pandikow, Lars and Kuijper, Arjan},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2018},
	keywords = {3D, Cesium, Geo-information, Unity3D, WebGL},
	pages = {23--35},
	file = {Full Text PDF:files/68/Gutbell et al. - 2018 - Web-Based Visualization Component for Geo-Information.pdf:application/pdf},
}

@inproceedings{qiu_unrealcv_2016,
	address = {Cham},
	title = {{UnrealCV}: {Connecting} {Computer} {Vision} to {Unreal} {Engine}},
	isbn = {978-3-319-49409-8},
	shorttitle = {{UnrealCV}},
	doi = {10.1007/978-3-319-49409-8_75},
	abstract = {Computer graphics can not only generate synthetic images and ground truth but it also offers the possibility of constructing virtual worlds in which: (i) an agent can perceive, navigate, and take actions guided by AI algorithms, (ii) properties of the worlds can be modified (e.g., material and reflectance), (iii) physical simulations can be performed, and (iv) algorithms can be learnt and evaluated. But creating realistic virtual worlds is not easy. The game industry, however, has spent a lot of effort creating 3D worlds, which a player can interact with. So researchers can build on these resources to create virtual worlds, provided we can access and modify the internal data structures of the games. To enable this we created an open-source plugin UnrealCV (Project website: http://unrealcv.github.io) for a popular game engine Unreal Engine 4 (UE4). We show two applications: (i) a proof of concept image dataset, and (ii) linking Caffe with the virtual world to test deep network algorithms.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Qiu, Weichao and Yuille, Alan},
	editor = {Hua, Gang and Jégou, Hervé},
	year = {2016},
	keywords = {Game Engine, Internal Data Structures, Single Virtual World, Synthetic Image Datasets, Unreal Engine (UE4)},
	pages = {909--916},
	file = {Full Text PDF:files/53/Qiu e Yuille - 2016 - UnrealCV Connecting Computer Vision to Unreal Engine.pdf:application/pdf},
}

@inproceedings{mora_anyboard_2016,
	address = {Cham},
	title = {Anyboard: {A} {Platform} for {Hybrid} {Board} {Games}},
	isbn = {978-3-319-46100-7},
	shorttitle = {Anyboard},
	doi = {10.1007/978-3-319-46100-7_14},
	abstract = {Making hybrid board games that mix the interactivity of video games with the social impact of board games is challenging. While the design process needs to take into account elements from the digital and analog domains, building prototypes requires dealing with diverse technologies in the field of Tangible Interfaces and Interactive Tabletop and Surfaces. Anyboard provides theoretical tools to map traditional board game interaction to the hybrid medium and lightweight technology tools to facilitate game prototyping. Our platform provides augmented game pieces that work with traditional cardboards, allowing designers to easily build collaborative interactive games without requiring engineering skills.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2016},
	publisher = {Springer International Publishing},
	author = {Mora, Simone and Fagerbekk, Tomas and Monnier, Matthias and Schroeder, Emil and Divitini, Monica},
	editor = {Wallner, Günter and Kriglstein, Simone and Hlavacs, Helmut and Malaka, Rainer and Lugmayr, Artur and Yang, Hyun-Seung},
	year = {2016},
	keywords = {Design, Hybrid game, Pervasive game, Platform, Prototyping, Tangible interface},
	pages = {161--172},
	file = {Full Text PDF:files/55/Mora et al. - 2016 - Anyboard A Platform for Hybrid Board Games.pdf:application/pdf},
}

@inproceedings{richter_playing_2016,
	address = {Cham},
	title = {Playing for {Data}: {Ground} {Truth} from {Computer} {Games}},
	isbn = {978-3-319-46475-6},
	shorttitle = {Playing for {Data}},
	doi = {10.1007/978-3-319-46475-6_7},
	abstract = {Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just \$\${\textbackslash}tfrac\{1\}\{3\}\$\$13of the CamVid training set outperform models trained on the complete CamVid training set.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Richter, Stephan R. and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Association Rule, Association Rule Mining, Graphic Hardware, Semantic Label, Visual Odometry},
	pages = {102--118},
	file = {Full Text PDF:files/58/Richter et al. - 2016 - Playing for Data Ground Truth from Computer Games.pdf:application/pdf},
}

@inproceedings{tree_author-driven_2019,
	address = {Cham},
	title = {Author-{Driven} {Approaches} to {Computational} {Narrative} {Design} for {Games}},
	isbn = {978-3-030-22646-6},
	doi = {10.1007/978-3-030-22646-6_43},
	abstract = {Accessible Head Mounted Displays (HMD) have provided mass access to Extended Reality (XR) content as never before. One of the key complaints from HMD owners, however, is the lack of substantial high-quality content (Moore 2017). Coupled with the domain-specific topic of presence, which describes a state beyond the concept of immersion instead with the user feeling part of the virtual world.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Perspectives} on {Design}},
	publisher = {Springer International Publishing},
	author = {Tree, David John and Malizia, Alessio},
	editor = {Kurosu, Masaaki},
	year = {2019},
	keywords = {Agency, Computational narrative, Interactive narrative},
	pages = {571--584},
	file = {Full Text PDF:files/60/Tree e Malizia - 2019 - Author-Driven Approaches to Computational Narrative Design for Games.pdf:application/pdf},
}

@inproceedings{kjellmo_3d_2014,
	address = {Cham},
	title = {{3D} {Design} for {Augmented} {Reality}},
	isbn = {978-3-319-07458-0},
	doi = {10.1007/978-3-319-07458-0_16},
	abstract = {How do you define a good concept when designing augmented reality apps for mobiles? This paper focuses on design processes technically, graphically and conceptually in the development of 3D content for Augmented Reality on mobile devices. Based on experiences in the development and implementation of a course in 3D design for Augmented Reality at NITH (The Norwegian School of IT), challenges and methods in creating concepts, optimized graphics and visually coherent content for AR will be discussed.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Designing} and {Developing} {Virtual} and {Augmented} {Environments}},
	publisher = {Springer International Publishing},
	author = {Kjellmo, Ivar},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2014},
	keywords = {3D design, Augmented Reality, Concepts, Education, Mixed reality, Presence in augmented and virtual reality, Virtual Reality},
	pages = {159--169},
	file = {Full Text PDF:files/61/Kjellmo - 2014 - 3D Design for Augmented Reality.pdf:application/pdf},
}

@inproceedings{jones_advanced_2006,
	address = {Berlin, Heidelberg},
	title = {Advanced {Data} {Driven} {Visualisation} for {Geo}-spatial {Data}},
	isbn = {978-3-540-34384-4},
	doi = {10.1007/11758532_77},
	abstract = {Most current 3D landscape visualisation systems either use bespoke hardware solutions, or offer a limited amount of interaction and detail when used in realtime mode. We are developing a modular, data driven 3D visualisation system that can be readily customised to specific requirements. By utilising the latest software engineering methods and bringing a dynamic data driven approach to geo-spatial data visualisation we will deliver an unparalleled level of customisation in near-photo realistic, realtime 3D landscape visualisation. In this paper we show the system framework and describe how this employs data driven techniques. In particular we discuss how data driven approaches are applied to the spatiotemporal management aspect of the application framework, and describe the advantages these convey.},
	language = {en},
	booktitle = {Computational {Science} – {ICCS} 2006},
	publisher = {Springer},
	author = {Jones, Anthony and Cornford, Dan},
	editor = {Alexandrov, Vassil N. and van Albada, Geert Dick and Sloot, Peter M. A. and Dongarra, Jack},
	year = {2006},
	keywords = {Application Framework, Binding System, Resource Description Framework, Scene Graph, Task System},
	pages = {586--592},
	file = {Full Text PDF:files/63/Jones e Cornford - 2006 - Advanced Data Driven Visualisation for Geo-spatial Data.pdf:application/pdf},
}

@inproceedings{bulk_testbed_2020,
	address = {Cham},
	title = {A {Testbed} for {Rapid} {Design} and {Evaluation} of {VR} {Navigation} {Techniques} for {Industrial} {Applications}},
	isbn = {978-3-030-50344-4},
	doi = {10.1007/978-3-030-50344-4_2},
	abstract = {VR is making inroads into industrial applications, especially in product design and presentation, training and simulation use cases. A focus of these applications is on the visualization, exploiting 3D immersive graphics as a central feature of VR. While navigation is often required in these applications to select a suitable location for observing the visualization or interacting with the simulation content it is not in the focus of the designers, often leading to the selection of techniques that are convenient during development because they are available in the implementation toolkit. However, these may be less than ideal for the industrial application context. A large variety of VR techniques for navigation have been proposed in the literature and implemented, both for motion control and guidance, but there is currently little established knowledge on their respective benefits and shortcoming, especially in non-gaming applications. To enable designers of industrial VR environments to make informed choices of navigation techniques we present a testbed that enables quick prototyping and comparative evaluation in the specific application context and with the intended target audience, making a user centered selection of navigation techniques viable.},
	language = {en},
	booktitle = {Distributed, {Ambient} and {Pervasive} {Interactions}},
	publisher = {Springer International Publishing},
	author = {Bulk, Jendrik and Paelke, Volker},
	editor = {Streitz, Norbert and Konomi, Shin'ichi},
	year = {2020},
	keywords = {Evaluation, Navigation, Testing, Virtual reality, VR},
	pages = {13--24},
	file = {Full Text PDF:files/65/Bulk e Paelke - 2020 - A Testbed for Rapid Design and Evaluation of VR Navigation Techniques for Industrial Applications.pdf:application/pdf},
}

@inproceedings{nakevska_using_2011,
	address = {Berlin, Heidelberg},
	title = {Using {Game} {Engines} in {Mixed} {Reality} {Installations}},
	isbn = {978-3-642-24500-8},
	doi = {10.1007/978-3-642-24500-8_65},
	abstract = {In mixed reality installations we have to integrate a variety of technologies such as virtual reality, augmented reality, animated virtual agents and robotic agents. In this paper we describe some of our explorations with a game engine as the driving software for mixed reality installations.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2011},
	publisher = {Springer},
	author = {Nakevska, Marija and Vos, Casper and Juarez, Alex and Hu, Jun and Langereis, Geert and Rauterberg, Matthias},
	editor = {Anacleto, Junia Coutinho and Fels, Sidney and Graham, Nicholas and Kapralos, Bill and Saif El-Nasr, Magy and Stanley, Kevin},
	year = {2011},
	keywords = {CAVE, game engine, Mixed reality, virtual reality},
	pages = {456--459},
	file = {Full Text PDF:files/67/Nakevska et al. - 2011 - Using Game Engines in Mixed Reality Installations.pdf:application/pdf},
}

@inproceedings{wei_real-time_2018,
	address = {Cham},
	title = {Real-{Time} {Hair} {Rendering} {Using} {Sequential} {Adversarial} {Networks}},
	isbn = {978-3-030-01225-0},
	doi = {10.1007/978-3-030-01225-0_7},
	abstract = {We present an adversarial network for rendering photorealistic hair as an alternative to conventional computer graphics pipelines. Our deep learning approach does not require low-level parameter tuning nor ad-hoc asset design. Our method simply takes a strand-based 3D hair model as input and provides intuitive user-control for color and lighting through reference images. To handle the diversity of hairstyles and its appearance complexity, we disentangle hair structure, color, and illumination properties using a sequential GAN architecture and a semi-supervised training approach. We also introduce an intermediate edge activation map to orientation field conversion step to ensure a successful CG-to-photoreal transition, while preserving the hair structures of the original input data. As we only require a feed-forward pass through the network, our rendering performs in real-time. We demonstrate the synthesis of photorealistic hair images on a wide range of intricate hairstyles and compare our technique with state-of-the-art hair rendering methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Wei, Lingyu and Hu, Liwen and Kim, Vladimir and Yumer, Ersin and Li, Hao},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {GAN, Hair rendering},
	pages = {105--122},
	file = {Full Text PDF:files/69/Wei et al. - 2018 - Real-Time Hair Rendering Using Sequential Adversarial Networks.pdf:application/pdf},
}

@inproceedings{griffith_leveraging_2017,
	address = {Cham},
	title = {Leveraging a {Virtual} {Environment} to {Prepare} for {School} {Shootings}},
	isbn = {978-3-319-57987-0},
	doi = {10.1007/978-3-319-57987-0_26},
	abstract = {Active-shooter incidents within a school setting involve a unique subset of active-shooter events. These events tend to have significant differences in duration and outcome to events that occur in other locations, often being resolved before, or when, first-responders arrive on the scene. The frequency and seriousness of these events inspired the US Department of Homeland Security, Science and Technology Directorate’s First Responder’s Group (DHS S\&T FRG) to leverage ongoing work with the US Army Research Laboratory, Human Research and Engineering Directorate, Advanced Training \& Simulation Division (ARL HRED ATSD) to establish a prototype virtual school environment to prepare teachers, administrators and staff on how to respond and work with Law Enforcement (LE) in the event of a school shooting. This virtual platform allows school staff and LE to practice various strategies and even supports analysis into how different security measures within the school environment might change the dynamic of an attack and response. The goal is to train affected groups together in advance of an attack to improve coordination and reduce response time and casualties. This paper illustrates design choices for training school teachers, administrators and other staff in a virtual environment in the event of a school shooting. These choices demonstrate unique development strategies related to controlling Artificial Intelligence (AI) through simple user interfaces, managing crowd behaviors and ultimately will include the ability to apply game engine level rules to different buildings or maps.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Griffith, Tami and Ablanedo, Jennie and Dwyer, Tabitha},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Crowd control, First responder, Game engine, School shooting, Virtual training},
	pages = {325--338},
	file = {Full Text PDF:files/70/Griffith et al. - 2017 - Leveraging a Virtual Environment to Prepare for School Shootings.pdf:application/pdf},
}

@inproceedings{lunden_interactive_2010,
	address = {Berlin, Heidelberg},
	title = {Interactive {Visualization} of {Interdependencies} and {Vulnerabilities} in {Constrained} {Environments}},
	isbn = {978-3-642-16806-2},
	doi = {10.1007/978-3-642-16806-2_12},
	abstract = {Many critical infrastructure assets from hospitals to industrial facilities rely on multiple infrastructure services whose close proximity can result in the failure of one component causing cascading failures in other assets. Ths effective analysis and mitigation of risks requires the consideration of numerous scenarios and input from domain experts.},
	language = {en},
	booktitle = {Critical {Infrastructure} {Protection} {IV}},
	publisher = {Springer},
	author = {Lunden, Nils and Sveen, Robin and Lund, Hans and Svendsen, Nils and Wolthusen, Stephen},
	editor = {Moore, Tyler and Shenoi, Sujeet},
	year = {2010},
	keywords = {concurrent models, distributed simulation, Visualization},
	pages = {171--183},
	file = {Full Text PDF:files/71/Lunden et al. - 2010 - Interactive Visualization of Interdependencies and Vulnerabilities in Constrained Environments.pdf:application/pdf},
}

@inproceedings{halvorsen_games_2014,
	address = {Berlin, Heidelberg},
	title = {Games for {Research}: {A} {Comparative} {Study} of {Open} {Source} {Game} {Projects}},
	isbn = {978-3-642-54420-0},
	shorttitle = {Games for {Research}},
	doi = {10.1007/978-3-642-54420-0_35},
	abstract = {Video games have proved to be an interesting platform for computer scientists as games demand the latest technology, fast response times and effective utilization of hardware. Finding the right games to perform experiments are however difficult. Some researchers create their own smaller prototype games to test their ideas, without performing tests in larger scale productions, which decreases the practical applicability of the conclusion. An important reason is the lack of suitable games for research. This paper proposes a list of qualities and features required by researchers for a video game to be suitable for computer science research. Further, it evaluates four games with open source code and discuss their usefulness. We also consider the current state of open source games and possibilities for enhanced cooperation between the professional and research communities.},
	language = {en},
	booktitle = {Euro-{Par} 2013: {Parallel} {Processing} {Workshops}},
	publisher = {Springer},
	author = {Halvorsen, Stig Magnus and Raaen, Kjetil},
	editor = {an Mey, Dieter and Alexander, Michael and Bientinesi, Paolo and Cannataro, Mario and Clauss, Carsten and Costan, Alexandru and Kecskemeti, Gabor and Morin, Christine and Ricci, Laura and Sahuquillo, Julio and Schulz, Martin and Scarano, Vittorio and Scott, Stephen L. and Weidendorfer, Josef},
	year = {2014},
	keywords = {Crystal Space, Game Developer, Open Source Code, Source Code, Video Game},
	pages = {353--362},
	file = {Full Text PDF:files/72/Halvorsen e Raaen - 2014 - Games for Research A Comparative Study of Open Source Game Projects.pdf:application/pdf},
}

@inproceedings{scacchi_modding_2011,
	address = {Berlin, Heidelberg},
	title = {Modding as an {Open} {Source} {Approach} to {Extending} {Computer} {Game} {Systems}},
	isbn = {978-3-642-24418-6},
	doi = {10.1007/978-3-642-24418-6_5},
	abstract = {This paper examines what is known so far about the role of open source software development within the world of game mods and modding practices. Game modding has become a leading method for developing games by customizing or creating OSS extensions to game software in general, and to proprietary closed source software games in particular. What, why, and how OSS and CSS come together within an application system is the subject for this study. The research method is observational and qualitative, so as to highlight current practices and issues that can be associated with software engineering and game studies foundations. Numerous examples of different game mods and modding practices are identified throughout.},
	language = {en},
	booktitle = {Open {Source} {Systems}: {Grounding} {Research}},
	publisher = {Springer},
	author = {Scacchi, Walt},
	editor = {Hissam, Scott A. and Russo, Barbara and de Mendonça Neto, Manoel G. and Kon, Fabio},
	year = {2011},
	keywords = {Game Development, Game Engine, Game Play, Modding Practice, Software Product Line},
	pages = {62--74},
	file = {Full Text PDF:files/73/Scacchi - 2011 - Modding as an Open Source Approach to Extending Computer Game Systems.pdf:application/pdf},
}

@inproceedings{valls_videogame_2016,
	address = {Cham},
	title = {Videogame {Technology} in {Architecture} {Education}},
	isbn = {978-3-319-39513-5},
	doi = {10.1007/978-3-319-39513-5_41},
	abstract = {Videogame technology is quickly maturing and approaching levels of realism once reserved to 3D rendering applications used in architecture, in real-time and with the capacity to react in real-time to user input. This paper describes an educational experience using videogame technology in architecture education, exploring its applicability in the field in architecture compared to more traditional media. A prototype application modeling a proposed urban space was developed using Unreal Engine and a group of architecture students were asked to use the software to navigate the virtual environment. The development process of the applications is discussed as well as the design of the survey to assess the participants’ experience in four key areas (a) player profile, (b) experience using the beta version, (c) use of videogame technology as an educational tool, and (d) applicability of game engines in Architecture.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Novel} {User} {Experiences}},
	publisher = {Springer International Publishing},
	author = {Valls, Francesc and Redondo, Ernest and Fonseca, David and Garcia-Almirall, Pilar and Subirós, Jordi},
	editor = {Kurosu, Masaaki},
	year = {2016},
	keywords = {Architecture, E-learning, Education, Gamification, Immersion, Interactivity, Urban space, Videogame engine, Visual representation},
	pages = {436--447},
	file = {Full Text PDF:files/74/Valls et al. - 2016 - Videogame Technology in Architecture Education.pdf:application/pdf},
}

@inproceedings{demeulemeester_icocoon_2012,
	address = {Berlin, Heidelberg},
	title = {The {ICOCOON} {Virtual} {Meeting} {Room}: {A} {Virtual} {Environment} as a {Support} {Tool} for {Multipoint} {Teleconference} {Systems}},
	isbn = {978-3-642-33542-6},
	shorttitle = {The {ICOCOON} {Virtual} {Meeting} {Room}},
	doi = {10.1007/978-3-642-33542-6_14},
	abstract = {Globalization and increasing collaboration between remote teams drive the need for teleconference systems. However, currently no videoconferencing system matches the face-to-face experience for a business meeting with many participants in a flexible and affordable manner. In search for a better solution, we created a Virtual Meeting Room (VMR) application that visualizes key events detected using computer vision (e.g., participant entering the meeting room, talking, presenting) in a 3D virtual environment. The goal was to provide a good sense of overview to users when many meeting participants - represented by 3D avatars - from remote locations join a teleconference. In this paper, a technical overview of the working prototype - built using 3D game technology - is presented. Also, feedback from multiple user tests performed during the development of the prototype is discussed and presented as a set of recommendations. From the technical perspective, we found that existing 3D game technology is mature, affordable and contains the features needed to build the VMR application. From the users’ and experts’ feedback, we conclude that the VMR has merits as a teleconferencing support tool accompanying a video stream that conveys more detailed non-verbal communication of the active speaker.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2012},
	publisher = {Springer},
	author = {Demeulemeester, Aljosha and Kilpi, Katriina and Elprama, Shirley A. and Lievens, Sammy and Hollemeersch, Charles-Frederik and Jacobs, An and Lambert, Peter and Van de Walle, Rik},
	editor = {Herrlich, Marc and Malaka, Rainer and Masuch, Maic},
	year = {2012},
	keywords = {Meeting Participant, Second Life, Video Aggregator, Video Stream, Virtual World},
	pages = {158--171},
	file = {Full Text PDF:files/75/Demeulemeester et al. - 2012 - The ICOCOON Virtual Meeting Room A Virtual Environment as a Support Tool for Multipoint Teleconfere.pdf:application/pdf},
}

@inproceedings{sreeram_parallelizing_2011,
	address = {Berlin, Heidelberg},
	title = {Parallelizing a {Real}-{Time} {Physics} {Engine} {Using} {Transactional} {Memory}},
	isbn = {978-3-642-23397-5},
	doi = {10.1007/978-3-642-23397-5_20},
	abstract = {The simulation of the dynamics and kinematics of solid bodies is an important problem in a wide variety of fields in computing ranging from animation and interactive environments to scientific simulations. While rigid body simulation has a significant amount of potential parallelism, efficiently synchronizing irregular accesses to the large amount of mutable shared data in such programs remains a hurdle. There has been a significant amount of interest in transactional memory systems for their potential to alleviate some of the problems associated with fine-grained locking and more broadly for writing correct and efficient parallel programs. While results so far are promising, the effectiveness of TM systems has so far been predominantly evaluated on small benchmarks and kernels.},
	language = {en},
	booktitle = {Euro-{Par} 2011 {Parallel} {Processing}},
	publisher = {Springer},
	author = {Sreeram, Jaswanth and Pande, Santosh},
	editor = {Jeannot, Emmanuel and Namyst, Raymond and Roman, Jean},
	year = {2011},
	keywords = {Collision Detection, Main Thread, Thread Pool, Transactional Memory, Work Thread},
	pages = {206--223},
	file = {Full Text PDF:files/76/Sreeram e Pande - 2011 - Parallelizing a Real-Time Physics Engine Using Transactional Memory.pdf:application/pdf},
}

@inproceedings{merabet_development_2016,
	address = {Cham},
	title = {Development of an {Audio}-{Haptic} {Virtual} {Interface} for {Navigation} of {Large}-{Scale} {Environments} for {People} {Who} {Are} {Blind}},
	isbn = {978-3-319-40238-3},
	doi = {10.1007/978-3-319-40238-3_57},
	abstract = {We are investigating cognitive spatial mapping skills in people who are blind through the use of virtual navigation and assessing the transference of acquired spatial knowledge in large-scale, real-world navigation tasks. Training is carried out with a user-centered, computer-based, navigation software platform called Haptic Audio Game Application (HAGA). This software was developed to assist in orientation and mobility (O\&M) training by introducing blind users to a spatial layout of a large-scale environment through immersive and simulation-based virtual navigation. As part of a self-directed, free exploration strategy, users interact with HAGA in order to navigate through a simulated indoor and outdoor virtual environment that represents an actual physical space. Navigation is based on the use of iconic and spatialized auditory cues and vibro-tactile feedback so as to build a cognitive spatial map of the surrounding environment. The ability to transfer acquired spatial information is then assessed in a series of physical navigation tasks carried out in the actual target environment explored virtually.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Users} and {Context} {Diversity}},
	publisher = {Springer International Publishing},
	author = {Merabet, Lotfi B. and Sánchez, Jaime},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2016},
	keywords = {Blind, Multimodal interfaces, Navigation, Spatial cognition},
	pages = {595--606},
	file = {Full Text PDF:files/77/Merabet e Sánchez - 2016 - Development of an Audio-Haptic Virtual Interface for Navigation of Large-Scale Environments for Peop.pdf:application/pdf},
}

@inproceedings{kocsis_multi-modal_2009,
	address = {Boston, MA},
	title = {Multi-modal {System} {Architecture} for {Serious} {Gaming}},
	isbn = {978-1-4419-0221-4},
	doi = {10.1007/978-1-4419-0221-4_52},
	abstract = {Human-computer interaction (HCI), especially in the games domain, targets to mimic as much as possible the natural human-to-human interaction, which is multimodal, involving speech, vision, haptic, etc. Furthermore, the domain of serious games, aiming to value-added games, makes use of additional inputs, such as biosensors, motion tracking equipment, etc. In this context, game development has become complex, expensive and burdened with a long development cycle. This creates barriers to independent game developers and inhibits the introduction of innovative games, or new game genres. In this paper the PlayMancer platform is introduced, a work in progress aiming to overcome such barriers by augmenting existing 3D game engines with innovative modes of interaction. Playmancer integrates open source existing systems, such as a game engine and a spoken dialog management system, extended by newly implemented components, supporting innovative interaction modalities, such as emotion recognition from audio data, motion tracking, etc, and advanced configuration tools.},
	language = {en},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations} {III}},
	publisher = {Springer US},
	author = {Kocsis, Otilia and Ganchev, Todor and Mporas, Iosif and Papadopoulos, George and Fakotakis, Nikos},
	editor = {{Iliadis} and {Maglogiann} and {Tsoumakasis} and {Vlahavas} and {Bramer}},
	year = {2009},
	keywords = {Emotion Recognition, Game Development, Game Engine, Multimodal Interaction, Voice Activity Detection},
	pages = {441--447},
	file = {Full Text PDF:files/98/Kocsis et al. - 2009 - Multi-modal System Architecture for Serious Gaming.pdf:application/pdf},
}

@inproceedings{herrlich_integration_2010,
	address = {Berlin, Heidelberg},
	title = {Integration of {CityGML} and {Collada} for {High}-{Quality} {Geographic} {Data} {Visualization} on the {PC} and {Xbox} 360},
	isbn = {978-3-642-15399-0},
	doi = {10.1007/978-3-642-15399-0_27},
	abstract = {Computer games and serious geographic information systems (GIS) share many requirements with regard to storage, exchange, and visualization of geographic data. Furthermore, there is a demand for high-fidelity photo-realistic and non-photo-realistic visualization. This poses at least two questions: Is there a single data format standard suitable for serious GIS-based applications and computer games that supports state-of-the-art visual quality? How can computer games and serious applications benefit from each other, especially platform-wise? In this paper we will investigate both questions by taking a closer look at the CityGML standard in comparison to COLLADA and we will report on our findings in integrating CityGML with mainstream game technology. The main contribution of this paper to the field is a suggested way of integrating an important features of CityGML and Collada for high-quality visualization, i.e. programmable shader effects, and demonstrating the feasibility of employing a game console as a cheap and widely available device for geodata visualization and possibly other geodata-centric applications.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2010},
	publisher = {Springer},
	author = {Herrlich, Marc and Holle, Henrik and Malaka, Rainer},
	editor = {Yang, Hyun Seung and Malaka, Rainer and Hoshino, Junichi and Han, Jung Hyun},
	year = {2010},
	keywords = {City Model, Computer Game, Geographic Information System, Open Geospatial Consortium},
	pages = {270--277},
	file = {Full Text PDF:files/100/Herrlich et al. - 2010 - Integration of CityGML and Collada for High-Quality Geographic Data Visualization on the PC and Xbox.pdf:application/pdf},
}

@inproceedings{pellerin_soundpark_2009,
	address = {Berlin, Heidelberg},
	title = {{SoundPark}: {Towards} {Highly} {Collaborative} {Game} {Support} in a {Ubiquitous} {Computing} {Architecture}},
	isbn = {978-3-642-02164-0},
	shorttitle = {{SoundPark}},
	doi = {10.1007/978-3-642-02164-0_12},
	abstract = {Ubiquitous computing architectures enable interaction and collaboration in multi-user applications. We explore the challenges of integrating the disparate services required in such architectures and describe how we have met these challenges in the context of a real-world application that operates on heterogeneous hardware and run-time environments. As a compelling example, we consider the role of ubiquitous computing to support the needs of a distributed multi-user game, including mobility, mutual awareness, and geo-localization. The game presented here, “SoundPark”, is played in a mixed-reality environment, in which the physical space is augmented with computer-generated audio and graphical content, and the players communicate frequently over a low-latency audio channel. Our experience designing and developing the game motivates significant discussion related to issues of general relevance to ubiquitous game architectures, including integration of heterogeneous components, monitoring, remote control and scalability.},
	language = {en},
	booktitle = {Distributed {Applications} and {Interoperable} {Systems}},
	publisher = {Springer},
	author = {Pellerin, Romain and Bouillot, Nicolas and Pietkiewicz, Tatiana and Wozniewski, Mike and Settel, Zack and Gressier-Soudan, Eric and Cooperstock, Jeremy R.},
	editor = {Senivongse, Twittie and Oliveira, Rui},
	year = {2009},
	keywords = {Mixed Reality, Mobile Device, Mobile Phone, Ubiquitous Computing, Virtual Object},
	pages = {157--170},
	file = {Full Text PDF:files/102/Pellerin et al. - 2009 - SoundPark Towards Highly Collaborative Game Support in a Ubiquitous Computing Architecture.pdf:application/pdf},
}

@inproceedings{helsing_noise_2015,
	address = {Cham},
	title = {Noise {Modeler}: {An} {Interactive} {Editor} and {Library} for {Procedural} {Terrains} via {Continuous} {Generation} and {Compilation} of {GPU} {Shaders}},
	isbn = {978-3-319-24589-8},
	shorttitle = {Noise {Modeler}},
	doi = {10.1007/978-3-319-24589-8_42},
	abstract = {In online procedural generation, content is generated as the game is running on the consumers computer. Our GPU-based Noise Modeler composites noise and other functions through a flow-graph editor similar to the ones used by procedural shader editors and offline terrain generators. Our framework enables non-programmers to edit models for procedural terrain while observing the effect of changes immediately in a real-time preview. Each time a change is made to the model, a corresponding GLSL shader function is automatically generated. The shader is then compiled, and used to render a real-time terrain preview.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Helsing, Johan K. and Elster, Anne C.},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {noise synthesis, Online terrain generation, real-time procedural content generation, stochastic implicit surface modeling},
	pages = {469--474},
	file = {Full Text PDF:files/99/Helsing e Elster - 2015 - Noise Modeler An Interactive Editor and Library for Procedural Terrains via Continuous Generation a.pdf:application/pdf},
}

@inproceedings{masek_critical_2010,
	address = {Berlin, Heidelberg},
	title = {Critical {Infrastructure} {Protection} {Risk} {Modelling} with {Games} {Technology}},
	isbn = {978-3-642-15479-9},
	doi = {10.1007/978-3-642-15479-9_34},
	abstract = {Threats to critical infrastructure are not passive. Trying to identify what is in fact ’critical’ is proving to be very difficult as threats constantly evolve. A major benefit of simulating the infrastructure is that security tests and risk modelling can be applied before infrastructure is built or its environment modified, allowing for lower cost design alterations to minimise vulnerabilities. By using the 3D environment of an existing Game Engine we can explore several possibilities for security analysis that existing tools, due to their global view of the problem, do not allow. Providing participants with a first-person view of the situation allows for more realistic role-play, whilst the networked gaming technology allows remote experts to interact in an intuitive environment and explore, identify and assess the critical components of the infrastructure.},
	language = {en},
	booktitle = {What {Kind} of {Information} {Society}? {Governance}, {Virtuality}, {Surveillance}, {Sustainability}, {Resilience}},
	publisher = {Springer},
	author = {Masek, Martin and Boeing, Adrian and Bailey, William},
	editor = {Berleur, Jacques and Hercheui, Magda David and Hilty, Lorenz M.},
	year = {2010},
	keywords = {Critical Infrastructure, Games Technology, Real-time Simulation, Risk Assessment},
	pages = {363--372},
	file = {Full Text PDF:files/101/Masek et al. - 2010 - Critical Infrastructure Protection Risk Modelling with Games Technology.pdf:application/pdf},
}

@inproceedings{kapralos_serious_2014,
	address = {Cham},
	title = {Serious {Games}: {Customizing} the {Audio}-{Visual} {Interface}},
	isbn = {978-3-319-07464-1},
	shorttitle = {Serious {Games}},
	doi = {10.1007/978-3-319-07464-1_18},
	abstract = {Serious games are gaining in popularity within a wide range of educational and training applications given their ability to engage and motivate learners in the educational process. Recent hardware and computational advancements are providing developers the opportunity to develop applications that employ a high level of fidelity (realism) and novel interaction techniques. However, despite these great advances in hardware and computational power, real-time high fidelity rendering of complex virtual environments (found in many serious games) across all modalities is still not feasible. Perceptual-based rendering exploits various aspects of the multi-modal perceptual system to reduce computational requirements without any resulting perceptual effects on the resulting scene. A series of human-based experiments demonstrated a potentially strong effect of sound on visual fidelity perception, and task performance. However, the resulting effects were subjective whereby the influence of sound was dependent on various individual factors including musical listening preferences. This suggests the importance of customizing (individualizing) a serious game’s virtual environment with respect to audio-visual fidelity, background sounds, etc. In this paper details regarding this series of audio-visual experiments will be provided followed by a description of current work that is examining the customization of a serious game’s virtual environment by each user through the use of a game-based calibration method.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} of {Virtual} and {Augmented} {Reality}},
	publisher = {Springer International Publishing},
	author = {Kapralos, Bill and Shewaga, Robert and Ng, Gary},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2014},
	keywords = {audio-visual fidelity, audio-visual interaction, calibration, Serious games, virtual simulation},
	pages = {190--199},
	file = {Full Text PDF:files/103/Kapralos et al. - 2014 - Serious Games Customizing the Audio-Visual Interface.pdf:application/pdf},
}

@inproceedings{garcia_data-driven_2014,
	address = {Cham},
	title = {A {Data}-{Driven} {Entity}-{Component} {Approach} to {Develop} {Universally} {Accessible} {Games}},
	isbn = {978-3-319-07440-5},
	doi = {10.1007/978-3-319-07440-5_49},
	abstract = {Design and implementing accessible games can be challenging, particularly when the designers wish to address different interaction capabilities. Universally-Accessible Games (UA-Games), for instance, follow the principles of the Design for All, aiming to enable the broadest audience as possible to play. Although there are papers regarding the design of UA-Games, the implementation can still be challenging. This paper presents a flexible and extensible approach to implement an UA-Game. The approach relies in a data-driven and component based architecture to allow game entities to be created, managed and customized during run-time. Doing so, it is possible to change the behavior and presentation of the game whilst it is running, allowing the game to adapt itself to better address the interaction needs of the user. Furthermore, being data-driven, it is possible to create and customize user profiles to address specific interaction requirements.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Universal} {Access} to {Information} and {Knowledge}},
	publisher = {Springer International Publishing},
	author = {Garcia, Franco Eusébio and de Almeida Neris, Vânia Paula},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {Game Accessibility, Game Design, Game Development, Universal Design, Universally-Accessible Game},
	pages = {537--548},
	file = {Full Text PDF:files/104/Garcia e de Almeida Neris - 2014 - A Data-Driven Entity-Component Approach to Develop Universally Accessible Games.pdf:application/pdf},
}

@inproceedings{esser_towards_2019,
	address = {Cham},
	title = {Towards {Learning} a {Realistic} {Rendering} of {Human} {Behavior}},
	isbn = {978-3-030-11012-3},
	doi = {10.1007/978-3-030-11012-3_32},
	abstract = {Realistic rendering of human behavior is of great interest for applications such as video animations, virtual reality and gaming engines. Commonly animations of persons performing actions are rendered by articulating explicit 3D models based on sequences of coarse body shape representations simulating a certain behavior. While the simulation of natural behavior can be efficiently learned, the corresponding 3D models are typically designed in manual, laborious processes or reconstructed from costly (multi-)sensor data. In this work, we present an approach towards a holistic learning framework for rendering human behavior in which all components are learned from easily available data. To enable control over the generated behavior, we utilize motion capture data and generate realistic motions based on user inputs. Alternatively, we can directly copy behavior from videos and learn a rendering of characters using RGB camera data only. Our experiments show that we can further improve data efficiency by training on multiple characters at the same time. Overall our approach shows a new path towards easily available, personalized avatar creation.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Esser, Patrick and Haux, Johannes and Milbich, Timo and Ommer, Björn},
	editor = {Leal-Taixé, Laura and Roth, Stefan},
	year = {2019},
	pages = {409--425},
	file = {Full Text PDF:files/105/Esser et al. - 2019 - Towards Learning a Realistic Rendering of Human Behavior.pdf:application/pdf},
}

@inproceedings{carrier-baudouin_solving_2016,
	address = {Cham},
	title = {Solving {Rendering} {Issues} in {Realistic} {3D} {Immersion} for {Visual} {Rehabilitation}},
	isbn = {978-3-319-48881-3},
	doi = {10.1007/978-3-319-48881-3_16},
	abstract = {When a person becomes visually impaired, intensive rehabilitation is required to learn the skills necessary to accurately interpret the sensory cues. 3D virtual immersion (3D VI) can provide a safe and rich world for rehabilitation by rendering a complex environment. 3D VI can stage challenges that can be faced with less stress than the real outdoor world. In order for 3D VI to be used efficiently, the visual rendering for people with low vision and the audio for both the blind and people with low vision must be as realistic as possible. Building a 3D environment at low cost imposes constraints both on the installation and the technical aspects. This paper describes our current work to solve projections issues for a realistic rendering when images are not aligned and deformed.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Carrier-Baudouin, Tristan and Chapdelaine, Claude and Lalonde, Marc and Quinn, Philippe and Foucher, Samuel},
	editor = {Hua, Gang and Jégou, Hervé},
	year = {2016},
	keywords = {3D immersion, Multi-projector calibration, Visual rehabilitation aid},
	pages = {223--237},
	file = {Full Text PDF:files/106/Carrier-Baudouin et al. - 2016 - Solving Rendering Issues in Realistic 3D Immersion for Visual Rehabilitation.pdf:application/pdf},
}

@inproceedings{te_brake_developing_2007,
	address = {Berlin, Heidelberg},
	title = {Developing {Adaptive} {Mobile} {Support} for {Crisis} {Response} in {Synthetic} {Task} {Environments}},
	isbn = {978-3-540-73289-1},
	doi = {10.1007/978-3-540-73289-1_59},
	abstract = {This paper presents an experimental platform for the development and evaluation of mobile decision support for crisis response operations. Using a game-engine, synthetic task environments can be created in which coordination support and the usability of adaptive user interfaces for first responders can be examined in a highly controlled manner. Results of the first experiment in which the platform was used to examine the influences of map size and spatial ability on task performance and situational awareness are presented, and ongoing work is described.},
	language = {en},
	booktitle = {Usability and {Internationalization}. {Global} and {Local} {User} {Interfaces}},
	publisher = {Springer},
	author = {te Brake, Guido and Smets, Nanja},
	editor = {Aykin, Nuray},
	year = {2007},
	keywords = {adaptive support systems, crisis response operations, synthetic task environments},
	pages = {510--519},
	file = {Full Text PDF:files/107/te Brake e Smets - 2007 - Developing Adaptive Mobile Support for Crisis Response in Synthetic Task Environments.pdf:application/pdf},
}

@inproceedings{ramos_continuous_2006,
	address = {Berlin, Heidelberg},
	title = {Continuous {Level} of {Detail} on {Graphics} {Hardware}},
	isbn = {978-3-540-47652-8},
	doi = {10.1007/11907350_39},
	abstract = {Recent advances in graphics hardware provide new possibilities to successfully integrate and improve multiresolution models. In this paper, we present a new continuous multiresolution model that maintains its geometry, based on triangle strips, in high-performance memory in the GPU. This model manages the level of detail by performing fast strip updating operations. We show how this approach takes advantage of the new capabilities of GPUs in an efficient manner.},
	language = {en},
	booktitle = {Discrete {Geometry} for {Computer} {Imagery}},
	publisher = {Springer},
	author = {Ramos, Francisco and Chover, Miguel and Ripolles, Oscar and Granell, Carlos},
	editor = {Kuba, Attila and Nyúl, László G. and Palágyi, Kálmán},
	year = {2006},
	keywords = {Continuous Level, Graphic Application, Graphic Hardware, Graphic Library, Polygonal Mesh},
	pages = {460--469},
	file = {Full Text PDF:files/108/Ramos et al. - 2006 - Continuous Level of Detail on Graphics Hardware.pdf:application/pdf},
}

@inproceedings{jailungka_intuitive_2018,
	address = {Cham},
	title = {Intuitive {3D} {Model} {Prototyping} with {Leap} {Motion} and {Microsoft} {HoloLens}},
	isbn = {978-3-319-91250-9},
	doi = {10.1007/978-3-319-91250-9_21},
	abstract = {This paper presents an advanced human-computer interaction system which supports the design of 3D model and makes 3D prototypes by merging technologies such as augmented reality (AR), hand gesture recognition, and 3D printing. This proposed development provides a system to enhance the user’s experience in designing of 3D model (3D prototypes). Beginners or novice designers can design 3D model intuitively. The proposed system provides an ability of manipulating with 3D model in 3D space such as translating, rotating, and scaling. Leap Motion was used to detect and recognize the hand gesture using the skeleton-based algorithm. This device sends the data of hand positions and gesture commands to display the virtual hands on the Microsoft HoloLens. The graphic manager manages the registration between the coordinate frame of Leap Motion and Microsoft HoloLens’ frame. The gesture-based modeling technique allows the user to design and manipulates 3D holographic objects. In addition, the HoloLens application is used to visualize holograms in the real environment’s scale. The designed holographic objects can be assembled, disassembled, interacted with the real environment surface. The holographic objects can be exported into the CAD file format from the mesh rendering to the ASCII STL structure which can be printed by a 3D printer automatically. In the experiment, the holographic objects can be modeled relatively to the physical objects. The system has been tested by eight participants. The purpose of this experiment was to explore the intuitive interaction techniques which facilitated in the designing and validated the relation between physical objects and holographic object. The output of the experiment was the real 3D printed prototype obtained from the designed holographics. The results of the system performance covered the operations such as translation, rotation, and scaling of holographic objects with respective to the actual object, the averaged time of designing, the precision of hand gesture interaction, and the usability.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Interaction} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Jailungka, Poonsiri and Charoenseang, Siam},
	editor = {Kurosu, Masaaki},
	year = {2018},
	keywords = {3D modeling, 3D printing, Augmented reality, Hand gesture recognition},
	pages = {269--284},
	file = {Full Text PDF:files/109/Jailungka e Charoenseang - 2018 - Intuitive 3D Model Prototyping with Leap Motion and Microsoft HoloLens.pdf:application/pdf},
}

@inproceedings{griffith_real-time_2018,
	address = {Cham},
	title = {Real-{Time} {Motion} {Capture} on a {Budget}},
	isbn = {978-3-319-91581-4},
	doi = {10.1007/978-3-319-91581-4_5},
	abstract = {The U.S. Army Research Laboratory’s Simulation \& Training Technology Center, along with Cole Engineering Services, Inc. and the University of Central Florida have set out to leverage commercial technology with the goal of improving realism, and reducing cost for Army training tasks. The focus of this task is to establish a prototype functionality that allows a live person to take control of a virtual character. This is done using the Enhanced Dynamic Geo-Social Environment, which is an Army-owned simulation built upon the Unreal Engine 4.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Interaction}, {Navigation}, {Visualization}, {Embodiment}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Griffith, Tami and Dwyer, Tabitha and Ablanedo, Jennie},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Avatar puppeteering, Real-time motion capture, Virtual humans, Virtual training},
	pages = {56--70},
	file = {Full Text PDF:files/130/Griffith et al. - 2018 - Real-Time Motion Capture on a Budget.pdf:application/pdf},
}

@inproceedings{narayanasamy_complex_2010,
	address = {Berlin, Heidelberg},
	title = {Complex {Game} {Design} {Modeling}},
	isbn = {978-3-642-15214-6},
	doi = {10.1007/978-3-642-15214-6_7},
	abstract = {This paper looks at the game design and engineering approach to model the game design. The game modeling framework discussed in this paper could be a systematic alternative for implementing in the game engine architecture. The suggested game modeling framework incorporates structural game component, temporal game component and boundary game component frameworks. It is suitable to model most complex games and game engines.},
	language = {en},
	booktitle = {Cultural {Computing}},
	publisher = {Springer},
	author = {Narayanasamy, Viknashvaran and Wong, Kok Wai and Rai, Shri and Chiou, Andrew},
	editor = {Nakatsu, Ryohei and Tosa, Naoko and Naghdy, Fazel and Wong, Kok Wai and Codognet, Philippe},
	year = {2010},
	keywords = {complex system, game component, game design and engineering approach, game modeling framework},
	pages = {65--74},
	file = {Full Text PDF:files/132/Narayanasamy et al. - 2010 - Complex Game Design Modeling.pdf:application/pdf},
}

@inproceedings{kweon_implementation_2018,
	address = {Cham},
	title = {Implementation of {Educational} {Drum} {Contents} {Using} {Mixed} {Reality} and {Virtual} {Reality}},
	isbn = {978-3-319-92279-9},
	doi = {10.1007/978-3-319-92279-9_40},
	abstract = {Since the revival of the arcade game market in Japan, a variety of sensory rhythm games have appeared. Among them, KONAMI’s ‘Drummania’, which provides a controller similar to a real drum, became a popular game that continued after its release in 1999. And ‘Drummania’ became the standard for later drum games. Conventional drum games provide players with information about their performances in GUI form through display. At this time, since the line of sight of the player is fixed on the game screen, recognition of each part of the drum is not intuitive. In this research, we propose more intuitive drum contents by applying mixed reality and virtual reality technology to solve this problem. In order to evaluate the intuitiveness of the proposed system, it is applied to the drum learning contents to verify the effectiveness.},
	language = {en},
	booktitle = {{HCI} {International} 2018 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Kweon, Yongjun and Kim, Sunmyeong and Yoon, Byounghyuk and Jo, Taeyang and Park, Changhoon},
	editor = {Stephanidis, Constantine},
	year = {2018},
	keywords = {Augmented Reality, Drum, Educational contents, Game, Tangible interface, Virtual Reality},
	pages = {296--303},
	file = {Full Text PDF:files/134/Kweon et al. - 2018 - Implementation of Educational Drum Contents Using Mixed Reality and Virtual Reality.pdf:application/pdf},
}

@inproceedings{jailungka_augmented_2020,
	address = {Cham},
	title = {Augmented {Reality} and {Microbit} for {Project}-{Based} {Learning}},
	isbn = {978-3-030-49698-2},
	doi = {10.1007/978-3-030-49698-2_15},
	abstract = {The research proposes an augmented reality system integrating with the BBC Microbit microcontroller for student’s project-based learning. It is a learning tool that helps the students to understand about the augmented reality (AR) and the Microbit technologies. To increase the student’s motivation, the final project was conducted as an AR projectile-based shooting game application. The Microbit was also used as an external remote controller. Each player has an AR marker to define the position and the orientation of the player’s avatar and the Microbit for shooting the bullets. The learning goal of this proposed system is to help the student to understand about physics simulation related to the projectile, augmented reality, C\# programming, the block-based programming with Microbit, basic electronics, hardware communication, computer graphics, painting, and the coordinate system. Furthermore, the proposed system was designed to support for various teaching and learning approaches including the interdisciplinary learning, project-based learning, and STEAM education. This proposed hands-on learning tool assists the learner to understand about the integration of core knowledges in science, engineering, arts, and math. The proposed system was used by 294 participants including Thai high school students, vocational students, and teachers from 3 provinces of Thailand. The experimental results showed that the learning achievement of students received improvement up to 19–36.28\% by comparing the average T-scores of pretest and posttest. In addition, the teachers and students showed a good level of satisfaction on using the proposed system in teaching and learning.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Industrial} and {Everyday} {Life} {Applications}},
	publisher = {Springer International Publishing},
	author = {Jailungka, Poonsiri and Charoenseang, Siam and Thammatinno, Chaowwalit},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Augmented reality, Interdisciplinary learning, Microbit, Project-based learning},
	pages = {219--235},
	file = {Full Text PDF:files/136/Jailungka et al. - 2020 - Augmented Reality and Microbit for Project-Based Learning.pdf:application/pdf},
}

@inproceedings{mcnamara_investigating_2016,
	address = {Cham},
	title = {Investigating {Low}-{Cost} {Virtual} {Reality} {Technologies} in the {Context} of an {Immersive} {Maintenance} {Training} {Application}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_59},
	abstract = {This study evaluates the feasibility and limitations of integrating low cost commercial off the shelf (COTS) virtual reality (VR) technologies into immersive maintenance training applications. The Oculus Rift DK2, Microsoft Kinect V2, and Unity 3D Game Engine were evaluated for positional accuracy and signal chain latency in two distinct studies. Furthermore, the integration between hardware and software was also assessed to determine the limitations and challenges of developing a low cost immersive VR training system. The positional accuracy results showed that the Spine Mid, Spine Shoulder, and Neck joints had the lowest mean error and standard deviation when considering all joints tracked by the Kinect. The signal chain latency results showed 173–186 ms of delay from the time a user performed an action to the time the action was reflected in the virtual environment displayed to the headset. Overall, the integration of a low cost VR system including the Microsoft Kinect V2 and the Oculus Rift DK2 is feasible and could provide a realistic training environment that is deployable and reconfigurable.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {McNamara, Courtney and Proetsch, Matthew and Lerma, Nelson},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {COTS, Immersive environments, Maintenance training, Microsoft Kinect, Oculus rift, Virtual reality},
	pages = {621--632},
	file = {Full Text PDF:files/138/McNamara et al. - 2016 - Investigating Low-Cost Virtual Reality Technologies in the Context of an Immersive Maintenance Train.pdf:application/pdf},
}

@inproceedings{lowe_fragment_2003,
	address = {Berlin, Heidelberg},
	title = {A {Fragment} {Culling} {Technique} for {Rendering} {Arbitrary} {Portals}},
	isbn = {978-3-540-44860-0},
	doi = {10.1007/3-540-44860-8_95},
	abstract = {Portal-based rendering traditionally describes techniques that involve rendering scenes that have been partitioned into cells connected by portals. The partition information is exploited to determine visibility information. Recently, portal-based rendering has also been used to describe scenes composed from cells and transformative portals. Interesting scenes can be composed by using cell topologies that would never emerge from scene partitioning. Although some constraints have been removed to allow for scene composition, many still exist. The surfaces of portals are necessarily planar and convex polygons, usually with a low maximum number of vertices. These constraints are imposed to simplify clipping that would otherwise become geometrically complex. In this paper, we analyze a technique to simulate complex geometric clipping using fragment culling and integrate this into an algorithm to render arbitrary portals. Finally we provide some examples of interesting portal-based environments that are enabled by our algorithm.},
	language = {en},
	booktitle = {Computational {Science} — {ICCS} 2003},
	publisher = {Springer},
	author = {Lowe, Nick and Datta, Amitava},
	editor = {Sloot, Peter M. A. and Abramson, David and Bogdanov, Alexander V. and Dongarra, Jack J. and Zomaya, Albert Y. and Gorbachev, Yuriy E.},
	year = {2003},
	keywords = {Depth Buffer, Destination Cell, Frame Buffer, Indoor Scene, Register Combiner},
	pages = {915--924},
	file = {Full Text PDF:files/140/Lowe e Datta - 2003 - A Fragment Culling Technique for Rendering Arbitrary Portals.pdf:application/pdf},
}

@inproceedings{muller_teaching_2019,
	address = {Cham},
	title = {Teaching {UAVs} to {Race}: {End}-to-{End} {Regression} of {Agile} {Controls} in {Simulation}},
	isbn = {978-3-030-11012-3},
	shorttitle = {Teaching {UAVs} to {Race}},
	doi = {10.1007/978-3-030-11012-3_2},
	abstract = {Automating the navigation of unmanned aerial vehicles (UAVs) in diverse scenarios has gained much attention in recent years. However, teaching UAVs to fly in challenging environments remains an unsolved problem, mainly due to the lack of training data. In this paper, we train a deep neural network to predict UAV controls from raw image data for the task of autonomous UAV racing in a photo-realistic simulation. Training is done through imitation learning with data augmentation to allow for the correction of navigation mistakes. Extensive experiments demonstrate that our trained network (when sufficient data augmentation is used) outperforms state-of-the-art methods and flies more consistently than many human pilots. Additionally, we show that our optimized network architecture can run in real-time on embedded hardware, allowing for efficient on-board processing critical for real-world deployment. From a broader perspective, our results underline the importance of extensive data augmentation techniques to improve robustness in end-to-end learning setups.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Müller, Matthias and Casser, Vincent and Smith, Neil and Michels, Dominik L. and Ghanem, Bernard},
	editor = {Leal-Taixé, Laura and Roth, Stefan},
	year = {2019},
	pages = {11--29},
	file = {Full Text PDF:files/142/Müller et al. - 2019 - Teaching UAVs to Race End-to-End Regression of Agile Controls in Simulation.pdf:application/pdf},
}

@inproceedings{cheung_lcrowdv_2016,
	address = {Cham},
	title = {{LCrowdV}: {Generating} {Labeled} {Videos} for {Simulation}-{Based} {Crowd} {Behavior} {Learning}},
	isbn = {978-3-319-48881-3},
	shorttitle = {{LCrowdV}},
	doi = {10.1007/978-3-319-48881-3_50},
	abstract = {We present a novel procedural framework to generate an arbitrary number of labeled crowd videos (LCrowdV). The resulting crowd video datasets are used to design accurate algorithms or training models for crowded scene understanding. Our overall approach is composed of two components: a procedural simulation framework for generating crowd movements and behaviors, and a procedural rendering framework to generate different videos or images. Each video or image is automatically labeled based on the environment, number of pedestrians, density, behavior (agent personality), flow, lighting conditions, viewpoint, noise, etc. Furthermore, we can increase the realism by combining synthetically-generated behaviors with real-world background videos. We demonstrate the benefits of LCrowdV over prior labeled crowd datasets, by augmenting real dataset with it and improving the accuracy in pedestrian detection. LCrowdV has been made available as an online resource.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Cheung, Ernest and Wong, Tsan Kwong and Bera, Aniket and Wang, Xiaogang and Manocha, Dinesh},
	editor = {Hua, Gang and Jégou, Hervé},
	year = {2016},
	keywords = {Crowd analysis, Crowd behaviors, Crowd datasets, Crowd rendering, Crowd simulation, Pedestrian detection},
	pages = {709--727},
	file = {Full Text PDF:files/164/Cheung et al. - 2016 - LCrowdV Generating Labeled Videos for Simulation-Based Crowd Behavior Learning.pdf:application/pdf},
}

@inproceedings{thammatinno_development_2018,
	address = {Cham},
	title = {Development of {Holographic} {Environment} for {Multi}-user {Virtual} {Robot} {Training} {System}},
	isbn = {978-3-319-91244-8},
	doi = {10.1007/978-3-319-91244-8_37},
	abstract = {This research presents the design and development of holographic environment for multi-user virtual robot training system. During On-the-Job Training (OJT), this proposed system assists the trainer to train the trainee for operating the virtual robot arm at the robot station. It is designed for multiple users to access the same augmented environment including the physics-based simulation at the same time. In the augmented environment, the trainer can demonstrate the operation of the robot through the hologram while the trainee can visualize and operate the virtual robot by interaction with the hologram. The result showed that the same augmented environment was interacted by the trainer and the trainee successfully. Hologram environment was accurately mapped to the real environment. In the future, the proposed system can send a set of commands to control the real robot as similar to the hologram version.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Interaction} in {Context}},
	publisher = {Springer International Publishing},
	author = {Thammatinno, Chaowwalit and Charoenseang, Siam},
	editor = {Kurosu, Masaaki},
	year = {2018},
	keywords = {Augmented reality, Hologram, Robot training, Simulation},
	pages = {466--478},
	file = {Full Text PDF:files/168/Thammatinno e Charoenseang - 2018 - Development of Holographic Environment for Multi-user Virtual Robot Training System.pdf:application/pdf},
}

@inproceedings{mateus_intelligent_2017,
	address = {Cham},
	title = {Intelligent {Virtual} {Environment} {Using} {Artificial} {Neural} {Networks}},
	isbn = {978-3-319-57987-0},
	doi = {10.1007/978-3-319-57987-0_4},
	abstract = {This paper describes an Intelligent Virtual Environment (IVE) which incorporates Artificial Neural Networks (ANN) in the perception and reasoning of a character in this virtual environment, in order to react intelligently to some given warning signs. First, we explore different types of ANN simulated in MATLAB to understand their operation in order to choose the one that fits to our virtual environment. The environment was created with the UDK game engine and it consists of a character that moves across a working environment to identify warning signals. Later we implemented the Multi-Layer Perceptron (MLP) ANN in this environment. MLP was selected according to data obtained in several tests. This implementation was done by integrating the ANN in the state machines in the source code of the game engine to perform several operations within a controlled work environment.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Mateus, Sandra and Branch, John},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Artificial neural network, Intelligent virtual environment, Multi-Layer perceptron, Perception, Reasoning},
	pages = {43--53},
	file = {Full Text PDF:files/172/Mateus e Branch - 2017 - Intelligent Virtual Environment Using Artificial Neural Networks.pdf:application/pdf},
}

@inproceedings{christiansen_game-based_2009,
	address = {Berlin, Heidelberg},
	title = {Game-{Based} {Simulation} for the {Evaluation} of {Threat} {Detection} in a {Seaport} {Environment}},
	isbn = {978-3-540-89222-9},
	doi = {10.1007/978-3-540-89222-9_28},
	abstract = {The ability to simulate a seaport environment, including illicit cargo and the sensors designed to detect such cargo, allows the evaluation of alternative detection methods in order to improve security at our nation’s seaports. We describe our progress towards this goal. Specifically, we describe our modeling of threats at a particle emission level, modeling of sensors as particle detectors, modeling of the seaport dynamics (e.g, ships, cargo containers, cranes, trucks), and how the particles interact with the various structures and materials in the seaport environment as the cargo moves through the seaport. Ultimately, this simulation will serve as a testbed for the evaluation of sensor network data collection, fusion and decision making for threat detection in a seaport environment.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2008},
	publisher = {Springer},
	author = {Christiansen, Allen and Johnson, Damian and Holder, Lawrence},
	editor = {Stevens, Scott M. and Saldamarco, Shirley J.},
	year = {2009},
	keywords = {particle propagation, seaport, security, sensor modeling, Simulation, threat detection},
	pages = {221--224},
	file = {Full Text PDF:files/175/Christiansen et al. - 2009 - Game-Based Simulation for the Evaluation of Threat Detection in a Seaport Environment.pdf:application/pdf},
}

@inproceedings{almeida_systematic_2013,
	address = {Berlin, Heidelberg},
	title = {A {Systematic} {Review} of {Game} {Design} {Methods} and {Tools}},
	isbn = {978-3-642-41106-9},
	doi = {10.1007/978-3-642-41106-9_3},
	abstract = {The game designer’s craft is very young if compared to filmmaking and software development. The knowledge base and formal techniques of these areas is far more comprehensive. Even after decades of evolution of the games production software, the range of design centered techniques and tools is still limited, as observed by many authors. Thereby, efforts have been made towards the establishment of game design formal methods. This paper presents a systematization over the contributions of researchers and designers towards conceptual and concrete tools. These efforts converge to two approaches: the build of a shared design vocabulary and a game design modeling language. While valuable, the existing implementations of these approaches are not mature enough to gain industry adepts, serving only as reference to future works. Moreover, it is needed to discover the designer’s particular methods, which may contribute to-wards the constitution of a unified design toolbox.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2013},
	publisher = {Springer},
	author = {Almeida, Marcos Silvano Orita and da Silva, Flávio Soares Corrêa},
	editor = {Anacleto, Junia C. and Clua, Esteban W. G. and da Silva, Flavio S. Correa and Fels, Sidney and Yang, Hyun S.},
	year = {2013},
	keywords = {Game design, game design methods, game design tools},
	pages = {17--29},
	file = {Full Text PDF:files/131/Almeida e da Silva - 2013 - A Systematic Review of Game Design Methods and Tools.pdf:application/pdf},
}

@inproceedings{zarraonandia_virtual_2009,
	address = {Berlin, Heidelberg},
	title = {A {Virtual} {Environment} for {Learning} {Aiport} {Emergency} {Management} {Protocols}},
	isbn = {978-3-642-02580-8},
	doi = {10.1007/978-3-642-02580-8_25},
	abstract = {This paper presents a virtual environment designed to enhance the learning of airport emergency management protocols. The learning is performed in an informal manner, with each learner playing a different role in a particular emergency simulation. Learners interact within the virtual environment, managing the available information and following the steps prescribed for each type of emergency in the Airport Emergency Plan of the Spanish Civil Defence Organization. The simulation can be run in different modes of difficulty, and can be used as a learning tool as well as an evaluation tool to measure the accuracy of the learner’s actuation within the protocol. It can also support stand-alone training having some of the emergency roles played out by the computer. The virtual environment has been built using DimensioneX, an open source multi-player online game engine.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Ambient}, {Ubiquitous} and {Intelligent} {Interaction}},
	publisher = {Springer},
	author = {Zarraonandia, Telmo and Vargas, Mario Rafael Ruiz and Díaz, Paloma and Aedo, Ignacio},
	editor = {Jacko, Julie A.},
	year = {2009},
	keywords = {emergency, game engine, simulation, Virtual environment},
	pages = {228--235},
	file = {Full Text PDF:files/133/Zarraonandia et al. - 2009 - A Virtual Environment for Learning Aiport Emergency Management Protocols.pdf:application/pdf},
}

@inproceedings{polverari_development_2007,
	address = {Berlin, Heidelberg},
	title = {Development of an {Autonomous} {Rescue} {Robot} {Within} the {USARSim} {3D} {Virtual} {Environment}},
	isbn = {978-3-540-74024-7},
	doi = {10.1007/978-3-540-74024-7_50},
	abstract = {The increasing interest towards rescue robotics and the complexity of typical rescue environments make it necessary to use high fidelity 3D simulators during the application development phase. USARSim is an open source high fidelity simulator for rescue environments, based on a commercial game engine. In this paper, we describe the development of an autonomous rescue robot within the USARSim simulation environment. We describe our rescue robotic system and present the extensions we made to USARSim in order to have a satisfying simulation of our robot. Moreover, as a case study, we present an algorithm to avoid obstacles invisible to our laser scanner based mapping process.},
	language = {en},
	booktitle = {{RoboCup} 2006: {Robot} {Soccer} {World} {Cup} {X}},
	publisher = {Springer},
	author = {Polverari, Giuliano and Calisi, Daniele and Farinelli, Alessando and Nardi, Daniele},
	editor = {Lakemeyer, Gerhard and Sklar, Elizabeth and Sorrenti, Domenico G. and Takahashi, Tomoichi},
	year = {2007},
	keywords = {Laser Range Finder, Mobile Robot, Real Robot, Robotic System, Stereo Vision},
	pages = {491--498},
	file = {Full Text PDF:files/135/Polverari et al. - 2007 - Development of an Autonomous Rescue Robot Within the USARSim 3D Virtual Environment.pdf:application/pdf},
}

@inproceedings{cavazza_developing_2004,
	address = {Boston, MA},
	title = {Developing {Re}-{Usable} {Interactive} {Storytelling} {Technologies}},
	isbn = {978-1-4020-8157-6},
	doi = {10.1007/978-1-4020-8157-6_6},
	abstract = {Despite the growing interest in Interactive Storytelling (IS), there have been only a small number of implemented demonstrators and few have attempted at developing a re-usable IS technology. In this paper we describe such an IS engine, which is the result of several years of experimentation in the field. The system is based on a game engine for its visualisation component, while the narrative generation component implements a variant of HTN Planning. After an introduction to the principles underlying the system, we introduce the associated production process and discuss authoring problems as well as tools we have developed to facilitate the use of the technology.},
	language = {en},
	booktitle = {Building the {Information} {Society}},
	publisher = {Springer US},
	author = {Cavazza, Marc and Charles, Fred and Mead, Steven J.},
	editor = {Jacquart, Renè},
	year = {2004},
	keywords = {Artificial Actors, Interactive Narratives, Virtual Storytelling},
	pages = {39--44},
	file = {Full Text PDF:files/137/Cavazza et al. - 2004 - Developing Re-Usable Interactive Storytelling Technologies.pdf:application/pdf},
}

@inproceedings{claire_battle_2015,
	address = {Cham},
	title = {A {Battle} of {Wit}: {Applying} {Computational} {Humour} to {Game} {Design}},
	isbn = {978-3-319-24589-8},
	shorttitle = {A {Battle} of {Wit}},
	doi = {10.1007/978-3-319-24589-8_6},
	abstract = {There is still a dearth of humour in computer games. To spur the use of humour in games and overcome some of the difficultiesin producing humour, we advance that game design can benefit from research in computational humour. The focus of this paper is thus on verbal humour and humour design. Integrating computational humour in games could facilitate humour scripting and solve one of the oldest problems in game humour related to repetition. A humour bot could enhance gamers’ experiences, by stimulating social bonding or supporting comic relief. We believe that the use of computational humour for game design would enhance players’ laughter and designers’ creativity. Last, as game design can benefit from advances in computational humour, so virtual agents can from game research.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Claire, Dormann},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {computational humour, computer games, humour design, one-liner, players, tools, verbal humour},
	pages = {72--85},
	file = {Full Text PDF:files/139/Claire - 2015 - A Battle of Wit Applying Computational Humour to Game Design.pdf:application/pdf},
}

@inproceedings{kastel_aristophanes_2013,
	address = {Berlin, Heidelberg},
	title = {{AR}’istophanes: {Mixed} {Reality} {Live} {Stage} {Entertainment} with {Spectator} {Interaction}},
	isbn = {978-3-642-39420-1},
	shorttitle = {{AR}’istophanes},
	doi = {10.1007/978-3-642-39420-1_41},
	abstract = {Mixed Reality and Augmented Reality for live stage productions have been used ever more frequently by artists over the past few years. AR’istophanes is an experimental stage production aimed at bringing the new technical possibilities of Mixed and Augmented Reality to the stages of this world. This document describes the first phase of pre-production from 2011 to 2012 and demonstrates the possibilities of integrating motion capturing and 3D animation. This also includes the use of Smartphone Apps and real-time rendering. Audience interaction is a key focus in this production – which means technical approaches are demonstrated and opinions were collected from potential viewers.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Systems} and {Applications}},
	publisher = {Springer},
	author = {Kastel, Thiemo and Kesmaecker, Marion and Mikolajczyk, Krzysztof and Duarte-Gonçalves, Bruno Filipe},
	editor = {Shumaker, Randall},
	year = {2013},
	keywords = {Augmented Reality, Interaction, Live Entertainment, Mixed Reality, Optical See-Through Glasses, Theatre},
	pages = {390--399},
	file = {Full Text PDF:files/141/Kastel et al. - 2013 - AR’istophanes Mixed Reality Live Stage Entertainment with Spectator Interaction.pdf:application/pdf},
}

@inproceedings{burney_measuring_2007,
	address = {Berlin, Heidelberg},
	title = {Measuring {Game}-{Play} {Performance} and {Perceived} {Immersion} in a {Domed} {Planetarium} {Projection} {Environment}},
	isbn = {978-3-540-74873-1},
	doi = {10.1007/978-3-540-74873-1_4},
	abstract = {Game playing in immersive projection environments such as caves and domes is assumed to offer an enhanced experience but there is little quantitative research that measures this. This paper reports on a study of user performance statistics while playing a computer game projected onto a planetarium dome and compares these with similar measurements taken in a conventional projected flat screen environment. A survey of users’ subjective impressions of immersion was also taken and used to compare these display modes. Analysis of users in each mode revealed differences in user experience and some aspects of performance. It was confirmed that dome projection enhanced the player’s sense of immersion when compared with flat projection. Navigation speed was found to decline in the dome while other performance metrics showed no significant difference between the environments.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2007},
	publisher = {Springer},
	author = {Burney, Timon and Lock, Phillip},
	editor = {Ma, Lizhuang and Rauterberg, Matthias and Nakatsu, Ryohei},
	year = {2007},
	keywords = {computer games, Dome-projection, game-play performance, immersion, user interfaces},
	pages = {22--27},
	file = {Full Text PDF:files/143/Burney e Lock - 2007 - Measuring Game-Play Performance and Perceived Immersion in a Domed Planetarium Projection Environmen.pdf:application/pdf},
}

@inproceedings{hermanns_screen_2016,
	address = {Cham},
	title = {Screen {Space} {Cone} {Tracing} for {Glossy} {Reflections}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_29},
	abstract = {Indirect lighting (also Global Illumination (GI)) is an important part of photo-realistic imagery and has become a widely used method in real-time graphics applications, such as Computer Aided Design (CAD), Augmented Reality (AR) and video games. Path tracing can already achieve photo-realism by shooting thousands or millions of rays into a 3D scene for every pixel, which results in computational overhead exceeding real-time budgets. However, with modern programmable shader pipelines, a fusion of ray-casting algorithms and rasterization is possible, i.e. methods, which are similar to testing rays against geometry, can be performed on the GPU within a fragment (or rather pixel-) shader. Nevertheless, many implementations for real-time GI still trace perfect specular reflections only. In this work the advantages and disadvantages of different reflection methods are exposed and a combination of some of these is presented, which circumvents artifacts in the rendering and provides a stable, temporally coherent image enhancement. The benefits and failings of this new method are clearly separated as well. Moreover the developed algorithm can be implemented as pure post-process, which can easily be integrated into an existing rendering pipeline.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Hermanns, Lukas and Franke, Tobias and Kuijper, Arjan},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {Global Illumination, Indirect Lighting, Planar Reflection, Reflective Geometry, Screen Space},
	pages = {308--318},
	file = {Full Text PDF:files/165/Hermanns et al. - 2016 - Screen Space Cone Tracing for Glossy Reflections.pdf:application/pdf},
}

@inproceedings{henden_surround_2009,
	address = {Berlin, Heidelberg},
	title = {A {Surround} {Display} {Warp}-{Mesh} {Utility} to {Enhance} {Player} {Engagement}},
	isbn = {978-3-540-89222-9},
	doi = {10.1007/978-3-540-89222-9_6},
	abstract = {Surround displays are used in simulation, training, and other applications based on virtual worlds. A wide-view display engages the viewer’s peripheral vision, providing a more accurate view of the virtual world and therefore a heightened sense of immersion. However, most commercially available surround displays are expensive and complex. We developed a low-cost alternative, which uses a standard digital projector, a hemispherical mirror, and any roughly spherical or cylindrical screen. The software can handle irregular surfaces and will be open-sourced in the next release of the CaveUT/VRGL freeware. We also conducted a pilot study comparing game play in our prototype and game play with a standard desktop monitor. Players using the surround display reported significantly shorter (P = 0.0051) perceived duration of time during play. Reduced awareness of the passage of time during game play was positively correlated with greater engagement and enjoyment.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2008},
	publisher = {Springer},
	author = {Henden, Charles and Champion, Erik and Muhlberger, Ralf and Jacobson, Jeffrey},
	editor = {Stevens, Scott M. and Saldamarco, Shirley J.},
	year = {2009},
	keywords = {CaveUT, Dome Projection, Immersion, Spherical Mirror Projection, Surround Gaming, Texture-Mesh, Virtual Environments, Virtual Reality, VRGL, Warp-Mesh},
	pages = {46--56},
	file = {Full Text PDF:files/167/Henden et al. - 2009 - A Surround Display Warp-Mesh Utility to Enhance Player Engagement.pdf:application/pdf},
}

@inproceedings{visser_topleague_2011,
	address = {Berlin, Heidelberg},
	title = {{TopLeague} and {Bundesliga} {Manager}: {New} {Generation} {Online} {Soccer} {Games}},
	isbn = {978-3-642-20217-9},
	shorttitle = {{TopLeague} and {Bundesliga} {Manager}},
	doi = {10.1007/978-3-642-20217-9_20},
	abstract = {This paper describes a new generation online soccer manager. More than 210,000 users operate TopLeague and the Official Bundesliga Manager, complex real-time soccer simulators that are based on actual data of real professional soccer players. The underlying technology is a hierarchical three-tier multiagent system that consists of autonomous BDI agents and allows dynamic group structures (e.g. an emergent situation for a wing attack). The German Bundesliga, one of the most prestigious soccer leagues in the world, adopted this AI system for their official web site. The online games run seamlessly in a web browser with a state-of-the-art 3D visualization engine. Fundamental research within the domain of the RoboCup simulation league is the basis for this technology. We will describe the architecture of the multiagent system (MAS) in this paper, discuss motion capture techniques for graphical animation, and reveal details about user acceptance of the games.},
	language = {en},
	booktitle = {{RoboCup} 2010: {Robot} {Soccer} {World} {Cup} {XIV}},
	publisher = {Springer},
	author = {Visser, Ubbo},
	editor = {Ruiz-del-Solar, Javier and Chown, Eric and Plöger, Paul G.},
	year = {2011},
	keywords = {Facial Expressions, Manager Games, Motion Capture, Robotic Entertainment, Soccer Simulation, Teamwork and Heterogeneous Agents},
	pages = {230--241},
	file = {Full Text PDF:files/170/Visser - 2011 - TopLeague and Bundesliga Manager New Generation Online Soccer Games.pdf:application/pdf},
}

@inproceedings{carlson_modeling_2019,
	address = {Cham},
	title = {Modeling {Camera} {Effects} to {Improve} {Visual} {Learning} from {Synthetic} {Data}},
	isbn = {978-3-030-11009-3},
	doi = {10.1007/978-3-030-11009-3_31},
	abstract = {Recent work has focused on generating synthetic imagery to increase the size and variability of training data for learning visual tasks in urban scenes. This includes increasing the occurrence of occlusions or varying environmental and weather effects. However, few have addressed modeling variation in the sensor domain. Sensor effects can degrade real images, limiting generalizability of network performance on visual tasks trained on synthetic data and tested in real environments. This paper proposes an efficient, automatic, physically-based augmentation pipeline to vary sensor effects – chromatic aberration, blur, exposure, noise, and color temperature – for synthetic imagery. In particular, this paper illustrates that augmenting synthetic training datasets with the proposed pipeline reduces the domain gap between synthetic and real domains for the task of object detection in urban driving scenes.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Carlson, Alexandra and Skinner, Katherine A. and Vasudevan, Ram and Johnson-Roberson, Matthew},
	editor = {Leal-Taixé, Laura and Roth, Stefan},
	year = {2019},
	keywords = {Deep learning, Image augmentation, Object detection},
	pages = {505--520},
	file = {Full Text PDF:files/173/Carlson et al. - 2019 - Modeling Camera Effects to Improve Visual Learning from Synthetic Data.pdf:application/pdf},
}

@inproceedings{mateus_virtual_2018,
	address = {Cham},
	title = {Virtual {Environment} for the {Treatment} of {Patients} with {Hemiparesis}},
	isbn = {978-3-319-92279-9},
	doi = {10.1007/978-3-319-92279-9_5},
	abstract = {This paper proposes the development of a Virtual Environment that facilitates the process of reeducation of the motor and sensory functions of a patient with hemiparesis, through the simulation of physiotherapy exercises. The methodology used for the development of the work was as follows: a characterization of the variables that affect patients with hemiparesis was realized, analyzing the factors that cause this condition, for the determination of the bases of the Virtual Environment; later a model of Virtual Reality Environment was designed that allows the interaction with the patient for its later implementation using a video game engine that leads us to obtain a prototype of applicability in the rehabilitation of patients. Finally, the results are validated with experts in physiotherapy using system tests to verify the rehabilitation of patients with hemiparesis, reviewing the acceptance criteria of the prototype.},
	language = {en},
	booktitle = {{HCI} {International} 2018 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Mateus, Sandra and García, Cristian and Martínez, Víctor},
	editor = {Stephanidis, Constantine},
	year = {2018},
	keywords = {Hemiparesis, Kinect, Physiotherapy, Rehabilitation, Virtual Environment (VE)},
	pages = {40--47},
	file = {Full Text PDF:files/176/Mateus et al. - 2018 - Virtual Environment for the Treatment of Patients with Hemiparesis.pdf:application/pdf},
}

@inproceedings{domik_coping_2010,
	address = {Berlin, Heidelberg},
	title = {Coping with {Complex} {Real}-{World} {Problems}: {Strategies} for {Developing} the {Competency} of {Transdisciplinary} {Collaboration}},
	isbn = {978-3-642-15378-5},
	shorttitle = {Coping with {Complex} {Real}-{World} {Problems}},
	doi = {10.1007/978-3-642-15378-5_9},
	abstract = {Real world problems are complex and therefore between and beyond disciplines. To solve them requires expertise across several disciplines. This paper argues that we need to teach students transdisciplinary collaboration as a competency demanded in future work places. We describe two learning strategies, ”breadth-first” and ”Long Tail”, to help develop these competencies in graduate students. An implementation of these strategies in a computer science course with 48 graduate students from various disciplines is described. Finally, implications and future opportunities of our approach are discussed.},
	language = {en},
	booktitle = {Key {Competencies} in the {Knowledge} {Society}},
	publisher = {Springer},
	author = {Domik, Gitta and Fischer, Gerhard},
	editor = {Reynolds, Nicholas and Turcsányi-Szabó, Márta},
	year = {2010},
	keywords = {21st century competencies, Lifelong Learning, Transdisciplinary Collaboration},
	pages = {90--101},
	file = {Full Text PDF:files/178/Domik e Fischer - 2010 - Coping with Complex Real-World Problems Strategies for Developing the Competency of Transdisciplina.pdf:application/pdf},
}

@inproceedings{tully_mesh_2015,
	address = {Cham},
	title = {Mesh {Extraction} from a {Regular} {Grid} {Structure} {Using} {Adjacency} {Matrix}},
	isbn = {978-3-319-21969-1},
	doi = {10.1007/978-3-319-21969-1_51},
	abstract = {Crisis management is a modern phenomenon brought about by natural disasters and acts of terrorism. Building a modern crisis management response program needs a multi-disciplinary architecture and accurate, up-to-date, real-world data. The creation of virtual environments depicting critical infrastructure buildings and conduits between these highly interconnected man-made structures is a complex procedure. The crossover between games technology and use of real-world map data for real-world simulations is becoming more common with the advancements of computer hardware and software, and the accuracy of real-world map data. However, there are many problems with using real-world map data for simulation due to the large potential of missing and error prone data involved in this big data. Within this work we use three types of data sets; Ordnance Survey data, LiDAR data, and OpenStreetMap data to provide accurate map and 3D environment information for crisis management systems. Combining these large data-sets can reduce errors and retrieve missing data for use within a modern game engine for visualization analysis. We propose a novel technique for data extraction using adjacency matrices for custom model generation corresponding to real-world structures such as landscapes, buildings, road systems, area boundaries, or a combination of these at different resolutions.},
	language = {en},
	booktitle = {Image and {Graphics}},
	publisher = {Springer International Publishing},
	author = {Tully, David and El Rhalibi, Abdennour and Pan, Zhigeng and Carter, Christopher and Sudirman, Sud},
	editor = {Zhang, Yu-Jin},
	year = {2015},
	keywords = {Adjacency matrix, Computer games, Crisis management, Visualization},
	pages = {562--572},
	file = {Full Text PDF:files/180/Tully et al. - 2015 - Mesh Extraction from a Regular Grid Structure Using Adjacency Matrix.pdf:application/pdf},
}

@inproceedings{croft_user_2019,
	address = {Cham},
	title = {User {Interface} {Prototyping} {Toolkit} ({UIPT})},
	isbn = {978-3-030-23570-3},
	doi = {10.1007/978-3-030-23570-3_15},
	abstract = {A design philosophy implementation in the form of a User Interface Prototyping Toolkit (UIPT) constitutes a software application for use by Fleet users, researchers, designers, and developers to aid in rapid engagement with end users in an agile design process. The toolkit allows rapid development of user interfaces (UIs) for collaborative exploration and validation of new concepts of operations (CONOPS) and new technology for decision-making and situational awareness. This improvement in exploration and analysis of software systems applies no matter the production stage. This product allows researchers, designers and developers to quickly develop new or modify existing UIs as designers work alongside Fleet users in a collaborative design effort. The design effort applies to existing or novel missions utilizing tasks and CONOPs as drivers for meaningful requirements and design input. The UIPT toolkit effort is targeted to Command, Control, Communications, Computers, Intelligence, Surveillance and Reconnaissance (C4ISR) and Cyber missions for the U.S. Navy. UIPT enables the Navy to significantly reduce the risks associated with pursuing revolutionary technology such as autonomous vehicles, artificial intelligence and distributed sensing in terms of the design of the user interface. The UIPT toolkit provides the ability to explore, analyze, develop and test systems that maximize human interaction and efficiency via the interface for these new and evolving CONOPs and systems.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {Design} {Philosophy} and {Theory}},
	publisher = {Springer International Publishing},
	author = {Croft, Bryan L. and Clarkson, Jeffrey D. and VonColln, Eric},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2019},
	keywords = {Command Center of the Future, Information systems, Rapid prototyping, Unity3D, User Interfaces},
	pages = {195--207},
	file = {Full Text PDF:files/182/Croft et al. - 2019 - User Interface Prototyping Toolkit (UIPT).pdf:application/pdf},
}

@inproceedings{perri_exploring_2024,
	address = {Cham},
	title = {Exploring the {Metaverse}: {Opportunities} for {Tourism} and {Territorial} {Development}},
	isbn = {978-3-031-65343-8},
	shorttitle = {Exploring the {Metaverse}},
	doi = {10.1007/978-3-031-65343-8_9},
	abstract = {This article conducts a preliminary study on the possible outcomes of applying very advanced computer graphics technologies, such as Immersive Virtual Reality, Virtual Reality, and Mixed Reality, to promote tourism and an area’s economy. A careful analysis of the existing literature has revealed an intense research activity that has explored the salient aspects of the interaction between modern technologies and tourism, analysing the changes resulting from their use. The Metaverse represents one of the most powerful and effective innovations in tourism. We describe a methodology for implementing metaverses and virtual scenarios using the open-source Blender modelling environment and Unity game engine and creation tool. The metaverse of Palazzo Bernabei in Assisi, the suggestive venue of the Degree Course in Tourism Economics and Management, was implemented as a use case. It is located in an area of unique interest next to the splendid and evocative Basilica of St. Francis of Assisi. These technologies can improve tourism since they provide a high sense of immersion, giving the user an experience that anticipates and enriches the live visit. The user can perform the virtual visit remotely with the help of an immersive virtual reality controller. A questionnaire was carried out on a group of users to assess the user experience and the level of satisfaction with the application. The discussion of the answers in the questionnaire shows that users are fascinated by the virtual experience and become more aware of the beauty of works of art and monuments than traditional media (magazines, brochures, books) and are more attracted to the real visit.},
	language = {en},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2024 {Workshops}},
	publisher = {Springer Nature Switzerland},
	author = {Perri, Damiano and Di Blasi, Sofia Rita and Forlani, Fabio and Gervasi, Osvaldo},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Garau, Chiara and Taniar, David and C. Rocha, Ana Maria A. and Faginas Lago, Maria Noelia},
	year = {2024},
	keywords = {Immersive Virtual Reality, Metaverse, Mixed Reality, Tourism},
	pages = {141--153},
	file = {Full Text PDF:files/185/Perri et al. - 2024 - Exploring the Metaverse Opportunities for Tourism and Territorial Development.pdf:application/pdf},
}

@inproceedings{ramoglu_programming_2017,
	address = {Cham},
	title = {Programming a {Robotic} {Toy} with a {Block} {Coding} {Application}: {A} {Usability} {Study} with {Non}-programmer {Adults}},
	isbn = {978-3-319-58634-2},
	shorttitle = {Programming a {Robotic} {Toy} with a {Block} {Coding} {Application}},
	doi = {10.1007/978-3-319-58634-2_47},
	abstract = {Recently, sophisticated robotic toys have commercially emerged into our lives. Apart from being only a toy, some of these smart devices are programmable for accomplishing commands given by the end-user. However, usually, end-users are not experts in robotics or programming. In order to explore the usability issues related to the non-programmers’ experience of controlling the robotic toys, we conducted a user study with non-programmers (N = 9) by using Sphero (a robotic toy) and tested its mobile application, called SPRK Lightning Lab for Sphero, which adopted visual programming language with a block-based coding interface. Our procedure consisted of a pre-test and a semi-structured post-test interview as well as an exploring session for the participants and three tasks with a short semi-structured interview at the end of each task. Our findings, which highlighted the usability issues of SPRK Lightning Lab for Sphero application, contribute to the field by providing design suggestions on using a digital medium and a tangible device together, the usability issues of block coding by non-programmers and learnability in a robotic toy application.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Theory}, {Methodology}, and {Management}},
	publisher = {Springer International Publishing},
	author = {Ramoğlu, Muhammet and Genç, Çağlar and Rızvanoğlu, Kerem},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2017},
	keywords = {Block coding, Mobile application, Robotic toys, Usability test, Visual programming},
	pages = {652--666},
	file = {Full Text PDF:files/207/Ramoğlu et al. - 2017 - Programming a Robotic Toy with a Block Coding Application A Usability Study with Non-programmer Adu.pdf:application/pdf},
}

@inproceedings{hess_large-scale_2018,
	address = {Cham},
	title = {Large-{Scale} {Stochastic} {Scene} {Generation} and {Semantic} {Annotation} for {Deep} {Convolutional} {Neural} {Network} {Training} in the {RoboCup} {SPL}},
	isbn = {978-3-030-00308-1},
	doi = {10.1007/978-3-030-00308-1_3},
	abstract = {Object detection and classification are essential tasks for any robotics scenario, where data-driven approaches, specifically deep learning techniques, have been widely adopted in recent years. However, in the context of the RoboCup standard platform league these methods have not yet gained comparable popularity in large part due to the lack of (publicly) available large enough data sets that involve a tedious gathering and error-prone manual annotation process. We propose a framework for stochastic scene generation, rendering and automatic creation of semantically annotated ground truth masks. Used as training data in conjunction with deep convolutional neural networks we demonstrate compelling classification accuracy on real-world data in a multi-class setting. An evaluation on multiple neural network architectures with varying depth and representational capacity, corresponding run-times on current NAO-H25 hardware, and required sampled training data is provided.},
	language = {en},
	booktitle = {{RoboCup} 2017: {Robot} {World} {Cup} {XXI}},
	publisher = {Springer International Publishing},
	author = {Hess, Timm and Mundt, Martin and Weis, Tobias and Ramesh, Visvanathan},
	editor = {Akiyama, Hidehisa and Obst, Oliver and Sammut, Claude and Tonidandel, Flavio},
	year = {2018},
	keywords = {Deep Convolutional Neural Networks, RoboCup SPL, Robotics, Standard Platform League (SPL), Static Head Pose},
	pages = {33--44},
	file = {Full Text PDF:files/210/Hess et al. - 2018 - Large-Scale Stochastic Scene Generation and Semantic Annotation for Deep Convolutional Neural Networ.pdf:application/pdf},
}

@inproceedings{kootbally_enabling_2014,
	address = {Berlin, Heidelberg},
	title = {Enabling {Codesharing} in {Rescue} {Simulation} with {USARSim}/{ROS}},
	isbn = {978-3-662-44468-9},
	doi = {10.1007/978-3-662-44468-9_54},
	abstract = {The Robot Operating System (ROS) has been steadily gaining popularity among robotics researchers as an open source framework for robot control. The Unified System for Automation and Robot Simulation (USARSim) has been used for many years by robotics researchers and developers as a validated framework for simulation. This paper presents a new ROS node that is designed to seamlessly interface between ROS and USARSim. It provides for automatic configuration of ROS transforms and topics to allow for full utilization of the simulated hardware. The design of the new node as well as examples of its use for mobile robot inside the RoboCup Rescue Simulation League are presented.},
	language = {en},
	booktitle = {{RoboCup} 2013: {Robot} {World} {Cup} {XVII}},
	publisher = {Springer},
	author = {Kootbally, Zeid and Balakirsky, Stephen and Visser, Arnoud},
	editor = {Behnke, Sven and Veloso, Manuela and Visser, Arnoud and Xiong, Rong},
	year = {2014},
	keywords = {Game Engine, Game Server, Mobile Robot, Robot Operating System, Virtual Robot},
	pages = {592--599},
	file = {Full Text PDF:files/213/Kootbally et al. - 2014 - Enabling Codesharing in Rescue Simulation with USARSimROS.pdf:application/pdf},
}

@inproceedings{dos_santos_nunes_interaction_2019,
	address = {Cham},
	title = {Interaction {Techniques} in {Three}-{Dimensional} {Virtual} {Environments} {Based} on {Games} to {Support} {Chronic} {Diseases} {Treatment}: {A} {Systematic} {Review}},
	isbn = {978-3-030-22602-2},
	shorttitle = {Interaction {Techniques} in {Three}-{Dimensional} {Virtual} {Environments} {Based} on {Games} to {Support} {Chronic} {Diseases} {Treatment}},
	doi = {10.1007/978-3-030-22602-2_25},
	abstract = {Games have quickly risen in culture, as well as serious games, which in addition to providing entertainment, also play the role of teaching resources. At the same time, the lack of information in children in relation to Diabetes has caused this disease to be treated with neglect. This paper presents the results of a Systematic Review on interaction and immersion strategies applied in Virtual Environments based on games or Virtual Reality proposed for children and adolescents in the context of the treatment of chronic diseases such as Diabetes. From the results obtained with the SR, it was possible to identify the main techniques that were applied to design the user-centered design project, including the experience of the stakeholders and their needs.},
	language = {en},
	booktitle = {{HCI} in {Games}},
	publisher = {Springer International Publishing},
	author = {dos Santos Nunes, Eunice P. and Gutierrez, Gabriel A. and Santos, Dayany A. C. and Viterbo, José and Trevisan, Daniela and Maciel, Cristiano and de Faria Borges, Luciana C. Lima},
	editor = {Fang, Xiaowen},
	year = {2019},
	keywords = {Chronic diseases, Diabetes, Serious game, Virtual Reality},
	pages = {333--350},
	file = {Full Text PDF:files/216/dos Santos Nunes et al. - 2019 - Interaction Techniques in Three-Dimensional Virtual Environments Based on Games to Support Chronic D.pdf:application/pdf},
}

@inproceedings{warren_linked_2014,
	address = {Cham},
	title = {Linked {Open} {Data} {Driven} {Game} {Generation}},
	isbn = {978-3-319-11915-1},
	doi = {10.1007/978-3-319-11915-1_23},
	abstract = {Linked Open Data provides a means of unified access to large and complex interconnected data sets that concern themselves with a surprising breath and depth of topics. This unified access in turn allows for the consumption of this data for modelling cultural heritage sites, historical events or creating serious games. In the following paper we present our work on simulating the terrain of a Great War battle using data from multiple Linked Open Data projects.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2014},
	publisher = {Springer International Publishing},
	author = {Warren, Rob and Champion, Erik},
	editor = {Mika, Peter and Tudorache, Tania and Bernstein, Abraham and Welty, Chris and Knoblock, Craig and Vrandečić, Denny and Groth, Paul and Noy, Natasha and Janowicz, Krzysztof and Goble, Carole},
	year = {2014},
	keywords = {Consuming Linked Open Data, Serious Games, Simulations},
	pages = {358--373},
	file = {Full Text PDF:files/166/Warren e Champion - 2014 - Linked Open Data Driven Game Generation.pdf:application/pdf},
}

@inproceedings{gutierrez_faars_2012,
	address = {Berlin, Heidelberg},
	title = {{fAARS}: {A} {Platform} for {Location}-{Aware} {Trans}-reality {Games}},
	isbn = {978-3-642-33542-6},
	shorttitle = {{fAARS}},
	doi = {10.1007/978-3-642-33542-6_16},
	abstract = {Users today can easily and intuitively record their real-world experiences through mobile devices, and commodity virtual worlds enable users from around the world to socialize in the context of realistic environments where they simulate real-world activities. This synergy of technological advances makes the design and implementation of trans-reality games, blending the boundaries of the real and virtual worlds, a compelling software-engineering problem. In this paper, we describe fAARS, a platform for developing and deploying trans-reality games that cut across the real and parallel virtual worlds, offering users a range of game-play modalities. We place fAARS in the context of recent related work, and we demonstrate its capabilities by discussing two different games developed on it, one with three different variants.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2012},
	publisher = {Springer},
	author = {Gutierrez, Lucio and Stroulia, Eleni and Nikolaidis, Ioanis},
	editor = {Herrlich, Marc and Malaka, Rainer and Masuch, Maic},
	year = {2012},
	keywords = {Game platform, mobile games, trans-reality games, virtual worlds},
	pages = {185--192},
	file = {Full Text PDF:files/169/Gutierrez et al. - 2012 - fAARS A Platform for Location-Aware Trans-reality Games.pdf:application/pdf},
}

@inproceedings{dos_anjos_mappets_2013,
	address = {Berlin, Heidelberg},
	title = {Mappets: {An} {Interactive} {Plugin} for {Transmedia} {Machinima} on {Unity3D}},
	isbn = {978-3-642-41106-9},
	shorttitle = {Mappets},
	doi = {10.1007/978-3-642-41106-9_8},
	abstract = {The popularity of Machinima movies has increased greatly in the recent years. From a transmedia point of view, there was little development regarding tools to assist the production of Machinima. These are still mainly focused on the gaming community, and 3D animators. The developed tool aims to bring the typical workflow present on a normal movie set, into a machinima creation environment, expanding possibilities for transmedia productions. With Mappets as a plugin for the Unity3D game engine, we allow a translation from the typical movie dimension to a virtual one. This work evaluates the current state of art of machinima development tools and presents a working solution more adequate for transmedia productions and non-expert users interested in the production of machinima.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2013},
	publisher = {Springer},
	author = {dos Anjos, Rafael Kuffner and Di Tullio, Eugenio and Prada, Rui},
	editor = {Anacleto, Junia C. and Clua, Esteban W. G. and da Silva, Flavio S. Correa and Fels, Sidney and Yang, Hyun S.},
	year = {2013},
	keywords = {Artificial Inteligence, Authoring System, Digital Entretainment, Entertainment, Entretainment Technology, Machinima, Narratives, TransMedia},
	pages = {69--74},
	file = {Full Text PDF:files/171/dos Anjos et al. - 2013 - Mappets An Interactive Plugin for Transmedia Machinima on Unity3D.pdf:application/pdf},
}

@inproceedings{sarinho_feature-based_2012,
	address = {Berlin, Heidelberg},
	title = {A {Feature}-{Based} {Environment} for {Digital} {Games}},
	isbn = {978-3-642-33542-6},
	doi = {10.1007/978-3-642-33542-6_67},
	abstract = {Digital games can be considered as an important software development area in our society. This paper proposes the Object Oriented Feature Modeling (OOFM) usage in the digital game domain. It aims to represent and manipulate distinct game features, defined by NESI and GDS models, in a parameterized and hierarchical way. As a result, a Feature-based Environment for Digital Games (FEnDiGa) is provided, a product line platform able to integrate and adapt represented game features in different types of available game engines.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2012},
	publisher = {Springer},
	author = {Sarinho, Victor T. and Apolinário, Antônio L. and Almeida, Eduardo S.},
	editor = {Herrlich, Marc and Malaka, Rainer and Masuch, Maic},
	year = {2012},
	keywords = {Digital game domain, FEnDiGa, game features, OOFM},
	pages = {518--523},
	file = {Full Text PDF:files/174/Sarinho et al. - 2012 - A Feature-Based Environment for Digital Games.pdf:application/pdf},
}

@inproceedings{souza_choosing_2014,
	address = {Cham},
	title = {Choosing a {Selection} {Technique} for a {Virtual} {Environment}},
	isbn = {978-3-319-07458-0},
	doi = {10.1007/978-3-319-07458-0_21},
	abstract = {Bearing in mind the difficulty required to create virtual environments, a platform for Setting-up Interactive Virtual Environments (pSIVE) was created to help non-specialists benefit from virtual applications involving virtual tours where users may interact with elements of the environment to extract contextual information. The platform allows creating virtual environments and setting up their aspects, interaction methods and hardware to be used. The construction of the world is done by loading 3D models and associating multimedia information (videos, texts or PDF documents) to them.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Designing} and {Developing} {Virtual} and {Augmented} {Environments}},
	publisher = {Springer International Publishing},
	author = {Souza, Danilo and Dias, Paulo and Sousa Santos, Beatriz},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2014},
	keywords = {Selection, Virtual Environments, Virtual Reality},
	pages = {215--225},
	file = {Full Text PDF:files/177/Souza et al. - 2014 - Choosing a Selection Technique for a Virtual Environment.pdf:application/pdf},
}

@inproceedings{carpin_bridging_2007,
	address = {Berlin, Heidelberg},
	title = {Bridging the {Gap} {Between} {Simulation} and {Reality} in {Urban} {Search} and {Rescue}},
	isbn = {978-3-540-74024-7},
	doi = {10.1007/978-3-540-74024-7_1},
	abstract = {Research efforts in urban search and rescue grew tremendously in recent years. In this paper we illustrate a simulation software that aims to be the meeting point between the communities of researchers involved in robotics and multi-agent systems. The proposed system allows the realistic modeling of robots, sensors and actuators, as well as complex unstructured dynamic environments. Multiple heterogeneous agents can be concurrently spawned inside the environment. We explain how different sensors and actuators have been added to the system and show how a seamless migration of code between real and simulated robots is possible. Quantitative results supporting the validation of simulation accuracy are also presented.},
	language = {en},
	booktitle = {{RoboCup} 2006: {Robot} {Soccer} {World} {Cup} {X}},
	publisher = {Springer},
	author = {Carpin, Stefano and Lewis, Mike and Wang, Jijun and Balakirsky, Steve and Scrapper, Chris},
	editor = {Lakemeyer, Gerhard and Sklar, Elizabeth and Sorrenti, Domenico G. and Takahashi, Tomoichi},
	year = {2007},
	keywords = {Messaging Protocol, RoboCup Rescue, Simulated Robot, Virtual Robot, World Modeling},
	pages = {1--12},
	file = {Full Text PDF:files/179/Carpin et al. - 2007 - Bridging the Gap Between Simulation and Reality in Urban Search and Rescue.pdf:application/pdf},
}

@inproceedings{herrlich_development_2010,
	address = {Berlin, Heidelberg},
	title = {Development of a {Virtual} {Electric} {Wheelchair} – {Simulation} and {Assessment} of {Physical} {Fidelity} {Using} the {Unreal} {Engine} 3},
	isbn = {978-3-642-15399-0},
	doi = {10.1007/978-3-642-15399-0_29},
	abstract = {This paper demonstrates how an existing game technology as a component off-the-shelf can be used as a basis to build a serious game for assistive technology for disabled people.Using the example of a virtual electric wheelchair simulator, we present how to use a computer game physics engine to achieve a realistic simulation of driving an electric wheelchair in a virtual environment. Focus of the paper is the conversion of driving characteristics of prevalently used electric wheelchairs into the virtual physics system of the used computer game engine. The used parameters are systematically balanced between the virtual and the real world to evaluate the realism of the driving characteristics of an electric wheelchair using the integrated physics simulation of the Unreal Engine 3.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2010},
	publisher = {Springer},
	author = {Herrlich, Marc and Meyer, Ronald and Malaka, Rainer and Heck, Helmut},
	editor = {Yang, Hyun Seung and Malaka, Rainer and Hoshino, Junichi and Han, Jung Hyun},
	year = {2010},
	keywords = {COTS, physics simulation, serious games, virtual electric wheelchair},
	pages = {286--293},
	file = {Full Text PDF:files/181/Herrlich et al. - 2010 - Development of a Virtual Electric Wheelchair – Simulation and Assessment of Physical Fidelity Using.pdf:application/pdf},
}

@inproceedings{thawonmas_rule-based_2010,
	address = {Berlin, Heidelberg},
	title = {Rule-{Based} {Camerawork} {Controller} for {Automatic} {Comic} {Generation} from {Game} {Log}},
	isbn = {978-3-642-15399-0},
	doi = {10.1007/978-3-642-15399-0_34},
	abstract = {We propose a rule-based camerawork controller for a recently proposed a comic generation system. Five camerawork rules are derived through an analysis of online-game webcomics about Lineage 2, one rule for each of the five event types: chatting, fighting, moving, approaching, and special. Each rule consists of three parts relating to the three camera parameters: camera angle, camera position, and zoom position. Each camera-parameter part contains multiples shot types whose value indicates the frequency of their usages in the analyzed webcomics. In this paper, comic frames generated with the proposed camerawork controller are shown and compared with those generated with our previous controller based on heuristic rules, confirming the effectiveness of the proposed camerawork controller.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2010},
	publisher = {Springer},
	author = {Thawonmas, Ruck and Oda, Ko and Shuda, Tomonori},
	editor = {Yang, Hyun Seung and Malaka, Rainer and Hoshino, Junichi and Han, Jung Hyun},
	year = {2010},
	keywords = {Camera Angle, Camera Parameter, Camera Position, Event Type, Game Engine},
	pages = {326--333},
	file = {Full Text PDF:files/183/Thawonmas et al. - 2010 - Rule-Based Camerawork Controller for Automatic Comic Generation from Game Log.pdf:application/pdf},
}

@inproceedings{kickmeier-rust_immersive_2007,
	address = {Berlin, Heidelberg},
	title = {Immersive {Digital} {Games}: {The} {Interfaces} for {Next}-{Generation} {E}-{Learning}?},
	isbn = {978-3-540-73283-9},
	shorttitle = {Immersive {Digital} {Games}},
	doi = {10.1007/978-3-540-73283-9_71},
	abstract = {The intrinsic motivation to play, and therefore to learn, that might be provided by digital educational games teases researchers and developers. However, existing educational games often fail in their attempt to compete with commercial games and to provide successful learning. Often some learning is added to digital games or some gameplay is added to educational applications. Successful educational games, however, require merging professional game design with sound pedagogical strategies, creating a new hybrid format. Moreover, a methodology is required that allows continuously balancing gaming and learning challenges and the learner’s abilities and knowledge in order to retain an immersive gaming experience. In this article we introduce approaches to game design and didactic design, as well as a framework for adaptive interventions in educational games.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Applications} and {Services}},
	publisher = {Springer},
	author = {Kickmeier-Rust, Michael D. and Peirce, Neil and Conlan, Owen and Schwarz, Daniel and Verpoorten, Dominique and Albert, Dietrich},
	editor = {Stephanidis, Constantine},
	year = {2007},
	keywords = {Adaptive intervention, Competence-based Knowledge Space Theory, Didactic design, Digital educational games, Game design, Microadaptivity},
	pages = {647--656},
	file = {Full Text PDF:files/184/Kickmeier-Rust et al. - 2007 - Immersive Digital Games The Interfaces for Next-Generation E-Learning.pdf:application/pdf},
}

@inproceedings{moreira_empirical_2013,
	address = {Berlin, Heidelberg},
	title = {An {Empirical} {Study} on {Immersive} {Prototyping} {Dimensions}},
	isbn = {978-3-642-39232-0},
	doi = {10.1007/978-3-642-39232-0_46},
	abstract = {Many aspects of the human experience of ubiquitous computing in built environments must be explored in the context of the target environment. However, delaying evaluation until a version of the system can be deployed can make redesign too costly. Prototypes have the potential to solve this problem by enabling evaluation before actual deployment. This paper presents a study of the design space of immersive prototyping for ubiquitous computing. It provides a framework to guide the alignment between specific evaluation goals and specific prototype properties. The goal is to understand the potential added-value of 3D simulation as a prototyping tool in the development process of ubiquitous computing environments.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Human}-{Centred} {Design} {Approaches}, {Methods}, {Tools}, and {Environments}},
	publisher = {Springer},
	author = {Moreira, Samuel and José, Rui and Campos, José Creissac},
	editor = {Kurosu, Masaaki},
	year = {2013},
	keywords = {3D environments, prototyping, ubiquitous computing},
	pages = {421--430},
	file = {Full Text PDF:files/186/Moreira et al. - 2013 - An Empirical Study on Immersive Prototyping Dimensions.pdf:application/pdf},
}

@inproceedings{sko_head_2009,
	address = {Berlin, Heidelberg},
	title = {Head {Tracking} in {First}-{Person} {Games}: {Interaction} {Using} a {Web}-{Camera}},
	isbn = {978-3-642-03655-2},
	shorttitle = {Head {Tracking} in {First}-{Person} {Games}},
	doi = {10.1007/978-3-642-03655-2_38},
	abstract = {Recent advances in face-tracking technology have made it possible to recognize head movements using a commodity web-camera. This development has created exciting possibilities for enhancing player enjoyment during computer game play. In order to ascertain the real-world potential for head gestural input to First Person Shooter games, we have developed seven diverse interaction techniques and integrated these with a modern games engine. Evaluation of the techniques was carried out with four focus groups made up of expert games developers and experienced end-users. One of the techniques was further refined and subjected to a follow-up comparison test with promising results. A set of guidelines for the future development of head interaction techniques for computer games has been derived from the studies. All of the techniques have been built upon freely available software and open-sourced to encourage further research in this area.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2009},
	publisher = {Springer},
	author = {Sko, Torben and Gardner, Henry J.},
	editor = {Gross, Tom and Gulliksen, Jan and Kotzé, Paula and Oestreicher, Lars and Palanque, Philippe and Prates, Raquel Oliveira and Winckler, Marco},
	year = {2009},
	keywords = {computer game, face tracking, first person shooter, game engine, head tracking, Input and interaction technology},
	pages = {342--355},
	file = {Full Text PDF:files/208/Sko e Gardner - 2009 - Head Tracking in First-Person Games Interaction Using a Web-Camera.pdf:application/pdf},
}

@inproceedings{yongliang_exploration_2019,
	address = {Cham},
	title = {Exploration of the {Interactive} {Narrative} {Modes} and {Application} {Form} of {AR} {Animations}},
	isbn = {978-3-030-23541-3},
	doi = {10.1007/978-3-030-23541-3_37},
	abstract = {The purpose of this paper is to explore the interactive narratives and applications of VR animations. Comparative study, literature survey, case study and other methods are adopted. It is demonstrated in the research that, in addition to the improved immersion, VR technology will surely cultivate an artistic experience of animation arts totally different from traditional ones. The paper thus concludes that the strong interactive capability of game engines provides unlimited possibilities for the future interactive design of VR animations while the technical innovation will promote the reform of interactive narration, which will embrace a great many approaches in forms, pushing itself to a deeper level. For example, the control of perspectives, the adjustment of story volume, the manipulation of narrative rhythms and the selection of endings will all help form a vigorous and personalized interactive narration and really hand over the story to the audience.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {User} {Experience} in {Advanced} {Technological} {Environments}},
	publisher = {Springer International Publishing},
	author = {Yongliang, Shen and Bosi, Niu},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2019},
	keywords = {Animation, Application form, Interactive mode, VR technology},
	pages = {517--528},
	file = {Full Text PDF:files/211/Yongliang e Bosi - 2019 - Exploration of the Interactive Narrative Modes and Application Form of AR Animations.pdf:application/pdf},
}

@inproceedings{ajisafe_development_2019,
	address = {Cham},
	title = {Development and {Usability} of a {Low}-{Cost} {Kinect} {Game} to {Promote} {Movement} {Competence} in {Children} with and {Without} {Intellectual} {Disability}},
	isbn = {978-3-030-22602-2},
	doi = {10.1007/978-3-030-22602-2_19},
	abstract = {Many children fail to meet recommendations for daily physical activity (PA). Skills like jumping and hopping indicate movement competence may positively shape PA trajectory. Because these skills are learned, it is important that children are exposed to them early. This paper presents the development and usability assessment of a low-cost 2.5D Kinect scroller obstacle avoidance game that integrates player ability and makes game object spatial properties editable using a text file. Seven children (age ± SD: 5.7 ± 1.5 years; height: 117.3 ± 12.4 cm; mass: 24.4 ± 8.0 kg) participated in the assessment. The game was developed using Unity game engine. The input device was Kinect v2. The objective of the game is to negotiate obstacles along the travel path and cross the finish line. Participants avoided obstacles by jumping, hopping, ducking, and sliding. Participants answered questions related to gameplay difficulty, discomfort, and desire to play the game again. Eighty six percent of participants had positive general impressions of the game, 14\% reported feeling dizzy or experiencing pain/discomfort during gameplay, and 86\% reported jumping as the difficult movement during gameplay. All participants reported that they would play again and felt the game could help them. It is concluded that young children found the game appealing and physically beneficial. Since the game is intended to help children practice joint stiffness regulation and improve their movement competence, future assessments should determine the longitudinal effect of exposure to the game on these parameters.},
	language = {en},
	booktitle = {{HCI} in {Games}},
	publisher = {Springer International Publishing},
	author = {Ajisafe, Toyin and Bethi, Rahul and King, Scott A. and Katangur, Ajay},
	editor = {Fang, Xiaowen},
	year = {2019},
	keywords = {Kinect, Leg stiffness, Locomotor skills, Movement competence, Physical activity},
	pages = {245--264},
	file = {Full Text PDF:files/214/Ajisafe et al. - 2019 - Development and Usability of a Low-Cost Kinect Game to Promote Movement Competence in Children with.pdf:application/pdf},
}

@inproceedings{redondo_edugame4city_2020,
	address = {Cham},
	title = {{EDUGAME4CITY}. {A} {Gamification} for {Architecture} {Students}. {Viability} {Study} {Applied} to {Urban} {Design}},
	isbn = {978-3-030-50506-6},
	doi = {10.1007/978-3-030-50506-6_22},
	abstract = {Hereby we present the first results of a Financed Research Project that focuses on the use of (Serious games) SG for training future architects and urban planner in the urban design context. After the first viability studies and tests in the creation of urban scenarios at intermediate scale, virtual simulation, vegetative environment and lighting, as well as basic gamification, the complete development of the teacher’s methodology proposal of the research project is outlined. First five urban scenarios have been selected in which the students of Máster y Grado de Arquitectura of the ETSABarcelona-UPC of landscape and urban design have developed of urban intervention projects from the local administration and neighborhood associations. These projects have been completed subsequently in different optional courses ICT, in which 3-D virtual scenarios have been created and rehearsed under a first basic gamification. Following with the project it has continued with other courses of Architectonic Representation degree, where final settings and designs have been created, according to the students´ interest in using Virtual Reality Technology. Afterwards the virtual models and scenarios were transferred to La Salle-Barcelona URL Architecture students in different courses of Representation tools, in order to continue with advanced gamification, HMD and analysis of its usage and their motivation. The results of these instructed processes have reached the administration, neighbors, professionals and general public.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Human} and {Technology} {Ecosystems}},
	publisher = {Springer International Publishing},
	author = {Redondo, Ernest and Fonseca, David and Sánchez-Sepúlveda, Mónica and Zapata, Héctor and Navarro, Isidro and Gimenez, Lluís and Pérez, Miguel Angel},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2020},
	keywords = {Educational research, Gamification, Interactive learning environment, Urban design},
	pages = {296--314},
	file = {Full Text PDF:files/217/Redondo et al. - 2020 - EDUGAME4CITY. A Gamification for Architecture Students. Viability Study Applied to Urban Design.pdf:application/pdf},
}

@inproceedings{dasgupta_user-centric_2019,
	address = {Cham},
	title = {A {User}-{Centric} {Design} {Framework} for {Smart} {Built} {Environments}},
	isbn = {978-3-030-21935-2},
	doi = {10.1007/978-3-030-21935-2_11},
	abstract = {Smart Built Environments (SBEs) empowered by the Internet of Things (IoT) dramatically augment the capabilities of traditional built environments by imbuing everyday objects with computational and communication capabilities. SBEs primarily consist of three types of components: architectural elements, embedded technology (smart objects) and enhanced interaction modalities. As smart objects hold the ability to change the state of the environment, inefficient design of smart configurations can lead to potentially harmful conditions affecting the safety and security of the inhabitants. The interaction scenarios and space use pattern of SBEs are also notably different from traditional built environments. But, to the best of our knowledge, there has been limited work on developing a consolidated design framework addressing the three interdependent SBE elements and evaluating the safety and security of the IoT application environment. We propose an SBE design framework based on the traditional architectural design process. The framework combines the technological aspects of SBEs with the traditional architectural design process while leveraging Building Information Modeling (BIM) and participatory design. We describe a Mixed Reality(MR)-based reference framework implementation that is particularly helpful for representing, visualizing and modeling the vast amount of data, digital components and novel SBE interaction scenarios.},
	language = {en},
	booktitle = {Distributed, {Ambient} and {Pervasive} {Interactions}},
	publisher = {Springer International Publishing},
	author = {Dasgupta, Archi and Handosa, Mohamed and Manuel, Mark and Gračanin, Denis},
	editor = {Streitz, Norbert and Konomi, Shin’ichi},
	year = {2019},
	keywords = {Human computer interaction, Human-centered computing, Internet of Things, Mixed Reality, Smart Built Environment},
	pages = {124--143},
	file = {Full Text PDF:files/219/Dasgupta et al. - 2019 - A User-Centric Design Framework for Smart Built Environments.pdf:application/pdf},
}

@inproceedings{lampen_context-aware_2020,
	address = {Cham},
	title = {A {Context}-{Aware} {Assistance} {Framework} for {Implicit} {Interaction} with an {Augmented} {Human}},
	isbn = {978-3-030-49698-2},
	doi = {10.1007/978-3-030-49698-2_7},
	abstract = {The automotive industry is currently facing massive challenges. Shorter product life cycles together with mass customization lead to a high complexity for manual assembly tasks. This induces the need for effective manual assembly assistances which guide the worker faultlessly through different assembly steps while simultaneously decrease their completion time and cognitive load. While in the literature a simulation-based assistance visualizing an augmented digital human was proposed, it lacks the ability to incorporate knowledge about the context of an assembly scenario through arbitrary sensor data. Within this paper, a general framework for the modular acquisition, interpretation and management of context is presented. Furthermore, a novel context-aware assistance application in augmented reality is introduced which enhances the previously proposed simulation-based assistance method by several context-aware features. Finally, a preliminary study (N = 6) is conducted to give a first insight into the effectiveness of context-awareness for the simulation-based assistance with respect to subjective perception criteria. The results suggest that the user experience is improved by context-awareness in general and the developed context-aware features were overall perceived as useful in terms of error, time and cognitive load reduction as well as motivational increase. However, the developed software architecture offers potential for improvement and future research considering performance parameters is mandatory.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Industrial} and {Everyday} {Life} {Applications}},
	publisher = {Springer International Publishing},
	author = {Lampen, Eva and Lehwald, Jannes and Pfeiffer, Thies},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Assistance, Augmented reality, Context-awareness, Human computer interaction, Human simulation},
	pages = {91--110},
	file = {Full Text PDF:files/221/Lampen et al. - 2020 - A Context-Aware Assistance Framework for Implicit Interaction with an Augmented Human.pdf:application/pdf},
}

@inproceedings{ye_interpretable_2018,
	address = {Cham},
	title = {Interpretable {Intuitive} {Physics} {Model}},
	isbn = {978-3-030-01258-8},
	doi = {10.1007/978-3-030-01258-8_6},
	abstract = {Humans have a remarkable ability to use physical commonsense and predict the effect of collisions. But do they understand the underlying factors? Can they predict if the underlying factors have changed? Interestingly, in most cases humans can predict the effects of similar collisions with different conditions such as changes in mass, friction, etc. It is postulated this is primarily because we learn to model physics with meaningful latent variables. This does not imply we can estimate the precise values of these meaningful variables (estimate exact values of mass or friction). Inspired by this observation, we propose an interpretable intuitive physics model where specific dimensions in the bottleneck layers correspond to different physical properties. In order to demonstrate that our system models these underlying physical properties, we train our model on collisions of different shapes (cube, cone, cylinder, spheres etc.) and test on collisions of unseen combinations of shapes. Furthermore, we demonstrate our model generalizes well even when similar scenes are simulated with different underlying properties.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Ye, Tian and Wang, Xiaolong and Davidson, James and Gupta, Abhinav},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Interpretable models, Intuitive physics, Physical properties},
	pages = {89--105},
	file = {Full Text PDF:files/223/Ye et al. - 2018 - Interpretable Intuitive Physics Model.pdf:application/pdf},
}

@inproceedings{vidal_hand_2020,
	address = {Cham},
	title = {Hand {Gesture} {Recognition} for {Smartphone}-{Based} {Augmented} {Reality} {Applications}},
	isbn = {978-3-030-49695-1},
	doi = {10.1007/978-3-030-49695-1_23},
	abstract = {Hand Gesture Recognition (HGR) is a principal input method in head-mounted Augmented Reality (AR) systems such as HoloLens, but the high cost and limited availability of such systems prevent HGR from becoming more prevalent. Alternatively, smartphones can be used to provide AR experiences, but current smartphones were not designed with HGR in mind, making development of HGR applications more challenging. This study develops a software-based framework that implements HGR as a principal input method for smartphone AR applications. This framework assumes a contemporary smartphone with dual back-facing cameras, which enable stereo imaging and thus allow extraction of limited depth information from the environment. Several image processing techniques, derived and improved from previous work, were used to filter the noisy depth information to segment the user’s hand from the rest of the environment, and then to extract the pose of the hand and fingers in real-time. The framework additionally facilitates the development of cross-platform AR applications for both head-mounted (HoloLens) and smartphone configurations. A user experiment is held to determine whether a smartphone-based AR application developed using our HGR framework is comparable in usability to the same application on the HoloLens. For each device, participants were asked to use the application and fill out a usability questionnaire. They were also asked to compare the two systems at the end. This experiment shows that, despite the current limitations of smartphone-based HGR, the smartphone system’s usability is competitive with that of the HoloLens. This study ends with recommendations for future development.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Design} and {Interaction}},
	publisher = {Springer International Publishing},
	author = {Vidal, Eric Cesar E. and Rodrigo, Ma. Mercedes T.},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Augmented Reality, Hand Gesture Recognition, Image processing, Usability},
	pages = {346--366},
	file = {Full Text PDF:files/246/Vidal e Rodrigo - 2020 - Hand Gesture Recognition for Smartphone-Based Augmented Reality Applications.pdf:application/pdf},
}

@inproceedings{hepp_learn--score_2018,
	address = {Cham},
	title = {Learn-to-{Score}: {Efficient} {3D} {Scene} {Exploration} by {Predicting} {View} {Utility}},
	isbn = {978-3-030-01267-0},
	shorttitle = {Learn-to-{Score}},
	doi = {10.1007/978-3-030-01267-0_27},
	abstract = {Camera equipped drones are nowadays being used to explore large scenes and reconstruct detailed 3D maps. When free space in the scene is approximately known, an offline planner can generate optimal plans to efficiently explore the scene. However, for exploring unknown scenes, the planner must predict and maximize usefulness of where to go on the fly. Traditionally, this has been achieved using handcrafted utility functions. We propose to learn a better utility function that predicts the usefulness of future viewpoints. Our learned utility function is based on a 3D convolutional neural network. This network takes as input a novel volumetric scene representation that implicitly captures previously visited viewpoints and generalizes to new scenes. We evaluate our method on several large 3D models of urban scenes using simulated depth cameras. We show that our method outperforms existing utility measures in terms of reconstruction performance and is robust to sensor noise.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Hepp, Benjamin and Dey, Debadeepta and Sinha, Sudipta N. and Kapoor, Ashish and Joshi, Neel and Hilliges, Otmar},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {3D CNN, 3D reconstruction, Active vision, Exploration},
	pages = {455--472},
	file = {Full Text PDF:files/250/Hepp et al. - 2018 - Learn-to-Score Efficient 3D Scene Exploration by Predicting View Utility.pdf:application/pdf},
}

@inproceedings{osudin_rendering_2019,
	address = {Cham},
	title = {Rendering {Non}-{Euclidean} {Space} in {Real}-{Time} {Using} {Spherical} and {Hyperbolic} {Trigonometry}},
	isbn = {978-3-030-22750-0},
	doi = {10.1007/978-3-030-22750-0_49},
	abstract = {We introduce a method of calculating and rendering shapes in a non-Euclidean 2D space in real-time using hyperbolic and spherical trigonometry. We record the objects’ parameters in a polar coordinate system and use azimuthal equidistant projection to render the space onto the screen. We discuss the complexity of this method, renderings produced, limitations and possible applications of the created software as well as potential future developments.},
	language = {en},
	booktitle = {Computational {Science} – {ICCS} 2019},
	publisher = {Springer International Publishing},
	author = {Osudin, Daniil and Child, Chris and He, Yang-Hui},
	editor = {Rodrigues, João M. F. and Cardoso, Pedro J. S. and Monteiro, Jânio and Lam, Roberto and Krzhizhanovskaya, Valeria V. and Lees, Michael H. and Dongarra, Jack J. and Sloot, Peter M.A.},
	year = {2019},
	keywords = {Azimuthal equidistant projection, Hyperbolic trigonometry, non-Euclidean geometry, Polar coordinate system, Real-time, Spherical trigonometry},
	pages = {543--550},
	file = {Full Text PDF:files/254/Osudin et al. - 2019 - Rendering Non-Euclidean Space in Real-Time Using Spherical and Hyperbolic Trigonometry.pdf:application/pdf},
}

@inproceedings{park_multiplatform_2016,
	address = {Cham},
	title = {Multiplatform {Game} {Type} of {Health} {Survey} on {Cancer} {Patient}’s {Stress} {Level}},
	isbn = {978-3-319-40542-1},
	doi = {10.1007/978-3-319-40542-1_54},
	abstract = {Monitoring the changing condition of cancer patients is a significant part of their treatment. Stress coming from the patient’s current situation may have a great effect on the spread of cancer cells. Thus, a cancer patient’s stress level is a crucial factor for doctors to refer to when deciding for an appropriate treatment. In order to gather these data, “Distress Thermometer and Problem List” distributed by NCCN (National Comprehensive Cancer Network) is used globally, but the participation rate among patients is low. This is because the survey is very tedious and provides no feedback, which makes the patients feel like it is a waste of time. This game is made based on “Distress Thermometer and Problem List”, and has entered in the 2016 Game4health competition in Utah, USA under the name of “Measure Your Stress Level”. This game uses fun and cute animations to identify with the patient’s situation and patients can participate in the survey in a more entertaining way. Also, at the end of the survey, a stress relieving game is provided as a package in order to make the survey more engaging. The game is based on a multiplatform game engine so it can be outputted to smartphone, website, exe file making the game accessible anywhere, at any time. Thus, it will be easier for the medical staff to gather more data in less time.},
	language = {en},
	booktitle = {{HCI} {International} 2016 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Park, Seong Kuk and Jeong, Sang Rak and Kim, Dong Gyun and Kim, Jae Hee and Lim, Yang Kyu and Moon, Han Byul and Park, Jin Wan},
	editor = {Stephanidis, Constantine},
	year = {2016},
	keywords = {Distress survey, Game design, Multiplatform game},
	pages = {330--334},
	file = {Full Text PDF:files/209/Park et al. - 2016 - Multiplatform Game Type of Health Survey on Cancer Patient’s Stress Level.pdf:application/pdf},
}

@inproceedings{mateus_integration_2015,
	address = {Cham},
	title = {Integration of {Artificial} {Intelligence} {Techniques} in a {Virtual} {Environment}},
	isbn = {978-3-319-21380-4},
	doi = {10.1007/978-3-319-21380-4_36},
	abstract = {In this article, two artificial intelligence techniques such as Artificial Neural Networks and Genetic Algorithms were incorporated into a 3D working environment and turned into a game engine, which simulates a working environment in order to obtain possible warning signs to different hazards. These techniques were incorporated in the perception and reasoning of a character in the virtual environment, in order to react intelligently to given warning signs.},
	language = {en},
	booktitle = {{HCI} {International} 2015 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Mateus, Sandra and Branch, John},
	editor = {Stephanidis, Constantine},
	year = {2015},
	keywords = {Artificial neural network, Genetic algorithm, Intelligent virtual environment},
	pages = {202--207},
	file = {Full Text PDF:files/212/Mateus e Branch - 2015 - Integration of Artificial Intelligence Techniques in a Virtual Environment.pdf:application/pdf},
}

@inproceedings{claudio_virtual_2013,
	address = {Berlin, Heidelberg},
	title = {Virtual {Environment} to {Treat} {Social} {Anxiety}},
	isbn = {978-3-642-39241-2},
	doi = {10.1007/978-3-642-39241-2_49},
	abstract = {The aim of our work is to propose a Virtual Reality solution to treat social anxiety, applying cognitive-behavioral therapies, that preserves the sense of immersion without requiring the use of expensive special purpose hardware. We have developed an application, called Virtual Spectators, that creates a simulation taking place in a virtual scenario inhabited by animated virtual humans whose behaviors are dynamically controlled by the therapist. To evaluate the effective usefulness of the tool from the point of view of the therapist, we performed an evaluation of the application with a set of these professionals familiarized with the use of exposure therapy. Their feedback was positive and they were enthusiastic about the possibility of using such a tool to support a session of exposure therapy.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {Health}, {Learning}, {Playing}, {Cultural}, and {Cross}-{Cultural} {User} {Experience}},
	publisher = {Springer},
	author = {Cláudio, Ana Paula and Carmo, Maria Beatriz and Pinheiro, Tânia and Esteves, Francisco and Lopes, Eder},
	editor = {Marcus, Aaron},
	year = {2013},
	keywords = {social anxiety, virtual humans, Virtual reality},
	pages = {442--451},
	file = {Full Text PDF:files/215/Cláudio et al. - 2013 - Virtual Environment to Treat Social Anxiety.pdf:application/pdf},
}

@inproceedings{thawonmas_comic_2008,
	address = {Boston, MA},
	title = {Comic {Layout} for {Automatic} {Comic} {Generation} from {Game} {Log}},
	isbn = {978-0-387-09701-5},
	doi = {10.1007/978-0-387-09701-5_10},
	abstract = {The paper presents our system for generating comics from game log. In particular, comic layout is focused. In order to achieve more comic-like expressivity, we extend an existing comic layout process proposed by Shamir et al. as follows. First, tiny frames are introduced for being placed vertically in the same row. Second, splash frames taking up space of several rows are introduced for emphasizing the corresponding frames. Third, slant frames are introduced for shooting events. Comic sequences generated with the proposed layout process and with the existing one are compared and discussed.},
	language = {en},
	booktitle = {New {Frontiers} for {Entertainment} {Computing}},
	publisher = {Springer US},
	author = {Thawonmas, Ruck and Shuda, Tomonori},
	editor = {Ciancarini, Paolo and Nakatsu, Ryohei and Rauterberg, Matthias and Roccetti, Marco},
	year = {2008},
	pages = {105--115},
	file = {Full Text PDF:files/218/Thawonmas e Shuda - 2008 - Comic Layout for Automatic Comic Generation from Game Log.pdf:application/pdf},
}

@inproceedings{blandford_knowledge_2008,
	address = {Berlin, Heidelberg},
	title = {Knowledge {Representation} {Environments}: {An} {Investigation} of the {CASSMs} between {Creators}, {Composers} and {Consumers}},
	isbn = {978-3-540-92698-6},
	shorttitle = {Knowledge {Representation} {Environments}},
	doi = {10.1007/978-3-540-92698-6_4},
	abstract = {Many systems form ‘chains’ whereby developers use one system (or ‘tool’) to create another system, for use by other people. For example, a web development tool is created by one development team then used by others to compose web pages for use by yet other people. Little work within Human-Computer Interaction (HCI) has considered how usability considerations propagate through such chains. In this paper, we discuss three-link chains involving people that we term Creators (commonly referred to as designers), Composers (users of the tool who compose artefacts for other users) and Consumers (end users of artefacts). We focus on usability considerations and how Creators can develop systems that are both usable themselves and also support Composers in producing further systems that Consumers can work with easily. We show how CASSM, an analytic evaluation method that focuses attention on conceptual structures for interactive systems, supports reasoning about the propagation of concepts through Creator-Composer-Consumer chains. We use as our example a knowledge representation system called Tallis, which includes specific implementations of these different perspectives. Tallis is promoting a development culture within which individuals are empowered to take on different roles in order to strengthen the ‘chain of comprehension’ between different user types.},
	language = {en},
	booktitle = {Engineering {Interactive} {Systems}},
	publisher = {Springer},
	author = {Blandford, Ann and Green, Thomas R. G. and Connell, Iain and Rose, Tony},
	editor = {Gulliksen, Jan and Harning, Morton Borup and Palanque, Philippe and van der Veer, Gerrit C. and Wesson, Janet},
	year = {2008},
	keywords = {CASSM, design chains, Usability evaluation methods},
	pages = {53--70},
	file = {Full Text PDF:files/220/Blandford et al. - 2008 - Knowledge Representation Environments An Investigation of the CASSMs between Creators, Composers an.pdf:application/pdf},
}

@inproceedings{zhou_user_2007,
	address = {Berlin, Heidelberg},
	title = {User {Studies} of a {Multiplayer} {First} {Person} {Shooting} {Game} with {Tangible} and {Physical} {Interaction}},
	isbn = {978-3-540-73335-5},
	doi = {10.1007/978-3-540-73335-5_80},
	abstract = {In this paper, we present a new immersive first-person shooting (FPS) game. Our system provides an intuitive way for the users to interact with the virtual world by physically moving around the real world and aiming freely with tangible objects. This encourages physical interaction between the players as they compete or collaborate with each other.},
	language = {en},
	booktitle = {Virtual {Reality}},
	publisher = {Springer},
	author = {Zhou, ZhiYing and Tedjokusumo, Jefry and Winkler, Stefan and Ni, Bingbing},
	editor = {Shumaker, Randall},
	year = {2007},
	keywords = {Augmented Reality, First Person Shooter, Physical Interaction, Tangible User Interface, Virtual Reality},
	pages = {738--747},
	file = {Full Text PDF:files/222/Zhou et al. - 2007 - User Studies of a Multiplayer First Person Shooting Game with Tangible and Physical Interaction.pdf:application/pdf},
}

@inproceedings{broy_3d-hudd_2015,
	address = {Cham},
	title = {{3D}-{HUDD} – {Developing} a {Prototyping} {Tool} for {3D} {Head}-{Up} {Displays}},
	isbn = {978-3-319-22723-8},
	doi = {10.1007/978-3-319-22723-8_24},
	abstract = {The ability of head-up displays (HUDs) to present information within the usual viewpoint of the user has led to a quick adoption in domains where attention is crucial, such as in the car. As HUDs employ 3D technology, further opportunities emerge: information can be structured and positioned in 3D space thus allowing important information to be perceived more easily and information can be registered with objects in the visual scene to communicate a relationship. This allows novel user interfaces to be built. As of today, however, no prototyping tools exist, that allow 3D UIs for HUDs to be sketched and tested prior to development. To close this gap, we report on the design and development of the 3D Head-Up Display Designer (3D-HUDD). In addition, we present an evaluation of the tool with 24 participants, comparing different input modalities and depth management modes.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2015},
	publisher = {Springer International Publishing},
	author = {Broy, Nora and Nefzger, Matthias and Alt, Florian and Hassib, Mariam and Schmidt, Albrecht},
	editor = {Abascal, Julio and Barbosa, Simone and Fetter, Mirko and Gross, Tom and Palanque, Philippe and Winckler, Marco},
	year = {2015},
	keywords = {Augmented Reality, Free Mode, Interaction Technique, Prototype Tool, Task Completion Time},
	pages = {300--318},
	file = {Full Text PDF:files/224/Broy et al. - 2015 - 3D-HUDD – Developing a Prototyping Tool for 3D Head-Up Displays.pdf:application/pdf},
}

@inproceedings{go_digital_2007,
	address = {Berlin, Heidelberg},
	title = {Digital {Game}-{Based} {Learning}: {An} {Agent} {Approach}},
	isbn = {978-3-540-73283-9},
	shorttitle = {Digital {Game}-{Based} {Learning}},
	doi = {10.1007/978-3-540-73283-9_65},
	abstract = {Digital game-based learning has proven to be a useful and cost-effective alternative to the traditional classroom-based experience. However, current digital learning methods for young learners fail to engage audiences accustomed to interactive media. Moreover, most edutainment games do not offer players a situated learning experience, and those few that do, do not leverage the immensely popular online game market. This paper introduces a Belief Desire Intention (BDI) agent architecture for an online game Non-Player Character that encourages and stimulates situational learning in an online Role-Playing Game.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Applications} and {Services}},
	publisher = {Springer},
	author = {Go, Christian Anthony L. and Lee, Won-Hyung},
	editor = {Stephanidis, Constantine},
	year = {2007},
	keywords = {BDI Agent, Digital Game-Based Learning, Intelligent Agent},
	pages = {588--597},
	file = {Full Text PDF:files/245/Go e Lee - 2007 - Digital Game-Based Learning An Agent Approach.pdf:application/pdf},
}

@inproceedings{westin_advances_2011,
	address = {Berlin, Heidelberg},
	title = {Advances in {Game} {Accessibility} from 2005 to 2010},
	isbn = {978-3-642-21663-3},
	doi = {10.1007/978-3-642-21663-3_43},
	abstract = {The research in the area of game accessibility has grown significantly since the last time it was examined in 2005. This paper examines the body of work between 2005 and 2010. We selected a set of papers on topics we felt represented the scope of the field, but were not able to include all papers on the subject. A summary of the research we examined is provided, along with suggestions for future work in game accessibility. It is hoped that this summary will prompt others to perform further research in this area.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Users} {Diversity}},
	publisher = {Springer},
	author = {Westin, Thomas and Bierre, Kevin and Gramenos, Dimitris and Hinn, Michelle},
	editor = {Stephanidis, Constantine},
	year = {2011},
	keywords = {accessibility, disability, game, multimodality},
	pages = {400--409},
	file = {Full Text PDF:files/248/Westin et al. - 2011 - Advances in Game Accessibility from 2005 to 2010.pdf:application/pdf},
}

@inproceedings{assogna_using_2008,
	address = {Boston, MA},
	title = {Using {3D} {Models} {And} {Discrete} {Simulations} {In} {Infrastructure} {Security} {Applications}},
	isbn = {978-0-387-88523-0},
	doi = {10.1007/978-0-387-88523-0_20},
	abstract = {Next generation systems for critical infrastructure protection must support capabilities such as behavior analysis, situation modeling and data mining integrated within sophisticated virtual or augmented reality interfaces. This paper describes the design goals and implementation of a platform for critical infrastructure security applications. The platform is designed to support semi-automated 3D modeling of infrastructures, 3D integration of sensor networks, situation modeling and visual simulation via 3D animation, and advanced situation analysis. Such a system would enable operators to recognize preliminary indications of crisis situations and promptly activate the appropriate countermeasures. It would also assist them in optimizing normal operations and conducting simulations for emergency planning and crisis management.},
	language = {en},
	booktitle = {Critical {Infrastructure} {Protection} {II}},
	publisher = {Springer US},
	author = {Assogna, Pierluigi and Bertocchi, Glauco and Paoluzzi, Alberto and Vicentino, Michele and Scorzelli, Giorgio and Zollo, Roberto},
	editor = {Papa, Mauricio and Shenoi, Sujeet},
	year = {2008},
	keywords = {Geometric modeling, infrastructure security, simulation},
	pages = {269--278},
	file = {Full Text PDF:files/251/Assogna et al. - 2008 - Using 3D Models And Discrete Simulations In Infrastructure Security Applications.pdf:application/pdf},
}

@inproceedings{gotsis_skyfarer_2014,
	address = {Cham},
	title = {Skyfarer: {Design} {Case} {Study} of a {Mixed} {Reality} {Rehabilitation} {Video} {Game}},
	isbn = {978-3-319-07626-3},
	shorttitle = {Skyfarer},
	doi = {10.1007/978-3-319-07626-3_66},
	abstract = {This paper outlines a design case study for Skyfarer, a mixed reality rehabilitation application developed for upper body exercise of individuals aging with disability. We describe how experience, experiential and participatory design methodologies were combined to develop a game, which was publicly exhibited at IEEE VR and ACM SIGGRAPH, and formally evaluated in a biomechanical study at Rancho Los Amigos National Rehabilitation Center RLANRC.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {User} {Experience} {Design} for {Diverse} {Interaction} {Platforms} and {Environments}},
	publisher = {Springer International Publishing},
	author = {Gotsis, Marientina and Lympouridis, Vangelis and Requejo, Phil and Haubert, Lisa L. and Poulos, Irina C. and Frangoudes, Fotos and Turpin, David and Jordan-Marsh, Maryalice},
	editor = {Marcus, Aaron},
	year = {2014},
	keywords = {experience design, experiential design, games, Mixed reality, participatory design, rehabilitation, spinal cord injury, virtual reality},
	pages = {699--710},
	file = {Full Text PDF:files/253/Gotsis et al. - 2014 - Skyfarer Design Case Study of a Mixed Reality Rehabilitation Video Game.pdf:application/pdf},
}

@inproceedings{abdul_ubba_2019,
	address = {Cham},
	title = {{UBBA}: {Unity} {Based} {BPMN} {Animator}},
	isbn = {978-3-030-21297-1},
	shorttitle = {{UBBA}},
	doi = {10.1007/978-3-030-21297-1_1},
	abstract = {In the last years BPMN became the most prominent notation for representing business processes, thanks to its wide usage in academic and industrial contexts. Despite BPMN is very intuitive, it’s way of representing activities with static flow charts may result effective just for the BPM experts. Stakeholders who are not too much aware of the BPMN notation could misread the behavior of the business process. To this aim, BPMN animation tools can help model comprehension. However they are mainly based on 2D diagrams, just few works investigate the use of a 3D world as an environment for closely portray the reality of the business process. In this paper, we propose our tool UBBA, which creates a custom 3D virtual world from an input .bpmn file. Besides this 3-dimensional view of the diagram, we also integrate into UBBA the semantics of the BPMN elements in order to enable the animation.},
	language = {en},
	booktitle = {Information {Systems} {Engineering} in {Responsible} {Information} {Systems}},
	publisher = {Springer International Publishing},
	author = {Abdul, Basit Mubeen and Corradini, Flavio and Re, Barbara and Rossi, Lorenzo and Tiezzi, Francesco},
	editor = {Cappiello, Cinzia and Ruiz, Marcela},
	year = {2019},
	keywords = {3D visualization, Animation, BPMN, Collaboration},
	pages = {1--9},
	file = {Full Text PDF:files/256/Abdul et al. - 2019 - UBBA Unity Based BPMN Animator.pdf:application/pdf},
}

@inproceedings{fox_element_2018,
	address = {Cham},
	title = {Element {Selection} of {Three}-{Dimensional} {Objects} in {Virtual} {Reality}},
	isbn = {978-3-319-91581-4},
	doi = {10.1007/978-3-319-91581-4_2},
	abstract = {The manipulation of three dimensional objects is vital to fields such as engineering and architecture, but understanding 3D models from images on 2D screens takes years of experience. Virtual reality offers a powerful tool for the observation and manipulation of 3D objects by giving its users a sense of depth perception and the ability to reach through objects. To understand specific pain points in 2D CAD software, we conducted interviews and a survey of students and professionals with experience using CAD software. We narrowed in on the ability to select interior or obscured elements, and created a VR prototype allowing users to do so. Our usability tests found that compared to 2D software, VR was easier to use, more intuitive, and less frustrating, thought slightly more physically uncomfortable. Finally, we created a set of recommendations for VR CAD programs around action feedback, environmental context, and the necessity of a tutorial.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Interaction}, {Navigation}, {Visualization}, {Embodiment}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Fox, Dylan and Park, Sophie So Yeon and Borcar, Amol and Brewer, Anna and Yang, Joshua},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {3D modeling, Computer-Aided Design (CAD), Element selection, Usability, Virtual reality},
	pages = {13--29},
	file = {Full Text PDF:files/258/Fox et al. - 2018 - Element Selection of Three-Dimensional Objects in Virtual Reality.pdf:application/pdf},
}

@inproceedings{froh_investigating_2020,
	address = {Cham},
	title = {Investigating the {Influence} of {Optical} {Stimuli} on {Human} {Decision} {Making} in {Dynamic} {VR}-{Environments}},
	isbn = {978-3-030-49695-1},
	doi = {10.1007/978-3-030-49695-1_30},
	abstract = {In this paper, we investigate the human decision making process in a virtual environment. The main goal is to find influencing optical and behavioral economical factors for intuitive decisions in Virtual Reality. We therefore place test persons in a virtual corridor with six visually and technically varying doors. The experimental task is to open one of the doors without overthinking or hesitating too long. This is repeated several times while randomizing and recombining optical features in every iteration. Our data shows that most of the introduced determinants actually do have an impact in the user’s decision. We can also observe different intensities of impact depending on the factor. It appears that color is by far the most influential component, followed by the complexity of the doors opening process. In contrast, position, spotlights and color brightness show only marginal correlation with choices made by the user.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Design} and {Interaction}},
	publisher = {Springer International Publishing},
	author = {Fröh, Stefanie and Heinzig, Manuel and Manthey, Robert and Roschke, Christian and Thomanek, Rico and Ritter, Marc},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Color influence, HCI, Opening doors, Rapid decision making, Virtual gesture interaction, Virtual Reality, Visual factors},
	pages = {453--467},
	file = {Full Text PDF:files/259/Fröh et al. - 2020 - Investigating the Influence of Optical Stimuli on Human Decision Making in Dynamic VR-Environments.pdf:application/pdf},
}

@inproceedings{oliveira_virtual_2018,
	address = {Cham},
	title = {Virtual and {Real} {Body} {Experience} {Comparison} {Using} {Mixed} {Reality} {Cycling} {Environment}},
	isbn = {978-3-319-99426-0},
	doi = {10.1007/978-3-319-99426-0_5},
	abstract = {Different solutions present the usage of bicycles with Head Mounted Display (HMD) in which virtual scenarios are visualized as background for athletes training or as cardiac patient rehabilitation systems. However, assessments on presence, degrees of immersion and user involvement with real bicycles in those virtual scenarios still are rare. In this paper we present a haptic interface of a real bicycle using HMDs as a mixed reality display using a procedural city as a background scenario. To measure and evaluate presence, two experiments had been conducted. One that simulates a virtual reality mode and a second that corresponds to a mixed reality mode. By think aloud method, it was possible to analyze the degree of presence, through control, focus, immersion and involvement factors. Six of the seven participants described that immersion is augmented as well as the feeling of presence in the mixed reality interface, feeling a better experience with the improvement of movements. Issues related to comfort and the visual graphic were also evaluated with some results on the stimulus that also opens new possibilities for future works in different areas.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2018},
	publisher = {Springer International Publishing},
	author = {Oliveira, Wesley and Gaisbauer, Werner and Tizuka, Michelle and Clua, Esteban and Hlavacs, Helmut},
	editor = {Clua, Esteban and Roque, Licinio and Lugmayr, Artur and Tuomi, Pauliina},
	year = {2018},
	keywords = {Haptic interface, Mixed reality, Procedural content generation (PCG), VR bicycle},
	pages = {52--63},
	file = {Full Text PDF:files/261/Oliveira et al. - 2018 - Virtual and Real Body Experience Comparison Using Mixed Reality Cycling Environment.pdf:application/pdf},
}

@inproceedings{sharma_emergency_2019,
	address = {Cham},
	title = {Emergency {Response} {Using} {HoloLens} for {Building} {Evacuation}},
	isbn = {978-3-030-21607-8},
	doi = {10.1007/978-3-030-21607-8_23},
	abstract = {Emergency response in indoor building evacuation is needed for rescue and safety management. One of the challenges is to provide user-specific personalized evacuation routes in real time. Early hands-on experiences with the Microsoft HoloLens augmented/mixed reality device have given promising results for building evacuation applications. A range of use cases are tested, including data visualization and immersive data spaces, in-situ visualization of 3D models and full-scale architectural form visualization. We present how the mixed reality technology can provide spatial contextualized 3D visualization that promotes knowledge acquisition and support cognitive mapping. Our HoloLens application gives a visual representation of a building in 3D space, allowing people to see where exits are in the building. It also gives a path to the various exits; a shortest path to the exits as well as directions to a safe zone from their current position. This paper describes the system architecture and implementation of the augmented reality application to leverage the Microsoft HoloLens for emergency response during a building evacuation. The experimental results show the effectiveness of our Microsoft HoloLens application in an emergency evacuation by offering 3D visualizations of multilevel spaces while adding the spatial context that allows the individual to better understand their position and evacuation path when evacuation is necessary. We believe that AR technologies like HoloLens could be adopted by people for building evacuating during emergencies as it offers enriched experience in navigating large-scale environments.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Multimodal} {Interaction}},
	publisher = {Springer International Publishing},
	author = {Sharma, Sharad and Bodempudi, Sri Teja and Scribner, David and Grynovicki, Jock and Grazaitis, Peter},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {Augmented reality, Building evacuation, Immersive AR, Microsoft HoloLens, Mixed reality},
	pages = {299--311},
	file = {Full Text PDF:files/263/Sharma et al. - 2019 - Emergency Response Using HoloLens for Building Evacuation.pdf:application/pdf},
}

@inproceedings{sinha_interactive_2017,
	address = {Cham},
	title = {An {Interactive} {Elementary} {Tutoring} {System} for {Oral} {Health} {Education} {Using} an {Augmented} {Approach}},
	isbn = {978-3-319-67684-5},
	doi = {10.1007/978-3-319-67684-5_26},
	abstract = {The conventional elementary education system in India is mostly guided by formal content development, focusing on areas like math, language, science and social-science. Children tend to retain very little knowledge about other important areas of learning like heath care, which needs to be developed in their foundation years. The education on oral health is one such example which is not given the focus they ought to be. Considering its importance in early education, we propose a learning environment where children would gain knowledge through constant interaction with an intelligent tutoring system. The system addresses the challenges in developing a learning environment for children by introducing audio-visual effects, 3D animations and customizing the tutoring process to provide user-controlled pace of learning. It also employs the Wii Remote for imparting a tangible hardware interaction with the interface. This paper describes the proposed system and the studies conducted on treatment and control groups to evaluate its efficacy and compare the learning outcome at various domains. Experimental results depict positive effects on learning in the proposed technology-enhanced environment and paves a way for the deployment of more interactive, technology-driven learning process in the elementary education system.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} - {INTERACT} 2017},
	publisher = {Springer International Publishing},
	author = {Sinha, Mitali and Deb, Suman},
	editor = {Bernhaupt, Regina and Dalvi, Girish and Joshi, Anirudha and K. Balkrishan, Devanuj and O'Neill, Jacki and Winckler, Marco},
	year = {2017},
	keywords = {Intelligent tutoring system, Tangible interaction, Wii remote},
	pages = {413--430},
	file = {Full Text PDF:files/265/Sinha e Deb - 2017 - An Interactive Elementary Tutoring System for Oral Health Education Using an Augmented Approach.pdf:application/pdf},
}

@inproceedings{oliveira_exergames_2019,
	address = {Cham},
	title = {Exergames: {Game} {Prototype} {Using} {Maker} {Movement} {Assets}},
	isbn = {978-3-030-22219-2},
	shorttitle = {Exergames},
	doi = {10.1007/978-3-030-22219-2_39},
	abstract = {The world population is expected to grow until the year 2100. Also, the number of senior individuals is expected to surpass the number of children and teens in the populations. This growth in the raw number and number of seniors creates huge challenges for countries - especially in cost terms. It’s well known that preventive healthcare could smooth those impacts through preventing disease. One of the most effective and easy to implement tools healthcare programs of this scope use is exercise. However, many individuals lack motivation for the practice. Since gaming is one of most used entertainment form, it seems a good way to attract and maintain people into exercise routines. Games of this type, that require the user movement as part of playing, are called ‘Exergames’. This paper is consequence of a larger research towards game design methods for this specific type. Since initial surveys showed some lack of information about this subject, the need for this project emerged. A problem found along the research development was which device would be used to conduct further testing with design methodologies. Therefore, this paper shows the process of using maker movement assets to create an open source device capable of transforming body movement through cycling into command input to a platform game.},
	language = {en},
	booktitle = {Digital {Human} {Modeling} and {Applications} in {Health}, {Safety}, {Ergonomics} and {Risk} {Management}. {Healthcare} {Applications}},
	publisher = {Springer International Publishing},
	author = {Oliveira, Bruno S. and Teofilo, Vania and Miranda, Juliana and Nesteriuk, Sergio},
	editor = {Duffy, Vincent G.},
	year = {2019},
	keywords = {Exergames, Health care, Maker movement},
	pages = {532--549},
	file = {Full Text PDF:files/287/Oliveira et al. - 2019 - Exergames Game Prototype Using Maker Movement Assets.pdf:application/pdf},
}

@inproceedings{whitlock_mrcat_2020,
	address = {Cham},
	title = {{MRCAT}: {In} {Situ} {Prototyping} of {Interactive} {AR} {Environments}},
	isbn = {978-3-030-49695-1},
	shorttitle = {{MRCAT}},
	doi = {10.1007/978-3-030-49695-1_16},
	abstract = {Augmented reality (AR) blends physical and virtual components to create a mixed reality experience. This unique display medium presents new opportunities for application design, as applications can move beyond the desktop and integrate with the physical environment. In order to build effective applications for AR displays, we need to be able to iteratively design for different contexts or scenarios. We present MRCAT (Mixed Reality Content Authoring Toolkit), a tool for in situ prototyping of mixed reality environments. We discuss the initial design of MRCAT and iteration after a study (\$\$N = 14\$\$) to evaluate users’ abilities to craft AR applications with MRCAT and with a 2D prototyping tool. We contextualize our system in a case study of museum exhibit development, identifying how existing ideation and prototyping workflows could be bolstered with the approach offered by MRCAT. With our exploration of in situ prototyping, we enumerate key aspects both of AR application design and targeted domains that help guide design of more effective AR prototyping tools.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Design} and {Interaction}},
	publisher = {Springer International Publishing},
	author = {Whitlock, Matt and Mitchell, Jake and Pfeufer, Nick and Arnot, Brad and Craig, Ryan and Wilson, Bryce and Chung, Brian and Szafir, Danielle Albers},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Augmented reality, Multimodal interaction, Prototyping},
	pages = {235--255},
	file = {Full Text PDF:files/291/Whitlock et al. - 2020 - MRCAT In Situ Prototyping of Interactive AR Environments.pdf:application/pdf},
}

@inproceedings{cowley_generalised_2020,
	address = {Cham},
	title = {Generalised {Player} {Modelling}: {Why} {Artificial} {Intelligence} in {Games} {Should} {Incorporate} {Meaning}, with a {Formalism} for so {Doing}},
	isbn = {978-3-030-50164-8},
	shorttitle = {Generalised {Player} {Modelling}},
	doi = {10.1007/978-3-030-50164-8_1},
	abstract = {General game-playing artificial intelligence (AI) has recently seen important advances due to the various techniques known as ‘deep learning’. However, in terms of human-computer interaction, the advances conceal a major limitation: these algorithms do not incorporate any sense of what human players find meaningful in games.},
	language = {en},
	booktitle = {{HCI} in {Games}},
	publisher = {Springer International Publishing},
	author = {Cowley, Benjamin Ultan},
	editor = {Fang, Xiaowen},
	year = {2020},
	keywords = {Artificial intelligence, Behavlets, Category theory, Formal models, Game AI, General player model, Player personality},
	pages = {3--22},
	file = {Full Text PDF:files/295/Cowley - 2020 - Generalised Player Modelling Why Artificial Intelligence in Games Should Incorporate Meaning, with.pdf:application/pdf},
}

@inproceedings{park_guidance_2014,
	address = {Cham},
	title = {Guidance {System} {Using} {Augmented} {Reality} for {Solving} {Rubik}’s {Cube}},
	isbn = {978-3-319-07857-1},
	doi = {10.1007/978-3-319-07857-1_111},
	abstract = {This paper proposes a guidance system to help to solve the Rubik’s cube. For the puzzle to be solved, each face must be returned to consisting of one color. But the problem is quite difficult because there are 4.3252x1019 different states that can be reached from any given configuration. Our system use augmented reality technology to recognize the placement of each square and provide intuitive and easily understandable guidance for solving procedure.},
	language = {en},
	booktitle = {{HCI} {International} 2014 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Park, Jaebum and Park, Changhoon},
	editor = {Stephanidis, Constantine},
	year = {2014},
	keywords = {Augmented Reality, Guidance System, Immersive, Intuitive, Rubik’s Cube, Solving Algorithm},
	pages = {631--635},
	file = {Full Text PDF:files/247/Park e Park - 2014 - Guidance System Using Augmented Reality for Solving Rubik’s Cube.pdf:application/pdf},
}

@inproceedings{karlsen_digital_2015,
	address = {Cham},
	title = {Digital {Art} {Application} {Development}: {A} {Project} to {Increase} {Motivation} in {Systems} {Development} {Courses} for {Bachelor} {Students} in {Computer} {Engineering}},
	isbn = {978-3-319-24589-8},
	shorttitle = {Digital {Art} {Application} {Development}},
	doi = {10.1007/978-3-319-24589-8_52},
	abstract = {In this demonstration, we present some in-progress results of using digital art application development as an example of entertainment computing for increasing motivation and participation in a computer engineering undergraduate systems development course, with the purpose of improving the chances of reaching the intended learning outcomes. By stimulating motivation and participation via an openly defined project description of making an interactive art application in a competitive context, a variety of interesting project outcomes were produced, despite the fact that the project did not count towards the final grading of the course. The students made their applications by combining existing programming skills with the programming language Processing, lessons in Human-Computer-Interaction and software development methodologies.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Karlsen, Anniken and Bye, Robin T.},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {Art and motivation, Creative activities, Digital creativity, Software engineering, Teaching},
	pages = {533--538},
	file = {Full Text PDF:files/249/Karlsen e Bye - 2015 - Digital Art Application Development A Project to Increase Motivation in Systems Development Courses.pdf:application/pdf},
}

@inproceedings{li_multidisciplinary_2020,
	address = {Cham},
	title = {Multidisciplinary {Iterative} {Design} of {Exergames} ({MIDE}): {A} {Framework} for {Supporting} the {Design}, {Development}, and {Evaluation} of {Exergames} for {Health}},
	isbn = {978-3-030-50164-8},
	shorttitle = {Multidisciplinary {Iterative} {Design} of {Exergames} ({MIDE})},
	doi = {10.1007/978-3-030-50164-8_9},
	abstract = {Exercise video games (exergames) are increasingly being employed as a complementary intervention to promote physical activity engagement in response to the need for creating sustainable strategies for supporting health. While exergames have shown that they can have comparable effects to conventional human-guided training programs in certain situations, its adoption in healthcare applications are still limited. This is in part because of a disconnection between the technology/content producers, healthcare providers, and end-users. Many design frameworks have been proposed to guide the process of creating games for health, however, what is missing is an integrated and multifaceted approach that includes the preliminary research and evaluation stages that are needed to create plausible solutions for exergames. Furthermore, relevant stakeholders are often not included throughout the entire process, neglecting the importance of transdisciplinary collaborations when creating exergames for health. This paper presents the Multidisciplinary Iterative Design of Exergames (MIDE) framework as a comprehensive, integrative, and specific framework for exergame design, development, and evaluation following different research methods, techniques and tools. The MIDE framework is intended to support researchers, healthcare professionals, and industrial experts in identifying the stages, processes, techniques, and key roles needed to create novel exergames for exercise promotion. As older adults are a key user group, applicability of the framework is illustrated using considerations for older adults and immersive experiences (e.g., virtual reality). A specific use case is presented at the end of the paper to illustrate the use of the MIDE framework in the context of a project of using virtual reality exergames for promoting exercise in people living with dementia.},
	language = {en},
	booktitle = {{HCI} in {Games}},
	publisher = {Springer International Publishing},
	author = {Li, Yirou and Muñoz, John and Mehrabi, Samira and Middleton, Laura and Cao, Shi and Boger, Jennifer},
	editor = {Fang, Xiaowen},
	year = {2020},
	keywords = {Exergames, Framework, Game design, Game evaluation, Iterative, Multidisciplinary, Older adults, Virtual reality (VR)},
	pages = {128--147},
	file = {Full Text PDF:files/252/Li et al. - 2020 - Multidisciplinary Iterative Design of Exergames (MIDE) A Framework for Supporting the Design, Devel.pdf:application/pdf},
}

@inproceedings{matsas_beware_2013,
	address = {Berlin, Heidelberg},
	title = {Beware of the {Robot}: {A} {Highly} {Interactive} and {Immersive} {Virtual} {Reality} {Training} {Application} in {Robotic} {Manufacturing} {Systems}},
	isbn = {978-3-642-40352-1},
	shorttitle = {Beware of the {Robot}},
	doi = {10.1007/978-3-642-40352-1_76},
	abstract = {A highly interactive and immersive Virtual Reality Training System (VRTS) is developed, in terms of an educational serious game that simulates the cooperation between industrial robotic manipulators and humans, executing manufacturing tasks. “BeWare of the robot” application ultimately aims at studying the acceptability of human-robot collaboration, when both human and robot share the same workspace. The initial version of the application was evaluated by a group of users. Experimental results on usability and technical aspects are presented and several remarks about users’ experience and behavior in the virtual world are discussed.},
	language = {en},
	booktitle = {Advances in {Production} {Management} {Systems}. {Competitive} {Manufacturing} for {Innovative} {Products} and {Services}},
	publisher = {Springer},
	author = {Matsas, Elias and Batras, Dimitrios and Vosniakos, George-Christopher},
	editor = {Emmanouilidis, Christos and Taisch, Marco and Kiritsis, Dimitris},
	year = {2013},
	keywords = {Interaction, Manufacturing Training, Safe Human-Robot Cooperation, Serious Game, Virtual Reality},
	pages = {606--613},
	file = {Full Text PDF:files/255/Matsas et al. - 2013 - Beware of the Robot A Highly Interactive and Immersive Virtual Reality Training Application in Robo.pdf:application/pdf},
}

@inproceedings{hughes_integrating_2009,
	address = {Berlin, Heidelberg},
	title = {Integrating and {Delivering} {Sound} {Using} {Motion} {Capture} and {Multi}-tiered {Speaker} {Placement}},
	isbn = {978-3-642-02771-0},
	doi = {10.1007/978-3-642-02771-0_21},
	abstract = {Creating effective and compelling soundscapes for simulations is a challenging process that requires non-traditional tools and techniques outside the scope of standard production methods. In an immersive simulation, sound is at least as important as graphics; auditory cues can be heard from behind walls, around corners, and out of the line of sight. This paper describes a novel approach to interactive 3D sound design utilizing vision-based motion capture and multi-tiered, configurable loudspeaker delivery.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality}},
	publisher = {Springer},
	author = {Hughes, Darin E.},
	editor = {Shumaker, Randall},
	year = {2009},
	keywords = {3D Sound, Immersive Simulation, Motion Capture, Sound Design, XACT},
	pages = {179--185},
	file = {Full Text PDF:files/257/Hughes - 2009 - Integrating and Delivering Sound Using Motion Capture and Multi-tiered Speaker Placement.pdf:application/pdf},
}

@inproceedings{villane_3d_2009,
	address = {Berlin, Heidelberg},
	title = {{3D} {Virtual} {Environments} for the {Rehabilitation} of the {Blind}},
	isbn = {978-3-642-02713-0},
	doi = {10.1007/978-3-642-02713-0_26},
	abstract = {The accretion of orientation and mobility skills in blind people is fundamental for the development of an independent life. To these ends, activities oriented towards reinforcing this line of knowledge require direct interactions with spaces in real places, and the assistance of an educator or a companion as well. The objective of this study was to design, implement and evaluate 3D virtual environments in order to evaluate orientation and mobility learning in an unfamiliar environment based on the use of such environments. The procedure was provided by a learning stage in which the user learned to move about in the virtual environments, followed by an interaction stage in which he/she traveled virtually through the environments, to then travel the virtual environments that had been navigated virtually in the real world. To simulate the virtual surroundings, Unreal Engine was used, which is a gaming engine that allows for the construction of scenarios through a graphic editor. The results obtained show that the users were able to run the established route without any difficulties, for which reason it can be established that it is possible to produce mental models of real places through virtual interactions guided only by auditory cues.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Applications} and {Services}},
	publisher = {Springer},
	author = {Villane, Julio and Sánchez, Jaime},
	editor = {Stephanidis, Constantine},
	year = {2009},
	keywords = {Cardinal Point, Mobility Learning, Unfamiliar Environment, Virtual Environment, Virtual Reality},
	pages = {246--255},
	file = {Full Text PDF:files/260/Villane e Sánchez - 2009 - 3D Virtual Environments for the Rehabilitation of the Blind.pdf:application/pdf},
}

@inproceedings{melzer_when_2009,
	address = {Berlin, Heidelberg},
	title = {When {Items} {Become} {Victims}: {Brand} {Memory} in {Violent} and {Nonviolent} {Games}},
	isbn = {978-3-540-89222-9},
	shorttitle = {When {Items} {Become} {Victims}},
	doi = {10.1007/978-3-540-89222-9_2},
	abstract = {This paper introduces the AdRacer system for multifaceted testing and in-depth analyses of game effects and in-game advertising efficiency. AdRacer combines an immersive driving simulator, 3D game environment, recording of players’ gaze directions, and application of memory tests. A pilot study tested the effects of game violence on memory for brands shown as billboard ads in a racing game. In contrast to findings with TV violence, game violence did not impede brand memory. Memory results were also not mediated by visual attention during encoding. Compared to a matching nonviolent version, playing a violent game resulted in superior brand retrieval, yet participants showed fewer and shorter eye fixations on the billboard ads. Hence, caution seems to be recommended in transferring standard results from the “passive” TV medium to the interactive game medium.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2008},
	publisher = {Springer},
	author = {Melzer, André and Bushman, Brad J. and Hofmann, Ulrich G.},
	editor = {Stevens, Scott M. and Saldamarco, Shirley J.},
	year = {2009},
	keywords = {Game violence, in-game advertising, media effects, memory},
	pages = {11--22},
	file = {Full Text PDF:files/262/Melzer et al. - 2009 - When Items Become Victims Brand Memory in Violent and Nonviolent Games.pdf:application/pdf},
}

@inproceedings{mancini_laugh_2014,
	address = {Berlin, Heidelberg},
	title = {Laugh {When} {You}’re {Winning}},
	isbn = {978-3-642-55143-7},
	doi = {10.1007/978-3-642-55143-7_3},
	abstract = {Developing virtual characters with naturalistic game playing capabilities is an increasingly researched topic in Human-Computer Interaction. Possible roles for such characters include virtual teachers, personal care assistants, and companions for children. Laughter is an under-investigated emotional expression both in Human-Human and Human-Computer Interaction. The EU Project ILHAIRE, aims to study this phenomena and endow machines with laughter detection and synthesis capabilities. The Laugh when you’re winning project, developed during the eNTERFACE 2013 Workshop in Lisbon, Portugal, aimed to set up and test a game scenario involving two human participants and one such virtual character. The game chosen, the yes/no game, induces natural verbal and non-verbal interaction between participants, including frequent hilarious events, e.g., one of the participants saying “yes” or “no” and so losing the game. The setup includes software platforms, developed by the ILHAIRE partners, allowing automatic analysis and fusion of human participants’ multimodal data (voice, facial expression, body movements, respiration) in real-time to detect laughter. Further, virtual characters endowed with multimodal skills were synthesised in order to interact with the participants by producing laughter in a natural way.},
	language = {en},
	booktitle = {Innovative and {Creative} {Developments} in {Multimodal} {Interaction} {Systems}},
	publisher = {Springer},
	author = {Mancini, Maurizio and Ach, Laurent and Bantegnie, Emeline and Baur, Tobias and Berthouze, Nadia and Datta, Debajyoti and Ding, Yu and Dupont, Stéphane and Griffin, Harry J. and Lingenfelser, Florian and Niewiadomski, Radoslaw and Pelachaud, Catherine and Pietquin, Olivier and Piot, Bilal and Urbain, Jérôme and Volpe, Gualtiero and Wagner, Johannes},
	editor = {Rybarczyk, Yves and Cardoso, Tiago and Rosas, João and Camarinha-Matos, Luis M.},
	year = {2014},
	keywords = {detection, fusion, game, HCI, laughter, multimodal, virtual characters},
	pages = {50--79},
	file = {Full Text PDF:files/264/Mancini et al. - 2014 - Laugh When You’re Winning.pdf:application/pdf},
}

@inproceedings{ullah_virtual_2019,
	address = {Cham},
	title = {A {Virtual} {Testbed} for {Critical} {Incident} {Investigation} with {Autonomous} {Remote} {Aerial} {Vehicle} {Surveying}, {Artificial} {Intelligence}, and {Decision} {Support}},
	isbn = {978-3-030-13453-2},
	doi = {10.1007/978-3-030-13453-2_18},
	abstract = {Autonomous robotics and artificial intelligence techniques can be used to support human personnel in the event of critical incidents. These incidents can pose great danger to human life. Some examples of such assistance include: multi-robot surveying of the scene; collection of sensor data and scene imagery, real-time risk assessment and analysis; object identification and anomaly detection; and retrieval of relevant supporting documentation such as standard operating procedures (SOPs). These incidents, although often rare, can involve chemical, biological, radiological/nuclear or explosive (CBRNE) substances and can be of high consequence. Real-world training and deployment of these systems can be costly and sometimes not feasible. For this reason, we have developed a realistic 3D model of a CBRNE scenario to act as a testbed for an initial set of assisting AI tools that we have developed (This research has received funding from the European Union’s Horizon 2020 Programme under grant agreement No. 700264.).},
	language = {en},
	booktitle = {{ECML} {PKDD} 2018 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Ullah, Ihsan and Abinesh, Sai and Smyth, David L. and Karimi, Nazli B. and Drury, Brett and Glavin, Frank G. and Madden, Michael G.},
	editor = {Alzate, Carlos and Monreale, Anna and Assem, Haytham and Bifet, Albert and Buda, Teodora Sandra and Caglayan, Bora and Drury, Brett and García-Martín, Eva and Gavaldà, Ricard and Koprinska, Irena and Kramer, Stefan and Lavesson, Niklas and Madden, Michael and Molloy, Ian and Nicolae, Maria-Irina and Sinn, Mathieu},
	year = {2019},
	pages = {216--221},
	file = {Full Text PDF:files/266/Ullah et al. - 2019 - A Virtual Testbed for Critical Incident Investigation with Autonomous Remote Aerial Vehicle Surveyin.pdf:application/pdf},
}

@inproceedings{semsar_virtual_2016,
	address = {Cham},
	title = {A {Virtual} {Testbed} for {Studying} {Trust} in {Ambient} {Intelligence} {Environments}},
	isbn = {978-3-319-39949-2},
	doi = {10.1007/978-3-319-39949-2_10},
	abstract = {Ambient Intelligence is a new paradigm in information technology that creates environments able to detect and respond to users’ needs, actions, behaviors and feelings. User trust plays an important role in accepting Ambient Intelligence environments. In this paper we describe the design and implementation of a virtual reality based testbed for studying trust in Ambient Intelligence Environments.},
	language = {en},
	booktitle = {Human {Aspects} of {IT} for the {Aged} {Population}. {Healthy} and {Active} {Aging}},
	publisher = {Springer International Publishing},
	author = {Semsar, Azin and Malek Makan, Morteza and Nazari Shirehjini, Ali Asghar and Malek Mohammadi, Zahra},
	editor = {Zhou, Jia and Salvendy, Gavriel},
	year = {2016},
	keywords = {Ambient intelligence, Interactive realistic virtual reality, Trust},
	pages = {101--111},
	file = {Full Text PDF:files/288/Semsar et al. - 2016 - A Virtual Testbed for Studying Trust in Ambient Intelligence Environments.pdf:application/pdf},
}

@inproceedings{manthey_exploratory_2017,
	address = {Cham},
	title = {An {Exploratory} {Comparison} of the {Visual} {Quality} of {Virtual} {Reality} {Systems} {Based} on {Device}-{Independent} {Testsets}},
	isbn = {978-3-319-57987-0},
	doi = {10.1007/978-3-319-57987-0_11},
	abstract = {Nowadays, several different devices exist to offer virtual, augmented and mixed reality to show artificial objects. Measurements of the quality or the correctness of their resulting visual structures are not developed as sophisticated as in the classical areas of 2D image and video processing. Common testsets for image and video processing frequently contain sequences from the real world to reproduce their intrinsic characteristics and properties as well as artificial structures to provoke potential visual errors (see Fig. 1a). These common but traditional testsets are nowadays faced with rapid technical developments and changes like HD, UHD etc. improved surround sound or multiple data streams. That results in a limitation of the testsets usability and their ability to evoke visual errors. To overcome those limitations, we developed a system to create device-independent testsets to be used in the area of virtual reality devices and 3D environments. We conduct an empirical evaluation of most recent virtual reality devices like HTC Vive and Zeiss Cinemizer OLED, aiming to explore whether the technical hardware properties of the devices or the provided software interfaces may introduce errors in the visual representation. The devices are going to be evaluated by a group with technical skills and mostly advanced knowledge in computer graphics. All perceived visual and technical saliences are recorded in order to evaluate the correctness and the quality of the devices and the constraints.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Manthey, Robert and Ritter, Marc and Heinzig, Manuel and Kowerko, Danny},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Fresnel Lens, Mixed Reality, Multimedia Data, Virtual Reality, Virtual Reality System},
	pages = {130--140},
	file = {Full Text PDF:files/292/Manthey et al. - 2017 - An Exploratory Comparison of the Visual Quality of Virtual Reality Systems Based on Device-Independe.pdf:application/pdf},
}

@inproceedings{bekele_multimodal_2014,
	address = {Cham},
	title = {Multimodal {Interfaces} and {Sensory} {Fusion} in {VR} for {Social} {Interactions}},
	isbn = {978-3-319-07458-0},
	doi = {10.1007/978-3-319-07458-0_2},
	abstract = {Difficulties in social interaction, verbal and non-verbal communications as well as repetitive and atypical patterns of behavior, are typical characteristics of Autism spectrum disorders (ASD). Advances in computer and robotic technology are enabling assistive technologies for intervention in psychiatric disorders such as autism spectrum disorders (ASD) and schizophrenia (SZ). A number of research studies indicate that many children with ASD prefer technology and this preference can be explored to develop systems that may alleviate several challenges of traditional treatment and intervention. The current work presents development of an adaptive virtual reality-based social interaction platform for children with ASD. It is hypothesized that endowing a technological system that can detect the feeling and mental state of the child and adapt its interaction accordingly is of great importance in assisting and individualizing traditional intervention approaches. The proposed system employs sensors such as eye trackers and physiological signal monitors and models the context relevant psychological state of the child from combination of these sensors. Preliminary affect recognition results indicate that psychological states could be determined from peripheral physiological signals and together with other modalities including gaze and performance of the participant, it is viable to adapt and individualize VR-based intervention paradigms.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Designing} and {Developing} {Virtual} and {Augmented} {Environments}},
	publisher = {Springer International Publishing},
	author = {Bekele, Esubalew and Wade, Joshua W. and Bian, Dayi and Zhang, Lian and Zheng, Zhi and Swanson, Amy and Sarkar, Medha and Warren, Zachary and Sarkar, Nilanjan},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2014},
	keywords = {adaptive interaction, autism intervention, eye tracking, multimodal system, physiological processing, sensor fusion, Social interaction, virtual reality},
	pages = {14--24},
	file = {Full Text PDF:files/294/Bekele et al. - 2014 - Multimodal Interfaces and Sensory Fusion in VR for Social Interactions.pdf:application/pdf},
}

@inproceedings{palazzi_learning_2017,
	address = {Cham},
	title = {Learning to {Map} {Vehicles} into {Bird}’s {Eye} {View}},
	isbn = {978-3-319-68560-1},
	doi = {10.1007/978-3-319-68560-1_21},
	abstract = {Awareness of the road scene is an essential component for both autonomous vehicles and Advances Driver Assistance Systems and is gaining importance both for the academia and car companies. This paper presents a way to learn a semantic-aware transformation which maps detections from a dashboard camera view onto a broader bird’s eye occupancy map of the scene. To this end, a huge synthetic dataset featuring 1M couples of frames, taken from both car dashboard and bird’s eye view, has been collected and automatically annotated. A deep-network is then trained to warp detections from the first to the second view. We demonstrate the effectiveness of our model against several baselines and observe that is able to generalize on real-world data despite having been trained solely on synthetic ones.},
	language = {en},
	booktitle = {Image {Analysis} and {Processing} - {ICIAP} 2017},
	publisher = {Springer International Publishing},
	author = {Palazzi, Andrea and Borghi, Guido and Abati, Davide and Calderara, Simone and Cucchiara, Rita},
	editor = {Battiato, Sebastiano and Gallo, Giovanni and Schettini, Raimondo and Stanco, Filippo},
	year = {2017},
	keywords = {Advanced Driver Assistance Systems, Bounding Box Coordinates, Dashboard Cameras, Road Obstacle Detection, Surround View},
	pages = {233--243},
	file = {Full Text PDF:files/297/Palazzi et al. - 2017 - Learning to Map Vehicles into Bird’s Eye View.pdf:application/pdf},
}

@inproceedings{maraj_performance_2020,
	address = {Cham},
	title = {Performance, {Simulator} {Sickness}, and {Immersion} of a {Ball}-{Sorting} {Task} in {Virtual} and {Augmented} {Realities}},
	isbn = {978-3-030-49695-1},
	doi = {10.1007/978-3-030-49695-1_35},
	abstract = {Virtual Reality (VR) and Augmented Reality (AR) can be defined by the amount of virtual elements displayed to a human’s senses: VR is completely synthetic and AR is partially synthetic. This paper compares VR and AR systems for variations of three ball-sorting task scenarios, and evaluates both user performance and reaction (i.e., simulator sickness and immersion). The VR system scored higher, with statistical significance, than the AR system in terms of effectiveness per each scenario and completion rate of all scenarios. The VR system also scored significantly lower than the AR system in terms of percentage error and total false positives. The VR group scored significantly lower than the AR group in efficiency performance: the VR group had less time spent in each scenario, less total time duration, and higher overall relative efficiency. Although post-scenario simulator sickness did not differ significantly between VR and AR, the VR condition had an increase in disorientation from pre-to-post scenarios. Significant correlations between performance effectiveness and post-scenario simulator sickness were not found. Finally, the AR system scored significantly higher on the immersion measure item for the level of challenge the scenarios provided. AR interface issues are discussed as a potential factor in performance decrement, and AR interface solutions are given. AR may be preferred over VR if disorientation is a concern. Study limits include causality ambiguity and experimental control. Next steps include testing VR or AR systems exclusively, and testing whether the increased challenge from the AR immersion is beneficial to educational applications.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Design} and {Interaction}},
	publisher = {Springer International Publishing},
	author = {Maraj, Crystal and Hurter, Jonathan and Murphy, Sean},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Augmented Reality, Immersion, Performance, Simulator Sickness, Virtual Reality},
	pages = {522--534},
	file = {Full Text PDF:files/299/Maraj et al. - 2020 - Performance, Simulator Sickness, and Immersion of a Ball-Sorting Task in Virtual and Augmented Reali.pdf:application/pdf},
}

@inproceedings{hernandez-ibanez_natural_2016,
	address = {Cham},
	title = {Natural {Interaction} and {Movement} {Paradigms}. {A} {Comparison} of {Usability} for a {Kinect} {Enabled} {Museum} {Installation}},
	isbn = {978-3-319-39483-1},
	doi = {10.1007/978-3-319-39483-1_14},
	abstract = {In this paper, the authors evaluate and compare two different paradigms of natural interaction, one metaphorical and one natural, in order to control the movement of the user inside a virtual model of an archaeological reconstruction. This work measures the two paradigm’s usability in order to use this installation in a museum environment. The system was implemented on a game engine enabling the use of a Kinect 2 depth camera to obtain user input by means of body gesture analysis.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Hernández-Ibáñez, Luis A. and Barneche-Naya, Viviana and Mihura-López, Rocío},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2016},
	keywords = {Game engine, Kinect, Museum, Natural interaction, System Usability Scale, Usability, Virtual archaeology, Virtual museum},
	pages = {145--155},
	file = {Full Text PDF:files/302/Hernández-Ibáñez et al. - 2016 - Natural Interaction and Movement Paradigms. A Comparison of Usability for a Kinect Enabled Museum In.pdf:application/pdf},
}

@inproceedings{rangelova_simulation_2018,
	address = {Cham},
	title = {Simulation {Sickness} {Evaluation} {While} {Using}   a {Fully} {Autonomous} {Car} in a {Head} {Mounted}   {Display} {Virtual} {Environment}},
	isbn = {978-3-319-91581-4},
	doi = {10.1007/978-3-319-91581-4_12},
	abstract = {Simulation sickness is a condition of physiological discomfort felt during or after exposure to a virtual environment. A virtual environment can be accessed through a head mounted display which provides the user with an entrance to the virtual world. The onset of simulation sickness is a main disadvantage of virtual reality (VR) systems. The proof-of-concept presented in this paper aims to provide new insights into development and evaluation of a VR driving simulation based on consumer electronics devices and a 3 Degrees-of-Freedom (3 DOF) motion platform. A small sample (n = 9) driving simulator pre-study with within-subjects design was conducted to explore simulation sickness outbreak, sense of presence and physiological responses induced by autonomous driving in a dynamic and static driving simulation. The preliminary findings show that users experienced no substantial simulation sickness while using an autonomous car when the VR simulation included a motion platform. This study is the basis for more extensive research in the future. Future studies will include more participants and investigate more factors that contribute to or mitigate the effects of simulation sickness.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Interaction}, {Navigation}, {Visualization}, {Embodiment}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Rangelova, Stanislava and Decker, Daniel and Eckel, Marc and Andre, Elisabeth},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Autonomous driving, Consumer electronics, Driving simulation, Head mounted display, Pilot study, Simulation sickness, Virtual reality},
	pages = {155--167},
	file = {Full Text PDF:files/304/Rangelova et al. - 2018 - Simulation Sickness Evaluation While Using   a Fully Autonomous Car in a Head Mounted   Display Virt.pdf:application/pdf},
}

@inproceedings{gao_energy-efficient_2018,
	address = {Cham},
	title = {Energy-{Efficient} and {Quality} of {Experience}-{Aware} {Resource} {Provisioning} for {Massively} {Multiplayer} {Online} {Games} in the {Cloud}},
	isbn = {978-3-030-03596-9},
	doi = {10.1007/978-3-030-03596-9_61},
	abstract = {Massively Multiplayer Online Games (MMOGs) routinely have millions of registered players and hundreds of thousands of active concurrent gamers. To guarantee quality of experience (QoE) to a highly variable number of concurrent players, MMOG infrastructure have converted nowadays into cloud computing paradigm. Many leading MMOG companies have begun to build increasing numbers of energy hungry data centers for running the MMOG services requested by the players. A main challenge for MMOG service providers is to find the best tradeoff between two contradictory aims: improving the QoE and reducing energy costs. In this paper, we propose a dynamic resource provisioning scheme for large-scale MMOG services implemented on top of cloud infrastructures which takes advantage of both virtual machine resizing and server consolidation to achieve energy efficiency and desired QoE requirements. Our experimental results indicate that, compared to an over-provisioning of infrastructural resources, our resource provisioning scheme can achieve up to 54.5\% energy savings while providing the just-good-enough QoE to gamers under rapidly changing workloads.},
	language = {en},
	booktitle = {Service-{Oriented} {Computing}},
	publisher = {Springer International Publishing},
	author = {Gao, Yongqiang and Wang, Lin and Xie, Zhulong and Guo, Wenhui and Zhou, Jiantao},
	editor = {Pahl, Claus and Vukovic, Maja and Yin, Jianwei and Yu, Qi},
	year = {2018},
	keywords = {Cloud computing, Dynamic resource provisioning, MMOG},
	pages = {854--869},
	file = {Full Text PDF:files/305/Gao et al. - 2018 - Energy-Efficient and Quality of Experience-Aware Resource Provisioning for Massively Multiplayer Onl.pdf:application/pdf},
}

@inproceedings{domeshek_lessons_2019,
	address = {Cham},
	title = {Lessons from {Building} {Diverse} {Adaptive} {Instructional} {Systems} ({AIS})},
	isbn = {978-3-030-22341-0},
	doi = {10.1007/978-3-030-22341-0_6},
	abstract = {This paper presents lessons learned from building a wide range of Adaptive Instructional Systems (AISs), ultimately bearing on the question of how to characterize the space of potential AISs to advance the cause of standardization and reuse. The AISs we consider support coached practice of complex decision-making skills—e.g., military tactical decision-making, situation assessment, and systems troubleshooting and management. We illustrate forces that affect system design and dimensions along which systems then vary.},
	language = {en},
	booktitle = {Adaptive {Instructional} {Systems}},
	publisher = {Springer International Publishing},
	author = {Domeshek, Eric and Ramachandran, Sowmya and Jensen, Randy and Ludwig, Jeremy and Ong, Jim and Stottler, Dick},
	editor = {Sottilare, Robert A. and Schwarz, Jessica},
	year = {2019},
	keywords = {Adaptive Instructional Systems, Application and project demands, Design and implementation choices, Ontologies and standardization},
	pages = {62--75},
	file = {Full Text PDF:files/308/Domeshek et al. - 2019 - Lessons from Building Diverse Adaptive Instructional Systems (AIS).pdf:application/pdf},
}

@inproceedings{krum_social_2017,
	address = {Cham},
	title = {Social {Impact} of {Enhanced} {Gaze} {Presentation} {Using} {Head} {Mounted} {Projection}},
	isbn = {978-3-319-58697-7},
	doi = {10.1007/978-3-319-58697-7_5},
	abstract = {Projected displays can present life-sized imagery of a virtual human character that can be seen by multiple observers. However, typical projected displays can only render that virtual human from a single viewpoint, regardless of whether head tracking is employed. This results in the virtual human being rendered from an incorrect perspective for most individuals in a group of observers. This could result in perceptual miscues, such as the “Mona Lisa” effect, causing the virtual human to appear as if it is simultaneously gazing and pointing at all observers in the room regardless of their location. This may be detrimental to training scenarios in which all trainees must accurately assess where the virtual human is looking or pointing a weapon. In this paper, we discuss our investigations into the presentation of eye gaze using REFLCT, a previously introduced head mounted projective display. REFLCT uses head tracked, head mounted projectors and retroreflective screens to present personalized, perspective correct imagery to multiple users without the occlusion of a traditional head mounted display. We examined how head mounted projection for enhanced presentation of eye gaze might facilitate or otherwise affect social interactions during a multi-person guessing game of “Twenty Questions.”},
	language = {en},
	booktitle = {Distributed, {Ambient} and {Pervasive} {Interactions}},
	publisher = {Springer International Publishing},
	author = {Krum, David M. and Kang, Sin-Hwa and Phan, Thai and Cairco Dukes, Lauren and Bolas, Mark},
	editor = {Streitz, Norbert and Markopoulos, Panos},
	year = {2017},
	keywords = {Enhanced gaze, Head mounted projection},
	pages = {61--76},
	file = {Full Text PDF:files/310/Krum et al. - 2017 - Social Impact of Enhanced Gaze Presentation Using Head Mounted Projection.pdf:application/pdf},
}

@inproceedings{zhu_penalizing_2018,
	address = {Cham},
	title = {Penalizing {Top} {Performers}: {Conservative} {Loss} for {Semantic} {Segmentation} {Adaptation}},
	isbn = {978-3-030-01234-2},
	shorttitle = {Penalizing {Top} {Performers}},
	doi = {10.1007/978-3-030-01234-2_35},
	abstract = {Due to the expensive and time-consuming annotations (e.g., segmentation) for real-world images, recent works in computer vision resort to synthetic data. However, the performance on the real image often drops significantly because of the domain shift between the synthetic data and the real images. In this setting, domain adaptation brings an appealing option. The effective approaches of domain adaptation shape the representations that (1) are discriminative for the main task and (2) have good generalization capability for domain shift. To this end, we propose a novel loss function, i.e., Conservative Loss, which penalizes the extreme good and bad cases while encouraging the moderate examples. More specifically, it enables the network to learn features that are discriminative by gradient descent and are invariant to the change of domains via gradient ascend method. Extensive experiments on synthetic to real segmentation adaptation show our proposed method achieves state of the art results. Ablation studies give more insights into properties of the Conservative Loss. Exploratory experiments and discussion demonstrate that our Conservative Loss has good flexibility rather than restricting an exact form.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Zhu, Xinge and Zhou, Hui and Yang, Ceyuan and Shi, Jianping and Lin, Dahua},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Conservative Loss, Domain Adaptation, Domain Invariant Representation, Gradient Ascend Method, Source Domain},
	pages = {587--603},
	file = {Full Text PDF:files/332/Zhu et al. - 2018 - Penalizing Top Performers Conservative Loss for Semantic Segmentation Adaptation.pdf:application/pdf},
}

@inproceedings{sisler_stories_2012,
	address = {Berlin, Heidelberg},
	title = {Stories from the {History} of {Czechoslovakia}, {A} {Serious} {Game} for {Teaching} {History} of the {Czech} {Lands} in the 20th {Century} – {Notes} on {Design} {Concepts} and {Design} {Process}},
	isbn = {978-3-642-33542-6},
	doi = {10.1007/978-3-642-33542-6_6},
	abstract = {In the context of curricular history education both commercial entertainment games as well as serious games specifically tailored for educational purposes were employed. Especially the latter types of games were reported as being promising concerning instructional effectiveness. Still, there are not many complex serious games for history education, particularly in the secondary schools context. In this work-in-progress paper, we report on the progress of project Stories from the History of Czechoslovakia, a serious game for teaching history of the Czech lands in the 20th century. We introduce main game concepts, describe two main design challenges we have been facing during the development and how we have addressed them and overview our feasibility study on 71 high-school students. This paper can be informative for researchers and designers working on similar projects.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2012},
	publisher = {Springer},
	author = {Šisler, Vít and Brom, Cyril and Cuhra, Jaroslav and Činátl, Kamil and Gemrot, Jakub},
	editor = {Herrlich, Marc and Malaka, Rainer and Masuch, Maic},
	year = {2012},
	keywords = {Educational Game, Game Concept, Game Genre, Interactive Game, Real Story},
	pages = {67--74},
	file = {Full Text PDF:files/289/Šisler et al. - 2012 - Stories from the History of Czechoslovakia, A Serious Game for Teaching History of the Czech Lands i.pdf:application/pdf},
}

@inproceedings{chapman_introduction_2009,
	address = {Berlin, Heidelberg},
	title = {Introduction},
	isbn = {978-3-642-03869-3},
	doi = {10.1007/978-3-642-03869-3_77},
	abstract = {During the past few years, mainstream computing has rapidly embraced multicore architectures and there is now considerable research into the design of such systems and into all aspects of their utilization. In particular, much work is needed in order to improve their programmability. Limitations on the amount of memory, bandwidth constraints, multiple layers of parallelism and heterogeneous components all introduce new challenges for application development and execution. In recognition of the importance of this architectural shift, multi-core programming was added to the collection of Euro-Par topics this year. The focus of the topic covered general-purpose multi-core programming techniques, models and languages, as well as those for multi-core embedded systems, and included related work on compilers, run-time systems, and performance and scalability studies.},
	language = {en},
	booktitle = {Euro-{Par} 2009 {Parallel} {Processing}},
	publisher = {Springer},
	author = {Chapman, Barbara and Kienhuis, Bart and Ayguadé, Eduard and Bodin, François and Plata, Oscar and Stotzer, Eric},
	editor = {Sips, Henk and Epema, Dick and Lin, Hai-Xiang},
	year = {2009},
	pages = {837--838},
	file = {Full Text PDF:files/290/Chapman et al. - 2009 - Introduction.pdf:application/pdf},
}

@inproceedings{kawai_creating_2017,
	address = {Cham},
	title = {Creating a {Regional} and {Historical} {Streetscape} {Simulation} {System}},
	isbn = {978-3-319-66715-7},
	doi = {10.1007/978-3-319-66715-7_62},
	abstract = {The purpose of this study is to create a streetscape simulation system to support local cultural succession by conveying—in a clear and simple way—local history and culture to interested members of the public. We developed an interactive system, based on a game engine, for the streetscape of Fujisawa-juku, which is a post-station town on the former Tokaido Road of the late Edo period. We designed 3D models for former architecture and urban facilities, using Ukiyo-e and old documents provided by the Fujisawa city archive as references. In addition, regarding dynamic spatial elemental components that encourage users to visually survey the area, we prepared non-player characters that walk in a specified range and communicate a fixed set of remarks. Furthermore, we created soundscapes, which correspond to particular locations, using sound effects in the 3D models by incorporating natural sounds. The developed streetscape simulation system is on permanent exhibition at the Fujisawa-juku Intersection Hall and is widely accessible to the public.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2017},
	publisher = {Springer International Publishing},
	author = {Kawai, Yasuo},
	editor = {Munekata, Nagisa and Kunita, Itsuki and Hoshino, Junichi},
	year = {2017},
	keywords = {Game engine, Post-station town, Streetscape simulation},
	pages = {456--459},
	file = {Full Text PDF:files/293/Kawai - 2017 - Creating a Regional and Historical Streetscape Simulation System.pdf:application/pdf},
}

@inproceedings{scacchi_collaborative_2008,
	address = {Boston, MA},
	title = {A {Collaborative} {Science} {Learning} {Game} {Environment} for {Informal} {Science} {Education}:{DinoQuest} {Online}},
	isbn = {978-0-387-09701-5},
	shorttitle = {A {Collaborative} {Science} {Learning} {Game} {Environment} for {Informal} {Science} {Education}},
	doi = {10.1007/978-0-387-09701-5_7},
	abstract = {We describe concepts and results that arose from the development and deployment of a large-scale collaborative game environment called DinoQuest Online. DQO provides an entertaining experience and approach to informal science education. DQO represents a collection of 13 games for helping school-age children to learn about science (or more specifically, life science and dinosaurs).In this paper, we identify and examine different collaborative group forms that emerged to play DQO. Along the way we provided examples of the collaborative groups and game play from DQO.},
	language = {en},
	booktitle = {New {Frontiers} for {Entertainment} {Computing}},
	publisher = {Springer US},
	author = {Scacchi, Walt and Nideffer, Robert and Adams, Joe},
	editor = {Ciancarini, Paolo and Nakatsu, Ryohei and Rauterberg, Matthias and Roccetti, Marco},
	year = {2008},
	keywords = {Computer Game, Computer Support Cooperative Work, Game Design, Game Play, Play Experience},
	pages = {71--82},
	file = {Full Text PDF:files/296/Scacchi et al. - 2008 - A Collaborative Science Learning Game Environment for Informal Science EducationDinoQuest Online.pdf:application/pdf},
}

@inproceedings{vidakis_combining_2014,
	address = {Cham},
	title = {Combining {Ludology} and {Narratology} in an {Open} {Authorable} {Framework} for {Educational} {Games} for {Children}: the {Scenario} of {Teaching} {Preschoolers} with {Autism} {Diagnosis}},
	isbn = {978-3-319-07440-5},
	shorttitle = {Combining {Ludology} and {Narratology} in an {Open} {Authorable} {Framework} for {Educational} {Games} for {Children}},
	doi = {10.1007/978-3-319-07440-5_57},
	abstract = {This paper presents the initial findings and the on-going work of IOLAOS project, a general open authorable framework for educational games for children. This framework features an editor, where the game narrative can be created or edited, according to specific needs. A ludic approach is also used both for the interface as well as for the game design. More specifically, by employing physical and natural user interface (NUI), we aim to achieve ludic interfaces. Moreover, by designing the educational game with playful elements, we follow a ludic design. This framework is then applied for the scenario of teaching preschoolers with autism diagnosis. Children with autism have been reported to exhibit deficits in the recognition of affective expressions and the perception of emotions. With the appropriate intervention, elimination of those deficits can be achieved. Interventions are proposed to start as early as possible. Computer-based programs have been widely used with success to teach people with autism to recognize emotions. However, those computer interventions require considerable skills for interaction. Such abilities are beyond very young children with autism as most probably they don’t have the skills to interact with computers. In this context, our approach with the suggested framework employs a ludic interface based on NUI, a ludic game design and takes account of the specific characteristics of preschoolers with autism diagnosis and their physical abilities for customizing accordingly the narrative of the game.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Universal} {Access} to {Information} and {Knowledge}},
	publisher = {Springer International Publishing},
	author = {Vidakis, Nikolas and Christinaki, Eirini and Serafimidis, Iosif and Triantafyllidis, Georgios},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {Autism Spectrum Condition, Autism Spectrum Disorder, Emotion Recognition, Game Design, Teaching Preschooler},
	pages = {626--636},
	file = {Full Text PDF:files/298/Vidakis et al. - 2014 - Combining Ludology and Narratology in an Open Authorable Framework for Educational Games for Childre.pdf:application/pdf},
}

@inproceedings{espinoza_videogaming_2014,
	address = {Cham},
	title = {Videogaming {Interaction} for {Mental} {Model} {Construction} in {Learners} {Who} {Are} {Blind}},
	isbn = {978-3-319-07440-5},
	doi = {10.1007/978-3-319-07440-5_48},
	abstract = {The purpose of this work is to present the design, development and evaluation of a videogame that allows users who are blind to gradually build up a mental model based on references between different points on a Cartesian plane, in a way that is both didactic and entertaining. Two prototypes were iteratively created, and were subjected to usability evaluations by the end users, who used the videogame in the context of a set of defined tasks. This allowed researchers to adjust, improve and validate various aspects of the interfaces that had been designed and implemented. In addition, the cognitive impact of the game on blind learners was evaluated, based on the use of the final version of the videogame, and leading to revealing results regarding the proposed objectives.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Universal} {Access} to {Information} and {Knowledge}},
	publisher = {Springer International Publishing},
	author = {Espinoza, Matías and Sánchez, Jaime and de Borba Campos, Márcia},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {Audio and haptic based interfaces, Mental model, People who are blind, Reference system, Videogame, Wiimote},
	pages = {525--536},
	file = {Full Text PDF:files/300/Espinoza et al. - 2014 - Videogaming Interaction for Mental Model Construction in Learners Who Are Blind.pdf:application/pdf},
}

@inproceedings{swarts_ultra_2013,
	address = {Berlin, Heidelberg},
	title = {Ultra {Low} {Cost} {Eye} {Gaze} {Tracking} for {Virtual} {Environments}},
	isbn = {978-3-642-39405-8},
	doi = {10.1007/978-3-642-39405-8_12},
	abstract = {In this paper we present an ultra-low cost eye gaze tracker specifically aimed at studying visual attention in 3D virtual environments. We capture camera view and user eye gaze for each frame and project vectors back into the environment to visualize where and what subjects view over time. Additionally we show one measure of calculating the accuracy in 3D space by creating vectors from the stored data and projecting them onto a fixed sphere. The ratio of hits to non-hits provides a measure of 3D sensitivity of the setup.},
	language = {en},
	booktitle = {Virtual {Augmented} and {Mixed} {Reality}. {Designing} and {Developing} {Augmented} and {Virtual} {Environments}},
	publisher = {Springer},
	author = {Swarts, Matthew and Noh, Jin},
	editor = {Shumaker, Randall},
	year = {2013},
	keywords = {Eye Tracking, Low Cost, Virtual Environments},
	pages = {94--102},
	file = {Full Text PDF:files/301/Swarts e Noh - 2013 - Ultra Low Cost Eye Gaze Tracking for Virtual Environments.pdf:application/pdf},
}

@inproceedings{maraj_usability_2020,
	address = {Cham},
	title = {Usability {Dimensions} of {Simulated} {Detectors} for {Improvised} {Explosive} {Devices}},
	isbn = {978-3-030-50788-6},
	doi = {10.1007/978-3-030-50788-6_10},
	abstract = {Buried explosives, such as Improvised Explosive Devices (IEDs), are a threat to operations in the military. This challenge is compounded by limits in training the military to detect IEDs using a handheld detector called the Minehound. Thus, a call for improved IED detector training is answered through testing Virtual Reality (VR) and Augmented Reality (AR) Minehound trainers: these trainers are subjected to a usability investigation. Further, the VR and AR developments are framed within a Systems Engineering Process Model. Following traditional Minehound instruction, a data collection event occurred over a two-day period, where ten Marines were asked to use the VR and AR Minehound trainers. Following the Marines’ interaction with the trainers, the Marines completed a usability questionnaire (i.e., agreement with the usefulness, ease of use, ease of learning, satisfaction, and effectiveness of the trainers; and responses to open-ended questions). Ratings indicated future iterations should not emphasize aspects of ease of use and ease of learning, such as for user interfaces, but emphasize challenging aspects, such as helping users accomplish training tasks. A lower mean score in the usefulness subscale may be linked to breaks in fidelity (e.g., lag issues, weight issues, and a non-standard Marine sweep technique). Primarily, considerations for usefulness, satisfaction, and effectiveness aspects should be highlighted in the future as per an iterative design process. A cost-benefit analysis is given to compare the traditional and experimental forms of training. Limits of the study include experimental, environmental, and technical issues.},
	language = {en},
	booktitle = {Adaptive {Instructional} {Systems}},
	publisher = {Springer International Publishing},
	author = {Maraj, Crystal and Hurter, Jonathan and Reed, Dean and Hoayun, Clive and Moodie, Adam and Eifert, Latika “Bonnie”},
	editor = {Sottilare, Robert A. and Schwarz, Jessica},
	year = {2020},
	keywords = {Augmented Reality, Simulation, Training, Usability, Virtual Reality},
	pages = {130--143},
	file = {Full Text PDF:files/303/Maraj et al. - 2020 - Usability Dimensions of Simulated Detectors for Improvised Explosive Devices.pdf:application/pdf},
}

@inproceedings{lyngsie_dont_2013,
	address = {Berlin, Heidelberg},
	title = {Don’t {Text} {While} {Driving}: {The} {Effect} of {Smartphone} {Text} {Messaging} on {Road} {Safety} during {Simulated} {Driving}},
	isbn = {978-3-642-40477-1},
	shorttitle = {Don’t {Text} {While} {Driving}},
	doi = {10.1007/978-3-642-40477-1_35},
	abstract = {Text messaging on smartphones uses a full soft keyboard instead of the numeric buttons on traditional mobile phones. While being more intuitive, the lack of tactile feedback from physical buttons increases the need for user focus, which may compromise safety in certain settings. This paper reports from an empirical study of the effect of text messaging on road safety. We compared the use of a traditional mobile phone and a smartphone for writing text messages during simulated driving. The results confirm that driver performance when texting decreases considerably as there are significant increases in reaction time, car-following distance, lane violation, number of crash/near-crash incidents, perceived task load and the amount of time the driver is looking away from the road. The results also show that smartphones makes this even worse; on key performance parameters they increase the threat from text messaging while driving. These results suggest that drivers should never text while driving, especially not with a smartphone.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2013},
	publisher = {Springer},
	author = {Lyngsie, Kaspar and Pedersen, Martin S. and Stage, Jan and Vestergaard, Kim F.},
	editor = {Kotzé, Paula and Marsden, Gary and Lindgaard, Gitte and Wesson, Janet and Winckler, Marco},
	year = {2013},
	keywords = {driver distraction, Driving, mobile phone, road safety, simulated driving experiment, smartphone, text messaging},
	pages = {546--563},
	file = {Full Text PDF:files/306/Lyngsie et al. - 2013 - Don’t Text While Driving The Effect of Smartphone Text Messaging on Road Safety during Simulated Dr.pdf:application/pdf},
}

@inproceedings{ogden_squad_2015,
	address = {Cham},
	title = {Squad {Overmatch}: {Using} {Virtual} {Technology} to {Enhance} {Live} {Training} {Environments}},
	isbn = {978-3-319-21067-4},
	shorttitle = {Squad {Overmatch}},
	doi = {10.1007/978-3-319-21067-4_31},
	abstract = {The application of virtual augmentation to the U.S. Army’s training continuum may reduce Post-Traumatic Stress (PTS) and suicides by increasing Soldiers’ resilience and cognitive skills at the squad level pre-deployment. This may be accomplished through current programs of record with technological injections, thereby enhancing the training experience improving involvement and retention. Virtual platforms also invite more skill and task repetitions at a much lower cost and reduced risk of injury.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Ogden, Patrick M. and Wollert, Terry N. and Butler, Paul and Salcedo, Julie N.},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2015},
	keywords = {Enemy Combatant, Training Environment, Virtual Technology, Virtual Training, Walter Reed Army Institute},
	pages = {300--308},
	file = {Full Text PDF:files/307/Ogden et al. - 2015 - Squad Overmatch Using Virtual Technology to Enhance Live Training Environments.pdf:application/pdf},
}

@inproceedings{klemke_transferring_2015,
	address = {Cham},
	title = {Transferring an {Educational} {Board} {Game} to a {Multi}-user {Mobile} {Learning} {Game} to {Increase} {Shared} {Situational} {Awareness}},
	isbn = {978-3-319-20609-7},
	doi = {10.1007/978-3-319-20609-7_55},
	abstract = {This paper analyses how multi-user mobile games can be beneficial to educational scenarios. It does so in several steps: Firstly, we introduce the field of logistics as a problem domain for an educational challenge. Secondly, we describe the design of an educational board game for the field of disruption handling in logistics processes, which aims to foster shared situational awareness (SSA). Thirdly, we introduce an open-source mobile serious games platform (ARLearn) and fourthly describe how the board game can be realized in this platform. The reader gets to know the problem situation of multi-stakeholder decision situations, learns about the design of a board game, and gets to know the open-source mobile serious game platform ARLearn.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Klemke, Roland and Kurapati, Shalini and Lukosch, Heide and Specht, Marcus},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2015},
	keywords = {Game-based learning, Logistics, Mobile learning, Multi-role game-design, Multi-user games},
	pages = {583--594},
	file = {Full Text PDF:files/309/Klemke et al. - 2015 - Transferring an Educational Board Game to a Multi-user Mobile Learning Game to Increase Shared Situa.pdf:application/pdf},
}

@inproceedings{berssenbrugge_interactive_2016,
	address = {Cham},
	title = {Interactive {VR}-based {Visualization} for {Material} {Flow} {Simulations}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_56},
	abstract = {The conventional way of visualizing the material flow in a production system is to use simulation tools and their integrated symbols and pictograms. By going this way, the reference to the real production system is very limited since conventional material flow models provide only an abstract view and are not very comprehensive for the user. This paper introduces a procedure which enables a Virtual Design Review of the planned process layout on a large-screen visualization facility. This enables production planners to conduct a virtual inspection of alternative concepts for a planned production system including the visualized material flow. As a result, planning certainty and system comprehension of all parties involved increase significantly, so that the presented procedure serves as a valuable decision support. This paper describes the steps to be taken from production data to an optimized material flow being verified by a Virtual Design Review.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Berssenbrügge, Jan and Stöcklein, Jörg and Köchling, Daniel},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {Material flow optimization, Virtual Design Review, Virtual Reality},
	pages = {587--596},
	file = {Full Text PDF:files/311/Berssenbrügge et al. - 2016 - Interactive VR-based Visualization for Material Flow Simulations.pdf:application/pdf},
}

@inproceedings{he_novel_2018,
	address = {Cham},
	title = {A {Novel} {Way} of {Estimating} a {User}’s {Focus} of {Attention} in a {Virtual} {Environment}},
	isbn = {978-3-319-91581-4},
	doi = {10.1007/978-3-319-91581-4_6},
	abstract = {Results from prior experiments suggested that measuring immersion objectively (using eye trackers) can be a very important supplement to subjective tests (with questionnaires). But, traditional eye trackers are not usable together with VR HMDs (Head Mounted Displays) because they cannot “see” an audience’s eyes occluded by helmets. The eye trackers compatible with HMDs are not easily accessible to students, researchers and developers in small studios because of the high prices. This paper explores a novel way of estimating a user’s focus of attention in a virtual environment. An experiment measuring the relationship between subject’s head movement and eyesight was conducted to investigate whether eye movement can be closely approximated by head rotation. The findings suggested that people’s eyesight tended to remain in the central area of the HMD when playing a VR game and the HMD orientation data was very close to the eyesight direction. And therefore, this novel way that employs no other equipment than HMDs themselves can hopefully be used to estimate a user’s focus of attention in a much more economic and convenient manner.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Interaction}, {Navigation}, {Visualization}, {Embodiment}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {He, Xuanchao and Liu, Zhejun},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Evaluation, Focus of attention, Head movement, HMD, VR games},
	pages = {71--81},
	file = {Full Text PDF:files/333/He e Liu - 2018 - A Novel Way of Estimating a User’s Focus of Attention in a Virtual Environment.pdf:application/pdf},
}

@inproceedings{gheorghe_rehabilitation_2015,
	address = {Cham},
	title = {Rehabilitation of {Balance}-{Impaired} {Stroke} {Patients} {Through} {Audio}-{Visual} {Biofeedback}},
	isbn = {978-3-319-20684-4},
	doi = {10.1007/978-3-319-20684-4_29},
	abstract = {This study explored how audio-visual biofeedback influences physical balance of seven balance-impaired stroke patients, between 33–70 years-of-age. The setup included a bespoke balance board and a music rhythm game. The procedure was designed as follows: (1) a control group who performed a balance training exercise without any technological input, (2) a visual biofeedback group, performing via visual input, and (3) an audio-visual biofeedback group, performing via audio and visual input. Results retrieved from comparisons between the data sets (2) and (3) suggested superior postural stability between test sessions for (2). Regarding the data set (1), the testers were less motivated to perform training exercises although their performance was superior to (2) and (3). Conclusions are that the audio component motivated patients to train although the physical performance was decreased.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Access} to {Learning}, {Health} and {Well}-{Being}},
	publisher = {Springer International Publishing},
	author = {Gheorghe, Cristina and Nissen, Thomas and Christensen, Daniel and Epure, Paula and Brooks, Anthony and Brooks, Eva Petersson},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2015},
	keywords = {Audio-visual biofeedback, Postural stability, Stroke rehabilitation},
	pages = {300--311},
	file = {Full Text PDF:files/335/Gheorghe et al. - 2015 - Rehabilitation of Balance-Impaired Stroke Patients Through Audio-Visual Biofeedback.pdf:application/pdf},
}

@inproceedings{visser_robocup_2015,
	address = {Cham},
	title = {{RoboCup} {Rescue} {Simulation} {Innovation} {Strategy}},
	isbn = {978-3-319-18615-3},
	doi = {10.1007/978-3-319-18615-3_54},
	abstract = {The RoboCup rescue simulation competitions have been held since 2001. The experience gained during these competitions has supported the development of multi-agent and robotics based solution for disaster mitigation. The league consists of three distinct competitions. These competitions are the agent competition, the virtual robots competition, and the infrastructure competition. The main goal of the infrastructure competition is to increase every year the challenge and to drive the innovation of the league, while the agent and virtual robot competition are focused on developing intelligent agents and robot control systems that can cope with those challenges. This paper provides an overview on the current state-of-the-art in the league and developments and innovations planned for the future.},
	language = {en},
	booktitle = {{RoboCup} 2014: {Robot} {World} {Cup} {XVIII}},
	publisher = {Springer International Publishing},
	author = {Visser, Arnoud and Ito, Nobuhiro and Kleiner, Alexander},
	editor = {Bianchi, Reinaldo A. C. and Akin, H. Levent and Ramamoorthy, Subramanian and Sugiura, Komei},
	year = {2015},
	keywords = {Fire Brigade, Incident Commander, Rescue Robot, Task Allocation, Traffic Simulation},
	pages = {661--672},
	file = {Full Text PDF:files/337/Visser et al. - 2015 - RoboCup Rescue Simulation Innovation Strategy.pdf:application/pdf},
}

@inproceedings{prado_desertesejo_2019,
	address = {Cham},
	title = {Desertesejo (2000/2014): {Notes} on the {Restoration} {Process}},
	isbn = {978-3-030-22636-7},
	shorttitle = {Desertesejo (2000/2014)},
	doi = {10.1007/978-3-030-22636-7_17},
	abstract = {The objective of this work is to present and discuss some issues of the Desertesejo project Desertesejo (Gilbertto Prado, 2000, developed at Program New Media Directions - Itaú Cultural, in Sao Paulo) and its restoration occurred in 2014. Desertesejo is an artistic interactive multi-user virtual environment initially developed in VRML. The project is a poetical exploration of geographical extension, the temporary ruptures, the loneliness, the constant reinvention and the proliferation of points of meeting and sharing.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Design} {Practice} in {Contemporary} {Societies}},
	publisher = {Springer International Publishing},
	author = {Prado, Gilbertto and Cuzziol, Marcos},
	editor = {Kurosu, Masaaki},
	year = {2019},
	keywords = {Art media, Digital poetry, Interactive installation, Poéticas digitais, Virtual reality},
	pages = {239--252},
	file = {Full Text PDF:files/339/Prado e Cuzziol - 2019 - Desertesejo (20002014) Notes on the Restoration Process.pdf:application/pdf},
}

@inproceedings{strentzsch_digital_2017,
	address = {Cham},
	title = {Digital {Map} {Table} {VR}: {Bringing} an {Interactive} {System} to {Virtual} {Reality}},
	isbn = {978-3-319-57987-0},
	shorttitle = {Digital {Map} {Table} {VR}},
	doi = {10.1007/978-3-319-57987-0_5},
	abstract = {Virtual Reality has the potential to replace large, multi-screen systems due to cost, flexibility, and mobility. It is desireable to keep interaction concepts from existing systems in virtual reality and avoid having to recreate existing software solutions. We recreated an interactive table in virtual reality using the same software used for the hardware table and conducted a user study to compare the two systems. Users were able to complete all tasks on both systems and results show little difference regarding preference between the two systems. While only a few of the advantages of virtual reality were taken advantage of, the results are promising for virtual display environments in general.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Strentzsch, Gunnar and van de Camp, Florian and Stiefelhagen, Rainer},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Interaction, Touchscreens, User study, Virtual reality},
	pages = {54--71},
	file = {Full Text PDF:files/341/Strentzsch et al. - 2017 - Digital Map Table VR Bringing an Interactive System to Virtual Reality.pdf:application/pdf},
}

@inproceedings{zhang_enabling_2019,
	address = {Cham},
	title = {Enabling {Immunology} {Learning} in {Virtual} {Reality} {Through} {Storytelling} and {Interactivity}},
	isbn = {978-3-030-21565-1},
	doi = {10.1007/978-3-030-21565-1_28},
	abstract = {Immunology concepts typically taught at the college level involve both factual and process-based knowledge and present learning barriers to college students. Immunology knowledge can be difficult for students to visualize and relate to. To help students better understand specific immunology concepts and increase their learning motivation and engagement, we designed the Immunology virtual reality (VR) application. Immunology VR leverages the rich interactivity and immersion offered by virtual reality systems to create a highly interactive and narrative-driven immersive VR experience that takes students on an exciting journey inside the human body. In this paper, we describe the design of the Immunology VR experience, focusing on our use of an interactive digital storytelling approach to enable learning.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Zhang, Lei and Bowman, Doug A. and Jones, Caroline N.},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {Education, Immersive virtual reality, Immunology, Instructional design, Interactivity, Learning, Storytelling},
	pages = {410--425},
	file = {Full Text PDF:files/343/Zhang et al. - 2019 - Enabling Immunology Learning in Virtual Reality Through Storytelling and Interactivity.pdf:application/pdf},
}

@inproceedings{bak_domain_2018,
	address = {Cham},
	title = {Domain {Adaptation} {Through} {Synthesis} for {Unsupervised} {Person} {Re}-identification},
	isbn = {978-3-030-01261-8},
	doi = {10.1007/978-3-030-01261-8_12},
	abstract = {Drastic variations in illumination across surveillance cameras make the person re-identification problem extremely challenging. Current large scale re-identification datasets have a significant number of training subjects, but lack diversity in lighting conditions. As a result, a trained model requires fine-tuning to become effective under an unseen illumination condition. To alleviate this problem, we introduce a new synthetic dataset that contains hundreds of illumination conditions. Specifically, we use 100 virtual humans illuminated with multiple HDR environment maps which accurately model realistic indoor and outdoor lighting. To achieve better accuracy in unseen illumination conditions we propose a novel domain adaptation technique that takes advantage of our synthetic data and performs fine-tuning in a completely unsupervised way. Our approach yields significantly higher accuracy than semi-supervised and unsupervised state-of-the-art methods, and is very competitive with supervised techniques.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Bąk, Sławomir and Carr, Peter and Lalonde, Jean-François},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Domain adaptation, Identification, Synthetic, Unsupervised},
	pages = {193--209},
	file = {Full Text PDF:files/345/Bąk et al. - 2018 - Domain Adaptation Through Synthesis for Unsupervised Person Re-identification.pdf:application/pdf},
}

@inproceedings{yu_minergate_2020,
	address = {Singapore},
	title = {{MinerGate}: {A} {Novel} {Generic} and {Accurate} {Defense} {Solution} {Against} {Web} {Based} {Cryptocurrency} {Mining} {Attacks}},
	isbn = {978-981-334-922-3},
	shorttitle = {{MinerGate}},
	doi = {10.1007/978-981-33-4922-3_5},
	abstract = {Web-based cryptocurrency mining attacks, also known as cryptojacking, become increasingly popular. A large number of diverse platforms (e.g., Windows, Linux, Android, and iOS) and devices (e.g., PC, smartphones, tablets, and even critical infrastructures) are widely impacted. Although a variety of detection approaches were recently proposed, it is challenging to apply these approaches to attack prevention directly.},
	language = {en},
	booktitle = {Cyber {Security}},
	publisher = {Springer},
	author = {Yu, Guorui and Yang, Guangliang and Li, Tongxin and Han, Xinhui and Guan, Shijie and Zhang, Jialong and Gu, Guofei},
	editor = {Lu, Wei and Wen, Qiaoyan and Zhang, Yuqing and Lang, Bo and Wen, Weiping and Yan, Hanbing and Li, Chao and Ding, Li and Li, Ruiguang and Zhou, Yu},
	year = {2020},
	keywords = {asm.js, Cryptojacking, WebAssembly},
	pages = {50--70},
	file = {Full Text PDF:files/347/Yu et al. - 2020 - MinerGate A Novel Generic and Accurate Defense Solution Against Web Based Cryptocurrency Mining Att.pdf:application/pdf},
}

@inproceedings{hashimoto_projection_2017,
	address = {Cham},
	title = {Projection {Simulator} to {Support} {Design} {Development} of {Spherical} {Immersive} {Display}},
	isbn = {978-3-319-58753-0},
	doi = {10.1007/978-3-319-58753-0_3},
	abstract = {This research aims to develop a simulator that supports the construction of a spherical immersive display, which is a system that can provide a realistic presence, as if the user exists in another space. In general, when developing a display, it is necessary to perform optical design of the projection system in considering special distortion correction on the dome screen. However, accuracy of the optical system that is actually manufactured is not guaranteed to be when it is simulated, and fine adjustment is again necessary when the display is used. In this research, we report on the development of a projection simulator that can perform optical system adjustment and distortion correction simultaneously during optical design of the projection system.},
	language = {en},
	booktitle = {{HCI} {International} 2017 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Hashimoto, Wataru and Mizutani, Yasuharu and Nishiguchi, Satoshi},
	editor = {Stephanidis, Constantine},
	year = {2017},
	keywords = {Distortion correction, Image registration, Optical design, Spatially immersive display},
	pages = {17--24},
	file = {Full Text PDF:files/334/Hashimoto et al. - 2017 - Projection Simulator to Support Design Development of Spherical Immersive Display.pdf:application/pdf},
}

@inproceedings{bradshaw_ora_2014,
	address = {Berlin, Heidelberg},
	title = {Ora – {Save} the {Forest}! {Designing} a {Social} {Impact} {Game}},
	isbn = {978-3-662-45212-7},
	doi = {10.1007/978-3-662-45212-7_11},
	abstract = {Computer models for designing educational games need to have practical applications as well as underlying theoretical principles. In this paper, we present the Structural Playability Process (SPP), a new approach for designing and implementing serious games. Using the SPP designed game Ora – Save the Forest! as a case study, we describe the four SPP spaces: education, translation, design and engine. Ora is a forest-pest-management game based on scientific models and intended to inform players about the complexities of ecosystem management. Preliminary user study results show that SPP is an effective method of producing motivating and successful learning environments.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2014},
	publisher = {Springer},
	author = {Bradshaw, Hazel and Holland, E. Penelope and Billinghurst, Mark},
	editor = {Pisan, Yusuf and Sgouros, Nikitas M. and Marsh, Tim},
	year = {2014},
	keywords = {Flow, Game-design, Game-play, Motivation, Serious/Educational games, Structural Playability (SPP)},
	pages = {84--91},
	file = {Full Text PDF:files/336/Bradshaw et al. - 2014 - Ora – Save the Forest! Designing a Social Impact Game.pdf:application/pdf},
}

@inproceedings{novick_usability_2020,
	address = {Cham},
	title = {Usability of the {Virtual} {Agent} {Interaction} {Framework}},
	isbn = {978-3-030-49695-1},
	doi = {10.1007/978-3-030-49695-1_9},
	abstract = {The Virtual Agent Interaction Framework (VAIF) is an authoring tool for creating virtual-reality applications with embodied conversational agents. VAIF is intended for use by both expert and non-expert users, in contrast with more sophisticated and complex development tools such as the Virtual Human Toolkit. To determine if VAIF is actually usable by a range of users, we conducted a two-phase summative usability test, with a total of 43 participants. We also tried porting to VAIF a scene from an earlier VR application. The results of the usability study suggest that people with little or even no experience in creating embodied conversational agents can install VAIF and build interaction scenes from scratch, with relatively low rates of encountering problem episodes. However, the usability testing disclosed aspects of VAIF and its user’s guide that could be improved to reduce the number of problem episodes that users encounter.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Design} and {Interaction}},
	publisher = {Springer International Publishing},
	author = {Novick, David and Afravi, Mahdokht and Martinez, Oliver and Rodriguez, Aaron and Hinojos, Laura J.},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Authoring systems, Embodied conversational agents, Virtual reality},
	pages = {123--134},
	file = {Full Text PDF:files/338/Novick et al. - 2020 - Usability of the Virtual Agent Interaction Framework.pdf:application/pdf},
}

@inproceedings{boldyreff_undergraduate_2009,
	address = {Berlin, Heidelberg},
	title = {Undergraduate {Research} {Opportunities} in {OSS}},
	isbn = {978-3-642-02032-2},
	doi = {10.1007/978-3-642-02032-2_30},
	abstract = {Using Open Source Software (OSS) in undergraduate teaching in universities is now commonplace. Students use OSS applications and systems in their courses on programming, operating systems, DBMS, web development to name but a few. Studying OSS projects from both a product and a process view also forms part of the software engineering curriculum at various universities. Many students have taken part in OSS projects as well as developers.},
	language = {en},
	booktitle = {Open {Source} {Ecosystems}: {Diverse} {Communities} {Interacting}},
	publisher = {Springer},
	author = {Boldyreff, Cornelia and Capiluppi, Andrea and Knowles, Thomas and Munro, James},
	editor = {Boldyreff, Cornelia and Crowston, Kevin and Lundell, Björn and Wasserman, Anthony I.},
	year = {2009},
	keywords = {Instant Messaging System, Open Source Soft Community, Open Source Software, Open Source Software Project, Student Researcher},
	pages = {340--350},
	file = {Full Text PDF:files/340/Boldyreff et al. - 2009 - Undergraduate Research Opportunities in OSS.pdf:application/pdf},
}

@inproceedings{cook_keep_2015,
	address = {Cham},
	title = {“{Keep} {What} {You}’ve {Earned}”: {Encouraging} {Sailors} to {Drink} {Responsibly}},
	isbn = {978-3-319-20889-3},
	shorttitle = {“{Keep} {What} {You}’ve {Earned}”},
	doi = {10.1007/978-3-319-20889-3_53},
	abstract = {The U.S. Navy contracted Booz Allen Hamilton, a strategy and technology consulting firm, to develop and implement a social marketing campaign to encourage Sailors to drink responsibly. The “Keep What You’ve Earned” campaign, launched in April 2013, aims to encourage Sailors to drink responsibly through the use of affirmative messaging, reminding them of all they have accomplished in their Navy careers. The primary product of the social marketing campaign is a mobile application game that combines role-playing with real-life tools to help encourage Sailors to drink responsibly. Navy leadership has indicated that the Keep What You’ve Earned campaign, in combination with other cultural and policy-related changes, contributed to a decline in the number of reported alcohol-related incidents.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Interactive} {Experience} {Design}},
	publisher = {Springer International Publishing},
	author = {Cook, Kristina and Brennan, Erin and Gray, Colleen and Kennard, Teha},
	editor = {Marcus, Aaron},
	year = {2015},
	keywords = {Alcohol abuse prevention, Behavior change, Gamification, Health communication model, Social marketing, U.S. Navy},
	pages = {575--586},
	file = {Full Text PDF:files/342/Cook et al. - 2015 - “Keep What You’ve Earned” Encouraging Sailors to Drink Responsibly.pdf:application/pdf},
}

@inproceedings{larpkiattaworn_multiuser_2020,
	address = {Cham},
	title = {Multiuser {Virtual} {Reality} for {Designing} and {Editing} {3D} {Models}},
	isbn = {978-3-030-50729-9},
	doi = {10.1007/978-3-030-50729-9_12},
	abstract = {This research proposes the development of a virtual reality system in which multiple users could collaboratively design and edit 3D models. In the virtual environment, users also obtained the rendered version of the actual environment using the Kinect’s point clouds. The proposed system provides several features of model building, coloring, realistic dimensions of actual environment, user’s collaboration, and STL file exporting for 3D printing.},
	language = {en},
	booktitle = {{HCI} {International} 2020 - {Posters}},
	publisher = {Springer International Publishing},
	author = {Larpkiattaworn, Noppasorn and Chareonwuttikajorn, Pitijit and Punya, Pattaraporn and Charoenseang, Siam},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2020},
	keywords = {3D model, Multiuser, Unity3D, Virtual reality},
	pages = {85--91},
	file = {Full Text PDF:files/344/Larpkiattaworn et al. - 2020 - Multiuser Virtual Reality for Designing and Editing 3D Models.pdf:application/pdf},
}

@inproceedings{kalay_vlc-enabled_2020,
	address = {Cham},
	title = {{VLC}-{Enabled} {Human}-{Aware} {Building} {Management} {System}},
	isbn = {978-3-030-50344-4},
	doi = {10.1007/978-3-030-50344-4_16},
	abstract = {“Smart” buildings that can sense and detect people’s presence have been in use for the past few decades, mostly using technologies that trigger reactive responses such as turning on/off heating/ventilating, lighting, security, etc. We argue that to be considered truly smart, buildings must become “aware” about the locations and activities of their inhabitants so they can proactively engage with the occupants and inform their decision making with respect to which actions to execute, by whom and where.},
	language = {en},
	booktitle = {Distributed, {Ambient} and {Pervasive} {Interactions}},
	publisher = {Springer International Publishing},
	author = {Kalay, Yehuda E. and Sathyanarayanan, Haripriya and Schaumann, Davide and Wang, Albert and Chen, Gang and Pai, Ramdas G.},
	editor = {Streitz, Norbert and Konomi, Shin'ichi},
	year = {2020},
	keywords = {Hospital environments, Human behavior simulation, Smart environments, Space utilization, Visible light communication},
	pages = {207--222},
	file = {Full Text PDF:files/346/Kalay et al. - 2020 - VLC-Enabled Human-Aware Building Management System.pdf:application/pdf},
}

@inproceedings{senette_interactive_2018,
	address = {Cham},
	title = {An {Interactive} {Cognitive}-{Motor} {Training} {System} for {Children} with {Intellectual} {Disability}},
	isbn = {978-3-319-92049-8},
	doi = {10.1007/978-3-319-92049-8_42},
	abstract = {It is increasingly evident that engaging in regular physical activity is important for people’s health and well-being. However, physical training is still a big challenge for individuals with cognitive disabilities since it is difficult to motivate them and provide them with sustained pleasant training experiences over time. Active Video Games and Exergames may help achieve this, especially in the younger population. This paper describes an accessible Interactive Cognitive-Motor Training system (ICMT) created to encourage physical activity in children with cognitive disabilities by combining cognitive and gross motor training. The system was developed at a low cost, on top of an open source rhythm game, which has built-in support for dance pads and large video screens. The application employs user profiling in order to deliver personalized training. Performance data are recorded for further analysis to verify the training’s efficacy and if needed, to tune the intervention. A pilot study showed the effectiveness of the proposed system, which by taking advantage of the positive effects of playing videogames, appears to encourage cognitively impaired people’s physical activity.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Methods}, {Technologies}, and {Users}},
	publisher = {Springer International Publishing},
	author = {Senette, Caterina and Trujillo, Amaury and Perrone, Erico and Bargagna, Stefania and Buzzi, Maria Claudia and Buzzi, Marina and Leporini, Barbara and Piatti, Alice Elena},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2018},
	keywords = {Cognitive impairment, Physical activity, Video game},
	pages = {571--582},
	file = {Full Text PDF:files/348/Senette et al. - 2018 - An Interactive Cognitive-Motor Training System for Children with Intellectual Disability.pdf:application/pdf},
}

@inproceedings{mehm_future_2012,
	address = {Berlin, Heidelberg},
	title = {Future {Trends} in {Game} {Authoring} {Tools}},
	isbn = {978-3-642-33542-6},
	doi = {10.1007/978-3-642-33542-6_70},
	abstract = {Authoring Tools for digital games are used to create games from scratch, integrate content and game mechanics easily and can assist in a multitude of ways in the production chain of a game. For example, they can allow non-programmers to work on the game logic by means of domain-specific or visual programming languages, increase the collaboration between team members by integrating computer-supported collaborative work techniques, assist in catching errors in the game by model checking and offer publishing to multiple platforms by saving games in an intermediate format which can be run on various systems. This already interesting and viable approach can be extended in a number of ways which we exemplify in this position paper to indicate possible future directions for game authoring tools.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2012},
	publisher = {Springer},
	author = {Mehm, Florian and Reuter, Christian and Göbel, Stefan and Steinmetz, Ralf},
	editor = {Herrlich, Marc and Malaka, Rainer and Masuch, Maic},
	year = {2012},
	keywords = {Authoring Tool, Domain-Specific Language, In-Game Editing, Multiplayer, Procedural Content Generation},
	pages = {536--541},
	file = {Full Text PDF:files/349/Mehm et al. - 2012 - Future Trends in Game Authoring Tools.pdf:application/pdf},
}

@inproceedings{kitagawa_vigor_2017,
	address = {Cham},
	title = {{VIGOR}: {Virtual} {Interaction} with {Gravitational} {Waves} to {Observe} {Relativity}},
	isbn = {978-3-319-57987-0},
	shorttitle = {{VIGOR}},
	doi = {10.1007/978-3-319-57987-0_33},
	abstract = {In 2015, a century after Albert Einstein published his theory of general relativity, the Laser Interferometer Gravitational-wave Observatory (LIGO) detected gravitational waves from binary black holes fully consistent with this theory. Our goal for VIGOR (Virtual-reality Interaction with Gravitational waves to Observe Relativity) is to communicate this revolutionary discovery to the public by visualizing the gravitational waves emitted by binary black holes. VIGOR has been developed using the Unity game engine and VR headsets (Oculus Rift DK2 and Samsung Gear VR). Wearing a VR headset, VIGOR users control an avatar to “fly” around binary black holes, experiment on the black holes by manipulating their total mass, mass ratio, and orbital separation, and witness how gravitational waves emitted by the black holes stretch and squeeze the avatar. We evaluated our prototype of VIGOR with high school students in 2016 and are further improving VIGOR based on our findings.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Kitagawa, Midori and Kesden, Michael and Tran, Ngoc and Sivampillai Venlayudam, Thulasi and Urquhart, Mary and Malina, Roger},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Binary black holes, Gravitational waves, Head-Mounted display, Physics education, Virtual reality},
	pages = {404--416},
	file = {Full Text PDF:files/350/Kitagawa et al. - 2017 - VIGOR Virtual Interaction with Gravitational Waves to Observe Relativity.pdf:application/pdf},
}

@inproceedings{cai_towards_2003,
	address = {Berlin, Heidelberg},
	title = {Towards {Biomedical} {Problem} {Solving} in a {Game} {Environment}},
	isbn = {978-3-540-44863-1},
	doi = {10.1007/3-540-44863-2_99},
	abstract = {Biomedical systems involve complex interactions between diverse components. Problem solving in such systems requires insight, i.e. the capability to make non-obvious connections. In this paper, we present a game-based problem solving environment, where users can explore biological interactions with navigation on atomic to macroscopic scales, role-play, and networked collaboration. The study investigates the system architecture of the biological game, bio-morphing characters, and bio-interactions with bio-sensing and bio-dynamics. The prototype has been implemented on PC and tested in a preschool environment where users have little knowledge in biology. The experiment shows that the game greatly inspired users both in concept learning and entertainment.},
	language = {en},
	booktitle = {Computational {Science} — {ICCS} 2003},
	publisher = {Springer},
	author = {Cai, Yang and Snel, Ingo and Bharathi, B. Suman and Klein, Clementine and Klein-Seetharaman, Judith},
	editor = {Sloot, Peter M. A. and Abramson, David and Bogdanov, Alexander V. and Gorbachev, Yuriy E. and Dongarra, Jack J. and Zomaya, Albert Y.},
	year = {2003},
	keywords = {Biological Character, Biomedical Problem, Implicit Learning, Pace Strategy, World Model},
	pages = {1005--1014},
	file = {Full Text PDF:files/351/Cai et al. - 2003 - Towards Biomedical Problem Solving in a Game Environment.pdf:application/pdf},
}

@inproceedings{schreiber_new_2009,
	address = {Berlin, Heidelberg},
	title = {New {Interaction} {Concepts} by {Using} the {Wii} {Remote}},
	isbn = {978-3-642-02577-8},
	doi = {10.1007/978-3-642-02577-8_29},
	abstract = {The interaction concept of the video game console Nintendo Wii has created a furor in the interface design community due to its intuitive interface: the Wii Remote. At the Institute of Ergonomics (IAD) of the Darmstadt University of Technology, several projects investigated the potential of interaction concepts with the Wii Remote, especially in nongaming contexts. In a first study an interactive whiteboard according to [1] was recreated, modified and evaluated. In this case, the Wii Remote is not the human-machine-interface but the sensor that detects an infrared emitting (IR) pencil. A survey with 15 subjects was conducted in which different IR pencils were evaluated. In a second study the potential of a gesture based human-computer interaction with the help of the Wii-Remote according to [2] was evaluated by using a multimedia software application. In a survey with 30 subjects, the Wii gesture interaction was compared to a standard remote control.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Novel} {Interaction} {Methods} and {Techniques}},
	publisher = {Springer},
	author = {Schreiber, Michael and von Wilamowitz-Moellendorff, Margeritta and Bruder, Ralph},
	editor = {Jacko, Julie A.},
	year = {2009},
	keywords = {gesture based interaction, interactive whiteboard, Wii, Wii Remote},
	pages = {261--270},
	file = {Full Text PDF:files/372/Schreiber et al. - 2009 - New Interaction Concepts by Using the Wii Remote.pdf:application/pdf},
}

@inproceedings{lv_domain_2019,
	address = {Cham},
	title = {Domain {Adaptive} {Semantic} {Segmentation} {Through} {Structure} {Enhancement}},
	isbn = {978-3-030-11012-3},
	doi = {10.1007/978-3-030-11012-3_13},
	abstract = {Although fully convolutional networks have recently achieved great advances in semantic segmentation, the performance leaps heavily rely on supervision with pixel-level annotations which are extremely expensive and time-consuming to collect. Training models on synthetic data is a feasible way to relieve the annotation burden. However, the domain shift between synthetic and real images usually lead to poor generalization performance. In this work, we propose an effective method to adapt the segmentation network trained on synthetic images to real scenarios in an unsupervised fashion. To improve the adaptation performance for semantic segmentation, we enhance the structure information of the target images at both the feature level and the output level. Specifically, we enforce the segmentation network to learn a representation that encodes the target images’ visual cues through image reconstruction, which is beneficial to the structured prediction of the target images. Further more, we implement adversarial training at the output space of the segmentation network to align the structured prediction of the source and target images based on the similar spatial structure they share. To validate the performance of our method, we conduct comprehensive experiments on the “GTA5 to Cityscapes” dataset which is a standard domain adaptation benchmark for semantic segmentation. The experimental results clearly demonstrate that our method can effectively bridge the synthetic and real image domains and obtain better adaptation performance compared with the existing state-of-the-art methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Lv, Fengmao and Lian, Qing and Yang, Guowu and Lin, Guosheng and Pan, Sinno Jialin and Duan, Lixin},
	editor = {Leal-Taixé, Laura and Roth, Stefan},
	year = {2019},
	keywords = {Deep learning, Semantic segmentation, Transfer learning, Unsupervised domain adaptation},
	pages = {172--179},
	file = {Full Text PDF:files/375/Lv et al. - 2019 - Domain Adaptive Semantic Segmentation Through Structure Enhancement.pdf:application/pdf},
}

@inproceedings{jin_interactive_2020,
	address = {Cham},
	title = {Interactive {Narrative} in {Augmented} {Reality}: {An} {Extended} {Reality} of the {Holocaust}},
	isbn = {978-3-030-49698-2},
	shorttitle = {Interactive {Narrative} in {Augmented} {Reality}},
	doi = {10.1007/978-3-030-49698-2_17},
	abstract = {In this research, the author descripted new narrative media known as Immersive Augmented Reality Environment (IARE) with HoloLens. Aarseth’s narrative model [17] and all available input design in IARE were reviewed and summarised. Based on these findings, The AR Journey, a HoloLens app aiming at interactive narrative for moral education purpose, was developed and assessed. Qualitative methods of interview and observation were used and the results were analysed. In general, narrative in IARE were proved to be valid for moral education purpose, and findings including valid narrative structure, input model, design guidelines were revealed.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Industrial} and {Everyday} {Life} {Applications}},
	publisher = {Springer International Publishing},
	author = {Jin, Yunshui and Ma, Minhua and Liu, Yun},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Augmented reality, Holocaust, Interactive narrative, Microsoft Hololens, Mixed reality, Moral education},
	pages = {249--269},
	file = {Full Text PDF:files/377/Jin et al. - 2020 - Interactive Narrative in Augmented Reality An Extended Reality of the Holocaust.pdf:application/pdf},
}

@inproceedings{sarkar_understanding_2015,
	address = {Cham},
	title = {Understanding and {Improving} {Collaborative} {Skills} {Among} {Individuals} with {ASD} in a {Distributed} {Virtual} {Environment}},
	isbn = {978-3-319-20684-4},
	doi = {10.1007/978-3-319-20684-4_64},
	abstract = {Individuals with Autism Spectrum Disorders (ASD) evidence core impairments regarding social interaction and communication. These impairments can inhibit the ability of individuals with ASD from effectively engaging with peers and collaborating on goal-oriented tasks. Recently collaborative virtual environment (CVE) in which individuals with ASD can interact with one another or with a therapist to achieve some common goal has been proposed for social competence interventions (SCI) for these individuals. In this paper, we present the design of a distributed CVE for playing the classic video game pong to be used for SCI. This collaborative game can be played at several different modes ranging from one player against an artificial agent in one computer to two players against each other in two different computers. The system functionality and robustness were validated through a small user study. In the future, this CVE will be evaluated with children and adolescents with ASD.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Access} to {Learning}, {Health} and {Well}-{Being}},
	publisher = {Springer International Publishing},
	author = {Sarkar, Arpan and Wade, Joshua and Warren, Zachary},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2015},
	keywords = {Autism Spectrum Disorder (ASD), Collaborative Virtual Environment (CVE)},
	pages = {669--680},
	file = {Full Text PDF:files/379/Sarkar et al. - 2015 - Understanding and Improving Collaborative Skills Among Individuals with ASD in a Distributed Virtual.pdf:application/pdf},
}

@inproceedings{bruder_touching_2013,
	address = {Berlin, Heidelberg},
	title = {Touching the {Void} {Revisited}: {Analyses} of {Touch} {Behavior} on and above {Tabletop} {Surfaces}},
	isbn = {978-3-642-40483-2},
	shorttitle = {Touching the {Void} {Revisited}},
	doi = {10.1007/978-3-642-40483-2_19},
	abstract = {Recent developments in touch and display technologies made it possible to integrate touch-sensitive surfaces into stereoscopic three-dimensional (3D) displays. Although this combination provides a compelling user experience, interaction with stereoscopically displayed objects poses some fundamental challenges. If a user aims to select a 3D object, each eye sees a different perspective of the same scene. This results in two distinct projections on the display surface, which raises the question where users would touch in 3D or on the two-dimensional (2D) surface to indicate the selection. In this paper we analyze the relation between the 3D positions of stereoscopically displayed objects and the on- as well as off-surface touch areas. The results show that 2D touch interaction works better close to the screen but also that 3D interaction is more suitable beyond 10cm from the screen. Finally, we discuss implications for the development of future touch-sensitive interfaces with stereoscopic display.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2013},
	publisher = {Springer},
	author = {Bruder, Gerd and Steinicke, Frank and Stuerzlinger, Wolfgang},
	editor = {Kotzé, Paula and Marsden, Gary and Lindgaard, Gitte and Wesson, Janet and Winckler, Marco},
	year = {2013},
	keywords = {3D interaction, stereoscopic displays, Touch-sensitive systems},
	pages = {278--296},
	file = {Full Text PDF:files/382/Bruder et al. - 2013 - Touching the Void Revisited Analyses of Touch Behavior on and above Tabletop Surfaces.pdf:application/pdf},
}

@inproceedings{adiani_usability_2019,
	address = {Cham},
	title = {Usability {Enhancement} and {Functional} {Extension} of a {Digital} {Tool} for {Rapid} {Assessment} of {Risk} for {Autism} {Spectrum} {Disorders} in {Toddlers} {Based} on {Pilot} {Test} and {Interview} {Data}},
	isbn = {978-3-030-23563-5},
	doi = {10.1007/978-3-030-23563-5_2},
	abstract = {Early accurate identification and treatment of young children with Autism Spectrum Disorder (ASD) represents a pressing public health and clinical care challenge. Unfortunately, large numbers of children are still not screened for ASD, waits for specialized diagnostic assessment can be very long, and the average age of diagnosis in the US remains between 4 to 5 years of age. In a step towards meaningfully addressing this issue, we previously developed Autoscreen: a digital tool for accurate and time-efficient screening, diagnostic triage, referral, and treatment engagement of young children with ASD concerns within community pediatric settings. In the current work, we significantly improve upon and expand Autoscreen based on usability data and interview data collected in a pilot investigation of pediatric healthcare providers using Autoscreen. The enhanced version of Autoscreen addresses limitations of the previous tool, such as scalability, and introduces important new features based on rigorous interviews with the target user population. Once validated on a large sample, Autoscreen could become an impactful tool for early ASD screening and targeted referral in primary care settings. The comprehensively-enhanced tool described in the current work will enable the investigative team to achieve this goal.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Multimodality} and {Assistive} {Environments}},
	publisher = {Springer International Publishing},
	author = {Adiani, Deeksha and Schmidt, Michael and Wade, Joshua and Swanson, Amy R. and Weitlauf, Amy and Warren, Zachary and Sarkar, Nilanjan},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2019},
	keywords = {Autism Spectrum Disorders, Digital screening, Scalability, Usability enhancement},
	pages = {13--22},
	file = {Full Text PDF:files/384/Adiani et al. - 2019 - Usability Enhancement and Functional Extension of a Digital Tool for Rapid Assessment of Risk for Au.pdf:application/pdf},
}

@inproceedings{nyamse_design_2013,
	address = {Berlin, Heidelberg},
	title = {The {Design} {Considerations} of a {Virtual} {Reality} {Application} for {Heart} {Anatomy} and {Pathology} {Education}},
	isbn = {978-3-642-39420-1},
	doi = {10.1007/978-3-642-39420-1_8},
	abstract = {Anatomy and pathology of the human body are complex subjects that cannot be elucidated easily to the medical students through traditional description and illustration methods. The proposed interactive system aims to present clear information on demand. For enhancing further the three-dimensional understanding of the anatomical information, a virtual reality environment was developed in order to accommodate different 3D models of the human body. In this case we opted for the heart model as it presents a unique section of the body that can produce motion and sound. The produced model was further simplified for use by patients who wish to understand better the generic anatomy and typical pathologies of the heart. Additionally the paper presents the data results of the system evaluation performed by ten users. The derived results although promising, highlighted some benefits and drawbacks of the proposed system that we aim, to improve in the near future. Finally the paper concludes with a plan of future work which will entail further interactivity through audio incorporation and gesture recognition.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Systems} and {Applications}},
	publisher = {Springer},
	author = {Nyamse, Victor and Charissis, Vassilis and Moore, J. David and Parker, Caroline and Khan, Soheeb and Chan, Warren},
	editor = {Shumaker, Randall},
	year = {2013},
	keywords = {3D Visualization, Anatomy, HCI, Heart Disease, Pathology, Virtual Reality},
	pages = {66--73},
	file = {Full Text PDF:files/386/Nyamse et al. - 2013 - The Design Considerations of a Virtual Reality Application for Heart Anatomy and Pathology Education.pdf:application/pdf},
}

@inproceedings{stahl_evaluation_2015,
	address = {Cham},
	title = {Evaluation of {Autonomous} {Approaches} {Using} {Virtual} {Environments}},
	isbn = {978-3-319-21067-4},
	doi = {10.1007/978-3-319-21067-4_51},
	abstract = {In this paper, we address the challenging problem of evaluating autonomous research approaches by the example of an online anomaly detection framework for dynamical real-time systems. We propose to use a virtual test environment that was conceptualized based on the specific evaluation requirements. The architecture is composed of all system parts required for evaluation: the operating system implementing the anomaly detection framework, reconfigurable autonomous applications, an execution platform device for the operating system and its applications, and the device’s environment. We demonstrate our concepts by the example of our miniature robot BeBot that acts as our virtual prototype (VP) to execute autonomous applications. With an interactive module, the virtual environment (VE) offers full control over the environment and the VP so that using different levels of hardware implementation for evaluation, but also failure injection at runtime becomes possible. Our architecture allows to determine clear system boundaries of the particular parts composed of perception function, decision making function and execution function which is essential for evaluating autonomous approaches. We define evaluation scenarios to show the effectiveness of each part of our approach and illustrate the powerfulness of applying virtual test environments to evaluate such approaches as the here referred one.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Stahl, Katharina and Stöcklein, Jörg and Li, Sijia},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2015},
	keywords = {Anomaly Detection, Evaluation Environment, Mechatronic System, Virtual Environment, Virtual Prototype},
	pages = {499--512},
	file = {Full Text PDF:files/388/Stahl et al. - 2015 - Evaluation of Autonomous Approaches Using Virtual Environments.pdf:application/pdf},
}

@inproceedings{silva_apex_2010,
	address = {Berlin, Heidelberg},
	title = {The {APEX} {Framework}: {Prototyping} of {Ubiquitous} {Environments} {Based} on {Petri} {Nets}},
	isbn = {978-3-642-16488-0},
	shorttitle = {The {APEX} {Framework}},
	doi = {10.1007/978-3-642-16488-0_2},
	abstract = {The user experience of ubiquitous environments is a determining factor in their success. The characteristics of such systems must be explored as early as possible to anticipate potential user problems, and to reduce the cost of redesign. However, the development of early prototypes to be evaluated in the target environment can be disruptive to the ongoing system and therefore unacceptable. This paper reports on an ongoing effort to explore how model-based rapid prototyping of ubiquitous environments might be used to avoid actual deployment while still enabling users to interact with a representation of the system. The paper describes APEX, a framework that brings together an existing 3D Application Server with CPN Tools. APEX-based prototypes enable users to navigate a virtual world simulation of the envisaged ubiquitous environment. The APEX architecture and the proposed CPN-based modelling approach are described. An example illustrates their use.},
	language = {en},
	booktitle = {Human-{Centred} {Software} {Engineering}},
	publisher = {Springer},
	author = {Silva, José Luís and Ribeiro, Óscar R. and Fernandes, João M. and Campos, José Creissac and Harrison, Michael D.},
	editor = {Bernhaupt, Regina and Forbrig, Peter and Gulliksen, Jan and Lárusdóttir, Marta},
	year = {2010},
	keywords = {Entry Gate, Ubiquitous Computing, Ubiquitous Environment, Ubiquitous System, Virtual Environment},
	pages = {6--21},
	file = {Full Text PDF:files/390/Silva et al. - 2010 - The APEX Framework Prototyping of Ubiquitous Environments Based on Petri Nets.pdf:application/pdf},
}

@inproceedings{sigitov_distributed_2016,
	address = {Cham},
	title = {Distributed {Unity} {Applications}},
	isbn = {978-3-319-40548-3},
	doi = {10.1007/978-3-319-40548-3_23},
	abstract = {There is a need for rapid prototyping tools for large, high-resolution displays (LHRDs) in both scientific and commercial domains. That is, the area of LHRDs is still poorly explored and possesses no established standards, thus developers have to experiment a lot with new interaction and visualization concepts. Therefore, a rapid prototyping tool for LHRDs has to undertake two functions: ease the process of application development, and make an application runnable on a broad range of LHRD setups. The latter comprises a challenge, since most LHRDs are driven by multiple compute nodes and require distributed applications.},
	language = {en},
	booktitle = {{HCI} {International} 2016 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Sigitov, Anton and Staadt, Oliver and Hinkenjann, André},
	editor = {Stephanidis, Constantine},
	year = {2016},
	keywords = {Distributed rendering, Large, high-resolution displays, Unity},
	pages = {138--143},
	file = {Full Text PDF:files/373/Sigitov et al. - 2016 - Distributed Unity Applications.pdf:application/pdf},
}

@inproceedings{fruth_e-learning_2013,
	address = {Berlin, Heidelberg},
	title = {E-{Learning} of {IT} {Security} {Threats}: {A} {Game} {Prototype} for {Children}},
	isbn = {978-3-642-40779-6},
	shorttitle = {E-{Learning} of {IT} {Security} {Threats}},
	doi = {10.1007/978-3-642-40779-6_14},
	abstract = {In this paper an e-learning game prototype for primary school children (aged between 7 and 9 years) is introduced. The game teaches children about IT security threats , which they encounter using the Internet. The game is separated into three mini games: virus infection of the computer, inviting somebody in social networks, chatting with strangers. The game design used metaphors and based on standard guidelines of infantile learning environments (e.g. paradigm of simplicity, multidimensional stimuli, characters). Furthermore, the results of a user study of 36 primary school children are presented. In the future, the prototype would be extended by additional metaphors.},
	language = {en},
	booktitle = {Communications and {Multimedia} {Security}},
	publisher = {Springer},
	author = {Fruth, Jana and Schulze, Carsten and Rohde, Marleen and Dittmann, Jana},
	editor = {De Decker, Bart and Dittmann, Jana and Kraetzer, Christian and Vielhauer, Claus},
	year = {2013},
	keywords = {children, e-learning, game, internet, IT security threats},
	pages = {162--172},
	file = {Full Text PDF:files/374/Fruth et al. - 2013 - E-Learning of IT Security Threats A Game Prototype for Children.pdf:application/pdf},
}

@inproceedings{harthoorn_interface_2011,
	address = {Berlin, Heidelberg},
	title = {Interface {Design} to {Support} {Situation} {Awareness} in {Virtual} {Puppetry}},
	isbn = {978-3-642-22095-1},
	doi = {10.1007/978-3-642-22095-1_23},
	abstract = {We propose virtual puppetry as a potential mechanism for enhancing students’ presence in a virtual learning environment. To develop this style of interaction will require substantial attention to the user interface in order to promote the operator’s situation awareness of both the real and virtual environments. This presentation describes the development of an initial prototype and some of the ongoing concerns for controlling virtual puppets.},
	language = {en},
	booktitle = {{HCI} {International} 2011 – {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer},
	author = {Harthoorn, Keisha and Hughes, Stephen},
	editor = {Stephanidis, Constantine},
	year = {2011},
	keywords = {Interface Design, Presence, Situation Awareness, Virtual Environments, Virtual Heritage, Virtual Puppetry},
	pages = {112--115},
	file = {Full Text PDF:files/376/Harthoorn e Hughes - 2011 - Interface Design to Support Situation Awareness in Virtual Puppetry.pdf:application/pdf},
}

@inproceedings{kim_emotional_2011,
	address = {Berlin, Heidelberg},
	title = {Emotional {Intelligent} {Contents}: {Expressing} {User}’s {Own} {Emotion} within {Contents}},
	isbn = {978-3-642-24500-8},
	shorttitle = {Emotional {Intelligent} {Contents}},
	doi = {10.1007/978-3-642-24500-8_49},
	abstract = {This paper presents an Emotionally Intelligent Contents (EIC) framework. This framework helps to create content that changes its elements (such as textures, color, light and sound) dynamically in response to a user’s emotional state. Also, this emotionally intelligent content allows users to add their own emotion characters at run-time. This paper presents an overview of the EIC framework designed to adapt a game environment to a user’s emotional state as measured physiologically or through an explicit rating of one’s affective state. It will then describe a couple of applications built with this framework.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2011},
	publisher = {Springer},
	author = {Kim, Minyoung and Park, Kyoung Shin and Kim, Dongkeun and Cho, Yongjoo},
	editor = {Anacleto, Junia Coutinho and Fels, Sidney and Graham, Nicholas and Kapralos, Bill and Saif El-Nasr, Magy and Stanley, Kevin},
	year = {2011},
	keywords = {Emotion, Emotional Characters, Emotional Intelligent Contents},
	pages = {391--394},
	file = {Full Text PDF:files/378/Kim et al. - 2011 - Emotional Intelligent Contents Expressing User’s Own Emotion within Contents.pdf:application/pdf},
}

@inproceedings{kim_virtual_2016,
	address = {Cham},
	title = {Virtual {Display} of {3D} {Computational} {Human} {Brain} {Using} {Oculus} {Rift}},
	isbn = {978-3-319-40406-6},
	doi = {10.1007/978-3-319-40406-6_24},
	abstract = {Creating a 3D environment by Game Engine is a useful way to integrate various types of information into one platform. As a result, it becomes more convenient and rapid to share massive information to the others. Also, using Virtual Reality technology makes visual information to be more detailed and intuitive. In this study, by combining these advantages of Game Engine and VR technology to present brain imaging technologies, we tried to provide more detail and more convenient information about cephalic anatomy or brain disease.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Technological} {Contexts}},
	publisher = {Springer International Publishing},
	author = {Kim, Seung-Wook and Seong, Joon-Kyung},
	editor = {Marcus, Aaron},
	year = {2016},
	keywords = {Brain imaging, Diffusion tensor imaging, Oculus rift, Unity3D, Virtual reality},
	pages = {258--265},
	file = {Full Text PDF:files/380/Kim e Seong - 2016 - Virtual Display of 3D Computational Human Brain Using Oculus Rift.pdf:application/pdf},
}

@inproceedings{hoda_recovery_2014,
	address = {Cham},
	title = {Recovery {Prediction} in the {Framework} of {Cloud}-{Based} {Rehabilitation} {Exergame}},
	isbn = {978-3-319-07446-7},
	doi = {10.1007/978-3-319-07446-7_25},
	abstract = {In this paper, we propose a framework of a cost-effective, entertaining, and motivating home-based upper limb rehabilitation system which consists of a cloud system and a client interface. The framework provides real-time feedback to the patient subject, summarizes the feed-back after each session, and predicts the rehabilitation performance. As an implementation of the framework, a Kinect sensor is used to collect real-time data for upper limb joints of the subjects while they are participating in rehabilitation exergames. The Dynamic Time Warping (DTW) algorithm is then applied to compare the movement pattern of a patient subject with the movement pattern of a healthy subject. Next, the Auto-Regressive Integrated Moving Average (ARIMA) is utilized to forecast the rehabilitation progress of the patients based on their performance history. The prototype of this system is tested on six healthy individuals and one patient. The results show that the patients’ movement patterns have a similar curve shape to the healthy individuals’ movement patterns and, hence, the DTW algorithm can be used as an effective index to describe the rehabilitation statuses of the subjects. The forecasting method is briefly tested by feeding the rehabilitation status history.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Aging} and {Assistive} {Environments}},
	publisher = {Springer International Publishing},
	author = {Hoda, Mohamad and Dong, Haiwei and El Saddik, Abdulmotaleb},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {ARIMA Prediction, Home-based Rehabilitation Framework, Model Matching, Virtual Reality},
	pages = {256--265},
	file = {Full Text PDF:files/381/Hoda et al. - 2014 - Recovery Prediction in the Framework of Cloud-Based Rehabilitation Exergame.pdf:application/pdf},
}

@inproceedings{dhiman_head-mounted_2018,
	address = {Cham},
	title = {Head-{Mounted} {Displays} in {Industrial} {AR}-{Applications}: {Ready} for {Prime} {Time}?},
	isbn = {978-3-319-91716-0},
	shorttitle = {Head-{Mounted} {Displays} in {Industrial} {AR}-{Applications}},
	doi = {10.1007/978-3-319-91716-0_6},
	abstract = {The latest generation of head-mounted displays such as HoloLens provide mixed reality capabilities that claim to better integrate the real and virtual worlds. In this paper, we would like the share our experiences in implementing a user interface for an assembly assistance system using the HoloLens. We carried out a preliminary evaluation of the applicability of mixed reality using the perspective of developers and expert users in an assembly scenario that allows us to operate and compare two interfaces - a state-of-the-art projector display system and the HoloLens. We believe our findings may contribute towards a better understanding of the effects of new display technologies such as the HoloLens in developing and using assistance systems in other fields as well. Areas that may be of future research are also highlighted.},
	language = {en},
	booktitle = {{HCI} in {Business}, {Government}, and {Organizations}},
	publisher = {Springer International Publishing},
	author = {Dhiman, Hitesh and Martinez, Sascha and Paelke, Volker and Röcker, Carsten},
	editor = {Nah, Fiona Fui-Hoon and Xiao, Bo Sophia},
	year = {2018},
	keywords = {Assembly assistance system, HoloLens, Human machine interaction, Qualitative study},
	pages = {67--78},
	file = {Full Text PDF:files/383/Dhiman et al. - 2018 - Head-Mounted Displays in Industrial AR-Applications Ready for Prime Time.pdf:application/pdf},
}

@inproceedings{gao_performance_2014,
	address = {Cham},
	title = {The {Performance} of {Self} in the {Context} of {Shopping} in a {Virtual} {Dressing} {Room} {System}},
	isbn = {978-3-319-07293-7},
	doi = {10.1007/978-3-319-07293-7_30},
	abstract = {This paper investigates the performance of self in a virtual dressing room based on a camera-based system reflecting a full body mirrored image of the self. The study was based on a qualitative research approach and a user-centered design methodology. 22 participants participated in design sessions, semi-structured interviews and a questionnaire investigation. The results showed that the system facilitated self-recognition, self-perception, and shared experience, which afforded an enriched experience of the performing self.},
	language = {en},
	booktitle = {{HCI} in {Business}},
	publisher = {Springer International Publishing},
	author = {Gao, Yi and Petersson Brooks, Eva and Brooks, Anthony Lewis},
	editor = {Nah, Fiona Fui-Hoon},
	year = {2014},
	keywords = {hedonic shopping experience, mirroring, self-perception, self-recognition, shared experience, Virtual dressing room},
	pages = {307--315},
	file = {Full Text PDF:files/385/Gao et al. - 2014 - The Performance of Self in the Context of Shopping in a Virtual Dressing Room System.pdf:application/pdf},
}

@inproceedings{fernandez-cervantes_grammar-based_2016,
	address = {Cham},
	title = {A {Grammar}-{Based} {Framework} for {Rehabilitation} {Exergames}},
	isbn = {978-3-319-46100-7},
	doi = {10.1007/978-3-319-46100-7_4},
	abstract = {Numerous serious exergames advocate the use of engaging avatars to motivate a consistent exercise regimen. However, the process of specifying the prescribed exercise, implementing it as avatar animation, and developing an accurate feedback-providing mechanism is complex and requires a high level of expertise in game engines, control languages, and hardware devices. Furthermore, in the context of rehabilitation exergames, the requirements for accurate assessment and timely and precise feedback can be quite stringent. At the same time, the Kinect\$\${\textasciicircum}\{TM\}\$\$ motion-capture sensor offers a natural interface to game consoles, and its affordability and wide availability represents a huge opportunity for at-home exergames. In this paper, we describe our work towards a system that envisions to simplify the process of developing rehabilitation exergames with Kinect\$\${\textasciicircum}\{TM\}\$\$. The system relies on a language for specifying postures and movements between them, and includes an editor that enables rehabilitation therapists to specify the prescribed exercise, by editing a demonstration of the exercise. This exercise-specification grammar is used to drive the animation of an avatar and the provision of quality feedback, by comparing the player’s postures (as captured by the Kinect\$\${\textasciicircum}\{TM\}\$\$) against those of the coaching avatar and the grammar.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2016},
	publisher = {Springer International Publishing},
	author = {Fernandez-Cervantes, Victor and Stroulia, Eleni and Hunter, Benjamin},
	editor = {Wallner, Günter and Kriglstein, Simone and Hlavacs, Helmut and Malaka, Rainer and Lugmayr, Artur and Yang, Hyun-Seung},
	year = {2016},
	keywords = {Interface, Kinect-based gameplay, Rehabilitation, Serious games},
	pages = {38--50},
	file = {Full Text PDF:files/387/Fernandez-Cervantes et al. - 2016 - A Grammar-Based Framework for Rehabilitation Exergames.pdf:application/pdf},
}

@inproceedings{paliokas_sense_2014,
	address = {Cham},
	title = {Sense of {Presence} and {Metacognition} {Enhancement} in {Virtual} {Reality} {Exposure} {Therapy} in the {Treatment} of {Social} {Phobias} and the {Fear} of {Flying}},
	isbn = {978-3-319-07464-1},
	doi = {10.1007/978-3-319-07464-1_30},
	abstract = {The aim of this research effort is to identify feeling-of-presence and metacognitive amplifiers over existing well-established VRET treatment methods. Patient real time projection in virtual environments during stimuli exposure and electroencephalography (EEG) report sharing are among the techniques, which have been used to achieve the desired result. Initialized from theoretical inferences, is moving towards a proof-of-concept prototype, which has been developed as a realization of the proposed method. The evaluation of the prototype made possible with an expert team of 28 therapists testing the fear of public speaking and fear of flying case studies.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} of {Virtual} and {Augmented} {Reality}},
	publisher = {Springer International Publishing},
	author = {Paliokas, Ioannis and Tsakiris, Athanasios and Vidalis, Athanasios and Tzovaras, Dimitrios},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2014},
	keywords = {Anxiety Disorders, Fear of Flying, Fear of Public Speech, Metacognition, Sense of Presence, Virtual Reality Exposure Therapy},
	pages = {316--328},
	file = {Full Text PDF:files/389/Paliokas et al. - 2014 - Sense of Presence and Metacognition Enhancement in Virtual Reality Exposure Therapy in the Treatment.pdf:application/pdf},
}

@inproceedings{nguyen_applying_2016,
	address = {Cham},
	title = {Applying {Virtual} {Reality} in {City} {Planning}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_69},
	abstract = {The rapid growth of virtual reality in recent years has brought this technology to a wide variety of common users. Realizing the potential of this technology in building and planning cities; the authors introduce a system in which architectures are brought together into a 3-dimensional virtual environment in order to collaborate in building cities. This system uses the Oculus Rift as the VR device, combining with Leap Motion to detect user’s hand-gestures. The authors walkthrough all the details in building such systems including object modeling, communication protocols and gesture recognition technique.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Nguyen, Minh-Tu and Nguyen, Hai-Khanh and Vo-Lam, Khanh-Duy and Nguyen, Xuan-Gieng and Tran, Minh-Triet},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {City planning, Collaborative environment, Human computer interaction, Urban planning, Virtual reality},
	pages = {724--735},
	file = {Full Text PDF:files/391/Nguyen et al. - 2016 - Applying Virtual Reality in City Planning.pdf:application/pdf},
}

@inproceedings{nishimoto_cave2tm_2016,
	address = {Cham},
	title = {From {CAVE2TM} to {Mobile}: {Adaptation} of {Hearts} and {Minds} {Virtual} {Reality} {Project} {Interaction}},
	isbn = {978-3-319-39516-6},
	shorttitle = {From {CAVE2TM} to {Mobile}},
	doi = {10.1007/978-3-319-39516-6_38},
	abstract = {Hearts and Minds: The Interrogations Project is an interactive performance made for the CAVE2TM [1] large-scale 320-degree panoramic virtual reality environment that describes veterans’ testimonies about military interrogations in Iraq during the American counter-insurgency campaign. The project is based on interviews of American soldiers and on their actual testimonies [2]. The project was achieved through technical innovation, cross-disciplinary and international collaboration. It was developed using a novel method for direct output of the Unity-based virtual reality projects into the CAVE2 environment. Other portable versions of the work were developed to reach new audiences across educational, arts and public arenas which include (1) personal computer version navigable using Xbox 360 controller; (2) web-based version available for free download; (3) Oculus Rift immersive virtual reality HMD version; (4) mobile version of the project for Apple iPad (in progress). This paper describes the development and compares the interaction experiences across platforms.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Interaction} {Platforms} and {Techniques}},
	publisher = {Springer International Publishing},
	author = {Nishimoto, Arthur and Tsoupikova, Daria and Rettberg, Scott and Coover, Roderick},
	editor = {Kurosu, Masaaki},
	year = {2016},
	keywords = {3D user interface, CAVE2TM, Immersive environments, Interaction, Navigation, Virtual reality},
	pages = {400--411},
	file = {Full Text PDF:files/392/Nishimoto et al. - 2016 - From CAVE2TM to Mobile Adaptation of Hearts and Minds Virtual Reality Project Interaction.pdf:application/pdf},
}

@inproceedings{starzyk_simulation_2013,
	address = {Berlin, Heidelberg},
	title = {Simulation of a {Motivated} {Learning} {Agent}},
	isbn = {978-3-642-41142-7},
	doi = {10.1007/978-3-642-41142-7_21},
	abstract = {In this paper we discuss how to design a simple motivated learning agent with symbolic I/O using a simulation environment within the NeoAxis game engine. The purpose of this work is to explore autonomous development of motivations and memory of agents in a virtual environment. The approach we took should speed up the development process, bypassing the need to create a physical embodied agent as well as reducing the learning effort. By rendering low-level motor actions such as grasping or walking into symbolic commands we remove the need to learn elementary motions. Instead, we use several basic primitive motor procedures, which can form more complex procedures. Furthermore, by simulating the agent’s environment, we both improve and simplify the learning process. There are a few adaptive learning variables associated with both the agent and its environment, and learning takes less time, than it would in a more complex real world environment.},
	language = {en},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer},
	author = {Starzyk, Janusz A. and Graham, James and Puzio, Leszek},
	editor = {Papadopoulos, Harris and Andreou, Andreas S. and Iliadis, Lazaros and Maglogiannis, Ilias},
	year = {2013},
	keywords = {cognitive architectures, embodied intelligence, Motivated learning, simulation, virtual agents},
	pages = {205--214},
	file = {Full Text PDF:files/393/Starzyk et al. - 2013 - Simulation of a Motivated Learning Agent.pdf:application/pdf},
}

@inproceedings{paul_simulation_2019,
	address = {Cham},
	title = {Simulation of {Reconfigurable} {Assembly} {Cells} with {Unity3D}},
	isbn = {978-3-030-30000-5},
	doi = {10.1007/978-3-030-30000-5_29},
	abstract = {This paper introduces a Unity3D-based simulation of reconfigurable assembly cells. A systematic approach defining flexibility ranges and comparing product requirements and cell capabilities allows the automated proposal of reconfigurations in the assembly cell. With this approach, the suitability of an existing cell can be examined for different variants of products while taking reconfiguration aspects into account. The simulation simplifies the process of introducing a new product to an assembly line. Through the virtual approach, designing of the product and line planning can be executed simultaneously, thereby decreasing cost and time-to-market for new variants in an existing production system.},
	language = {en},
	booktitle = {Advances in {Production} {Management} {Systems}. {Production} {Management} for the {Factory} of the {Future}},
	publisher = {Springer International Publishing},
	author = {Paul, Magdalena and Leiber, Daria and Pleli, Julian and Reinhart, Gunther},
	editor = {Ameri, Farhad and Stecke, Kathryn E. and von Cieminski, Gregor and Kiritsis, Dimitris},
	year = {2019},
	keywords = {Assembly, Reconfigurability, Simulation, Unity3D},
	pages = {223--230},
	file = {Full Text PDF:files/394/Paul et al. - 2019 - Simulation of Reconfigurable Assembly Cells with Unity3D.pdf:application/pdf},
}

@inproceedings{cao_song_2018,
	address = {Cham},
	title = {Song of {Red} {Pine} {Woods} - {Design} and {Study} of {Digital} {Picture} {Books} for {Preschool} {Children} on {iPad}},
	isbn = {978-3-319-91806-8},
	doi = {10.1007/978-3-319-91806-8_13},
	abstract = {The cultivation of children’s early reading ability is of vital importance in their entire life. As preschool education attracts more and more attention, the primary challenge for many designers of children’s books is how to better guide children to read with higher quality. This paper takes the design of Song of Red Pine Woods as an example, discusses the actual effects and realization way of interactive design in preschool children’s picture books. Based on the author’s experiment results in preschool, the paper proves that digital picture books have unique advantages compared to traditional paper picture books, and also proposes new ideas on how to improve children’s reading interest and reading experience.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Users}, {Contexts} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Cao, Qi and Han, Jing-Hua and Ding, Yu-Yi and Huang, Shi and Liu, Chao},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2018},
	keywords = {Cognitive features, Digital picture book, Interactive design, Preschool children},
	pages = {158--169},
	file = {Full Text PDF:files/415/Cao et al. - 2018 - Song of Red Pine Woods - Design and Study of Digital Picture Books for Preschool Children on iPad.pdf:application/pdf},
}

@inproceedings{bottino_smart_2014,
	address = {Cham},
	title = {{SMART} {VIEW}: {A} {Serious} {Game} {Supporting} {Spatial} {Orientation} of {Subjects} with {Cognitive} {Impairments}},
	isbn = {978-3-319-07440-5},
	shorttitle = {{SMART} {VIEW}},
	doi = {10.1007/978-3-319-07440-5_45},
	abstract = {The paper presents SMART VIEW a serious game developed with the aim of helping young people with moderate cognitive disabilities acquire those spatial abilities that are key prerequisites to autonomous mobility. The game was conceived for cognitively impaired teenagers; it proposes exercises supporting the acquisition and consolidation of competences related to space awareness and self-perception in the space; such skills are necessary to develop the sense of spatial orientation, which is critical for the target population. SMART VIEW makes use of Touch Screen tables so to allow easier access to the game content and augmented interaction. Particular attention has been devoted to the game interface design, so to make it free from cognitive barriers and fully accessible to the target population. Contents are as close as possible to reality and the educational strategy entails slow and gradual increase of the game complexity, so to properly sustain the users’ cognitive effort.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Universal} {Access} to {Information} and {Knowledge}},
	publisher = {Springer International Publishing},
	author = {Bottino, Rosa Maria and Canessa, Andrea and Ott, Michela and Tavella, Mauro},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {Cognitive Disabilities, E-inclusion, Perspective Taking, Serious Games, Spatial Orientation, Technology Enhanced Learning},
	pages = {489--500},
	file = {Full Text PDF:files/417/Bottino et al. - 2014 - SMART VIEW A Serious Game Supporting Spatial Orientation of Subjects with Cognitive Impairments.pdf:application/pdf},
}

@inproceedings{odwyer_mixed_2020,
	address = {Cham},
	title = {Mixed {Reality} and {Volumetric} {Video} in {Cultural} {Heritage}: {Expert} {Opinions} on {Augmented} and {Virtual} {Reality}},
	isbn = {978-3-030-50267-6},
	shorttitle = {Mixed {Reality} and {Volumetric} {Video} in {Cultural} {Heritage}},
	doi = {10.1007/978-3-030-50267-6_16},
	abstract = {Mixed reality (MR) technology is currently growing in popularity for applications in the cultural heritage domain. Furthermore, with the ability to be viewed with six degrees of freedom, volumetric video (VV) is presently being explored as a viable approach to content creation within this area. When combined, MR technology and VV present both practitioners and audiences with innovative approaches to the creation and consumption of both tangible and intangible representations of cultural significance. While there are some existing quantitative studies appraising these new technologies, the precise effects of MR in a cultural heritage context have yet to be fully explored. Here we show the results of a systematic evaluation of MR technology as applied in a cultural heritage context, where subject matter expert interviews were conducted to identify how virtual reality and augmented reality technologies are influencing the creative practices of domain experts and audience engagements with modern dramatic literature. Gathered from high-level stakeholders within the cultural heritage domain, our results highlighted the problems, concerns, and desires of users who must consider this technology in practice. We found that MR and VV content were considered by many to be disruptive technologies for the future of film, theater, and performance practice from the perspectives of both practitioners and audiences. We anticipate that these results will help future MR and VV projects to create meaningful content that is sympathetic to the needs and requirements of creators and audiences.},
	language = {en},
	booktitle = {Culture and {Computing}},
	publisher = {Springer International Publishing},
	author = {O’Dwyer, Néill and Young, Gareth W. and Johnson, Nicholas and Zerman, Emin and Smolic, Aljosa},
	editor = {Rauterberg, Matthias},
	year = {2020},
	keywords = {Cultural heritage, Mixed reality, Subject matter expert interviews},
	pages = {195--214},
	file = {Full Text PDF:files/419/O’Dwyer et al. - 2020 - Mixed Reality and Volumetric Video in Cultural Heritage Expert Opinions on Augmented and Virtual Re.pdf:application/pdf},
}

@inproceedings{bellekens_cyber-security_2019,
	address = {Cham},
	title = {From {Cyber}-{Security} {Deception} to {Manipulation} and {Gratification} {Through} {Gamification}},
	isbn = {978-3-030-22351-9},
	doi = {10.1007/978-3-030-22351-9_7},
	abstract = {With the ever growing networking capabilities and services offered to users, attack surfaces have been increasing exponentially, additionally, the intricacy of network architectures has increased the complexity of cyber-defenses, to this end, the use of deception has recently been trending both in academia and industry. Deception enables to create proactive defense systems, luring attackers in order to better defend the systems at hand. Current applications of deception, only rely on static, or low interactive environments. In this paper we present a platform that combines human-computer-interaction, analytics, gamification and deception to lure malicious users into selected traps while piquing their interests. Furthermore we analyse the interactive deceptive aspects of the platform through the addition of a narrative, further engaging malicious users into following a predefined path and deflecting attacks from key network systems.},
	language = {en},
	booktitle = {{HCI} for {Cybersecurity}, {Privacy} and {Trust}},
	publisher = {Springer International Publishing},
	author = {Bellekens, Xavier and Jayasekara, Gayan and Hindy, Hanan and Bures, Miroslav and Brosset, David and Tachtatzis, Christos and Atkinson, Robert},
	editor = {Moallem, Abbas},
	year = {2019},
	keywords = {Cyber-security, Deception, Interactive defense, Manipulation},
	pages = {99--114},
	file = {Full Text PDF:files/421/Bellekens et al. - 2019 - From Cyber-Security Deception to Manipulation and Gratification Through Gamification.pdf:application/pdf},
}

@inproceedings{senanayake_revolutionizing_2019,
	address = {Cham},
	title = {Revolutionizing the {Visual} {Design} of {Capture} the {Flag} ({CTF}) {Competitions}},
	isbn = {978-3-030-22351-9},
	doi = {10.1007/978-3-030-22351-9_23},
	abstract = {There are a variety of cyber-security challenge tournaments held within the INFOSEC and Hacker communities, which among their benefits help to promote and identify emerging talent. Unfortunately, most of these competitions are rather narrow in reach, being of interest primarily to those enthusiasts who are already well versed in cyber security. To attract a broader pool of younger generation participants requires one to make such events more engaging and intellectually accessible. The way these tournaments are currently conducted and presented to live audiences is rather opaque, if not unintelligible to most who encounter them. This paper presents an ongoing effort to bridge the presentation gap necessary to make cyber security competitions more attractive and accessible to a broader audience. We present the design of a new but familiar model for capturing the interplay, individual achievements, and tactical drama that transpires during one form of cyber security competition. The main user interface and presentation paradigm in this research borrows from those of established e-sports, such as League of Legends and Overwatch. Our motivation is to elevate the current format of cyber security competition events to incorporate design and presentation elements that are informed by techniques that have evolved within the e-sports community. We apply the physics models and battlefield visualizations of virtual world gaming environments in a manner that captures the intellectual challenges, team achievements, and tactical gameplay that occur in a popular form of cyber security tournament, called the Capture The Flag (CTF) competition. Our goal is to make these events intellectually accessible to broader audiences, to engage a broader and more diverse talent pool of competitors, and to increase the awareness and interest in cyber security among the general public.},
	language = {en},
	booktitle = {{HCI} for {Cybersecurity}, {Privacy} and {Trust}},
	publisher = {Springer International Publishing},
	author = {Senanayake, Rukman and Porras, Phillip and Kaehler, Jason},
	editor = {Moallem, Abbas},
	year = {2019},
	keywords = {Capture the Flag, Cyber education, Cyber security, National cyber league, Visualization},
	pages = {339--352},
	file = {Full Text PDF:files/423/Senanayake et al. - 2019 - Revolutionizing the Visual Design of Capture the Flag (CTF) Competitions.pdf:application/pdf},
}

@inproceedings{wu_dcan_2018,
	address = {Cham},
	title = {{DCAN}: {Dual} {Channel}-{Wise} {Alignment} {Networks} for {Unsupervised} {Scene} {Adaptation}},
	isbn = {978-3-030-01228-1},
	shorttitle = {{DCAN}},
	doi = {10.1007/978-3-030-01228-1_32},
	abstract = {Harvesting dense pixel-level annotations to train deep neural networks for semantic segmentation is extremely expensive and unwieldy at scale. While learning from synthetic data where labels are readily available sounds promising, performance degrades significantly when testing on novel realistic data due to domain discrepancies. We present Dual Channel-wise Alignment Networks (DCAN), a simple yet effective approach to reduce domain shift at both pixel-level and feature-level. Exploring statistics in each channel of CNN feature maps, our framework performs channel-wise feature alignment, which preserves spatial structures and semantic information, in both an image generator and a segmentation network. In particular, given an image from the source domain and unlabeled samples from the target domain, the generator synthesizes new images on-the-fly to resemble samples from the target domain in appearance and the segmentation network further refines high-level features before predicting semantic maps, both of which leverage feature statistics of sampled images from the target domain. Unlike much recent and concurrent work relying on adversarial training, our framework is lightweight and easy to train. Extensive experiments on adapting models trained on synthetic segmentation benchmarks to real urban scenes demonstrate the effectiveness of the proposed framework.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Wu, Zuxuan and Han, Xintong and Lin, Yen-Liang and Uzunbas, Mustafa Gökhan and Goldstein, Tom and Lim, Ser Nam and Davis, Larry S.},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	pages = {535--552},
	file = {Full Text PDF:files/426/Wu et al. - 2018 - DCAN Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation.pdf:application/pdf},
}

@inproceedings{alce_design_2019,
	address = {Cham},
	title = {Design and {Evaluation} of {Three} {Interaction} {Models} for {Manipulating} {Internet} of {Things} ({IoT}) {Devices} in {Virtual} {Reality}},
	isbn = {978-3-030-29390-1},
	doi = {10.1007/978-3-030-29390-1_15},
	abstract = {More and more things are getting connected to the internet, including lights, speakers, and refrigerators. These connected things are an example of what a smart home system that is part of the Internet of Things (IoT) can incorporate. IoT enables advanced services by interconnecting physical and virtual things. But, building interactive prototypes for smart home systems can be difficult and costly, since it involves a number of different devices and systems with varying technological readiness level. Virtual reality (VR) is a technology that can create computer-generated environments and has been used as a design tool in many different domains, such as architecture, city planning, and industrial design. However, the focus has traditionally been on visualizing design proposals rather than letting the intended users directly interact with them. Recently, we have seen an intensified development of VR headsets such as HTC Vive and Oculus Rift. These headsets come with relatively well-developed hand controllers, which can be used to interact with the virtual environment. This opens up opportunities to develop and evaluate interactive virtual smart home systems.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2019},
	publisher = {Springer International Publishing},
	author = {Alce, Günter and Ternblad, Eva-Maria and Wallergård, Mattias},
	editor = {Lamas, David and Loizides, Fernando and Nacke, Lennart and Petrie, Helen and Winckler, Marco and Zaphiris, Panayiotis},
	year = {2019},
	keywords = {Interaction, Internet of Things, Method, Virtual Reality},
	pages = {267--286},
	file = {Full Text PDF:files/416/Alce et al. - 2019 - Design and Evaluation of Three Interaction Models for Manipulating Internet of Things (IoT) Devices.pdf:application/pdf},
}

@inproceedings{radianti_co-design_2018,
	address = {Cham},
	title = {Co-design of a {Virtual} {Training} {Tool} with {Emergency} {Management} {Stakeholders} for {Extreme} {Weather} {Response}},
	isbn = {978-3-319-91797-9},
	doi = {10.1007/978-3-319-91797-9_14},
	abstract = {Emergency services usually prepare for the most frequent or predictable types of disasters, such as fires. However, preparation for complex, unpredictable disaster scenarios is infrequent, probably because of high resource demand and difficulty of covering dynamic training needs of multiple stakeholders. The use of serious game techniques as the core of simulated or virtual training tools opens for new ways of training and learning in emergency and crisis scenarios. However, the number of virtual training tools customized to specific disaster or crisis scenario that address needs of diverse user groups is limited. Existing tools are often tailored with a particular geographical setting and local threats, requiring an extensive adaptation outside the pre-defined settings. This paper describes the co-design process aimed at the creation of an emergency and disaster virtual training tool prototype linked to a Norwegian context. Two co-design workshops were run involving local emergency management actors. The general setting included an extreme weather scenario because of its high probability of occurrence and societal impact. The first workshop was used to gather end-user requirements for the training tool, explore the current gaps in the training practices, information needs and elements to improve training on decision making. In the second workshop, we focused on scrutinizing the detailed design, user interface, training use-case and learning points. Finally, we ran a small-scale usability testing of the initial prototype using SeGUE (Serious Game Usability Evaluator) instrument. The results of the prototype activities and the testing are reported in this paper.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Theory} and {Practice}},
	publisher = {Springer International Publishing},
	author = {Radianti, Jaziar and Martinez, Santiago Gil and Munkvold, Bjørn Erik and Konnestad, Morgan},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2018},
	keywords = {Co-design, Emergency management, Extreme weather, SeGUE, Serious game, Training tool, User testing},
	pages = {185--202},
	file = {Full Text PDF:files/418/Radianti et al. - 2018 - Co-design of a Virtual Training Tool with Emergency Management Stakeholders for Extreme Weather Resp.pdf:application/pdf},
}

@inproceedings{bolivar_3d_2019,
	address = {Cham},
	title = {{3D} {Interaction} for {Computer} {Science} {Educational} {VR} {Game}},
	isbn = {978-3-030-23560-4},
	doi = {10.1007/978-3-030-23560-4_30},
	abstract = {We propose a full immersion 3D environment in the form of a video game. The environment offers the player the opportunity to explore basic Computer Science (CS) concepts without removing any of the entertaining aspects of games. We believe in creating a software that can be enjoyed by anyone regardless of age and at the same time can increase CS awareness. We developed a complete 3D game emulating an Escape Room. We aim to capture the attention of not only teenagers but also adults. The solution to each room is presented as puzzles based on the background concepts of computer science. These concepts are in the form of every day decisions to bring familiarity into the game play. The games aim is to be inviting and fun. Ultimately, giving each player the opportunity to be engaged into computer science concepts as they go thought the game and sparking interest towards CS.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Theory}, {Methods} and {Tools}},
	publisher = {Springer International Publishing},
	author = {Bolivar, Santiago and Perez, Daniel and Carrasquillo, Armando and Williams, Adam S. and Rishe, Naphtali D. and Ortega, Francisco R.},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2019},
	keywords = {Computer science education, Education, Virtual Reality},
	pages = {408--419},
	file = {Full Text PDF:files/420/Bolivar et al. - 2019 - 3D Interaction for Computer Science Educational VR Game.pdf:application/pdf},
}

@inproceedings{raaen_measuring_2015,
	address = {Cham},
	title = {Measuring {Latency} in {Virtual} {Reality} {Systems}},
	isbn = {978-3-319-24589-8},
	doi = {10.1007/978-3-319-24589-8_40},
	abstract = {Virtual Reality(VR) systems have the potential to revolutionise how we interact with computers. However motion sickness and discomfort are currently severely impeding the adoption. Traditionally the focus of optimising VR systems have been on frame-rate. Delay and frame-rate are however not equivalent. Latency may occur in several steps in image processing, and a frame-rate measure only picks up some of them. We have made an experimental setup to physically measure the actual delay from the user moves the head until the screen of the VR device is updated. Our results show that while dedicated VR-equipment had very low delay, smartphones are in general not ready for VR-applications.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Raaen, Kjetil and Kjellmo, Ivar},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {Framerate, Latency, Mixed Reality, Virtual Reality},
	pages = {457--462},
	file = {Full Text PDF:files/422/Raaen e Kjellmo - 2015 - Measuring Latency in Virtual Reality Systems.pdf:application/pdf},
}

@inproceedings{babu_votre_2017,
	address = {Cham},
	title = {{VoTrE}: {A} {Vocational} {Training} and {Evaluation} {System} to {Compare} {Training} {Approaches} for the {Workplace}},
	isbn = {978-3-319-57987-0},
	shorttitle = {{VoTrE}},
	doi = {10.1007/978-3-319-57987-0_16},
	abstract = {Extensive research has been carried out in using computer-based techniques to train and prepare workers for various industry positions. Most of this research focuses on how to best enable the workers to perform a type of task safely and efficiently. In fact, many of the accidents in manufacturing and construction environments are due to the lack of proper training needed for employees. In this study, we compare the impact of three types of training approaches on the planning and problem-solving abilities of a trainee while he/she performs the Towers of Hanoi (TOH) task. The three approaches are (a) traditional (with a human trainer), (b) gamification (game-based training simulation), and (c) computer-aided training. The aim of this study is to evaluate a worker’s level of functioning and problem-solving skills based on a specific training approach. Exact assessment of functional capacities is an important prerequisite to ensure effective and personalized training. The study uses workplace simulation to collect different types of performance data and assess the impact of these training approaches.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Babu, Ashwin Ramesh and Rajavenkatanarayanan, Akilesh and Abujelala, Maher and Makedon, Fillia},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Cognitive issues, Computer-aided training, Gamification, Performance, Towers of Hanoi, Vocational training},
	pages = {203--214},
	file = {Full Text PDF:files/424/Babu et al. - 2017 - VoTrE A Vocational Training and Evaluation System to Compare Training Approaches for the Workplace.pdf:application/pdf},
}

@inproceedings{fu_novel_2015,
	address = {Cham},
	title = {A {Novel} {3D} {Wheelchair} {Simulation} {System} for {Training} {Young} {Children} with {Severe} {Motor} {Impairments}},
	isbn = {978-3-319-21380-4},
	doi = {10.1007/978-3-319-21380-4_62},
	abstract = {Young children with severe motor impairments face a higher risk of secondary impairments in the development of social, cognitive, and motor skills, owing to the lack of independent mobility. Although power wheelchairs are typical tools for providing independent mobility, the steep learning curve, safety concerns, and high cost may prevent children aged 2–5 years from using them. We have developed a 3D wheelchair simulation system using gaming technologies for these young children to learn fundamental wheelchair driving skills in a safe, affordable, and entertaining environment. Depending on the skill level, the simulation system offers different options ranging from automatic control (i.e., the artificial intelligent (AI) module fully controls the wheelchair) to manual control (i.e., human users are fully responsible for controlling the wheelchair). Optimized AI algorithms were developed to make the simulation system easy and efficient to use. We have conducted experiments to evaluate the simulation system. The results demonstrate that the simulation system is promising to overcome the limitations associated with real wheelchairs meanwhile providing a safe, affordable, and exciting environment to train young children.},
	language = {en},
	booktitle = {{HCI} {International} 2015 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Fu, Jicheng and Garien, Cole and Smith, Sean and Zeng, Wenxi and Jones, Maria},
	editor = {Stephanidis, Constantine},
	year = {2015},
	keywords = {A*, Artificial intelligence, Gaming technology, Power wheelchair, Secondary impairment, Severe motor impairment, Simulation},
	pages = {366--371},
	file = {Full Text PDF:files/425/Fu et al. - 2015 - A Novel 3D Wheelchair Simulation System for Training Young Children with Severe Motor Impairments.pdf:application/pdf},
}

@inproceedings{robertson_visual_2015,
	address = {Cham},
	title = {The {Visual} {Design} and {Implementation} of an {Embodied} {Conversational} {Agent} in a {Shared} {Decision}-{Making} {Context} ({eCoach})},
	isbn = {978-3-319-20609-7},
	doi = {10.1007/978-3-319-20609-7_40},
	abstract = {This paper outlines the design process and challenges of creating a character for our implementation of an embodied conversational agent (ECA), specifically integrating diverse views from focus groups consisting of individuals representing different levels of socio-economic status and health literacy. Initial focus groups consisting of members from both higher and lower socio-economic status and health literacy found the stylized ECA to be unappealing. Later focus groups conducted after completion of the educational intervention better accepted the ECA, reporting it to be acceptable.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Robertson, Scott and Solomon, Rob and Riedl, Mark and Gillespie, Theresa Wicklin and Chociemski, Toni and Master, Viraj and Mohan, Arun},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2015},
	keywords = {Computer supported collaborative learning, Design and evaluation of collaboration technology, Interdisciplinary studies on collaboration technology and learning, Methodologies for the study of computer supported collaborative learning and /or technology-enhanced learning},
	pages = {427--437},
	file = {Full Text PDF:files/427/Robertson et al. - 2015 - The Visual Design and Implementation of an Embodied Conversational Agent in a Shared Decision-Making.pdf:application/pdf},
}

@inproceedings{sciarini_initial_2015,
	address = {Cham},
	title = {Initial {Evaluation} of a {Modern} {Augmented} {Reality} {Display} for {Deployable} {Embedded} {Training} {System}},
	isbn = {978-3-319-21380-4},
	doi = {10.1007/978-3-319-21380-4_40},
	abstract = {When flight time is not available, flight simulators are an effective task rehearsal tool used by the military to train and maintaining aviator proficiency. Unfortunately, the physical characteristics of traditional simulator architectures prevent their use in most operational environments. Previous research has demonstrated that the embedded training (ET) simulator concept is viable but also has limitations in the display of immersive visuals (Lennerton, 2004). Recent advancements in virtual display devices and aircraft design can overcome challenges of the past and should rapidly advance the realization of ET simulators. However, the ability of technology to provide an ET solution must be supported by user acceptance and confidence that effective training transfer will occur with such a system. This effort explored the feasibility of using a modern, user worn, 3D, projection based Augmented Reality (AR) system as the visual interface for a hypothetical ET system with two fixed wing and one rotary wing aircraft. Eight Naval Aviation Subject Matter Experts (SMEs) were given a preflight questionnaire, participated in a simulated flight using the AR display, and completed a post-flight questionnaire. Results indicated that both the ET concept and the prototype AR system were highly regarded.},
	language = {en},
	booktitle = {{HCI} {International} 2015 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Sciarini, Lee and Elfe, Jason and Shilling, Tim and Martin, Eric},
	editor = {Stephanidis, Constantine},
	year = {2015},
	keywords = {Embedded Training, Prototype Casting, Rotary Wing Aircraft, Subject Matter Experts (SMEs), Users Wear},
	pages = {226--231},
	file = {Full Text PDF:files/428/Sciarini et al. - 2015 - Initial Evaluation of a Modern Augmented Reality Display for Deployable Embedded Training System.pdf:application/pdf},
}

@inproceedings{maxwell_application_2016,
	address = {Cham},
	title = {Application of {Virtual} {Environments} for {Infantry} {Soldier} {Skills} {Training}: {We} are {Doing} it {Wrong}},
	isbn = {978-3-319-39907-2},
	shorttitle = {Application of {Virtual} {Environments} for {Infantry} {Soldier} {Skills} {Training}},
	doi = {10.1007/978-3-319-39907-2_41},
	abstract = {Simulation based training (SBT) technology has been shown to be effective in a number of domains such as for pilot training and ground vehicle operator training. In the dismounted infantry soldier skills domain, the low hanging fruit for effective use of SBT is equipment operations training. However, the complexities of the operational environment are often too difficult to replicate in current virtual environments to present an accurate or effective training for the skills requiring identification of enemy activity or reacting to enemy contact. The U.S. Army Research Laboratory and the University of Central Florida have been conducting studies using large numbers of soldiers to determine how effective virtual training methods are in comparison to traditional training methods for dismounted infantry soldier skills. This paper will discuss recommendations for changes in the employment of virtual training systems that could have a meaningful impact on the performance of soldiers.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Maxwell, Douglas B.},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {Collective training, Infantry soldier skills training, Leadership training, Simulation based training, Virtual environments},
	pages = {424--432},
	file = {Full Text PDF:files/429/Maxwell - 2016 - Application of Virtual Environments for Infantry Soldier Skills Training We are Doing it Wrong.pdf:application/pdf},
}

@inproceedings{garcia_strategies_2019,
	address = {Cham},
	title = {Strategies for {Inclusive} {End}-{User} {Co}-{Creation} of {Inclusive} {Storytelling} {Games}},
	isbn = {978-3-030-34644-7},
	doi = {10.1007/978-3-030-34644-7_16},
	abstract = {As gaming acquires new purposes (for instance, entertainment, education and healthcare), game accessibility becomes increasingly important for multiple domains. For broader inclusion, game accessibility encompass both creation and play. Towards this goal, we have defined a framework to enable more people to create and play digital games In this paper, we present strategies resulting from developing and evaluating Lepi, an inclusive end-user tool for co-creation of inclusive storytelling-based games. Adults with heterogeneous interaction needs, levels of literacy and experience with computers used Lepi to co-create their games over ten creation workshops. Using Lepi and following the framework practices, the participants managed to co-create games accessible for themselves and their peers. From this experience, we have identified some strategies (Creation Commands, Interaction Alternatives for Input, Slots, Creation Alternatives, Assisted and Collaborative Co-Creation, Gentle Slopes, Multimodal Features, Playing Commands and Presenting Game Content) which can contribute towards more inclusive practices of game creation and play of storytelling-based games.},
	language = {en},
	booktitle = {Entertainment {Computing} and {Serious} {Games}},
	publisher = {Springer International Publishing},
	author = {Garcia, Franco Eusébio and de Almeida Neris, Vânia Paula},
	editor = {van der Spek, Erik and Göbel, Stefan and Do, Ellen Yi-Luen and Clua, Esteban and Baalsrud Hauge, Jannicke},
	year = {2019},
	keywords = {End-User Development, Game accessibility, Game development, Human-Centered Computing, Meta-Design, Universal Design},
	pages = {201--213},
	file = {Full Text PDF:files/430/Garcia e de Almeida Neris - 2019 - Strategies for Inclusive End-User Co-Creation of Inclusive Storytelling Games.pdf:application/pdf},
}

@inproceedings{bekele_responses_2014,
	address = {Cham},
	title = {Responses during {Facial} {Emotional} {Expression} {Recognition} {Tasks} {Using} {Virtual} {Reality} and {Static} {IAPS} {Pictures} for {Adults} with {Schizophrenia}},
	isbn = {978-3-319-07464-1},
	doi = {10.1007/978-3-319-07464-1_21},
	abstract = {Technology-assisted intervention has the potential to adaptively individualize and improve outcomes of traditional schizophrenia (SZ) intervention. Virtual reality (VR) technology, in particular, has the potential to simulate real world social and communication interactions and hence could be useful as a therapeutic platform for SZ. Emotional face recognition is considered among the core building blocks of social communication. Studies have shown that emotional face processing and understanding is impaired in patients with SZ. The current study develops a novel VR-based system that presents avatars that can change their facial emotion dynamically for emotion recognition tasks. Additionally, this system allows real-time measurement of physiological signals and eye gaze during the emotion recognition tasks, which can be used to gain insight about the emotion recognition process in SZ population. This study further compares VR-based facial emotion recognition with that of the more traditional emotion recognition from static faces using a small usability study. Results from the usability study suggest that VR could be a viable platform for SZ intervention and implicit signals such as physiological signals and eye gaze can be utilized to better understand the underlying pattern that is not available from user reports and performance alone.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} of {Virtual} and {Augmented} {Reality}},
	publisher = {Springer International Publishing},
	author = {Bekele, Esubalew and Bian, Dayi and Zheng, Zhi and Peterman, Joel and Park, Sohee and Sarkar, Nilanjan},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2014},
	keywords = {adaptive interaction, emotion recognition, eye tracking, facial expression, IAPS, physiological processing, schizophrenia intervention, virtual reality},
	pages = {225--235},
	file = {Full Text PDF:files/431/Bekele et al. - 2014 - Responses during Facial Emotional Expression Recognition Tasks Using Virtual Reality and Static IAPS.pdf:application/pdf},
}

@inproceedings{griffith_bio-reckoning_2013,
	address = {Berlin, Heidelberg},
	title = {Bio-reckoning: {Perceptual} {User} {Interface} {Design} for {Military} {Training}},
	isbn = {978-3-642-39454-6},
	shorttitle = {Bio-reckoning},
	doi = {10.1007/978-3-642-39454-6_4},
	abstract = {Simulation based training is one way to attain operational realism for training complex military tasks in a safe, task relevant manner. For successful transfer of knowledge, skills, and abilities to the dynamically changing military environment, the human-computer interface should minimally support learning during the training process and provide congruent action plans that facilitate understanding of the overall training goal. While there are emerging controller technologies, simulators still rely on such input devices as mouse and keyboard. These devices potentially cause information and training bottlenecks as they limit naturalistic interactivity within the more advanced serious gaming platforms. Given the shortcomings of current interface design, we suggest a human-computer interface framework that includes perceptual user interface components and an open source serious game testbed. We discuss a multimodal framework called bio-reckoning that integrates brain-computer interface techniques, eye tracking, and facial recognition within EDGE, the U.S. Army’s newest serious game based training tool.},
	language = {en},
	booktitle = {Foundations of {Augmented} {Cognition}},
	publisher = {Springer},
	author = {Griffith, Tami and Rumble, Deanna and Mahajan, Pankaj and Fidopiastis, Cali M.},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2013},
	keywords = {augmented cognition, braincomputer interfaces, military training, perceptual user interfaces, serious games, simulation based training},
	pages = {31--40},
	file = {Full Text PDF:files/432/Griffith et al. - 2013 - Bio-reckoning Perceptual User Interface Design for Military Training.pdf:application/pdf},
}

@inproceedings{trindade_tourism_2018,
	address = {Cham},
	title = {Tourism and {Virtual} {Reality}: {User} {Experience} {Evaluation} of a {Virtual} {Environment} {Prototype}},
	isbn = {978-3-319-91806-8},
	shorttitle = {Tourism and {Virtual} {Reality}},
	doi = {10.1007/978-3-319-91806-8_57},
	abstract = {Tourism is one of the largest industries and one of the fastest growing sectors in the world. At present and due to accessibility and improvement of technology, the tourism industry has been using experiences based on Virtual Reality, because they offer immersive experiences that absorb the user through sensory stimulation like sight and sound. The game, plays an essential role in the imaginary constructive process. By engaging fascination through interactivity, digital games introduce a sense of experimentation that facilitates immersion. In this context, the present preliminary study sought to assess the user experience through development of an immersive virtual reality game, to share the cultural aspects of São Tomé island and engaging the participants to visit the island. The goal of this study is to evaluate usability and user experience in a three-dimensional virtual reality environment prototype of a beach place, in a traditional hostel. Concerning usability, we evaluated the participants interaction in virtual world through their behavior, and responses to questionnaires of presence and simulator sickness. To evaluate the user experience, we evaluated emotions through use of a scale of valence and arousal (SAM) and categories of emotions elicited by the VR simulation using the Geneva Emotion Wheel. The results reveal that the three-dimensional virtual reality environment prototype provided high level of immersion, the control and sensory factors also present high value, while the distraction factors recorded reveal that they were not significant, and participant also felt that the experience had a high level of realism. No significant simulator sickness problems were reported. User experience was emotionally positive with high values of pleasure and contentment. In conclusion, the 3D virtual reality environment prototype was able to engage the participants and provide interactive experiences capable to seducing and arousing interest in the São Tomé and Príncipe islands.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Users}, {Contexts} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Trindade, Yanick and Rebelo, Francisco and Noriega, Paulo},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2018},
	keywords = {Design, Tourism, Usability methods and tools, Virtual reality},
	pages = {730--742},
	file = {Full Text PDF:files/433/Trindade et al. - 2018 - Tourism and Virtual Reality User Experience Evaluation of a Virtual Environment Prototype.pdf:application/pdf},
}

@inproceedings{velho_live_2018,
	address = {Cham},
	title = {Live {Probabilistic} {Editing} for {Virtual} {Cinematography}},
	isbn = {978-3-319-99426-0},
	doi = {10.1007/978-3-319-99426-0_4},
	abstract = {This paper introduces Probabilistic Editing for Virtual Cinematography. It is part of the VR Kino+Theater platform and provides high level authoring tools for cinematic presentations. The director acts as a DJ controlling in real-time an A/V switcher interface that selects the camera views of a theatrical performance in a virtual reality experience.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2018},
	publisher = {Springer International Publishing},
	author = {Velho, Luiz and Carvalho, Leonardo and Lucio, Djama},
	editor = {Clua, Esteban and Roque, Licinio and Lugmayr, Artur and Tuomi, Pauliina},
	year = {2018},
	keywords = {Immersive theatre, Montage, Real-time cinema, Storytelling, Virtual reality},
	pages = {40--51},
	file = {Full Text PDF:files/434/Velho et al. - 2018 - Live Probabilistic Editing for Virtual Cinematography.pdf:application/pdf},
}

@inproceedings{huttunen_enhancing_2024,
	address = {Cham},
	title = {Enhancing {Independent} {Auditory} and {Speechreading} {Training} – {Two} {Finnish} {Free} {Mobile} {Applications} {Constructed} for {Deaf} and {Hard} of {Hearing} {Children} and {Adults}},
	isbn = {978-3-031-59080-1},
	doi = {10.1007/978-3-031-59080-1_21},
	abstract = {The users of hearing technology often need auditory training for getting used to their hearing devices and maximally benefiting from them. Because auditory training given by professionals is only sparsely available, there is a great need for materials and applications with which self-training is possible. Moreover, deaf and hard-of-hearing persons need to improve their speechreading skills to help in speech reception and children to strengthen their reading skills. We describe the background, contents, construction and features of two Finnish free applications: Auditory Track for auditory training and Optic Track for speechreading (lip reading) training. Both can be used by children and adults, even though the Auditory Track is mainly aimed at adults and the Optic Track at primary school age children. The features of both applications include exercises carefully selected based on extensive knowledge of the acoustic and visual characteristics of speech. In addition, during the implementation of both applications, careful attention has been paid to the usability, accessibility, gamification and construction of feedback systems. The applications developed can be used in independent training, clinical use and research.},
	language = {en},
	booktitle = {Digital {Health} and {Wireless} {Solutions}},
	publisher = {Springer Nature Switzerland},
	author = {Huttunen, Kerttu and Kauramäki, Jaakko and Pajo, Kati and Saalasti, Satu},
	editor = {Särestöniemi, Mariella and Keikhosrokiani, Pantea and Singh, Daljeet and Harjula, Erkki and Tiulpin, Aleksei and Jansson, Miia and Isomursu, Minna and van Gils, Mark and Saarakkala, Simo and Reponen, Jarmo},
	year = {2024},
	keywords = {auditory training, lip reading, speechreading},
	pages = {284--302},
	file = {Full Text PDF:files/435/Huttunen et al. - 2024 - Enhancing Independent Auditory and Speechreading Training – Two Finnish Free Mobile Applications Con.pdf:application/pdf},
}

@inproceedings{maraj_oculus_2019,
	address = {Cham},
	title = {Oculus {Rift} {Versus} {HTC} {Vive}: {Usability} {Assessment} from a {Teleportation} {Task}},
	isbn = {978-3-030-21607-8},
	shorttitle = {Oculus {Rift} {Versus} {HTC} {Vive}},
	doi = {10.1007/978-3-030-21607-8_19},
	abstract = {Virtual Reality (VR) technology has shown impressive growth in recent years, extending to industrial, military, and rehabilitation occupations. However, despite such growth, there is little research on the usability aspects associated with different VR devices. This paper investigates subjective and objective usability differences between two commercial Head-Mounted Display (HMD) systems, the HTC Vive and Oculus Rift, using a between-subjects experimental design on three teleportation task scenarios. Each scenario had a time limit of five minutes and sequentially increased in complexity. Objective usability was evaluated through performance measures, including per scenario effectiveness, time duration, total time duration, completion rate, and time-based efficiency. Subjective usability was evaluated by users after the three scenarios, via a questionnaire formed of ease of use, comfort, effectiveness, and visual quality subscales. The results, interpreted using Mann-Whitney U Tests, indicated significant differences between the HTC Vive and Oculus Rift: in terms of objective usability, Vive’s overmatch in scenario three effectiveness suggests harder tasks in the Rift may require additional aids; in terms of subjective usability, Vive’s overmatch in effectiveness questionnaire items suggests it is a preferred choice for a range of applications, as well as for learning real-world skills. In terms of significant Spearman’s rho correlations, more HMD comfort is aligned with completion rates, within the Vive; different contexts may lead to a reversal effect, where visual quality can either relate to negative or positive performance, depending on the headset; and overall, many different usability aspects positively relate to total time-based efficiency of the teleportation task.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Multimodal} {Interaction}},
	publisher = {Springer International Publishing},
	author = {Maraj, Crystal and Hurter, Jonathan and Ferrante, Schuyler and Horde, Lauren and Carter, Jasmine and Murphy, Sean},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {Head-Mounted Displays, Usability analysis, Usability games, Virtual Reality},
	pages = {247--257},
	file = {Full Text PDF:files/436/Maraj et al. - 2019 - Oculus Rift Versus HTC Vive Usability Assessment from a Teleportation Task.pdf:application/pdf},
}

@inproceedings{zhou_effective_2009,
	address = {Berlin, Heidelberg},
	title = {Effective and {Efficient} {Tracking} and {Ego}-{Motion} {Recovery} for {Mobile} {Cameras}},
	isbn = {978-3-642-11164-8},
	doi = {10.1007/978-3-642-11164-8_56},
	abstract = {Estimating 3-D structure and camera motion from 2-D image sequences is an important problem in computer vision. In this paper we present an effective approach to tracking and recovery of ego-motion from an image sequence acquired by a single camera attached to a pedestrian. Our approach consists of two stages. In the first phase, human gait analysis is performed and human gait parameters are estimated by frame-by-frame analysis utilising a generalised least squares technique. In the second phase, the gait model is employed within a “predict-correct” framework using a maximum a posteriori expectation maximisation strategy to recover ego-motion and scene structure, while continuously refining the gait model. Experiments on synthetic and real image sequences confirm that the use of the gait model allows for effective tracking while also reducing the computational complexity.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Machine} {Intelligence}},
	publisher = {Springer},
	author = {Zhou, Huiyu and Schaefer, Gerald},
	editor = {Chaudhury, Santanu and Mitra, Sushmita and Murthy, C. A. and Sastry, P. S. and Pal, Sankar K.},
	year = {2009},
	keywords = {Feature Tracking, Gait Parameter, Generalise Little Square, Roll Angle, Structure From Motion},
	pages = {345--350},
	file = {Full Text PDF:files/437/Zhou e Schaefer - 2009 - Effective and Efficient Tracking and Ego-Motion Recovery for Mobile Cameras.pdf:application/pdf},
}

@inproceedings{brahnam_hci_2014,
	address = {Cham},
	title = {{HCI} {Prototyping} and {Modeling} of {Future} {Psychotherapy} {Technologies} in {Second} {Life}},
	isbn = {978-3-319-07233-3},
	doi = {10.1007/978-3-319-07233-3_26},
	abstract = {This paper describes the virtual MSU SL Prototyping Center for Psychotherapy Technologies in development at Missouri State University and explores the value of using Second Life (SL) as a prototyping tool for HCI research. The power of SL is illustrated in our use of it to envision applications and usage scenarios for an integrative system for psychotherapy technologies called MyPsySpace, a highly flexible and customizable system that can be used by independent therapists trained in a wide range of theoretical orientations.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Theories}, {Methods}, and {Tools}},
	publisher = {Springer International Publishing},
	author = {Brahnam, Sheryl},
	editor = {Kurosu, Masaaki},
	year = {2014},
	keywords = {drama therapy, expressive arts therapy, futures studies, psychotherapy, scenarios, second life prototyping, virtual reality},
	pages = {273--284},
	file = {Full Text PDF:files/438/Brahnam - 2014 - HCI Prototyping and Modeling of Future Psychotherapy Technologies in Second Life.pdf:application/pdf},
}

@inproceedings{thalen_virtual_2014,
	address = {Cham},
	title = {Virtual {Personas}: {A} {Case} {Study} on {Truck} {Cabin} {Design}},
	isbn = {978-3-319-07635-5},
	shorttitle = {Virtual {Personas}},
	doi = {10.1007/978-3-319-07635-5_35},
	abstract = {User involvement can help designers reach beyond functionality and usability, and identify the user’s deeper needs for a pleasurable product experience. In practice, direct user involvement can be limited by a lack of knowledge of appropriate techniques, confidentiality constraints or limited access to end-users. Alternatively, personas can be used as a substitute for direct user involvement. Personas, however, often end up as posters in the hallway of a design department without being used, for instance because personas are not sufficiently realistic, or because the personas are insufficiently communicated within the design department. This paper presents a case study featuring Virtual Personas. This application allows designers to create and review use scenarios in a virtual world, featuring digital avatars. Although the application has been successfully deployed, it was found that additional effort is required for designers to really reach beyond the level of functionality and usability.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {User} {Experience} {Design} for {Everyday} {Life} {Applications} and {Services}},
	publisher = {Springer International Publishing},
	author = {Thalen, Jos and van der Voort, Mascha},
	editor = {Marcus, Aaron},
	year = {2014},
	keywords = {case study, personas, roleplaying, scenarios, virtual reality},
	pages = {357--368},
	file = {Full Text PDF:files/439/Thalen e van der Voort - 2014 - Virtual Personas A Case Study on Truck Cabin Design.pdf:application/pdf},
}

@inproceedings{flotynski_multi-platform_2014,
	address = {Berlin, Heidelberg},
	title = {Multi-platform {Semantic} {Representation} of {Interactive} {3D} {Content}},
	isbn = {978-3-642-54734-8},
	doi = {10.1007/978-3-642-54734-8_8},
	abstract = {In this paper, a semantic approach to building multi-platform 3D content is proposed. The presented solution is intended to enable flexible and efficient creation of 3D presentations covering a wide range of target platforms – visualisation tools, content representation languages and programming libraries. Referring to the semantics of particular content elements can facilitate conceptual knowledge-based content creation at arbitrarily high levels of abstraction, and it can improve indexing, searching and analysis of 3D content in a variety of application domains on the web.},
	language = {en},
	booktitle = {Technological {Innovation} for {Collective} {Awareness} {Systems}},
	publisher = {Springer},
	author = {Flotyński, Jakub and Walczak, Krzysztof},
	editor = {Camarinha-Matos, Luis M. and Barrento, Nuno S. and Mendonça, Ricardo},
	year = {2014},
	keywords = {3D content, 3D web, multi-platform, ontology, Semantic Web},
	pages = {63--72},
	file = {Full Text PDF:files/440/Flotyński e Walczak - 2014 - Multi-platform Semantic Representation of Interactive 3D Content.pdf:application/pdf},
}

@inproceedings{lorenzini_aditho_2015,
	address = {Cham},
	title = {{ADITHO} – {A} {Serious} {Game} for {Training} and {Evaluating} {Medical} {Ethics} {Skills}},
	isbn = {978-3-319-24589-8},
	doi = {10.1007/978-3-319-24589-8_5},
	abstract = {This paper presents “A Day In The HOspital”, a Digital Serious Game aiming at providing a technological tool for both evaluating and training ethical skills of medical staff personnel. During the game, the player interprets the role of a physician who has to perform a decision-making process that involves his ethical and medical skills. Usability and sense of Presence have been assessed through a specific post-game Likert-questionnaire.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Lorenzini, Cristian and Faita, Claudia and Barsotti, Michele and Carrozzino, Marcello and Tecchia, Franco and Bergamasco, Massimo},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {Biomedical Ethics, Collaborative Training, Decision Making, Digital Serious Game, Ethical Evaluation, Medical Training, Physician-Patient Relationship, Virtual Environments, Virtual Reality},
	pages = {59--71},
	file = {Full Text PDF:files/461/Lorenzini et al. - 2015 - ADITHO – A Serious Game for Training and Evaluating Medical Ethics Skills.pdf:application/pdf},
}

@inproceedings{flor_3d_2015,
	address = {Cham},
	title = {{3D} {Virtual} {Worlds}: {An} {Ethnography} of {Key} {Artifacts} and {Processes}},
	isbn = {978-3-319-20367-6},
	shorttitle = {{3D} {Virtual} {Worlds}},
	doi = {10.1007/978-3-319-20367-6_3},
	abstract = {The development of an educational 3D virtual world requires a complex skillset, which includes programming, modeling, texturing, animating, and quest/level designing. When these skills are distributed across multiple workers, the workers must negotiate a shared understanding of their intermediate work products, which ultimately culminate in the virtual world. This paper is an ethnography of the intermediate work products (“artifacts”) and processes used in the development of Virtual Energy World—a 3D virtual world for instruction on sustainable energy issues. The resulting artifacts and processes have utility as a general development framework for both educational virtual worlds and video games.},
	language = {en},
	booktitle = {Social {Computing} and {Social} {Media}},
	publisher = {Springer International Publishing},
	author = {Flor, Nick V.},
	editor = {Meiselwitz, Gabriele},
	year = {2015},
	pages = {20--29},
	file = {Full Text PDF:files/462/Flor - 2015 - 3D Virtual Worlds An Ethnography of Key Artifacts and Processes.pdf:application/pdf},
}

@inproceedings{helgath_investigating_2018,
	address = {Cham},
	title = {Investigating the {Effect} of {Different} {Autonomy} {Levels} on {User} {Acceptance} and {User} {Experience} in {Self}-driving {Cars} with a {VR} {Driving} {Simulator}},
	isbn = {978-3-319-91806-8},
	doi = {10.1007/978-3-319-91806-8_19},
	abstract = {The possible transition to fully autonomous cars represents a paradigm shift, which is likely to have a profound impact on driving experience and automobile technology acceptance. Using an online questionnaire, Rödel et al. [7] have found that measures for User Acceptance (UA) and User Experience (UX) decline with increasing autonomy level. In this study, we investigate the differences in UA and UX for vehicles with different levels of automation in a more immersive context. We used a simple driving simulator setup in a virtual reality environment (using an Oculus Rift headset). We designed three tasks which each represented a different level of automation and asked participants (N = 17) to fill out the Car Technology Acceptance Model (CTAM) questionnaire after using each autonomy level. The immersion of the simulator setup was assessed with a standardized questionnaire. In contrast to Rödel et al. [7] results do not show a general decline in UA and UX with increasing autonomy, but suggest that Performance Expectancy, Perceived Safety and Social Influence are significantly higher for the fully automated condition than for no automation. The scores for immersion ranging about the average of benchmark evaluations indicate that the users felt quite immersed, but that there is still room for improving the VR setup.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Users}, {Contexts} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Helgath, Jana and Braun, Philip and Pritschet, Andreas and Schubert, Maximilian and Böhm, Patricia and Isemann, Daniel},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2018},
	keywords = {Car technology acceptance model (CTAM), Driving automation levels, User Acceptance, User Experience, VR driving simulator},
	pages = {247--256},
	file = {Full Text PDF:files/463/Helgath et al. - 2018 - Investigating the Effect of Different Autonomy Levels on User Acceptance and User Experience in Self.pdf:application/pdf},
}

@inproceedings{liu_virtual_2019,
	address = {Cham},
	title = {Virtual {Dome} {System} {Using} {HMDs}: {An} {Alternative} to the {Expensive} and {Less} {Accessible} {Physical} {Domes}},
	isbn = {978-3-030-21565-1},
	shorttitle = {Virtual {Dome} {System} {Using} {HMDs}},
	doi = {10.1007/978-3-030-21565-1_21},
	abstract = {It is known that the dome display, as one of the Immersive Virtual Environments (IVEs), has its prominent advantages that viewers can experience highly immersive sensation thanks to the frameless image with a wide Field of View (FOV). However, fulldome projection systems have limited accessibility because of the technical complexity and high construction cost. Recently, the development and popularization of Head-Mounted Displays (HMDs) have changed the ways of inquiry and production in many media industries. In this research, the authors developed a virtual dome system using HMDs and assessed it quantitatively from the perspective of user immersion. An experiment was conducted to compare the difference of users’ immersion between a physical dome and a virtual one using modified Immersive Tendency Questionnaire (ITQ) and the Virtual Reality Immersion (VRI) Questionnaire. 44 participants took part in this research. In conclusion, as far as user immersion is concerned, the virtual dome system is capable of bringing a similar, if not better, experience when compared with the physical one. Although the defects intrinsic to a virtual dome system, such as the limited resolution, uncomfortableness to wear and the lack of shared experience should not be overlooked, this paper proved that the virtual dome system can be a relatively low-cost and more accessible alternative for one to experience a fulldome movie, and thus worth further study and application.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Liu, Yun and Liu, Zhejun and Jin, Yunshui},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {Dome display, Head-Mounted Display (HMD), Immersion, Immersive Virtual Environment, Virtual dome system},
	pages = {312--328},
	file = {Full Text PDF:files/464/Liu et al. - 2019 - Virtual Dome System Using HMDs An Alternative to the Expensive and Less Accessible Physical Domes.pdf:application/pdf},
}

@inproceedings{boonbrahm_assembly_2014,
	address = {Cham},
	title = {Assembly of the {Virtual} {Model} with {Real} {Hands} {Using} {Augmented} {Reality} {Technology}},
	isbn = {978-3-319-07458-0},
	doi = {10.1007/978-3-319-07458-0_31},
	abstract = {In the past few years, studying in the field of Augmented Reality (AR) has been expanded from technical aspect such as tracking system, authoring tools and etc. to applications ranging from the fields of education, entertainment, medicine to manufacturing. In manufacturing, which relies on assembly process, AR is used for assisting staffs in the field of maintenance and assembly. Usually, it has been used as a guidance system, for example using graphical instructions for advising the users with the steps in performing the maintenance or assembly operation. In assembly training, especially for small, expensive or harmful devices, interactive technique using real hands may be suitable than the guiding technique. Using tracking algorithm to track both hands in real time, interaction can occurs by the execution of grasp and release gestures. Bare hand tracking technique, which uses gesture recognition to enable interaction with augmented objects are also possible. In this paper, we attempted to use marker based AR technique to assemble 3D virtual objects using natural hand interaction. By applying the markers to fit on fingertip and assigned the corresponding virtual 3D finger that have physical properties such as surface, volume, density, friction and collision detection properties to them, interaction between fingers and objects could be executed. This setup was designed on a PC based system but could be ported to iOS or Android, so that it would work on tablet or mobile phones as well. Unity 3D game engine was used with Vuforia AR platform. In order to grab and move the virtual object by hand, the shape of the virtual finger (Vulforia’s target) has been investigated. Appropriate friction coefficient were applied to both virtual fingers and the object and then at least two virtual fingers were force to press on the 3D virtual object in opposite directions so that frictional force is more than gravitational force. To test this method, virtual model of LEGO’s mini-figures which composed of five pieces, was used and the assembly could be done in just a short time. Comparing with other popular technique such as “gestures recognition”, we have found that our technique could provide more efficient result in term of cost and natural feeling.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Designing} and {Developing} {Virtual} and {Augmented} {Environments}},
	publisher = {Springer International Publishing},
	author = {Boonbrahm, Poonpong and Kaewrat, Charlee},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2014},
	keywords = {Assembly Process, Augmented Reality, Manufacturing, Virtual Object Assembly},
	pages = {329--338},
	file = {Full Text PDF:files/465/Boonbrahm e Kaewrat - 2014 - Assembly of the Virtual Model with Real Hands Using Augmented Reality Technology.pdf:application/pdf},
}

@inproceedings{velloso_empirical_2015,
	address = {Cham},
	title = {An {Empirical} {Investigation} of {Gaze} {Selection} in {Mid}-{Air} {Gestural} {3D} {Manipulation}},
	isbn = {978-3-319-22668-2},
	doi = {10.1007/978-3-319-22668-2_25},
	abstract = {In this work, we investigate gaze selection in the context of mid-air hand gestural manipulation of 3D rigid bodies on monoscopic displays. We present the results of a user study with 12 participants in which we compared the performance of Gaze, a Raycasting technique (2D Cursor) and a Virtual Hand technique (3D Cursor) to select objects in two 3D mid-air interaction tasks. Also, we compared selection confirmation times for Gaze selection when selection is followed by manipulation to when it is not. Our results show that gaze selection is faster and more preferred than 2D and 3D mid-air-controlled cursors, and is particularly well suited for tasks in which users constantly switch between several objects during the manipulation. Further, selection confirmation times are longer when selection is followed by manipulation than when it is not.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2015},
	publisher = {Springer International Publishing},
	author = {Velloso, Eduardo and Turner, Jayson and Alexander, Jason and Bulling, Andreas and Gellersen, Hans},
	editor = {Abascal, Julio and Barbosa, Simone and Fetter, Mirko and Gross, Tom and Palanque, Philippe and Winckler, Marco},
	year = {2015},
	keywords = {3D user interfaces, Eye tracking, Mid-air gestures},
	pages = {315--330},
	file = {Full Text PDF:files/466/Velloso et al. - 2015 - An Empirical Investigation of Gaze Selection in Mid-Air Gestural 3D Manipulation.pdf:application/pdf},
}

@inproceedings{torok_mobile_2015,
	address = {Cham},
	title = {A {Mobile} {Game} {Controller} {Adapted} to the {Gameplay} and {User}’s {Behavior} {Using} {Machine} {Learning}},
	isbn = {978-3-319-24589-8},
	doi = {10.1007/978-3-319-24589-8_1},
	abstract = {When playing games, the user expects an easy and intuitive interaction. While current controllers are physical hardware components with a default configuration of buttons, different games use different buttons and demand different interaction methods. Besides, the player style varies according to personal characteristics or past gaming experiences. In previous works we proposed a novel virtual controller based on a common touchscreen device, such as smartphone or tablet, that is used as a gamepad to control a game on a computer or game console. In this work we include machine-learning techniques for an intelligent adaption of the layout and control elements distribution, minimizing errors and providing an enjoyable experience for individual users. We also present different usability tests and show considerable improvements in the precision and game performance of the user. We expect to open a new way of designing console and desktop games, allowing game designers to project individual controllers for each game.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Torok, Leonardo and Pelegrino, Mateus and Trevisan, Daniela G. and Clua, Esteban and Montenegro, Anselmo},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {games and play, input and interaction technologies, machine learning and data mining, Touch surfaces and touch interaction},
	pages = {3--16},
	file = {Full Text PDF:files/467/Torok et al. - 2015 - A Mobile Game Controller Adapted to the Gameplay and User’s Behavior Using Machine Learning.pdf:application/pdf},
}

@inproceedings{wang_study_2009,
	address = {Berlin, Heidelberg},
	title = {A {Study} on the {Design} of {Augmented} {Reality} {User} {Interfaces} for {Mobile} {Learning} {Systems} in {Heritage} {Temples}},
	isbn = {978-3-642-02771-0},
	doi = {10.1007/978-3-642-02771-0_32},
	abstract = {In order to reduce switching attention and increase the performance and pleasure of mobile learning in heritage temples, the objective of this research was to employ the technology of Augmented Reality (AR) on the user interfaces of mobile devices. Based on field study and literature review, three user interface prototypes were constructed. They both offered two service modes but differed in the location of navigation bars and text display approaches. The results of experiment showed that users preferred animated and interactive virtual objects or characters with sound effects. In addition, transparent background of images and text message boxes were better. The superimposed information should not cover more than thirty percents of the screen so that users could still see the background clearly.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality}},
	publisher = {Springer},
	author = {Wang, Kuo-Hsiung and Chen, Li-Chieh and Chu, Po-Ying and Cheng, Yun-Maw},
	editor = {Shumaker, Randall},
	year = {2009},
	keywords = {Augmented Reality, Mobile Learning, User Interface Design},
	pages = {282--290},
	file = {Full Text PDF:files/468/Wang et al. - 2009 - A Study on the Design of Augmented Reality User Interfaces for Mobile Learning Systems in Heritage T.pdf:application/pdf},
}

@inproceedings{ploss_case_2009,
	address = {Berlin, Heidelberg},
	title = {A {Case} {Study} on {Using} {RTF} for {Developing} {Multi}-player {Online} {Games}},
	isbn = {978-3-642-00955-6},
	doi = {10.1007/978-3-642-00955-6_44},
	abstract = {Real-Time Online Interactive Applications (ROIA) include a broad spectrum of online computer games, as well as challenging distributed e-learning applications, like virtual classrooms and collaborative environments. Development of ROIA poses several complex tasks that currently are addressed at a low level of abstraction. In our previous work, we presented the Real-Time Framework (RTF) - a novel middleware for a high-level development and execution of ROIA in single- and multi-server environments. This paper describes a case study in which a simple but representative online computer game is developed using RTF. We explain how RTF supports the design of data structures and their automatic serialization for network transmission, as well as determining and processing user actions when computing a new game state; the challenge is to provide the state updates to all players in real time at a very high frequency.},
	language = {en},
	booktitle = {Euro-{Par} 2008 {Workshops} - {Parallel} {Processing}},
	publisher = {Springer},
	author = {Ploss, Alexander and Glinka, Frank and Gorlatch, Sergei},
	editor = {César, Eduardo and Alexander, Michael and Streit, Achim and Träff, Jesper Larsson and Cérin, Christophe and Knüpfer, Andreas and Kranzlmüller, Dieter and Jha, Shantenu},
	year = {2009},
	keywords = {Game Logic, Game State, Game World, Online Game, State Update},
	pages = {390--400},
	file = {Full Text PDF:files/469/Ploss et al. - 2009 - A Case Study on Using RTF for Developing Multi-player Online Games.pdf:application/pdf},
}

@inproceedings{martis_going_2017,
	address = {Cham},
	title = {Going to a {Virtual} {Supermarket}: {Comparison} of {Different} {Techniques} for {Interacting} in a {Serious} {Game} for the {Assessment} of the {Cognitive} {Status}},
	isbn = {978-3-319-70742-6},
	shorttitle = {Going to a {Virtual} {Supermarket}},
	doi = {10.1007/978-3-319-70742-6_26},
	abstract = {An increasing number of people suffers from cognitive impairments, also related to aging. Several approaches are used to evaluate the mental status of people affected by cognitive diseases, and there is a growing interest toward approaches that allow a quantitative and personalized evaluation of such impairments. Such approaches comprise serious games and VR-based cognitive assessment systems. Nevertheless, few works attempt to understand how people interact in such systems, and which human-computer interaction modalities are to be preferred when targeting impaired people. The aim of this work is to quantitative and qualitative compare two solutions to play in a virtual supermarket (a PC and a tablet-based solution). The obtained results can be used as a starting point to design VR-based serious games to be used instead of questionnaire-based approaches, thus improving both clinical evaluation performances and patients’ motivation.},
	language = {en},
	booktitle = {New {Trends} in {Image} {Analysis} and {Processing} – {ICIAP} 2017},
	publisher = {Springer International Publishing},
	author = {Martis, Alice E. and Bassano, Chiara and Solari, Fabio and Chessa, Manuela},
	editor = {Battiato, Sebastiano and Farinella, Giovanni Maria and Leo, Marco and Gallo, Giovanni},
	year = {2017},
	keywords = {Human-computer interaction, Neuropsychological assessment, Serious games, Virtual reality, VR-based cognitive assessment},
	pages = {281--289},
	file = {Full Text PDF:files/470/Martis et al. - 2017 - Going to a Virtual Supermarket Comparison of Different Techniques for Interacting in a Serious Game.pdf:application/pdf},
}

@inproceedings{miranda_evaluation_2016,
	address = {Cham},
	title = {Evaluation of {Information} {Visualization} {Interaction} {Techniques} {Using} {Gestures} and {Widgets} in {3D} {Environments}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_24},
	abstract = {This paper presents the results of usability evaluation of an Information Visualization tool with touchless gestural commands. The tool has well-known visualizations tasks implemented in itself, allowing users to interact on a 3D scatterplot visualization technique. The chosen usability evaluation was the Think Aloud protocol, together with questionnaires conducted by an interviewer, both of them performed with five participants in a controlled environment.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Miranda, Brunelli and Santos, Carlos and Carneiro, Nikolas and Araújo, Tiago and Marques, Anderson and Mota, Marcelle and Neto, Nelson and Meiguins, Bianchi},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {3D interactions, Gestural commands, Information visualization, Usability evaluation},
	pages = {255--265},
	file = {Full Text PDF:files/471/Miranda et al. - 2016 - Evaluation of Information Visualization Interaction Techniques Using Gestures and Widgets in 3D Envi.pdf:application/pdf},
}

@inproceedings{dos_santos_digital_2019,
	address = {Cham},
	title = {Digital {Empathic} {Games} and {Their} {Relation} with {Mortality}: {Analysis} of {Discussion} {Forums}},
	isbn = {978-3-030-22602-2},
	shorttitle = {Digital {Empathic} {Games} and {Their} {Relation} with {Mortality}},
	doi = {10.1007/978-3-030-22602-2_23},
	abstract = {Digital empathy games have attracted the attention of players since they present topics associated with human frailty, such as death and mortality. Thus this type of game invites users to engage in reflection and they identify with scenes and images of the narrated world. This study presents an analysis of the perception of users of this type of game, in order to identify relevant aspects of its design and evaluation. The object studied by this article is the empathy game Valiant Hearts: The Great War, and user reviews were collected and analyzed through comments found in internet forums. These discussion boards contain user opinions ranging from technical issues and quality to matters related to mortality and death. The research is based on netnography, by means of qualitative and quantitative manual analysis of user comments. In general, the study presented multiple aspects involved in this type of game and generated considerable reflection and future studies.},
	language = {en},
	booktitle = {{HCI} in {Games}},
	publisher = {Springer International Publishing},
	author = {dos Santos, Danilo Barros and Maciel, Cristiano and Pereira, Vinicius Carvalho and dos Santos Nunes, Eunice Pereira},
	editor = {Fang, Xiaowen},
	year = {2019},
	keywords = {Death, Digital empathy games, Discussion forums, Internet forums, Mortality},
	pages = {307--319},
	file = {Full Text PDF:files/472/dos Santos et al. - 2019 - Digital Empathic Games and Their Relation with Mortality Analysis of Discussion Forums.pdf:application/pdf},
}

@inproceedings{maruyama_mocap-based_2015,
	address = {Cham},
	title = {{MoCap}-{Based} {Adaptive} {Human}-{Like} {Walking} {Simulation} in {Laser}-{Scanned} {Large}-{Scale} as-{Built} {Environments}},
	isbn = {978-3-319-21070-4},
	doi = {10.1007/978-3-319-21070-4_20},
	abstract = {Accessibility evaluation to enhance accessibility and safety for the elderly and disabled is increasing in importance. Accessibility must be assessed not only from the general standard aspect but also in terms of physical and cognitive friendliness for users of different ages, genders, and abilities. Human behavior simulation has been progressing in crowd behavior analysis and emergency evacuation planning. This research aims to develop a virtual accessibility evaluation by combining realistic human behavior simulation using a digital human model (DHM) with as-built environmental models. To achieve this goal, we developed a new algorithm for generating human-like DHM walking motions, adapting its strides and turning angles to laser-scanned as-built environments using motion-capture (MoCap) data of flat walking. Our implementation quickly constructed as-built three-dimensional environmental models and produced a walking simulation speed sufficient for real-time applications. The difference in joint angles between the DHM and MoCap data was sufficiently small. Demonstrations of our environmental modeling and walking simulation in an indoor environment are illustrated.},
	language = {en},
	booktitle = {Digital {Human} {Modeling}. {Applications} in {Health}, {Safety}, {Ergonomics} and {Risk} {Management}: {Ergonomics} and {Health}},
	publisher = {Springer International Publishing},
	author = {Maruyama, Tsubasa and Kanai, Satoshi and Date, Hiroaki},
	editor = {Duffy, Vincent G.},
	year = {2015},
	keywords = {Accessibility evaluation, Laser-scanning, Motion capture, Walking simulation},
	pages = {193--204},
	file = {Full Text PDF:files/473/Maruyama et al. - 2015 - MoCap-Based Adaptive Human-Like Walking Simulation in Laser-Scanned Large-Scale as-Built Environment.pdf:application/pdf},
}

@inproceedings{park_augmented_2016,
	address = {Cham},
	title = {Augmented {Reality} {Based} {Guidance} for {Solving} {Rubik}’s {Cube} {Using} {HMD}},
	isbn = {978-3-319-40542-1},
	doi = {10.1007/978-3-319-40542-1_85},
	abstract = {This paper proposes a guidance system to help to solve the Rubik’s cube using Head Mounted Display (HMD) and gesture interface. Our system use augmented reality technology to recognize the placement of each square and provide intuitive and easily understandable guidance for solving procedure. Also, this system aims to improve the user experience of guidance system for solving Rubik’s Cube by allowing the user pose naturally.},
	language = {en},
	booktitle = {{HCI} {International} 2016 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Park, Jaebum and Park, Changhoon},
	editor = {Stephanidis, Constantine},
	year = {2016},
	keywords = {Augmented reality, Gesture interface, Guidance system, Head mounted display, HMD, Intuitive, Rubik’s cube},
	pages = {524--529},
	file = {Full Text PDF:files/474/Park e Park - 2016 - Augmented Reality Based Guidance for Solving Rubik’s Cube Using HMD.pdf:application/pdf},
}

@inproceedings{mazalek_pictures_2012,
	address = {Berlin, Heidelberg},
	title = {Pictures at an {Exhibition}: {Design} of a {Hybrid} {Puppetry} {Performance} {Piece}},
	isbn = {978-3-642-33542-6},
	shorttitle = {Pictures at an {Exhibition}},
	doi = {10.1007/978-3-642-33542-6_12},
	abstract = {Pictures at an Exhibition is a physical/digital puppetry piece that uses tangible interface puppets to modify a virtual scene projected at the back of the stage in real-time. The piece merges traditional puppeteering practices with tangible interaction technologies and virtual environments to create a novel performance for the live stage. This paper describes the design and development of piece, as well as our lessons learned from this process and from on-stage performances of Pictures at an Exhibition in a puppetry theatre.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2012},
	publisher = {Springer},
	author = {Mazalek, Ali and Nitsche, Michael and Rébola, Claudia and Clifton, Paul and Wu, Andy and Poirier, Nick and Peer, Firaz},
	editor = {Herrlich, Marc and Malaka, Rainer and Masuch, Maic},
	year = {2012},
	keywords = {experimental theatre, performance, physical/digital puppetry, tangible interaction, virtual space},
	pages = {130--143},
	file = {Full Text PDF:files/475/Mazalek et al. - 2012 - Pictures at an Exhibition Design of a Hybrid Puppetry Performance Piece.pdf:application/pdf},
}

@inproceedings{wade_design_2014,
	address = {Cham},
	title = {Design of a {Virtual} {Reality} {Driving} {Environment} to {Assess} {Performance} of {Teenagers} with {ASD}},
	isbn = {978-3-319-07440-5},
	doi = {10.1007/978-3-319-07440-5_43},
	abstract = {Autism Spectrum Disorder (ASD) is an extremely common and costly neurodevelopmental disorder. While significant research has been devoted to addressing social communication skill deficits of people with ASD, relatively less attention has been paid to improving their deficits in daily activities such as driving. Only two empirical studies have investigated driving performance in individuals with ASD—both employing proprietary driving simulation software. We designed a novel Virtual Reality (VR) driving simulator so that we could integrate various sensory modules directly into our system as well as to define task-oriented protocols that would not be otherwise possible using commercial software. We conducted a small user study with a group of individuals with ASD and a group of typically developing community controls. We found that our system was capable of distinguishing behavioral patterns between both groups indicating that it is suitable for use in designing a protocol aimed at improving driving performance.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Universal} {Access} to {Information} and {Knowledge}},
	publisher = {Springer International Publishing},
	author = {Wade, Joshua and Bian, Dayi and Zhang, Lian and Swanson, Amy and Sarkar, Medha and Warren, Zachary and Sarkar, Nilanjan},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {Adaptive task, Autism intervention, Eye gaze, Physiological signals, Virtual Reality},
	pages = {466--474},
	file = {Full Text PDF:files/476/Wade et al. - 2014 - Design of a Virtual Reality Driving Environment to Assess Performance of Teenagers with ASD.pdf:application/pdf},
}

@inproceedings{tran_robust_2009,
	address = {Berlin, Heidelberg},
	title = {Robust {Hybrid} {Tracking} with {Life}-{Size} {Avatar} in {Mixed} {Reality} {Environment}},
	isbn = {978-3-642-02771-0},
	doi = {10.1007/978-3-642-02771-0_56},
	abstract = {We have developed a system which enables us to track participant-observers accurately in a large area for the purpose of immersing them in a mixed reality environment. This system is robust even under uncompromising lighting conditions. Accurate tracking of the observer’s spatial and orientation point of view is achieved by using hybrid inertial sensors and computer vision techniques. We demonstrate our results by presenting life-size, animated human avatars sitting in real chairs, in a stable and low-jitter manner. The system installation allows the observers to freely walk around and navigate themselves in the environment even while still being able to see the avatars from various angles. The project installation provides and exciting way for cultural and historical narratives to be presented vividly in the real present world.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality}},
	publisher = {Springer},
	author = {Tran, Qui Cong Thien and Lee, Shang Ping and Pensyl, W. Russell and Jernigan, Daniel},
	editor = {Shumaker, Randall},
	year = {2009},
	keywords = {Augmented Reality, Human Avatar, Inertial Sensor, Mixed Reality, Virtual Character},
	pages = {503--510},
	file = {Full Text PDF:files/477/Tran et al. - 2009 - Robust Hybrid Tracking with Life-Size Avatar in Mixed Reality Environment.pdf:application/pdf},
}

@inproceedings{martin_after_2011,
	address = {Berlin, Heidelberg},
	title = {An {After} {Action} {Review} {Engine} for {Training} in {Multiple} {Areas}},
	isbn = {978-3-642-21741-8},
	doi = {10.1007/978-3-642-21741-8_64},
	abstract = {The notion of after action review (AAR) is known in the military where it is used to develop a common picture of what happened and why. Recently, the concept has been rediscovered by other domains. Obviously, a review within these domains would be different. This paper addresses development of an AAR engine. By “AAR engine” we mean a system that provides the common functionalities across all AAR systems into a single foundation for training. Regardless of the domain, there are capabilities needed in an AAR system (e.g. recording and playback of scenario data). On the other hand, there are also features specific for each domain. In this paper we first review the infrastructure of our AAR engine. Then advantages of such a system for addressing various AAR systems are reviewed. Additional advanced functions are then presented and reviewed in light of how the engine can easily provide these enhancements.},
	language = {en},
	booktitle = {Engineering {Psychology} and {Cognitive} {Ergonomics}},
	publisher = {Springer},
	author = {Martin, Glenn A. and Daly, Jason and Thurston, Casey},
	editor = {Harris, Don},
	year = {2011},
	keywords = {AAR, After Action Review, Simulation, Software Infrastructures, Training},
	pages = {598--607},
	file = {Full Text PDF:files/478/Martin et al. - 2011 - An After Action Review Engine for Training in Multiple Areas.pdf:application/pdf},
}

@inproceedings{botev_immersive_2020,
	address = {Cham},
	title = {Immersive {Telepresence} {Framework} for {Remote} {Educational} {Scenarios}},
	isbn = {978-3-030-50506-6},
	doi = {10.1007/978-3-030-50506-6_26},
	abstract = {Social robots have an enormous potential for educational applications, allowing cognitive outcomes similar to those with human involvement. Enabling instructors and learners to directly control a social robot and immersively interact with their students and peers opens up new possibilities for effective lesson delivery and better participation in the classroom.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Human} and {Technology} {Ecosystems}},
	publisher = {Springer International Publishing},
	author = {Botev, Jean and Rodríguez Lera, Francisco J.},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2020},
	keywords = {Education, Human-robot interaction, Immersive telepresence, Social robotics, Teleoperation, UI design, Virtual reality},
	pages = {373--390},
	file = {Full Text PDF:files/479/Botev e Rodríguez Lera - 2020 - Immersive Telepresence Framework for Remote Educational Scenarios.pdf:application/pdf},
}

@inproceedings{saleh_effective_2018,
	address = {Cham},
	title = {Effective {Use} of {Synthetic} {Data} for {Urban} {Scene} {Semantic} {Segmentation}},
	isbn = {978-3-030-01216-8},
	doi = {10.1007/978-3-030-01216-8_6},
	abstract = {Training a deep network to perform semantic segmentation requires large amounts of labeled data. To alleviate the manual effort of annotating real images, researchers have investigated the use of synthetic data, which can be labeled automatically. Unfortunately, a network trained on synthetic data performs relatively poorly on real images. While this can be addressed by domain adaptation, existing methods all require having access to real images during training. In this paper, we introduce a drastically different way to handle synthetic images that does not require seeing any real images at training time. Our approach builds on the observation that foreground and background classes are not affected in the same manner by the domain shift, and thus should be treated differently. In particular, the former should be handled in a detection-based manner to better account for the fact that, while their texture in synthetic images is not photo-realistic, their shape looks natural. Our experiments evidence the effectiveness of our approach on Cityscapes and CamVid with models trained on synthetic data only.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Saleh, Fatemeh Sadat and Aliakbarian, Mohammad Sadegh and Salzmann, Mathieu and Petersson, Lars and Alvarez, Jose M.},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Instance-level annotation, Object detection, Semantic segmentation, Synthetic data},
	pages = {86--103},
	file = {Full Text PDF:files/480/Saleh et al. - 2018 - Effective Use of Synthetic Data for Urban Scene Semantic Segmentation.pdf:application/pdf},
}

@inproceedings{kim_serious_2015,
	address = {Cham},
	title = {Serious {Game} for the {Evaluation} of {Cognitive} {Function} of {Kids}},
	isbn = {978-3-319-21380-4},
	doi = {10.1007/978-3-319-21380-4_64},
	abstract = {This paper describes the serious game contents for the evaluation of cognitive function for kids. The game contents were designed for measuring and enhancing the cognitive function of the kids (ages 5–7). We clustered the measurable cognitive functions as auditory attention, visual attention, attention shift, and impulse control. This study is based on the advisory of the Department of Psychiatry and Behavioral Science, Seoul National University College of Medicine. In impulse control task, we applied the vision based head tracking technology. This study is meaningful on the point view that we can evaluate and enhance the cognitive function of kids who are familiar with the computer environments.},
	language = {en},
	booktitle = {{HCI} {International} 2015 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Kim, Donghan and Lim, C. J.},
	editor = {Stephanidis, Constantine},
	year = {2015},
	keywords = {Attention shift, Auditory attention, Cognitive function, Impulse control, Serious game, Visual attention},
	pages = {377--382},
	file = {Full Text PDF:files/501/Kim e Lim - 2015 - Serious Game for the Evaluation of Cognitive Function of Kids.pdf:application/pdf},
}

@inproceedings{vourvopoulos_multimodal_2019,
	address = {Cham},
	title = {Multimodal {Head}-{Mounted} {Virtual}-{Reality} {Brain}-{Computer} {Interface} for {Stroke} {Rehabilitation}},
	isbn = {978-3-030-21607-8},
	doi = {10.1007/978-3-030-21607-8_13},
	abstract = {Rehabilitation after stroke requires the exploitation of active movement by the patient in order to efficiently re-train the affected side. Individuals with severe stroke cannot benefit from many training solutions since they have paresis and/or spasticity, limiting volitional movement. Nonetheless, research has shown that individuals with severe stroke may have modest benefits from action observation, virtual reality, and neurofeedback from brain-computer interfaces (BCIs). In this study, we combined the principles of action observation in VR together with BCI neurofeedback for stroke rehabilitation to try to elicit optimal rehabilitation gains. Here, we illustrate the development of the REINVENT platform, which takes post-stroke brain signals indicating an attempt to move and drives a virtual avatar arm, providing patient-driven action observation in head-mounted VR. We also present a longitudinal case study with a single individual to demonstrate the feasibility and potentially efficacy of the REINVENT system.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Multimodal} {Interaction}},
	publisher = {Springer International Publishing},
	author = {Vourvopoulos, Athanasios and Marin-Pardo, Octavio and Neureither, Meghan and Saldana, David and Jahng, Esther and Liew, Sook-Lei},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {Brain-computer interfaces, Stroke rehabilitation, Virtual reality},
	pages = {165--179},
	file = {Full Text PDF:files/502/Vourvopoulos et al. - 2019 - Multimodal Head-Mounted Virtual-Reality Brain-Computer Interface for Stroke Rehabilitation.pdf:application/pdf},
}

@inproceedings{rodil_new_2011,
	address = {Berlin, Heidelberg},
	title = {A {New} {Visualization} {Approach} to {Re}-{Contextualize} {Indigenous} {Knowledge} in {Rural} {Africa}},
	isbn = {978-3-642-23771-3},
	doi = {10.1007/978-3-642-23771-3_23},
	abstract = {Current views of sustainable development recognize the importance of accepting the Indigenous Knowledge (IK) of rural people. However, there is an increasing technological gap between Elder IK holders and the younger generation and a persistent incompatibility between IK and the values, logics and literacies embedded, and supported by ICT. Here, we present an evaluation of new technology that might bridge generations and preserve key elements of local IK in Namibia. We describe how we applied insights, generated by ethnographic, dialogical and participatory action research, in designing a structure in which users can store, organize and retrieve user-generated videos in ways that are compatible with their knowledge system. The structure embeds videos in a scenario-based 3D visualization of a rural village. It accounts for some of the ways this rural community manages information, socially, spatially and temporally and provides users with a recognizable 3D simulated environment in which to re-contextualize de-contextualized video clips. Our formative in situ evaluation of a prototype suggests the visualization is legible to community members, provokes participation in design discussions, offers opportunities for local appropriation and may facilitate knowledge sharing between IK holders and more youthful IK assimilators. Simultaneously differing interpretations of scenarios and modeled objects reveal the limitations of our modeling decisions and raises various questions regarding graphic design details and regional transferability.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2011},
	publisher = {Springer},
	author = {Rodil, Kasper and Winschiers-Theophilus, Heike and Bidwell, Nicola J. and Eskildsen, Søren and Rehm, Matthias and Kapuire, Gereon Koch},
	editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
	year = {2011},
	keywords = {3D visualization, Africa, design, indigenous knowledge, rural},
	pages = {297--314},
	file = {Full Text PDF:files/503/Rodil et al. - 2011 - A New Visualization Approach to Re-Contextualize Indigenous Knowledge in Rural Africa.pdf:application/pdf},
}

@inproceedings{iqbal_new_2013,
	address = {Berlin, Heidelberg},
	title = {A {New} {Chess} {Variant} for {Gaming} {AI}},
	isbn = {978-3-642-41106-9},
	doi = {10.1007/978-3-642-41106-9_2},
	abstract = {In this article, we describe a newly-invented chess variant called Switch-Side Chain-Chess that is demonstrably more challenging for humans and computers than the standard, international version of the game. A new rule states that players have the choice to switch sides with each other if a continuous link of pieces is created on the board. This simple rule increases significantly the complexity of chess, as perceived by the players, but not the actual size of its game tree. The new variant therefore more easily allows board game researchers to focus on the ‘higher level’ aspects of intelligence such as perception and intuition without being constrained by a larger search space as they would be if using a game like Go or Arimaa. They can also immediately build upon the tried and tested approaches already being used in strong chess engines instead of having to start from scratch or a lower level of progress as is the case with other games of this type.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2013},
	publisher = {Springer},
	author = {Iqbal, Azlan},
	editor = {Anacleto, Junia C. and Clua, Esteban W. G. and da Silva, Flavio S. Correa and Fels, Sidney and Yang, Hyun S.},
	year = {2013},
	keywords = {Chess, complexity, intelligence, intuition, perception, variant},
	pages = {9--16},
	file = {Full Text PDF:files/504/Iqbal - 2013 - A New Chess Variant for Gaming AI.pdf:application/pdf},
}

@inproceedings{tezza_brain_2019,
	address = {Cham},
	title = {Brain {eRacing}: {An} {Exploratory} {Study} on {Virtual} {Brain}-{Controlled} {Drones}},
	isbn = {978-3-030-21565-1},
	shorttitle = {Brain {eRacing}},
	doi = {10.1007/978-3-030-21565-1_10},
	abstract = {As Brain-Computer Interface (BCI) technology become more ubiquitous, lower cost and expands from research laboratories to user’s home, there is an emerging field and application on brain-controlled games. The use of BCI for gaming does not only allows an extra channel of communication between player and game systems, but it also extends its use to users with physical disabilities. This paper presents a brain-controlled drone game, where users can control a drone avatar with their brain-waves through motor imagery. To control the game, players wear a non-invasive BCI device, which measures and decodes the brain activity into game commands. Furthermore, this paper presents the results of a exploratory study performed to evaluate the gameplay experience, how the game changes participants affective state, and evaluate the user’s perception towards brain-controlled games. Our findings show that players had a statistically significant increase in their positive affect score during the gameplay, as well as an increase in alertness, attentiveness, and inspiration.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Tezza, Dante and Garcia, Sarah and Hossain, Tamjid and Andujar, Marvin},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {Accessibility, Brain-computer interfaces, e-sports, User experience, Video games, Virtual reality},
	pages = {150--162},
	file = {Full Text PDF:files/505/Tezza et al. - 2019 - Brain eRacing An Exploratory Study on Virtual Brain-Controlled Drones.pdf:application/pdf},
}

@inproceedings{mapes_geppetto_2011,
	address = {Berlin, Heidelberg},
	title = {Geppetto: {An} {Environment} for the {Efficient} {Control} and {Transmission} of {Digital} {Puppetry}},
	isbn = {978-3-642-22024-1},
	shorttitle = {Geppetto},
	doi = {10.1007/978-3-642-22024-1_30},
	abstract = {An evolution of remote control puppetry systems is presented. These systems have been designed to provide high quality trainer to trainee communication in game scenarios containing multiple digital puppets with interaction occurring over long haul networks. The design requirements were to support dynamic switching of control between multiple puppets; suspension of disbelief when communicating through puppets; sensitivity to network bandwidth requirements; and as an affordable tool for professional interactive trainers (Interactors). The resulting system uses a novel pose blending solution guided by a scaled down desktop range motion capture controller as well as traditional button devices running on an standard game computer. This work incorporates aspects of motion capture, digital puppet design and rigging, game engines, networking, interactive performance, control devices and training.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality} - {Systems} and {Applications}},
	publisher = {Springer},
	author = {Mapes, Daniel P. and Tonner, Peter and Hughes, Charles E.},
	editor = {Shumaker, Randall},
	year = {2011},
	keywords = {avatar, Digital puppetry, gesture, motion capture},
	pages = {270--278},
	file = {Full Text PDF:files/506/Mapes et al. - 2011 - Geppetto An Environment for the Efficient Control and Transmission of Digital Puppetry.pdf:application/pdf},
}

@inproceedings{eubanks_full-body_2016,
	address = {Cham},
	title = {Full-{Body} {Portable} {Virtual} {Reality} for {Personal} {Protective} {Equipment} {Training}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_47},
	abstract = {This paper presents a full-body, portable virtual reality prototype for training personal protective equipment procedures to surgical staff. The system consists of a head-mounted display for viewing the virtual world, inertial measurement units that track the user’s full-body movements, and handheld controllers for bimanual interactions. With these capabilities, our system affords the development of gross psychomotor skills, in addition to knowledge-based cognitive skills. The system hardware is designed to be portable and does not require a dedicated space for usage. The system software includes two training modules. An instructional module uses an error-avoidant training approach to teach trainees correct actions. A practice module uses an error-management approach that allows trainees to rehearse their skills while receiving automated assessments on their mistakes. Future work includes an evaluation of the prototype with medical subject-matter experts.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Eubanks, James Coleman and Somareddy, Veena and McMahan, Ryan P. and Lopez, Alfonso A.},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {Personal protective equipment, Training, Virtual reality},
	pages = {490--501},
	file = {Full Text PDF:files/507/Eubanks et al. - 2016 - Full-Body Portable Virtual Reality for Personal Protective Equipment Training.pdf:application/pdf},
}

@inproceedings{richard_virtual_2018,
	address = {Cham},
	title = {A {Virtual} {Kitchen} for {Cognitive} {Rehabilitation} of {Alzheimer} {Patients}},
	isbn = {978-3-319-92043-6},
	doi = {10.1007/978-3-319-92043-6_36},
	abstract = {This article presents an innovative interactive tool that has been designed and developed in the context of the preventive treatment of Alzheimer’s disease. This tool allows simulating different cooking tasks that the patient has to perform with the computer mouse. The virtual environment is visualized on a simple computer screen. Gradual assistance is provided to the patient so that he/she trains and learns to perform the tasks requested. In order for the training to be relevant and effective, no errors are allowed by the system.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Interaction}, {Visualization}, and {Analytics}},
	publisher = {Springer International Publishing},
	author = {Richard, Paul and Foloppe, Déborah and Allain, Philippe},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2018},
	keywords = {Activity of daily living, Alzheimer disease, Cognitive rehabilitation, Errorless learning, Virtual environment},
	pages = {426--435},
	file = {Full Text PDF:files/508/Richard et al. - 2018 - A Virtual Kitchen for Cognitive Rehabilitation of Alzheimer Patients.pdf:application/pdf},
}

@inproceedings{wen_fighting_2014,
	address = {Cham},
	title = {Fighting {Technology} {Dumb} {Down}: {Our} {Cognitive} {Capacity} for {Effortful} {AR} {Navigation} {Tools}},
	isbn = {978-3-319-07227-2},
	shorttitle = {Fighting {Technology} {Dumb} {Down}},
	doi = {10.1007/978-3-319-07227-2_50},
	abstract = {By overlaying virtual guidance information directly over the surrounding environment, Augmented Reality (AR) is seen as an easy alternative to maps for pedestrians navigating in unfamiliar urban environments. It is hypothesized, however, that easing navigation tasks would result in weaker cognitive maps, leaving users more vulnerable to becoming lost should their navigation device fail. We describe an outdoor navigation study that highlighted the gap between theoretical expectations and real world testing with navigation tools. We addressed the issues by creating a simulation system for testing navigation tools and report on the results of a study comparing AR with maps. We then extended the system to support simultaneous secondary tasks to assess relative workload. We present this as a way of objectively measuring relative cognitive effort expended on navigation tool use. Our findings are helpful in the design of mobile pedestrian navigation tools seeking to balance navigational efficiency with mental map formation.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Applications} and {Services}},
	publisher = {Springer International Publishing},
	author = {Wen, James and Deneka, Agnes and Helton, William S. and Dünser, Andreas and Billinghurst, Mark},
	editor = {Kurosu, Masaaki},
	year = {2014},
	keywords = {augmented reality, cognitive load, maps, pedestrian navigation, spatial knowledge acquisition, virtual environment},
	pages = {525--536},
	file = {Full Text PDF:files/509/Wen et al. - 2014 - Fighting Technology Dumb Down Our Cognitive Capacity for Effortful AR Navigation Tools.pdf:application/pdf},
}

@inproceedings{becker_haptic_2022,
	address = {Cham},
	title = {Haptic {Guidance} for {Teleoperation}: {Optimizing} {Performance} and {User} {Experience}},
	isbn = {978-3-031-06249-0},
	shorttitle = {Haptic {Guidance} for {Teleoperation}},
	doi = {10.1007/978-3-031-06249-0_15},
	abstract = {Haptic guidance in teleoperation (e.g. of robotic systems) is a pioneering approach to successfully combine automation and human competencies. In the current user study, various forms of haptic guidance were evaluated in terms of user performance and experience. Twenty-six participants completed an obstacle avoidance task and a peg-in-hole task in a virtual environment using a seven DoF force feedback device. Three types of haptic guidance (translational, rotational, combination of both, i.e. 6 DoF) and three guidance forces and torques (stiffnesses) were compared. Moreover, a secondary task paradigm was utilized to explore the effects of additional cognitive load. The results show that haptic guidance significantly improves performance (i.e. completion times, collision forces). Best results were obtained when the guidance forces were set to a medium or high value. Additionally, feelings of control were significantly increased during higher cognitive load conditions when being supported by translational haptic guidance.},
	language = {en},
	booktitle = {Haptics: {Science}, {Technology}, {Applications}},
	publisher = {Springer International Publishing},
	author = {Becker, Leonie and Weber, Bernhard and Bechtel, Nicolai},
	editor = {Seifi, Hasti and Kappers, Astrid M. L. and Schneider, Oliver and Drewing, Knut and Pacchierotti, Claudio and Abbasimoshaei, Alireza and Huisman, Gijs and Kern, Thorsten A.},
	year = {2022},
	keywords = {Haptic guidance, Haptic shared control, Teleoperation, Virtual fixtures, Virtual reality},
	pages = {129--137},
	file = {Full Text PDF:files/510/Becker et al. - 2022 - Haptic Guidance for Teleoperation Optimizing Performance and User Experience.pdf:application/pdf},
}

@inproceedings{kade_low-cost_2016,
	address = {Cham},
	title = {Low-{Cost} {Mixed} {Reality} {Simulator} for {Industrial} {Vehicle} {Environments}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_57},
	abstract = {High-end industrial vehicle simulators are generally expensive and aim at providing a high level of realism. The access to such simulators is often a limited resource to researchers and developers who find themselves using a PC-based simulator instead. We challenge this approach by introducing a low-cost mixed reality simulator for industrial vehicles that allows to test new vehicle control concepts and design ideas in a rapid prototyping manner. Our simulator prototype consists of a head-mounted projection display, a CAVE-like room covered with a retro-reflective cloth and a rotatable chair with controls to steer an industrial vehicle. The created digital environment represents an obstacle course for an excavator and can be controlled by a joystick, a keyboard and can be explored by natural head movements.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Kade, Daniel and Wallmyr, Markus and Holstein, Tobias and Lindell, Rikard and Ürey, Hakan and Özcan, Oğuzhan},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {Head-mounted projection display, Industrial vehicle simulator, Mixed reality},
	pages = {597--608},
	file = {Full Text PDF:files/511/Kade et al. - 2016 - Low-Cost Mixed Reality Simulator for Industrial Vehicle Environments.pdf:application/pdf},
}

@inproceedings{adiani_design_2019,
	address = {Cham},
	title = {Design of a {Novel} {Web} {Utility} that {Provides} {Multi}-lingual {Word} {Definitions} for {Child} {E}-{Book} {Applications}},
	isbn = {978-3-030-21817-1},
	doi = {10.1007/978-3-030-21817-1_1},
	abstract = {The use of mobile computing devices to gain access to the Internet and to interact with a range of applications has become ubiquitous, impacting on education, entertainment, healthcare, and many other domains. Engaging applications such as e-books used by children and their parents or educators have also become increasingly common, especially in the context of childhood education. An e-book that presents the reader with challenging words has the potential to improve and expand vocabulary. However, the seamless combination of methods for definition-retrieval, word sense disambiguation, and multi-lingual support are not currently available in a simple tool for the specific application of children’s e-book reading. In this work, we present WordWeaver, an open-source tool for use in child reading scenarios where context-sensitive, multi-lingual definitions of unfamiliar words are determined and provided to the user via a simple web API. Our proof-of-concept design includes support for English, Spanish, and French language definitions. Preliminary results support the feasibility of WordWeaver including excellent levels of usability based on the System Usability Scale (i.e., mean SUS = 87.17). Future work will include extending support to include additional languages, definition selection tailored to individual reading skill level, and the ability to address more complicated cases of word and part-of-speech disambiguation.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Ubiquitous} and {Virtual} {Environments} for {Learning} and {Collaboration}},
	publisher = {Springer International Publishing},
	author = {Adiani, Deeksha and Lewis, Daniel and Serao, Vanessa and Barrett, Kevin and Bennett, Amelia and Hambly, Derick and Shenoda, Martina and West, Samuel and Coulter, Garrett and Shagal, Sultan and Biala, Toheeb and Sarkar, Medha and Wade, Joshua and Sarkar, Nilanjan},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2019},
	keywords = {Context-sensitive, Dictionary, E-book, Multi-lingual, Vocabulary acquisition},
	pages = {3--12},
	file = {Full Text PDF:files/512/Adiani et al. - 2019 - Design of a Novel Web Utility that Provides Multi-lingual Word Definitions for Child E-Book Applicat.pdf:application/pdf},
}

@inproceedings{rao_fernandes_delta_2019,
	address = {Cham},
	title = {\$\${\textbackslash}delta \$\$-logit : {Dynamic} {Difficulty} {Adjustment} {Using} {Few} {Data} {Points}},
	isbn = {978-3-030-34644-7},
	shorttitle = {\$\${\textbackslash}delta \$\$-logit},
	doi = {10.1007/978-3-030-34644-7_13},
	abstract = {Difficulty is a fundamental factor of enjoyment and motivation in video games. Thus, many video games use Dynamic Difficulty Adjustment systems to provide players with an optimal level of challenge. However, many of these systems are either game specific, limited to a specific range of difficulties, or require much more data than one can track during a short play session. In this paper, we introduce the \$\${\textbackslash}delta \$\$-logit algorithm. It can be used on many game types, allows a developer to set the game’s difficulty to any level, with, in our experiment, a player failure error prediction rate lower than 20\% in less than two minutes of playtime. In order to roughly estimate the difficulty as quickly as possible, \$\${\textbackslash}delta \$\$-logit drives a single metavariable to adjust the game’s difficulty. It starts with a simple +/\$\$-\$\$\$\${\textbackslash}delta \$\$algorithm to gather a few data points and then uses logistic regression to estimate the players failure probability when the smallest required amount of data has been collected. The goal of this paper is to describe \$\${\textbackslash}delta \$\$-logit and estimate its accuracy and convergence speed with a study on 37 participants playing a tank shooter game.},
	language = {en},
	booktitle = {Entertainment {Computing} and {Serious} {Games}},
	publisher = {Springer International Publishing},
	author = {Rao Fernandes, William and Levieux, Guillaume},
	editor = {van der Spek, Erik and Göbel, Stefan and Do, Ellen Yi-Luen and Clua, Esteban and Baalsrud Hauge, Jannicke},
	year = {2019},
	keywords = {Difficulty, Dynamic difficulty adjustment, Game balancing, Motivation, Player modeling, Video games},
	pages = {158--171},
	file = {Full Text PDF:files/513/Rao Fernandes e Levieux - 2019 - \$\$delta \$\$-logit  Dynamic Difficulty Adjustment Using Few Data Points.pdf:application/pdf},
}

@inproceedings{antle_balancing_2011,
	address = {Berlin, Heidelberg},
	title = {Balancing {Act}: {Enabling} {Public} {Engagement} with {Sustainability} {Issues} through a {Multi}-touch {Tabletop} {Collaborative} {Game}},
	isbn = {978-3-642-23771-3},
	shorttitle = {Balancing {Act}},
	doi = {10.1007/978-3-642-23771-3_16},
	abstract = {Despite a long history of using participatory methods to enable public engagement with issues of societal importance, interactive displays have only recently been explored for this purpose. In this paper, we evaluate a tabletop game called Futura, which was designed to engage the public with issues of sustainability. Our design is grounded in prior research on public displays, serious games, and computer supported collaborative learning. We suggest that a role-based, persistent simulation style game implemented on a multi-touch tabletop affords unique opportunities for a walk-up-and-play style of public engagement. We report on a survey-based field study with 90 participants at the 2010 Vancouver Winter Olympics (Canada). The study demonstrated that small groups of people can be immediately engaged, participate collaboratively, and can master basic awareness outcomes around sustainability issues. However, it is difficult to design feedback that disambiguates between individual and group actions, and shows the temporal trajectory of activity.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2011},
	publisher = {Springer},
	author = {Antle, Alissa N. and Tanenbaum, Theresa Jean and Bevans, Allen and Seaborn, Katie and Wang, Sijie},
	editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
	year = {2011},
	keywords = {collaborative learning, digital tabletops, group interaction, interactive surfaces, multi-touch interaction, Public displays, public engagement, public participation, serious games, sharable displays, simulations, social issues, sustainability},
	pages = {194--211},
	file = {Full Text PDF:files/514/Antle et al. - 2011 - Balancing Act Enabling Public Engagement with Sustainability Issues through a Multi-touch Tabletop.pdf:application/pdf},
}

@inproceedings{rouillard_rapid_2014,
	address = {Cham},
	title = {Rapid {Prototyping} for {Mobile} {Serious} {Games}},
	isbn = {978-3-319-07485-6},
	doi = {10.1007/978-3-319-07485-6_20},
	abstract = {Mobile Serious Games are new kind of Serious Games which are running on mobile devices, mainly on Smartphones. With continuously increased power and User Interface facilities, they constitute an alternative to the usual entertainment applications proposed on Smartphones. To design and implement such applications, a methodological assistance and development support are required. In this paper, we present our contribution to rapid prototyping for Mobile Serious Games in which we propose to augment App Inventor for Android framework with a methodological assistance. This proposition is based on a study in which we asked to 116 students to use this framework for the development of mobile applications. The results are presented (thematic domain, targeted users, components used...) and we discuss the relevance of using such a tool to achieve rapid prototyping for mobile Serious Game.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Technology}-{Rich} {Environments} for {Learning} and {Collaboration}},
	publisher = {Springer International Publishing},
	author = {Rouillard, José and Serna, Audrey and David, Bertrand and Chalon, René},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2014},
	keywords = {App Inventor, Human-computer interaction, Mobile learning, Prototyping, Serious Games},
	pages = {194--205},
	file = {Full Text PDF:files/515/Rouillard et al. - 2014 - Rapid Prototyping for Mobile Serious Games.pdf:application/pdf},
}

@inproceedings{werrlich_assembly_2018,
	address = {Cham},
	title = {Assembly {Training}: {Comparing} the {Effects} of {Head}-{Mounted} {Displays} and {Face}-to-{Face} {Training}},
	isbn = {978-3-319-91581-4},
	shorttitle = {Assembly {Training}},
	doi = {10.1007/978-3-319-91581-4_35},
	abstract = {Due to increasing complexity of assembly tasks at manual workplaces, intensive training of new employees is absolutely essential to ensure high process and product quality. Interactive assistive systems are becoming more and more important as they can support workers during manual procedural tasks. New assistive technologies such as Augmented Reality (AR) are introduced to the industrial domain, especially in the automotive industry. AR allows for enriching our real world with additional virtual information. We are observing a trend in using head-mounted displays (HMDs) in order to support new employees during assembly training tasks. This technology claims to improve the efficiency and quality of assembly and maintenance tasks but so far, HMDs have not been scientifically compared against face-to-face training. In this paper, we aim to close this gap in research by comparing HMD instructions to face-to-face training using a real-life engine assembly task. We executed a training-session with a total of 36 participants. Results showed that trainees who performed the assembly training with HMD support made 10\% less picking mistakes, 5\% less assembly mistakes and 60\% caused less rework but they are significantly slower compared to face-to-face training. We further aimed to rate user satisfaction by using the system usability scale (SUS) questionnaire. Results indicated an average SUS of 73,5 which means ‘good’. These and further findings are presented in this paper.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Interaction}, {Navigation}, {Visualization}, {Embodiment}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Werrlich, Stefan and Lorber, Carolin and Nguyen, Phuc-Anh and Yanez, Carlos Emilio Franco and Notni, Gunther},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Augmented Reality, Evaluation, Head-mounted displays, Training},
	pages = {462--476},
	file = {Full Text PDF:files/516/Werrlich et al. - 2018 - Assembly Training Comparing the Effects of Head-Mounted Displays and Face-to-Face Training.pdf:application/pdf},
}

@inproceedings{amigoni_robocup_2013,
	address = {Berlin, Heidelberg},
	title = {{RoboCup} 2012 {Rescue} {Simulation} {League} {Winners}},
	isbn = {978-3-642-39250-4},
	doi = {10.1007/978-3-642-39250-4_3},
	abstract = {Inside the RoboCup Rescue Simulation League, the mission is to use robots to rescue as many victims as possible after a disaster. The research challenge is to let the robots cooperate as a team. This year in total 15 teams from 8 different countries have been active in the competitions. This paper highlights the approaches of the winners of the virtual robot competition, the infrastructure competition, and the agent competition.},
	language = {en},
	booktitle = {{RoboCup} 2012: {Robot} {Soccer} {World} {Cup} {XVI}},
	publisher = {Springer},
	author = {Amigoni, Francesco and Visser, Arnoud and Tsushima, Masatoshi},
	editor = {Chen, Xiaoping and Stone, Peter and Sucar, Luis Enrique and van der Zant, Tijn},
	year = {2013},
	keywords = {Line Segment, Path Planning, Rescue Team, Robot Team, Semantic Place},
	pages = {20--35},
	file = {Full Text PDF:files/537/Amigoni et al. - 2013 - RoboCup 2012 Rescue Simulation League Winners.pdf:application/pdf},
}

@inproceedings{liu_applying_2020,
	address = {Cham},
	title = {Applying {Social} {Gamification} in a {Gamified} {Point} {System}},
	isbn = {978-3-030-50164-8},
	doi = {10.1007/978-3-030-50164-8_10},
	abstract = {User engagement measures whether users find value in a product or service, which is highly correlated with overall profitability. If users choose to spend their time on a particular application or website, it means that they found value in it. This allows businesses to monetize products or services through advertising, subscriptions or sales. To increase user engagement, it is necessary to meet user needs to improve their experience. Recently, gamification has become increasingly popular because it applies game mechanics to non-gaming environment like education and shopping to attract and motivate participants. In this paper, we explore using social gamification in a gamified point system. In particular, we focus on two types of user interaction, namely competitive and non-competitive interaction. In preliminary experiments, we obtained positive results from the experimenters.},
	language = {en},
	booktitle = {{HCI} in {Games}},
	publisher = {Springer International Publishing},
	author = {Liu, Boyang and Tanaka, Jiro},
	editor = {Fang, Xiaowen},
	year = {2020},
	keywords = {Competitive and non-competitive interaction, Gamification, Motivation, Social incentives, User engagement},
	pages = {148--161},
	file = {Full Text PDF:files/539/Liu e Tanaka - 2020 - Applying Social Gamification in a Gamified Point System.pdf:application/pdf},
}

@inproceedings{chen_museum_2020,
	address = {Cham},
	title = {The {Museum} {Guidance} {System} in {Gamification} {Design}},
	isbn = {978-3-030-50732-9},
	doi = {10.1007/978-3-030-50732-9_37},
	abstract = {Gamification is the application of game-oriented design approaches or game-inspired mechanics to otherwise non-game contexts. Mobile guiding system is the design process of information interactions. It is the integration of information design, interaction design, and sensorial design. The e-learning system of mobile guide is able to be loaded gamification concepts and let mobile learning interestingly, diversely, and validly. The problem of the research was if a guided system is combined the concept of gamification design into museum guide services for non-commercial purposes, it also provided the same benefits to the promotion of museum learning and knowledge, integrating mobile devices as navigation media. It would improve more users to participate in a museum and use the guide system actively, and then arise their interest and achievement. The research method was adopted prototyping implementation and user test. The result was to establish a preliminary model for developing a museum mobile guide system of gamification design.},
	language = {en},
	booktitle = {{HCI} {International} 2020 - {Posters}},
	publisher = {Springer International Publishing},
	author = {Chen, Zi-Ru},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2020},
	keywords = {Gamification, Multimedia guided system, Museum learning},
	pages = {278--285},
	file = {Full Text PDF:files/541/Chen - 2020 - The Museum Guidance System in Gamification Design.pdf:application/pdf},
}

@inproceedings{pardos_introducing_2020,
	address = {Cham},
	title = {Introducing an {Edge}-{Native} {Deep} {Learning} {Platform} for {Exergames}},
	isbn = {978-3-030-49186-4},
	doi = {10.1007/978-3-030-49186-4_8},
	abstract = {The recent advancements in the areas of computer vision and deep learning with the development of convolutional neural networks and the profusion of highly accurate general purpose pre-trained models, create new opportunities for the interaction of humans with systems and facilitate the development of advanced features for all types of platforms and applications. Research, consumer and industrial applications increasingly integrate deep learning frameworks into their operational flow, and as a result of the availability of high performance hardware (Computer Boards, GPUs, TPUs) also for individual consumers and home use, this functionality has been moved closer to the end-users, at the edge of the network. In this work, we exploit the aforementioned approaches and tools for the development of an edge-native platform for exergames, which includes innovative gameplay and features for the users. A prototype game was created using the platform that was deployed in the real-world scenario of a rehabilitation center. The proposed approach provides advanced user experience based on the automated, real-time pose and gesture detection, and in parallel maintains low-cost to enable wide adoption in multiple applications across domains and usage scenarios.},
	language = {en},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Pardos, Antonis and Menychtas, Andreas and Maglogiannis, Ilias},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
	year = {2020},
	keywords = {Computer vision, Convolutional neural networks, Deep learning, Edge computing, Exergames, Rehabilitation},
	pages = {88--98},
	file = {Full Text PDF:files/543/Pardos et al. - 2020 - Introducing an Edge-Native Deep Learning Platform for Exergames.pdf:application/pdf},
}

@inproceedings{gieser_comparing_2016,
	address = {Cham},
	title = {Comparing {Objective} and {Subjective} {Metrics} {Between} {Physical} and {Virtual} {Tasks}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_1},
	abstract = {Virtual Reality (VR) is becoming a tool that is more often used in various types of activities, including rehabilitation. However, studies using VR rehabilitation mainly focus on comparing the performances of participants, but not their opinions. In this paper, we present a virtual version of the Box and Blocks Test. We also present the results of a pilot study where participants completed a physical version of the Box and Blocks Test and the virtual version, comparing their scores and opinions. We also compare how the participants viewed the passage of time while performing both versions as a way to see how engaged they were during the task.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Gieser, S. N. and Gentry, Caleb and LePage, James and Makedon, Fillia},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {Box and blocks test, Gamification, Leap motion, Time perception, Upper extremity rehabilitation, Virtual reality},
	pages = {3--13},
	file = {Full Text PDF:files/538/Gieser et al. - 2016 - Comparing Objective and Subjective Metrics Between Physical and Virtual Tasks.pdf:application/pdf},
}

@inproceedings{coelho_body_2014,
	address = {Berlin, Heidelberg},
	title = {Body {Ownership} of {Virtual} {Avatars}: {An} {Affordance} {Approach} of {Telepresence}},
	isbn = {978-3-642-55143-7},
	shorttitle = {Body {Ownership} of {Virtual} {Avatars}},
	doi = {10.1007/978-3-642-55143-7_1},
	abstract = {Virtual environments are an increasing trend in today’s society. In this scope, the avatar is the representation of the user in the virtual world. However, that relationship lacks empirical studies regarding the nature of the interaction between avatars and human beings. For that purpose it was studied how the avatar’s modeled morphology and dynamics affect its control by the user. An experiment was conducted to measure telepresence and ownership on participants who used a Kinect Natural User Interface (NUI). The body ownership of different avatars was assessed through a behavioral parameter, based on the concept of affordances, and a questionnaire of presence. The results show that the feelings of telepresence and ownership seem to be greater when the kinematics and the avatar’s proportions are closer to those of the user.},
	language = {en},
	booktitle = {Innovative and {Creative} {Developments} in {Multimodal} {Interaction} {Systems}},
	publisher = {Springer},
	author = {Coelho, Tiago and de Oliveira, Rita and Cardoso, Tiago and Rybarczyk, Yves},
	editor = {Rybarczyk, Yves and Cardoso, Tiago and Rosas, João and Camarinha-Matos, Luis M.},
	year = {2014},
	keywords = {affordances, Avatar, natural user interface, ownership, telepresence, virtual environment},
	pages = {3--19},
	file = {Full Text PDF:files/540/Coelho et al. - 2014 - Body Ownership of Virtual Avatars An Affordance Approach of Telepresence.pdf:application/pdf},
}

@inproceedings{harman_improved_2017,
	address = {Cham},
	title = {Improved {Memory} {Elicitation} in {Virtual} {Reality}: {New} {Experimental} {Results} and {Insights}},
	isbn = {978-3-319-67684-5},
	shorttitle = {Improved {Memory} {Elicitation} in {Virtual} {Reality}},
	doi = {10.1007/978-3-319-67684-5_9},
	abstract = {Eliciting accurate and complete knowledge from individuals is a non-trivial challenge. In this paper, we present the evaluation of a virtual-world based approach, informed by situated cognition theory, which aims to assist with knowledge elicitation. In this approach, we place users into 3D virtual worlds which represent real-world locations and ask users to describe information related to tasks completed in those locations. Through an empirical A/B evaluation of 62 users, we explore the differences in recall ability and behaviour of those viewing the virtual world via a virtual reality headset and those viewing the virtual world on a monitor. Previous results suggest that the use of a virtual reality headset was able to meaningfully improve memory recall ability within the given scenario. In this study, we adjust experiment protocol to explore the potential confounds of time taken and tool usability. After controlling for these possible confounds, we once again found that those given a virtual reality headset were able to recall more information about the given task than those viewing the virtual world on a monitor.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} - {INTERACT} 2017},
	publisher = {Springer International Publishing},
	author = {Harman, Joel and Brown, Ross and Johnson, Daniel},
	editor = {Bernhaupt, Regina and Dalvi, Girish and Joshi, Anirudha and K. Balkrishan, Devanuj and O'Neill, Jacki and Winckler, Marco},
	year = {2017},
	pages = {128--146},
	file = {Full Text PDF:files/542/Harman et al. - 2017 - Improved Memory Elicitation in Virtual Reality New Experimental Results and Insights.pdf:application/pdf},
}

@inproceedings{zhang_story_2019,
	address = {Cham},
	title = {Story {Envisioning} {Framework} for {Visualized} {Collective} {Storytelling} in {Conversation}},
	isbn = {978-3-030-22660-2},
	doi = {10.1007/978-3-030-22660-2_17},
	abstract = {In this paper, we introduce the method of story envisioning as an efficient approach to collective storytelling in daily conversation and training procedure. The goal of story envisioning is to provide intuitive, concrete and consistent visualization contents as interactive drama for storytellers who are not professional at storytelling. We employ graphic recording to symbolize and visualize the stories written in natural language. We built a partially automated framework TSEiA that allows storytellers to visualize and edit the stories. By traversing through the whole story structure, storytellers are also able to compare and analyze different consequences of branches. Preliminary experiments showed that using visualized interactive drama can help participants have better understanding to the conversation from culture and personality viewpoint. Simultaneously, story envisioning could help the storytellers organize the story structure in a better way and enhance the immersion of their stories. Game platform powered by TSEiA was built to illustrate how story envisioning could be considered as an effective tool for collective storytelling and content generation.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Visual} {Information} and {Knowledge} {Management}},
	publisher = {Springer International Publishing},
	author = {Zhang, Qiang and Sadat Mirzaei, Maryam and Huang, Hung-Hsuan and Nishida, Toyoaki},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2019},
	keywords = {Conversation analysis, Story envisioning, Storytelling},
	pages = {250--261},
	file = {Full Text PDF:files/544/Zhang et al. - 2019 - Story Envisioning Framework for Visualized Collective Storytelling in Conversation.pdf:application/pdf},
}

@inproceedings{jia_multimodal_2020,
	address = {Cham},
	title = {A {Multimodal} {Human}-{Computer} {Interaction} {System} and {Its} {Application} in {Smart} {Learning} {Environments}},
	isbn = {978-3-030-51968-1},
	doi = {10.1007/978-3-030-51968-1_1},
	abstract = {A multimodal human-computer interaction system is composed of the comprehensive usage of various input and output channels. For the information input, apart from the traditional keyboard typing, mouse clicking, screen touching, the latest speech and face recognition technology can be used. For the output, the traditional screen display, the latest speech and facial expression synthesis and gesture generation can be used. After literature review of related works, this paper at first presents such a system, MMISE (Multimodal Interaction System for Education), about its architecture and working mechanism, POOOIIM (Pedagogical Objective Oriented Output, Input and Implementation Mechanism) illustrated with practical examples. Then this paper introduces this system’s pilot applications in the epidemic time of novel coronavirus in 2020.},
	language = {en},
	booktitle = {Blended {Learning}. {Education} in a {Smart} {Learning} {Environment}},
	publisher = {Springer International Publishing},
	author = {Jia, Jiyou and He, Yunfan and Le, Huixiao},
	editor = {Cheung, Simon K. S. and Li, Richard and Phusavat, Kongkiti and Paoprasert, Naraphorn and Kwok, Lam‑For},
	year = {2020},
	keywords = {Intelligent tutoring system, Learning of English as a foreign language, Mathematics instruction, Multimodal human-computer interaction, Smart learning environment},
	pages = {3--14},
	file = {Full Text PDF:files/545/Jia et al. - 2020 - A Multimodal Human-Computer Interaction System and Its Application in Smart Learning Environments.pdf:application/pdf},
}

@inproceedings{fragomeni_training_2015,
	address = {Cham},
	title = {Training {Effectiveness} {Evaluation}: {Call} for {Fire} {Trainer} – {Augmented} {Virtuality} ({CFFT}-{AV})},
	isbn = {978-3-319-21067-4},
	shorttitle = {Training {Effectiveness} {Evaluation}},
	doi = {10.1007/978-3-319-21067-4_27},
	abstract = {As emerging technologies continue to modernize battlefield systems, the use of Mixed Reality (MR) training has been increasingly proposed as a lower cost and more time-effective alternative to live training. However, there has been minimal empirical data to demonstrate the effectiveness of MR type training which leaders require to make informed decisions about training device acquisition. In an effort to assist in the decision making process of future training system acquisition a Training Effectiveness Evaluation (TEE) is being conducted by U.S. Army Research Laboratory (ARL) Human Research and Engineering Directorate (HRED), Simulation and Training Technology Center (STTC) on the Call for Fire Trainer – Augmented Virtuality (CFFT-AV). This paper describes the methodology of the TEE with regard to the effectiveness of AV as a platform within the Call for Fire (CFF) task domain and how AV technologies and methods can impact CFF training.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Fragomeni, Gino and Lackey, Stephanie J. and Champney, Roberto and Salcedo, Julie Nanette and Serge, Stephen},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2015},
	keywords = {Augmented virtuality, Joint forward observer, Simulation-Based training},
	pages = {263--272},
	file = {Full Text PDF:files/566/Fragomeni et al. - 2015 - Training Effectiveness Evaluation Call for Fire Trainer – Augmented Virtuality (CFFT-AV).pdf:application/pdf},
}

@inproceedings{kim_card_2010,
	address = {Berlin, Heidelberg},
	title = {A {Card} {Playing} {Humanoid} for {Understanding} {Socio}-emotional {Interaction}},
	isbn = {978-3-642-15399-0},
	doi = {10.1007/978-3-642-15399-0_2},
	abstract = {This paper describes the groundwork for designing a social and emotional interaction between a human and robot in game-playing. We considered that understanding deception in terms of mind reading plays a key role in realistic interactions for social robots. In order to understand the human mind, the humanoid robot observes nonverbal deception cues through multimodal perception during poker playing which is one of human social activities. Additionally, the humanoid manipulates the real environment which includes not only the game but also people to create a feeling of interacting with life-like machine and drive affective responses in determining the reaction.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2010},
	publisher = {Springer},
	author = {Kim, Min-Gyu and Suzuki, Kenji},
	editor = {Yang, Hyun Seung and Malaka, Rainer and Hoshino, Junichi and Han, Jung Hyun},
	year = {2010},
	keywords = {Deception, Human-robot social interaction, Humanoid playmate, Mind reading},
	pages = {9--19},
	file = {Full Text PDF:files/568/Kim e Suzuki - 2010 - A Card Playing Humanoid for Understanding Socio-emotional Interaction.pdf:application/pdf},
}

@inproceedings{kickmeier-rust_personalized_2010,
	address = {Berlin, Heidelberg},
	title = {Personalized {Support}, {Guidance}, and {Feedback} by {Embedded} {Assessment} and {Reasoning}: {What} {We} {Can} {Learn} from {Educational} {Computer} {Games}},
	isbn = {978-3-642-15231-3},
	shorttitle = {Personalized {Support}, {Guidance}, and {Feedback} by {Embedded} {Assessment} and {Reasoning}},
	doi = {10.1007/978-3-642-15231-3_15},
	abstract = {Software that intelligently interprets the goals and needs of its users on the basis of their behaviors without interrupting the work flow and consequently disturbing concentration and software that can support the users in a personalized, smart, yet unostentatious way is a desirable vision, for sure. One attempt to such support system was Microsoft’s famous paperclip. The underlying logic, unfortunately, was rather simple and the users did not accept the feature very well. This paper introduces a psychologically and formally sound approach to a non-invasive, hidden assessment of very specific needs of the users as well as their competencies and corresponding tailored support and feedback. The approach was developed in the context of adaptive digital educational games and is based on the concepts of Competence-based Knowledge Space Theory as well as that of Problem Spaces. The purpose of this paper is to broaden the concept and elucidate a possible bridge from computer games to regular software tools.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}},
	publisher = {Springer},
	author = {Kickmeier-Rust, Michael D. and Albert, Dietrich},
	editor = {Forbrig, Peter and Paternó, Fabio and Mark Pejtersen, Annelise},
	year = {2010},
	keywords = {Embedded Assessment, Feedback, Micro Adaptation, Support Methods, User Model},
	pages = {142--151},
	file = {Full Text PDF:files/569/Kickmeier-Rust e Albert - 2010 - Personalized Support, Guidance, and Feedback by Embedded Assessment and Reasoning What We Can Learn.pdf:application/pdf},
}

@inproceedings{matthews_trust_2018,
	address = {Cham},
	title = {Trust in {Autonomous} {Systems} for {Threat} {Analysis}: {A} {Simulation} {Methodology}},
	isbn = {978-3-319-91584-5},
	shorttitle = {Trust in {Autonomous} {Systems} for {Threat} {Analysis}},
	doi = {10.1007/978-3-319-91584-5_27},
	abstract = {Human operators will increasingly team with autonomous systems in military and security settings, for example, evaluation and analysis of threats. Determining whether humans are threatening is a particular challenge to which future autonomous systems may contribute. Optimal trust calibration is critical for mission success, but most trust research has addressed conventional automated systems of limited intelligence. This article identifies multiple factors that may influence trust in autonomous systems. Trust may be undermined by various sources of demand and uncertainty. These include the cognitive demands resulting from the complexity and unpredictability of the system, “social” demands resulting from the system’s capacity to function as a team-member, and self-regulative demands associated with perceived threats to personal competence. It is proposed that existing gaps in trust research may be addressed using simulation methodologies. A simulated environment developed by the research team is described. It represents a “town-clearing” task in which the human operator teams with a robot that can be equipped with various sensors, and software for intelligent analysis of sensor data. The functionality of the simulator is illustrated, together with future research directions.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Applications} in {Health}, {Cultural} {Heritage}, and {Industry}},
	publisher = {Springer International Publishing},
	author = {Matthews, Gerald and Panganiban, April Rose and Bailey, Rachel and Lin, Jinchao},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Autonomous systems, Cognitive processes, Simulation, Threat detection, Trust},
	pages = {341--353},
	file = {Full Text PDF:files/572/Matthews et al. - 2018 - Trust in Autonomous Systems for Threat Analysis A Simulation Methodology.pdf:application/pdf},
}

@inproceedings{bekele_step_2013,
	address = {Berlin, Heidelberg},
	title = {A {Step} towards {Adaptive} {Multimodal} {Virtual} {Social} {Interaction} {Platform} for {Children} with {Autism}},
	isbn = {978-3-642-39191-0},
	doi = {10.1007/978-3-642-39191-0_51},
	abstract = {Recent advances in computer and robotic technology are enabling the application of such technology in assisting traditional intervention in developmental disorders such as autism spectrum disorders (ASD). A number of research studies indicate that many children with ASD prefer technology and this preference can be explored to develop systems that may alleviate several challenges of traditional treatment and intervention. The current work proposes to develop an adaptive virtual reality-based social interaction platform for children with ASD. It is hypothesized that endowing a technological system that can detect the feeling and state of the child and adapt its interaction accordingly is of great importance in assisting and individualization of traditional intervention approaches. The proposed system employs sensors such as eye trackers and physiological signal monitors and models the context relevant psychological state of the user from combination of these sensors together with the performance of the participant.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {User} and {Context} {Diversity}},
	publisher = {Springer},
	author = {Bekele, Esubalew and Young, Mary and Zheng, Zhi and Zhang, Lian and Swanson, Amy and Johnston, Rebecca and Davidson, Julie and Warren, Zachary and Sarkar, Nilanjan},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2013},
	keywords = {adaptive interaction, autism intervention, eye tracking, multimodal system, physiological processing, Social interaction, virtual reality},
	pages = {464--473},
	file = {Full Text PDF:files/573/Bekele et al. - 2013 - A Step towards Adaptive Multimodal Virtual Social Interaction Platform for Children with Autism.pdf:application/pdf},
}

@inproceedings{stavroulia_potential_2017,
	address = {Cham},
	title = {On the {Potential} of {Using} {Virtual} {Reality} for {Teacher} {Education}},
	isbn = {978-3-319-58509-3},
	doi = {10.1007/978-3-319-58509-3_15},
	abstract = {Virtual reality technology has the potential to be used in teacher training as it can provide innovative virtual teaching environments, offering teachers the ability to gain in-training feedback and knowledge that can be transferred and applied to real-life situations. As part of an initial investigation into the applicability of using VR for teacher training, two experiments were conducted. The first experiment had to do with teachers’ understanding and detection of students’ possible disorders such as vision disorders. The second experiment had to do the ability of teachers to identify and deal with bullying-related activities among students. The results indicated that through a VR-based role-changing mechanism teachers could enter students’ position and understand their problems, while they experienced incidents that were like real-life incidents making the application a valuable training tool. The overall results of the preliminary investigations in combination with the findings of a related survey, highlight the potential of using VR for implementing real-life tools for teacher professional training. Building on the results of the preliminary experiments, a new application is currently under development aiming to address the lack of practice in teacher training and provide to young but also experienced teachers a VR-based school environment that represents real-life situations and will allow them to be trained, experiment, test their skills, make mistakes and learn from them but without the risk of harming real students.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Novel} {Learning} {Ecosystems}},
	publisher = {Springer International Publishing},
	author = {Stavroulia, Kalliopi-Evangelia and Lanitis, Andreas},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2017},
	keywords = {Head Mounted Display (HMD), Teacher preparation, Teacher training, Virtual classroom, Virtual reality},
	pages = {173--186},
	file = {Full Text PDF:files/575/Stavroulia e Lanitis - 2017 - On the Potential of Using Virtual Reality for Teacher Education.pdf:application/pdf},
}

@inproceedings{armitage_client-side_2008,
	address = {Berlin, Heidelberg},
	title = {Client-{Side} {Adaptive} {Search} {Optimisation} for {Online} {Game} {Server} {Discovery}},
	isbn = {978-3-540-79549-0},
	doi = {10.1007/978-3-540-79549-0_43},
	abstract = {This paper describes a client-side, adaptive search technique to reduce both the time taken to discover playable online First Person Shooter (FPS) game servers and the number of network flows created during game server discovery. Online FPS games usually use a client-server model, with thousands of game servers active at any time. Traditional FPS server discovery probes all available servers over multiple minutes in no particular order, creating thousands of short-lived UDP flows. Probing triggers rapid consumption of longer-lived per-flow state memory in NAT devices between a client and the internet. Using server discovery data from Valve’s Counterstrike:Source and idSoftware’s Wolfenstein Enemy Territory this paper demonstrates that pre-probing a subset of game servers can be used to re-order and optimise the overall probe sequence. Game servers are now probed in approximately ascending latency, expediting the location of playable servers. Discovery of playable servers may now take less than 20\% of the time and network traffic of conventional game server discovery. The worst case converges to (without exceeding) the behaviour of conventional game server discovery.},
	language = {en},
	booktitle = {{NETWORKING} 2008 {Ad} {Hoc} and {Sensor} {Networks}, {Wireless} {Networks}, {Next} {Generation} {Internet}},
	publisher = {Springer},
	author = {Armitage, Grenville},
	editor = {Das, Amitabha and Pung, Hung Keng and Lee, Francis Bu Sung and Wong, Lawrence Wai Choong},
	year = {2008},
	keywords = {latency estimation, search optimisation, Server discovery},
	pages = {494--505},
	file = {Full Text PDF:files/597/Armitage - 2008 - Client-Side Adaptive Search Optimisation for Online Game Server Discovery.pdf:application/pdf},
}

@inproceedings{yang_egocentric_2018,
	address = {Cham},
	title = {Egocentric {Distance} {Perception} {Control} for {Direct} {Manipulation} {Interaction} on {HMD} {Platform}},
	isbn = {978-3-319-92285-0},
	doi = {10.1007/978-3-319-92285-0_20},
	abstract = {At present, the simulation technique of sensory stimulation of virtual reality technology has limitations to reproduce imperfectly the sensory feedback learned while human grows. Therefore, the inconsistency of the sensory feedback is obstructing the popularization of the virtual reality service. This study deals with visual distance perception or sensory inconsistency experienced by users when using HMD. In addition, the feature of interaction that we study in this study is to restore objects in real space to virtual space and to manipulate the connected object in real time by hand. We carried out near-body-space interaction experiments to analyze the performance characteristics of subjects with various environments consisting of HMD-oriented hardware and a software focused on the 3D game engine, which is representing the VR market. We are investigating algorithms that can minimize the error of matching task in a virtual environment. We found the world scale parameter in terms of geometric field of view as a significant control factor. We report the results of applying the current research results to an indoor VR theme park application. The user can experience the visual feedback of the precisely associated virtual space while manipulating the physical object and receiving simultaneously the haptic feedback that interacts with other objects in the real space.},
	language = {en},
	booktitle = {{HCI} {International} 2018 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Yang, Ungyeon and Kim, Nam-Gyu and Kim, Ki-Hong},
	editor = {Stephanidis, Constantine},
	year = {2018},
	keywords = {Distance perception, Head mounted display, Virtual reality},
	pages = {137--142},
	file = {Full Text PDF:files/599/Yang et al. - 2018 - Egocentric Distance Perception Control for Direct Manipulation Interaction on HMD Platform.pdf:application/pdf},
}

@inproceedings{re_natural_2014,
	address = {Cham},
	title = {A {Natural} {User} {Interface} for {Navigating} in {Organized} {3D} {Virtual} {Contents}},
	isbn = {978-3-319-07458-0},
	doi = {10.1007/978-3-319-07458-0_10},
	abstract = {The research activity presented in this paper aim at extending the traditional planar navigation, which is adopted by many desktop applications for searching information, to an experience in a Virtual Reality (VR) environment. In particular, the work proposes a system that allows the user to navigate in virtual environments, in which the objects are spatially organized and sorted. The visualization of virtual object has been designed and an interaction method, based on gestures, has been proposed to trigger the navigation in the environment. The article describes the design and the development of the system, by starting from some considerations about the intuitiveness and naturalness required for a three-dimensional navigation. In addition, an initial case study has been carried out and consists in using the system in a virtual 3D catalogue of furniture.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Designing} and {Developing} {Virtual} and {Augmented} {Environments}},
	publisher = {Springer International Publishing},
	author = {Re, Guido Maria and Bordegoni, Monica},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2014},
	keywords = {Gestures, Natural User Interfaces, Navigation, Virtual Catalogue, Virtual Reality},
	pages = {93--104},
	file = {Full Text PDF:files/603/Re e Bordegoni - 2014 - A Natural User Interface for Navigating in Organized 3D Virtual Contents.pdf:application/pdf},
}

@inproceedings{flynn_maritime_2011,
	address = {Berlin, Heidelberg},
	title = {Maritime {City}: {Using} {Games} {Technology} to {Train} {Social} {Workers} – {Some} {Initial} {Results}},
	isbn = {978-3-642-24500-8},
	shorttitle = {Maritime {City}},
	doi = {10.1007/978-3-642-24500-8_55},
	abstract = {Maritime City is an educational tool for training Healthcare professionals, currently targeted towards Social Workers. It has its roots in the serious games area and is developed in a commercial game engine. This paper presents some initial results from testing the game with in-practice Social Workers in the context of child protection, with the aim to show equivalence between this approach and more traditional methods. The testing found that the game was realistic (apart from in some areas, which has provided new directions for the research) and also showed that there are many areas of the game experience that impact on the learning in ways similar to that of traditional role-play.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2011},
	publisher = {Springer},
	author = {Flynn, Ryan and McKinnon, Lachlan and Bacon, Elizabeth and Webb, Janet},
	editor = {Anacleto, Junia Coutinho and Fels, Sidney and Graham, Nicholas and Kapralos, Bill and Saif El-Nasr, Magy and Stanley, Kevin},
	year = {2011},
	keywords = {Games technology, health, serious game, social work},
	pages = {415--418},
	file = {Full Text PDF:files/606/Flynn et al. - 2011 - Maritime City Using Games Technology to Train Social Workers – Some Initial Results.pdf:application/pdf},
}

@inproceedings{boonbrahm_interactive_2016,
	address = {Cham},
	title = {Interactive {Augmented} {Reality}: {A} {New} {Approach} for {Collaborative} {Learning}},
	isbn = {978-3-319-39483-1},
	shorttitle = {Interactive {Augmented} {Reality}},
	doi = {10.1007/978-3-319-39483-1_11},
	abstract = {This study aims at using new technology, interactive augmented reality (AR), to establish collaborative learning. Interactive augmented reality means that virtual objects can be interacted and displayed in real world which made them easy to understand or easy to work with. In this paper, we have developed a system to support collaborative work in which users can be in different locations and use interactive augmented reality technology to help them doing collaborative work which in this case is completing a 3D jigsaw puzzle. For the interactive AR system, Unity 3D game engine was used on a Vuforia platform. Apples’ iPads were chosen as the device to perform the task due to ease of use, good camera quality and good display for AR applications. The results show that the two users can collaborate by helping each other to get the job done in AR environment.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Boonbrahm, Poonpong and Kaewrat, Charlee and Boonbrahm, Salin},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2016},
	keywords = {Augmented reality, Collaboration learning, Interactive},
	pages = {115--124},
	file = {Full Text PDF:files/608/Boonbrahm et al. - 2016 - Interactive Augmented Reality A New Approach for Collaborative Learning.pdf:application/pdf},
}

@inproceedings{walter_1st_2011,
	address = {Berlin, Heidelberg},
	title = {1st {Workshop} on {Game} {Development} and {Model}-{Driven} {Software} {Development}},
	isbn = {978-3-642-24500-8},
	doi = {10.1007/978-3-642-24500-8_66},
	abstract = {This workshop brings together game development professionals and experts of model-driven software development (MDSD). MDSD can improve the way software is developed by providing a higher level of abstraction and model-to-model transformation respectively model-to-text generation by automated tools. With this workshop, we want to identify how game development can benefit from MDSD. Thereby, the workshop focuses on a strong collaboration of both expert groups to determine the chances, challenges, and boundaries of introducing MDSD to the game domain.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2011},
	publisher = {Springer},
	author = {Walter, Robert and Masuch, Maic and Merks, Ed},
	editor = {Anacleto, Junia Coutinho and Fels, Sidney and Graham, Nicholas and Kapralos, Bill and Saif El-Nasr, Magy and Stanley, Kevin},
	year = {2011},
	keywords = {Game Development, Model-Driven Game Development, Model-Driven Software Development},
	pages = {460--463},
	file = {Full Text PDF:files/611/Walter et al. - 2011 - 1st Workshop on Game Development and Model-Driven Software Development.pdf:application/pdf},
}

@inproceedings{el-nasr_body_2011,
	address = {Berlin, Heidelberg},
	title = {Body {Buddies}: {Social} {Signaling} through {Puppeteering}},
	isbn = {978-3-642-22024-1},
	shorttitle = {Body {Buddies}},
	doi = {10.1007/978-3-642-22024-1_31},
	abstract = {While virtual worlds have evolved to provide a good medium for social communication, they are very primitive in their social and affective communication design. The social communication methods within these worlds have progressed from early text-based social worlds, e.g. MUDS (multi-user dungeons) to 3D graphical interfaces with avatar control, such as Second Life. Current communication methods include triggering gestures by typed commands, and/or selecting a gesture by name through the user interface. There are no agreed-upon standards for organizing such gestures or interfaces. In this paper, we address this problem by discussing a Unity-based avatar pupeteering prototype we developed called Body Buddies. Body Buddies sits on top of the communication program Skype, and provides additional modalities for social signaling through avatar pupeteering. Additionally, we discuss results from an exploratory study we conducted to investigate how people use the interface. We also outline steps to continuously develop and evolve Body Buddies.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality} - {Systems} and {Applications}},
	publisher = {Springer},
	author = {El-Nasr, Magy Seif and Isbister, Katherine and Ventrella, Jeffery and Aghabeigi, Bardia and Hash, Chelsea and Erfani, Mona and Morie, Jacquelyn and Bishko, Leslie},
	editor = {Shumaker, Randall},
	year = {2011},
	keywords = {avatar design, avatar nonverbal communication, avatar pupeteering, CVE (Collaborative Virtual Environment), social communication with avatars},
	pages = {279--288},
	file = {Full Text PDF:files/614/El-Nasr et al. - 2011 - Body Buddies Social Signaling through Puppeteering.pdf:application/pdf},
}

@inproceedings{hurst_predicting_2015,
	address = {Cham},
	title = {Predicting and {Visualising} {City} {Noise} {Levels} to {Support} {Tinnitus} {Sufferers}},
	isbn = {978-3-319-21969-1},
	doi = {10.1007/978-3-319-21969-1_53},
	abstract = {On a daily basis, urban residents are unconsciously exposed to hazardous noise levels. This has a detrimental effect on the ear-drum, with symptoms often not apparent till later in life. The impact of harmful noises levels has a damaging impact on wellbeing. It is estimated that 10 million people suffer from damaged hearing in the UK alone, with 6.4 million of retirement age or above. With this number expected to increase significantly by 2031, the demand and cost for healthcare providers is expected to intensify. Tinnitus affects about 10 percent of the UK population, with the condition ranging from mild to severe. The effects can have psychological impact on the patient. Often communication becomes difficult, and the sufferer may also be unable to use a hearing aid due to buzzing, ringing or monotonous sounds in the ear. Action on Hearing Loss states that sufferers of hearing related illnesses are more likely to withdraw from social activities. Tinnitus sufferers are known to avoid noisy environments and busy urban areas, as exposure to excessive noise levels exacerbates the symptoms. In this paper, an approach for evaluating and predicting urban noise levels is put forward. The system performs a data classification process to identify and predict harmful noise areas at diverse periods. The goal is to provide Tinnitus sufferers with a real-time tool, which can be used as a guide to find quieter routes to work; identify harmful areas to avoid or predict when noise levels on certain roads will be dangerous to the ear-drum. Our system also performs a visualisation function, which overlays real-time noise levels onto an interactive 3D map.},
	language = {en},
	booktitle = {Image and {Graphics}},
	publisher = {Springer International Publishing},
	author = {Hurst, William and Davis, Graham and El Rhalibi, Abdennour and Tully, David and Pan, Zhigeng},
	editor = {Zhang, Yu-Jin},
	year = {2015},
	keywords = {Data classification, Hazardous noise levels, Hearing loss, Prediction, Real-Time, Tinnitus, Visualisation},
	pages = {583--598},
	file = {Full Text PDF:files/637/Hurst et al. - 2015 - Predicting and Visualising City Noise Levels to Support Tinnitus Sufferers.pdf:application/pdf},
}

@inproceedings{foloppe_new_2019,
	address = {Cham},
	title = {A {New} {Motion}-{Based} {Tool} for {Occupation} and {Monitoring} of {Residents} in {Nursing} {Homes}},
	isbn = {978-3-030-22649-7},
	doi = {10.1007/978-3-030-22649-7_37},
	abstract = {Population ageing bring new challenges in healthcare and has raised issues concerning innovative solutions to optimize the management of elderly. As recommended, new interactive tools must be accessible to users, acceptable, easy to use, motivating and useful for both residents and staff. Virtual Reality is a good candidate to fulfill these specifications. Based on our expertise in Human Computer Interaction and Neuropsychology of ageing, we are developing a platform to offer interactive activities adapted to very-old and dependent people living in nursing homes. It is based on the use of a low-cost markerless RGB-D sensor (AstraTM, Orbbec) to track user body motion. Implemented activities were designed to involve various cognitive abilities, such as sorting game, search game, ball game. In addition, a module records several biomechanical data and generates reports for caregivers. This paper aims to discuss the special needs of research context and to present the designed interaction platform.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Information} in {Intelligent} {Systems}},
	publisher = {Springer International Publishing},
	author = {Foloppe, Déborah Alexandra and Richard, Paul and Allain, Philippe and Calenda, Alphonse},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2019},
	keywords = {Application software, Dependent aged resident, Health monitoring, Physical activity, RGB-D sensor},
	pages = {469--481},
	file = {Full Text PDF:files/641/Foloppe et al. - 2019 - A New Motion-Based Tool for Occupation and Monitoring of Residents in Nursing Homes.pdf:application/pdf},
}

@inproceedings{cone_cyber_2006,
	address = {Boston, MA},
	title = {Cyber {Security} {Training} and {Awareness} {Through} {Game} {Play}},
	isbn = {978-0-387-33406-6},
	doi = {10.1007/0-387-33406-8_37},
	abstract = {Although many of the concepts included in staff cyber-security awareness training are universal, such training often must be tailored to address the policies and requirements of a particular organization. In addition, many forms of training fail because they are rote and do not require users to think about and apply security concepts. A flexible, highly interactive video game, CyberCIEGE, is described as a security awareness tool that can support organizational security training objectives while engaging typical users in an engaging security adventure.},
	language = {en},
	booktitle = {Security and {Privacy} in {Dynamic} {Environments}},
	publisher = {Springer US},
	author = {Cone, Benjamin D. and Thompson, Michael F. and Irvine, Cynthia E. and Nguyen, Thuy D.},
	editor = {Fischer-Hübner, Simone and Rannenberg, Kai and Yngström, Louise and Lindskog, Stefan},
	year = {2006},
	keywords = {Computer Security, Information Assurance, Information System Security, Malicious Software, Naval Postgraduate School},
	pages = {431--436},
	file = {Full Text PDF:files/567/Cone et al. - 2006 - Cyber Security Training and Awareness Through Game Play.pdf:application/pdf},
}

@inproceedings{boonbrahm_using_2017,
	address = {Cham},
	title = {Using {Augmented} {Reality} {Interactive} {System} to {Support} {Digital} {Electronics} {Learning}},
	isbn = {978-3-319-58515-4},
	doi = {10.1007/978-3-319-58515-4_1},
	abstract = {The fundamental theories and concepts of digital electronics which usually being taught at entry level electronics courses is designed to help the students develop solid underlying knowledge of how computer works. The topics usually covered Boolean algebra, Logic Gates and etc. Beside Theory, which are being taught in the lectures, practical skill is required by doing the experiments. In the past the digital electronics laboratory is needed for this purpose, but recently, it was replaced by the software that can simulate the digital circuits. The advantage of this software is that, the students can study the behavior of a systems without building them which save a lot of time and expense. The drawback of these simulators is that, it lacks real charisma in performing the experiments and difficult to interpret. In this research, we have applied interactive marker-based augmented reality (AR) to solve these problems. The advantage of using this marker based techniques in this research is that we can implant the functions of each logic gates in the marker and program these markers to interact with others just like in the real logic gates laboratory. The other advantage is that, markers can be in any shape or have any pictures or symbol on it which make user recognize them easily. In this research, we have created 9 markers which represent 7 basic Logic gates i.e. AND gate, OR gate, NOT gate, NAND gate, NOR gate, XOR gate and XNOR gate. Since we can implant the functions of each logic gates in the marker, the output from these markers will represent the function of each logic gate. By adding them together, the output will be the same as adding the logic gates together. The advantage of using this marker based AR techniques is that we can construct the digital circuit using these markers and the output will be just the same as using the real logic gates, except there is no need for power supply and can be done anywhere and anytime.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Technology} in {Education}},
	publisher = {Springer International Publishing},
	author = {Boonbrahm, Poonpong and Kaewrat, Charlee and Boonbrahm, Salin},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2017},
	keywords = {Augmented reality, Boolean algebra, Interactive, Logic gate, Marker},
	pages = {3--11},
	file = {Full Text PDF:files/570/Boonbrahm et al. - 2017 - Using Augmented Reality Interactive System to Support Digital Electronics Learning.pdf:application/pdf},
}

@inproceedings{de_melo_version_2014,
	address = {Cham},
	title = {Version {Control} {System} {Gamification}: {A} {Proposal} to {Encourage} the {Engagement} of {Developers} to {Collaborate} in {Software} {Projects}},
	isbn = {978-3-319-07632-4},
	shorttitle = {Version {Control} {System} {Gamification}},
	doi = {10.1007/978-3-319-07632-4_52},
	abstract = {This paper proposes to use gamification for recognition of software developers’ collaboration and commitment. In order to improve productivity, the paper also evaluates the users’ engagement in a software development project. The idea is to use the information extracted from source repositories where developers realize their commits. A tool proposes ranking via news feed that will extract information from the source repository by using software engineering metrics, such as McCabe’s cyclomatic complexity, in order to build a ranking system, which highlights and rewards the most active developers. The ultimate goal is to determine whether the use of gamification encourages collaboration and commitment of all involved in software development projects.},
	language = {en},
	booktitle = {Social {Computing} and {Social} {Media}},
	publisher = {Springer International Publishing},
	author = {de Melo, Alexandre Altair and Hinz, Mauro and Scheibel, Glaucio and Diacui Medeiros Berkenbrock, Carla and Gasparini, Isabela and Baldo, Fabiano},
	editor = {Meiselwitz, Gabriele},
	year = {2014},
	keywords = {collaboration, gamification, interface, software engineering, version control},
	pages = {550--558},
	file = {Full Text PDF:files/571/de Melo et al. - 2014 - Version Control System Gamification A Proposal to Encourage the Engagement of Developers to Collabo.pdf:application/pdf},
}

@inproceedings{schmitz_spheres_2015,
	address = {Cham},
	title = {Spheres of {Play}: {Designing} {Games} and {Interfaces} for {Media} {Architectures}},
	isbn = {978-3-319-24589-8},
	shorttitle = {Spheres of {Play}},
	doi = {10.1007/978-3-319-24589-8_45},
	abstract = {The paper describes a game-based interaction scenario around an existing media architecture, developed to integrate aesthetic, social, and technological dynamics. On screen, the game unfolded as users moved across a globe, using a spherical input device to direct their avatars across a dynamic world of obstacles. Recalling the singularity and site-specificity of a performative intervention, the multidisciplinary project is part of a larger research effort that explores the use of media facades as an infrastructural core of complex interfaces for multiple forms of engagement and the co-creation of transmedial scenarios.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Schmitz, Michael and Scholl, Dominik and Saraceni, Julian and Klein, Pascal and Blaser, Carsten and Olmeda, Jorge and Zehle, Soenke and Miede, André},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {Game design, Media facade, Urban HCI, User interfaces},
	pages = {490--495},
	file = {Full Text PDF:files/574/Schmitz et al. - 2015 - Spheres of Play Designing Games and Interfaces for Media Architectures.pdf:application/pdf},
}

@inproceedings{nisi_yasmines_2015,
	address = {Cham},
	title = {Yasmine’s {Adventures}: {An} {Interactive} {Urban} {Experience} {Exploring} the {Sociocultural} {Potential} of {Digital} {Entertainment}},
	isbn = {978-3-319-24589-8},
	shorttitle = {Yasmine’s {Adventures}},
	doi = {10.1007/978-3-319-24589-8_26},
	abstract = {Urban computing systems impact quality of life in densely populated areas. With the widespread availability of wireless networks and portable devices, urban areas are fast becoming a hybrid of the physical environment and the digital datasphere. This paper describes Yasmine’s Adventures, a location aware storytelling platform that leverages on urban computing strategies to create an interactive walk through the Mehringplatz area, surrounding the Jewish Museum in Berlin. Yasmine’s Adventures (YA) is a mobile application that delivers a sequence of animations clips tailored specifically to the Mehringplatz neighbourhood. The story follows an adventurous local girl as she walks home alone, visiting local landmarks. Yasmine’s perceptions of the landmarks, identified by community members in an earlier workshop, reflect the real concerns of the community. This interactive experience was created to engage visitors of the Jewish Museum to explore the relatively neglected streets of the area in which the museum is situated.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Nisi, Valentina and Dionisio, Mara and Hanna, Julian and Ferreira, Luis and Nunes, Nuno},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {Digital storytelling, Interactive narrative, Location aware virtual reality, Mobile socially driven entertainment, Urban computing},
	pages = {343--356},
	file = {Full Text PDF:files/576/Nisi et al. - 2015 - Yasmine’s Adventures An Interactive Urban Experience Exploring the Sociocultural Potential of Digit.pdf:application/pdf},
}

@inproceedings{wade_virtual_2015,
	address = {Cham},
	title = {A {Virtual} {Reality} {Driving} {Environment} for {Training} {Safe} {Gaze} {Patterns}: {Application} in {Individuals} with {ASD}},
	isbn = {978-3-319-20684-4},
	shorttitle = {A {Virtual} {Reality} {Driving} {Environment} for {Training} {Safe} {Gaze} {Patterns}},
	doi = {10.1007/978-3-319-20684-4_66},
	abstract = {It has been well established that adolescents with Autism Spectrum Disorders (ASD) present social and behavioral characteristics that differ significantly from those of their peers without ASD. A growing number of recent studies have begun to look closely at automobile operation characteristics in individuals diagnosed with ASD. Some of this work has suggested that certain driving behaviors demonstrated by those with ASD may pose significant safety concerns to both themselves and other drivers. Expanding on previous work, we designed and tested a gaze-contingent driving intervention system in which drivers were required to not only perform well, but also to look at key regions of interest in the environment such as traffic lights, stop signs, pedestrians and side-view mirrors. We present preliminary results from a study comparing performance outcomes and eye gaze patterns in a group using the gaze-contingent system and a group using a gaze-insensitive, performance-based system.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Access} to {Learning}, {Health} and {Well}-{Being}},
	publisher = {Springer International Publishing},
	author = {Wade, Joshua and Bian, Dayi and Fan, Jing and Zhang, Lian and Swanson, Amy and Sarkar, Medha and Weitlauf, Amy and Warren, Zachary and Sarkar, Nilanjan},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2015},
	keywords = {Autism intervention, Eye gaze, Virtual reality},
	pages = {689--697},
	file = {Full Text PDF:files/598/Wade et al. - 2015 - A Virtual Reality Driving Environment for Training Safe Gaze Patterns Application in Individuals wi.pdf:application/pdf},
}

@inproceedings{zhang_research_2018,
	address = {Cham},
	title = {Research on {Application} of {Gesture} {Recognition} {Technology} in {Traditional} {Puppet} {Show}},
	isbn = {978-3-319-91803-7},
	doi = {10.1007/978-3-319-91803-7_38},
	abstract = {This article uses gesture recognition technology to study the culture, structure and performing form of traditional Chinese puppet show. It aims at annotating traditional culture by using new media art language. Taking “Puppet” as an example, this article summarizes the characteristics of controlling the marionette under gesture recognition technology as well as the binding point design in both traditional way and new technology by analyzing the characters. Through communication and interviews with the inheritors of the intangible cultural heritage, this paper discusses the new and old performing form and controlling features. This research lowers the threshold of learning puppet show, and puppet show fans only need to use simple operations to perform complicated movements. The cost of learning the puppet show decreases because of HCI. This article annotates the traditional meaning of gesture control under new technology background by comparing the new and old controlling form.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Designing} {Interactions}},
	publisher = {Springer International Publishing},
	author = {Zhang, Mu and Dong, Zhanjun},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2018},
	keywords = {Gesture recognition, New media art, Puppet, Puppet show, Traditional culture},
	pages = {498--512},
	file = {Full Text PDF:files/601/Zhang e Dong - 2018 - Research on Application of Gesture Recognition Technology in Traditional Puppet Show.pdf:application/pdf},
}

@inproceedings{abrar_augmented_2019,
	address = {Cham},
	title = {Augmented {Reality} in {Education}: {A} {Study} on {Preschool} {Children}, {Parents}, and {Teachers} in {Bangladesh}},
	isbn = {978-3-030-21565-1},
	shorttitle = {Augmented {Reality} in {Education}},
	doi = {10.1007/978-3-030-21565-1_14},
	abstract = {Augmented reality (AR) is a technology that is being used in various aspects of life, including education. Many studies have been performed to investigate the effectiveness of using AR in educational settings. The purpose of this study is to investigate the effectiveness of AR in teaching preschool children in a developing country such as Bangladesh. To conduct the study, we have developed two AR-based apps for Android using marker-based tracking techniques. We have run our study in a classroom of a school in Bangladesh where 25 students, 13 parents, and three teachers voluntarily participated. We taught students using our AR apps and evaluated their learning improvements through pre- and post-test results. The results show at least 30\% learning improvement. We have observed children’s reaction and engagement, and surveyed parents and teachers for acceptance of such technology and suggestions for improvement. Our preliminary study finds that AR can be useful for preschool students learning in a developing country such as Bangladesh. Through our study, we also identify a list of requirements for designing and developing an AR app for education.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Abrar, Mohammad Fahim and Islam, Md. Rakibul and Hossain, Md. Sabir and Islam, Mohammad Mainul and Kabir, Muhammad Ashad},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {AR in education, Augmented reality, Bangladesh, Early childhood education, Learning technology, Preschool teaching},
	pages = {217--229},
	file = {Full Text PDF:files/604/Abrar et al. - 2019 - Augmented Reality in Education A Study on Preschool Children, Parents, and Teachers in Bangladesh.pdf:application/pdf},
}

@inproceedings{sarkar_data-driven_2018,
	address = {Cham},
	title = {A {Data}-{Driven} {Mobile} {Application} for {Efficient}, {Engaging}, and {Accurate} {Screening} of {ASD} in {Toddlers}},
	isbn = {978-3-319-92049-8},
	doi = {10.1007/978-3-319-92049-8_41},
	abstract = {Early detection of Autism Spectrum Disorder (ASD) followed by targeted intervention has been shown to yield meaningful improvements in outcomes for individuals with ASD. However, despite the potential to curtail developmental delays, constrained clinical resources and barriers to access for some populations prevent many families from obtaining these services. In response, we have developed a tablet-based ASD screening tool called Autoscreen that uses machine learning methods and a data-driven design with the ultimate goal of efficiently triaging toddlers with ASD concerns based on an engaging and non-technical administration procedure. The current paper describes the design of Autoscreen as well as a pilot evaluation to assess the feasibility of the novel approach. Preliminary results suggest the potential for robust risk classification (i.e., F1 score = 0.94), adequate levels of usability based on the System Usability Scale (M = 87.19, 100 point scale), and adequate levels of acceptability based on a novel instrument called ALFA-Q (M = 85.94, 100 point scale). These results, combined with participant feedback, will be used to improve Autoscreen prior to evaluation with the target population of toddlers with concerns for ASD.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Methods}, {Technologies}, and {Users}},
	publisher = {Springer International Publishing},
	author = {Sarkar, Arpan and Wade, Joshua and Swanson, Amy and Weitlauf, Amy and Warren, Zachary and Sarkar, Nilanjan},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2018},
	keywords = {Autism screening, Autism Spectrum Disorder, Machine learning},
	pages = {560--570},
	file = {Full Text PDF:files/607/Sarkar et al. - 2018 - A Data-Driven Mobile Application for Efficient, Engaging, and Accurate Screening of ASD in Toddlers.pdf:application/pdf},
}

@inproceedings{seele_cognitive_2012,
	address = {Berlin, Heidelberg},
	title = {Cognitive {Agents} for {Microscopic} {Traffic} {Simulations} in {Virtual} {Environments}},
	isbn = {978-3-642-33542-6},
	doi = {10.1007/978-3-642-33542-6_27},
	abstract = {Traffic simulations in current open world video games and driving simulators are still limited with respect to the complexity of the behavior of simulated agents. These limitations are typically due to scarce computational resources, but also to the applied methodologies. We suggest adding cognitive components to traffic agents in order to achieve more realistic behavior, such as opting for risky actions or occasionally breaking traffic rules. To achieve this goal, we start by adding a personality profile to each agent, which is based on the “Five Factor Model” from psychology. We test our enhancement on a specific traffic scenario where simplistic behaviors would lead to a complete standstill of traffic. Our results show that the approach resolves critical situations and keeps traffic flowing.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2012},
	publisher = {Springer},
	author = {Seele, Sven and Herpers, Rainer and Bauckhage, Christian},
	editor = {Herrlich, Marc and Malaka, Rainer and Masuch, Maic},
	year = {2012},
	keywords = {cognitive agents, multi-agent systems, traffic simulation, virtual environments},
	pages = {318--325},
	file = {Full Text PDF:files/610/Seele et al. - 2012 - Cognitive Agents for Microscopic Traffic Simulations in Virtual Environments.pdf:application/pdf},
}

@inproceedings{rauterberg_entertainment_2008,
	address = {Boston, MA},
	title = {Entertainment {Computing} in the {Orbit}},
	isbn = {978-0-387-09701-5},
	doi = {10.1007/978-0-387-09701-5_6},
	abstract = {During ultra long space missions (i.e. to Mars), the isolated space environmentaffects a number of physiological, psychosocial and mental processescritically involved in human performance, and it is vital to missions’ success tounderstand the psychological limits. Past experiences in space have shown that themental health of a crew can have a great effect on the success or failure of a mission.Latent and overt stress factors are mental strain, interpersonal problems, andlack of capability to rescue crew members, isolation, monotony, and tedium of lifeaboard an autonomous shuttle. Abstract\_\_ These issues develop very slowly overtime and are very difficult to detect and remedy for observers on the ground. E.g.long-term isolation can lead to sleep deprivation, depression, irritability, anxiety,impaired cognition, and even hostility. Providing astronauts with entertainmentproducts can help to maintain the mental health of the crew. The results of thisproject will deepen the understanding of intra- and inter-individual crew behaviourand related performance, and provide the technical platform for a new type of crew assistance tools based on multi-user computer games.},
	language = {en},
	booktitle = {New {Frontiers} for {Entertainment} {Computing}},
	publisher = {Springer US},
	author = {Rauterberg, Matthias and Neerincx, Mark and Tuyls, Karl and Loon, Jack van},
	editor = {Ciancarini, Paolo and Nakatsu, Ryohei and Rauterberg, Matthias and Roccetti, Marco},
	year = {2008},
	keywords = {astronaut, entertainment, game, mental health, space research},
	pages = {59--70},
	file = {Full Text PDF:files/613/Rauterberg et al. - 2008 - Entertainment Computing in the Orbit.pdf:application/pdf},
}

@inproceedings{park_developing_2016,
	address = {Cham},
	title = {Developing a {Human} {Behavior} {Simulation} {Technology} in the {Real}-{Time} {Manner} {Based} on {BIM}},
	isbn = {978-3-319-40542-1},
	doi = {10.1007/978-3-319-40542-1_84},
	abstract = {This study intends to develop a human behavior simulation system based on building information modeling (BIM) in a real-time manner in the early stages of the architectural design process. This paper introduces the system developed, named SafeBIM, and a design process based on it. The proposed system is an add-on module for Revit Architecture, one of the most popular BIM authoring tools, which supports both architectural design and real-time human behavior simulation of design alternatives. With SafeBIM, architects can perform user behavior simulations at any point in the design process, and the autonomous, human-shaped characters in SafeBIM provide important information related to human behavior by their actions. Additionally, both architects and non-professionals can participate in the design process by using an immersive head-mounted display.},
	language = {en},
	booktitle = {{HCI} {International} 2016 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Park, Changhoon and Lee, Yun Gil},
	editor = {Stephanidis, Constantine},
	year = {2016},
	keywords = {Architectural design, Building information modeling, Human behavior simulation},
	pages = {519--523},
	file = {Full Text PDF:files/616/Park e Lee - 2016 - Developing a Human Behavior Simulation Technology in the Real-Time Manner Based on BIM.pdf:application/pdf},
}

@inproceedings{takagi_voice_2020,
	address = {Cham},
	title = {Voice and {Speech} {Training} {System} for the {Hearing}-{Impaired} {Children} {Using} {Tablet} {Terminal}},
	isbn = {978-3-030-50732-9},
	doi = {10.1007/978-3-030-50732-9_17},
	abstract = {Speech is an important human capability, especially as a learning base for expression and communication. However, it is very difficult for children with hearing disorders to obtain this basic ability while they are young.},
	language = {en},
	booktitle = {{HCI} {International} 2020 - {Posters}},
	publisher = {Springer International Publishing},
	author = {Takagi, Hiroki and Sasaki, Shun and Kaneko, Megumi and Itoh, Takayuki and Sasaki, Kazuo and Ueki, Kazuya and Kuwahara, Meeko},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2020},
	keywords = {Speech training of children with hearing disorders, Tablet terminal application, Voice and speech training system},
	pages = {121--127},
	file = {Full Text PDF:files/639/Takagi et al. - 2020 - Voice and Speech Training System for the Hearing-Impaired Children Using Tablet Terminal.pdf:application/pdf},
}

@inproceedings{salo_backend_2016,
	address = {Cham},
	title = {Backend {Infrastructure} {Supporting} {Audio} {Augmented} {Reality} and {Storytelling}},
	isbn = {978-3-319-40397-7},
	doi = {10.1007/978-3-319-40397-7_31},
	abstract = {Today, museums are looking for new ways to attract and engage audience. These include virtual exhibitions, augmented reality and 3D modelling based applications, and interactive digital storytelling. The target of all these activities is to provide better experiences for audiences that are very familiar with the digital world. In augmented reality (AR) and interactive digital storytelling (IDS) systems, visual presentation has been dominant. In contrast to this trend, we have chosen to concentrate on auditory presentation. A key element for this is a backend service supporting different client applications. This paper discusses our experiences from designing a portable open source based audio digital asset management system (ADAM), which supports interaction with smart phones and tablets containing audio augmented reality and audio story applications. We have successfully implemented ADAM system and evaluated it in the Museum of Technology in Helsinki, Finland.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}: {Applications} and {Services}},
	publisher = {Springer International Publishing},
	author = {Salo, Kari and Giova, Diana and Mikkonen, Tommi},
	editor = {Yamamoto, Sakae},
	year = {2016},
	keywords = {Audio augmented reality, Digital asset management, Metadata, Open source DAM, Soundscape},
	pages = {325--335},
	file = {Full Text PDF:files/643/Salo et al. - 2016 - Backend Infrastructure Supporting Audio Augmented Reality and Storytelling.pdf:application/pdf},
}

@inproceedings{pena_designing_2019,
	address = {Cham},
	title = {Designing {Educational} {Virtual} {Environments} for {Construction} {Safety}: {A} {Case} {Study} in {Contextualizing} {Incident} {Reports} and {Engaging} {Learners}},
	isbn = {978-3-030-21565-1},
	shorttitle = {Designing {Educational} {Virtual} {Environments} for {Construction} {Safety}},
	doi = {10.1007/978-3-030-21565-1_23},
	abstract = {Safety education is important in the construction industry, with many onsite injuries and fatalities. Reviewing incident reports can be valuable in preventing the same mistakes from reoccurring and in reinforcing the concept of designing for construction safety. However, the required information can be difficult for students and non-experts to understand in a meaningful way without instructor facilitation. Recently research has shifted into using 3D virtual environments for safety education, with applications teaching learners how to identify hazards and operating procedures. While there are exploratory results on student engagement and overall learning, there is less focus on how the design influences the learning outcomes. For these reasons we conducted a case study in system design to understand how to effectively contextualize raw incident reports into a meaningful 3D educational experience. From our case study, we present a single-learner educational application with both a desktop computer and VR version. The desktop version was used in development of the application’s design framework and in a controlled study testing how interaction techniques influence learning and behavioral outcomes. The results showed that interaction technique did significantly affect total time spent using the application, but did not affect remembering and understanding. We discuss how lessons learned from the user study were applied to the VR version, what designs revisions needed to be made, and overall usability. Lastly, we summarize the experiences and evaluations from the case study by listing design guidelines for creating educational virtual environments from existing 2D information records.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Peña, Alyssa M. and Ragan, Eric D. and Kang, Julian},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {Construction safety, Educational technology, Virtual environments, Virtual reality},
	pages = {338--354},
	file = {Full Text PDF:files/646/Peña et al. - 2019 - Designing Educational Virtual Environments for Construction Safety A Case Study in Contextualizing.pdf:application/pdf},
}

@inproceedings{cuculo_openfacs_2019,
	address = {Cham},
	title = {{OpenFACS}: {An} {Open} {Source} {FACS}-{Based} {3D} {Face} {Animation} {System}},
	isbn = {978-3-030-34110-7},
	shorttitle = {{OpenFACS}},
	doi = {10.1007/978-3-030-34110-7_20},
	abstract = {We present OpenFACS, an open source FACS-based 3D face animation system. OpenFACS is a software that allows the simulation of realistic facial expressions through the manipulation of specific action units as defined in the Facial Action Coding System. OpenFACS has been developed together with an API which is suitable to generate real-time dynamic facial expressions for a three-dimensional character. It can be easily embedded in existing systems without any prior experience in computer graphics. In this note, we discuss the adopted face model, the implemented architecture and provide additional details of model dynamics. Finally, a validation experiment is proposed to assess the effectiveness of the model.},
	language = {en},
	booktitle = {Image and {Graphics}},
	publisher = {Springer International Publishing},
	author = {Cuculo, Vittorio and D’Amelio, Alessandro},
	editor = {Zhao, Yao and Barnes, Nick and Chen, Baoquan and Westermann, Rüdiger and Kong, Xiangwei and Lin, Chunyu},
	year = {2019},
	keywords = {3D facial animation, Emotion, Facial expression, FACS, HCI},
	pages = {232--242},
	file = {Full Text PDF:files/649/Cuculo e D’Amelio - 2019 - OpenFACS An Open Source FACS-Based 3D Face Animation System.pdf:application/pdf},
}

@inproceedings{manero_xavier_2019,
	address = {Cham},
	title = {Xavier {Electromyographic} {Wheelchair} {Control} and {Virtual} {Training}},
	isbn = {978-3-030-21607-8},
	doi = {10.1007/978-3-030-21607-8_10},
	abstract = {For a variety of individuals, limited hand dexterity yields complications to independently control a power wheelchair. Neurological conditions, including quadriplegia, and traumatic brain injuries can all reduce or eliminate fine motor skills necessary to operate a joystick. For patients with progressive disorders, this acute or chronic progression can affect the hands and limbs at an early stage of the disease. In an effort to extend independence and autonomous mobility, the Xavier system was developed to utilize electromyography signals measured on the temporalis muscles on the face to enable control of a power wheelchair. This study looks to document the human to machine interaction and control scheme as well as discuss the development of a clinical trial protocol to quantify the effectiveness and meaning of the technology on patients. Patient selection in this pilot study was focused on people living with Amyotrophic Lateral Sclerosis (ALS) in conjunction with Mayo Clinic Jacksonville. A review of the clinical protocol and assessment techniques is joined with a discussion about the role of virtual training via designed vehicle simulation to develop muscle isolation and driving mechanics.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Multimodal} {Interaction}},
	publisher = {Springer International Publishing},
	author = {Manero, Albert and Oskarsson, Bjorn and Sparkman, John and Smith, Peter A. and Dombrowski, Matt and Peddinti, Mrudula and Rodriguez, Angel and Vila, Juan and Jones, Brendan},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {ALS, Electromyography, Mobility},
	pages = {133--142},
	file = {Full Text PDF:files/672/Manero et al. - 2019 - Xavier Electromyographic Wheelchair Control and Virtual Training.pdf:application/pdf},
}

@inproceedings{pereira_de_aguiar_educational_2018,
	address = {Cham},
	title = {Educational {Digital} {Games}: {A} {Theoretical} {Framework} {About} {Design} {Models}, {Learning} {Theories} and {User} {Experience}},
	isbn = {978-3-319-91797-9},
	shorttitle = {Educational {Digital} {Games}},
	doi = {10.1007/978-3-319-91797-9_13},
	abstract = {This article presents a study on design models for Educational Digital Games, taking into account learning theories, game design models, game elements and user experience. Through a theoretical framework the research seeks to understand the different perspectives involved in these models, considering the interdisciplinary character of the developer teams and their target users. The aim of the research is to identify similarities, differences, and gaps between the models investigated. Thus, a literature review was conducted and four models for analysis were identified. As a method, both a comparative and a qualitative analysis of these models were used, based on a data analysis spiral combined with an analysis protocol. The analysis sought to identify pedagogical approaches, instructional design aspects and other elements involved in the design process. Game elements and user experience were also considered relevant for the analysis. As a result, this research presents a hybrid model which complements the analyzed models, using the investigated framework as a structural basis to assist developers and content specialists when designing educational digital games for teachers and students, according to the learning objectives and the theme addressed in the game. Finally, this article presents a brief analysis of the results obtained and considers how the present study may contribute to future investigations.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Theory} and {Practice}},
	publisher = {Springer International Publishing},
	author = {Pereira de Aguiar, Michelle and Winn, Brian and Cezarotto, Matheus and Battaiola, André Luiz and Varella Gomes, Péricles},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2018},
	keywords = {Educational Digital Games, Game design models, User experience},
	pages = {165--184},
	file = {Full Text PDF:files/676/Pereira de Aguiar et al. - 2018 - Educational Digital Games A Theoretical Framework About Design Models, Learning Theories and User E.pdf:application/pdf},
}

@inproceedings{wallmyr_360_2018,
	address = {Cham},
	title = {360 {Degree} {Mixed} {Reality} {Environment} to {Evaluate} {Interaction} {Design} for {Industrial} {Vehicles} {Including} {Head}-{Up} and {Head}-{Down} {Displays}},
	isbn = {978-3-319-91584-5},
	doi = {10.1007/978-3-319-91584-5_30},
	abstract = {Designing and testing new information and safety features for industrial vehicles do not need to involve the realization of high-fidelity and expensive simulators. We propose a low-cost mixed reality environment which allows for rapid development and rearrangement of a virtual and physical setup of a simulator for industrial vehicles.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Applications} in {Health}, {Cultural} {Heritage}, and {Industry}},
	publisher = {Springer International Publishing},
	author = {Wallmyr, Markus and Kade, Daniel and Holstein, Tobias},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Design evaluation, Head-up display, Head-worn projection display, Mixed reality, Rapid prototyping environment},
	pages = {377--391},
	file = {Full Text PDF:files/681/Wallmyr et al. - 2018 - 360 Degree Mixed Reality Environment to Evaluate Interaction Design for Industrial Vehicles Includin.pdf:application/pdf},
}

@inproceedings{queiroz_developing_2022,
	address = {Cham},
	title = {Developing a {VR} {Tool} to {Support} {Repeat} {Pattern} {Design} {Learning}},
	isbn = {978-3-031-05675-8},
	doi = {10.1007/978-3-031-05675-8_9},
	abstract = {Virtual Reality (VR) applications have been progressively adopted in design industry and education, and are often associated with increased engagement, creativity, and spatial awareness skills. This study investigates the development and use of a bespoke VR application in textiles and fashion design education, designed to support the teaching and learning of repeat pattern design principles and techniques, transposing the limitations of traditional monitor displays and image editor software. Aiming at identifying potential benefits for students and educators, we have surveyed and observed students who explored the application to visualize their design outputs, applying their pattern designs onto real-size virtual objects and environments. Our findings suggests that VR tools have a positive effect in both learning and design process, allowing students to identify design shortcomings and technical issues, as well as fostering self-evaluation and reflection on their work. Moreover, although findings on spatial awareness are inconclusive, they indicate that the use of the VR application to estimate final dimensions of repeat patterns allows students to identify and correct patterns that have been inaccurately designed.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Novel} {Technological} {Environments}},
	publisher = {Springer International Publishing},
	author = {Queiroz, Francisco and dos Santos Lonsdale, Maria and Henry, Phillip},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2022},
	keywords = {Higher education, Textiles design, Virtual reality, XR and Immersive learning},
	pages = {97--111},
	file = {Full Text PDF:files/705/Queiroz et al. - 2022 - Developing a VR Tool to Support Repeat Pattern Design Learning.pdf:application/pdf},
}

@inproceedings{canossa_play-personas_2009,
	address = {Berlin, Heidelberg},
	title = {Play-{Personas}: {Behaviours} and {Belief} {Systems} in {User}-{Centred} {Game} {Design}},
	isbn = {978-3-642-03658-3},
	shorttitle = {Play-{Personas}},
	doi = {10.1007/978-3-642-03658-3_55},
	abstract = {Game designers attempt to ignite affective, emotional responses from players via engineering game designs to incite definite user experiences. Theories of emotion state that definite emotional responses are individual, and caused by the individual interaction sequence or history. Engendering desired emotions in the audience of traditional audiovisual media is a considerable challenge; however it is potentially even more difficult to achieve the same goal for the audience of interactive entertainment, because a substantial degree of control rests in the hand of the end user rather than the designer. This paper presents a possible solution to the challenge of integrating the user in the design of interactive entertainment such as computer games by employing the "persona" framework introduced by Alan Cooper. This approach is already in use in interaction design. The method can be improved by complementing the traditional narrative description of personas with quantitative, data-oriented models of predicted patterns of user behaviour for a specific computer game Additionally, persona constructs can be applied both as design-oriented metaphors during the development of games, and as analytical lenses to existing games, e.g. for evaluation of patterns of player behaviour.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2009},
	publisher = {Springer},
	author = {Canossa, Alessandro and Drachen, Anders},
	editor = {Gross, Tom and Gulliksen, Jan and Kotzé, Paula and Oestreicher, Lars and Palanque, Philippe and Prates, Raquel Oliveira and Winckler, Marco},
	year = {2009},
	keywords = {emotion, game design, Play persona, user centered design, user experience design},
	pages = {510--523},
	file = {Full Text PDF:files/600/Canossa e Drachen - 2009 - Play-Personas Behaviours and Belief Systems in User-Centred Game Design.pdf:application/pdf},
}

@inproceedings{kjellmo_educational_2013,
	address = {Berlin, Heidelberg},
	title = {Educational: {3D} {Design} for {Mobile} {Augmented} {Reality}},
	isbn = {978-3-642-41106-9},
	shorttitle = {Educational},
	doi = {10.1007/978-3-642-41106-9_30},
	abstract = {Using Unity and the Vuforia platform in the course ”3D design for Mobile Augmented reality” at The Norwegian School of IT. In the course students learned to make efficient, optimized and visually coherent content for Augmented Reality apps for mobile devices.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2013},
	publisher = {Springer},
	author = {Kjellmo, Ivar},
	editor = {Anacleto, Junia C. and Clua, Esteban W. G. and da Silva, Flavio S. Correa and Fels, Sidney and Yang, Hyun S.},
	year = {2013},
	keywords = {3D, Android, Augmented Reality, Educational, IoS, Mobile, Unity, Vuforia},
	pages = {200--203},
	file = {Full Text PDF:files/602/Kjellmo - 2013 - Educational 3D Design for Mobile Augmented Reality.pdf:application/pdf},
}

@inproceedings{lima_innovation_2013,
	address = {Berlin, Heidelberg},
	title = {Innovation in {Learning} – {The} {Use} of {Avatar} for {Sign} {Language}},
	isbn = {978-3-642-39262-7},
	doi = {10.1007/978-3-642-39262-7_49},
	abstract = {This paper presents the steps followed in developing an avatar-interpreter of the Brazilian sign language for deaf (LIBRAS), applied to an electro technical glossary. The research was done in collaboration between The Surface Interaction and Displays Division (DSID) and The National Service for Industrial Apprenticeship (SENAI “Ítalo Bologna”), a reference center in the attendance of people having physical or mental incapability. This work makes use of advanced techniques of motion capture, treatment of images and virtualization, to produce an avatar that mimics a teacher-interpreter of the specific electro technical signs of LIBRAS during the lesson.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Applications} and {Services}},
	publisher = {Springer},
	author = {Lima, Tania and Rocha, Mario Sandro and Santos, Thebano Almeida and Benetti, Angelo and Soares, Evandro and de Oliveira, Helvecio Siqueira},
	editor = {Kurosu, Masaaki},
	year = {2013},
	pages = {428--433},
	file = {Full Text PDF:files/605/Lima et al. - 2013 - Innovation in Learning – The Use of Avatar for Sign Language.pdf:application/pdf},
}

@inproceedings{lange_leveraging_2011,
	address = {Berlin, Heidelberg},
	title = {Leveraging {Unencumbered} {Full} {Body} {Control} of {Animated} {Virtual} {Characters} for {Game}-{Based} {Rehabilitation}},
	isbn = {978-3-642-22024-1},
	doi = {10.1007/978-3-642-22024-1_27},
	abstract = {The use of commercial video games as rehabilitation tools, such as the Nintendo® Wii FitTM, has recently gained much interest in the physical therapy arena. However, physical rehabilitation requires accurate and appropriate tracking and feedback of performance, often not provided by existing commercial console devices or games. This paper describes the development of an application that leverages recent advances in commercial video game technology to provide fullbody control of animated virtual characters with low cost markerless tracking. The aim of this research is to develop and evaluate an interactive game-based rehabilitation tool for balance training of adults with neurological injury. This paper outlines the development and evaluation of a game-based rehabilitation tool using the PrimeSense depth sensing technology, designed to elicit specific therapeutic motions when controlling a virtual avatar in pursuit of in-game goals. A sample of nine adults participated in the initial user testing, providing feedback on the hardware and software prototype.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality} - {Systems} and {Applications}},
	publisher = {Springer},
	author = {Lange, Belinda and Suma, Evan A. and Newman, Brad and Phan, Thai and Chang, Chien-Yen and Rizzo, Albert and Bolas, Mark},
	editor = {Shumaker, Randall},
	year = {2011},
	keywords = {balance, camera tracking, stoke, video game},
	pages = {243--252},
	file = {Full Text PDF:files/609/Lange et al. - 2011 - Leveraging Unencumbered Full Body Control of Animated Virtual Characters for Game-Based Rehabilitati.pdf:application/pdf},
}

@inproceedings{singh_strategic_2009,
	address = {Berlin, Heidelberg},
	title = {Strategic {Path} {Planning} on the {Basis} of {Risk} vs. {Time}},
	isbn = {978-3-540-89222-9},
	doi = {10.1007/978-3-540-89222-9_9},
	abstract = {The selection of path in an urban combat setting determines the survival to a greater extent. In this paper we propose an algorithm that finds strategic paths inside a map with a set of enemies without using predetermined waypoints. The strategic path calculation is based upon the hit probability calculated for each enemy’s weapons and the risk vs. time preference and it is done at multiple levels of abstractions to address trade-off of efficiency and accuracy and the strategic path calculation minimizes both time and risk as per mission objectives.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2008},
	publisher = {Springer},
	author = {Singh, Ashish C. and Holder, Lawrence},
	editor = {Stevens, Scott M. and Saldamarco, Shirley J.},
	year = {2009},
	keywords = {Heuristic Space Search (HSS), Line-of-Sight (LOS), Military perations on Urban Terrain (MOUT), Non-player character (NPC), Risk, Strategic Path Planning, Time, Visibility Algorithm},
	pages = {77--87},
	file = {Full Text PDF:files/612/Singh e Holder - 2009 - Strategic Path Planning on the Basis of Risk vs. Time.pdf:application/pdf},
}

@inproceedings{lucht_exer-learning_2010,
	address = {Berlin, Heidelberg},
	title = {Exer-{Learning} {Games}: {Transferring} {Hopscotch} from the {Schoolyard} to the {Classroom}},
	isbn = {978-3-642-15286-3},
	shorttitle = {Exer-{Learning} {Games}},
	doi = {10.1007/978-3-642-15286-3_3},
	abstract = {In this paper we present the idea of Exer-learning games - integrating the element of exercise into serious games. The combination of motion, learning and playing is assumed to facilitate intrinsic motivation and learning achievements. For the application of exer-learning games, the concept of HOPSCOTCH is introduced that is inspired by the popular childrens game. Two demonstrators of this concept have been realized and evaluated: HOPSCOTCHpad and HOPSCOTCHmobile. First results show a positive feedback from scholars and teachers. Finally, future directions for our research on HOPSCOTCH are described that could model research on exer-learning games as well as their application in the classroom.},
	language = {en},
	booktitle = {Artificial {Intelligence} in {Theory} and {Practice} {III}},
	publisher = {Springer},
	author = {Lucht, Martina and Domagk, Steffi and Mohring, Martin},
	editor = {Bramer, Max},
	year = {2010},
	keywords = {digital game-based learning, exer-learning games, exercise, HOPSCOTCH, learning, mobile learning, playing, serious games},
	pages = {25--34},
	file = {Full Text PDF:files/615/Lucht et al. - 2010 - Exer-Learning Games Transferring Hopscotch from the Schoolyard to the Classroom.pdf:application/pdf},
}

@inproceedings{dulic_future_2011,
	address = {Berlin, Heidelberg},
	title = {Future {Delta} {Motivating} {Climate} {Change} {Action} {Grounded} in {Place}},
	isbn = {978-3-642-24500-8},
	doi = {10.1007/978-3-642-24500-8_24},
	abstract = {In this paper we discuss the Future Delta game, as a time-forward 3-D visualization and simulation tool that aims to motivate actions and behavioral changes and to educate players about climate change mitigation and adaptations solutions and challenges. The game simulation is situated in a recognizable community locale: the flood-prone neighborhood of Delta, BC. Combining climate change modeling, socioeconomic scenario analysis and 3D modeling of real places with engaging soundscapes and imagery, our game is designed to make climate change science and solutions more salient and understandable to the layperson. The project comprises a game simulation and dynamic 3D visualizations of future local climate change scenarios to provide an environment for experiential learning tied to place attachment. The project builds on a foundation rich in research, experimentation, and production in the topic of climate change in Delta, but extends previous work into a new representational platform of virtual game. An initial testing of the game shows that engaging with the game strengthened the user’s belief that action can be taken to mitigate climate change and increased their support for more transformative social changes to achieve climate mitigation and adaptation.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2011},
	publisher = {Springer},
	author = {Dulic, Aleksandra and Schroth, Olaf and Shirley, Maggie and Sheppard, Stephen},
	editor = {Anacleto, Junia Coutinho and Fels, Sidney and Graham, Nicholas and Kapralos, Bill and Saif El-Nasr, Magy and Stanley, Kevin},
	year = {2011},
	keywords = {Attitudes, Behavioral Change, Climate Change, Experience Design, Immersion, Interactivity, Landscape Visualization, Serious Games},
	pages = {228--234},
	file = {Full Text PDF:files/638/Dulic et al. - 2011 - Future Delta Motivating Climate Change Action Grounded in Place.pdf:application/pdf},
}

@inproceedings{gordon_flexible_2017,
	address = {Cham},
	title = {Flexible {Virtual} {Environments}: {Gamifying} {Immersive} {Learning}},
	isbn = {978-3-319-58753-0},
	shorttitle = {Flexible {Virtual} {Environments}},
	doi = {10.1007/978-3-319-58753-0_18},
	abstract = {The availability of Virtual Reality (VR) and Virtual Environment (VE) equipment - with the launch of domestic technologies such as the Oculus Rift, Microsoft Hololens and Sony Playstation VR) - offer new ways to enable interactive immersive experiences [16]. The opportunities these create in learning and training applications are immense: but create new challenges. Meanwhile, current virtual learning environments are typically web or app based technologies, sometimes perceived as having little value added from a user perspective beyond improved User Interfaces to access some content [6]. The challenge is how the human computer interaction features of such VE platforms may be used in education in a way that adds value, especially for computer mediated instruction. This paper will outline some of the issues, and opportunities, as well as some of the open questions about how such technologies can be used effectively in a higher education context, along with a proposed framework for embedding a learning engine within a virtual reality or environment system.},
	language = {en},
	booktitle = {{HCI} {International} 2017 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Gordon, Neil and Brayshaw, Mike},
	editor = {Stephanidis, Constantine},
	year = {2017},
	keywords = {Flexible pedagogy, Gamification, Virtual environments},
	pages = {115--121},
	file = {Full Text PDF:files/642/Gordon e Brayshaw - 2017 - Flexible Virtual Environments Gamifying Immersive Learning.pdf:application/pdf},
}

@inproceedings{lopes_natural_2017,
	address = {Cham},
	title = {A {Natural} {Interaction} {VR} {Environment} for {Surgical} {Instrumentation} {Training}},
	isbn = {978-3-319-58637-3},
	doi = {10.1007/978-3-319-58637-3_39},
	abstract = {This paper details the process of prototyping a Surgical Instrumentation Simulator using Virtual Reality and a gesture-based natural interaction. Our prototype used a cost-efficient mobile headset along with a telephone screen for a stereoscopic display, thus creating a low-cost. We applied an iterative approach to our prototyping process, and user testing included both students and professionals from the Health field. Results showed proposed interactions techniques as satisfactory according to users. In addition, the low-cost hardware choice proved sufficient in quality to support an immersive experience.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Designing} {Pleasurable} {Experiences}},
	publisher = {Springer International Publishing},
	author = {Lopes, Adalberto and Harger, Antônio and Breyer, Felipe and Kelner, Judith},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2017},
	keywords = {HMD, Interaction design, Natural user interfaces},
	pages = {499--509},
	file = {Full Text PDF:files/645/Lopes et al. - 2017 - A Natural Interaction VR Environment for Surgical Instrumentation Training.pdf:application/pdf},
}

@inproceedings{saenz_see_2015,
	address = {Cham},
	title = {See the {Flex}: {Investigating} {Various} {Display} {Settings} for {Different} {Study} {Conditions}},
	isbn = {978-3-319-21383-5},
	shorttitle = {See the {Flex}},
	doi = {10.1007/978-3-319-21383-5_50},
	abstract = {We present FlexAR, a kinetic tangible augmented reality (TAR) [5] application for anatomy education in varied learning situations. Learning anatomy is fundamental to every health profession as well as related domains such as performance, physical therapy, art, and animation. For example, dancers need to learn anatomy to care for their bodies and learn to move efficiently. Anatomy has traditionally been taught in two dimensions, particularly for those in non-medical fields such as artists and physical education practitioners. Medical students often gain hands-on experience through cadaver dissections [8]. However, with dissection becoming less practical, researchers have begun evaluating techniques for teaching anatomy through technology. Our goal is to develop TAR interfaces to enhance the effectiveness of learning gross anatomy in group and individual study settings. We believe that once expanded FlexAR could be effective as a standalone or supplementary tool for both group and individual learning.},
	language = {en},
	booktitle = {{HCI} {International} 2015 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Saenz, Michael and Strunk, Joshua and Maset, Kelly and Malone, Erica and Seo, Jinsil Hwaryoung},
	editor = {Stephanidis, Constantine},
	year = {2015},
	keywords = {Augmented reality, Education, Human anatomy, Tangible user interface},
	pages = {295--300},
	file = {Full Text PDF:files/648/Saenz et al. - 2015 - See the Flex Investigating Various Display Settings for Different Study Conditions.pdf:application/pdf},
}

@inproceedings{rocha_facanha_editor_2019,
	address = {Cham},
	title = {Editor of {O} \& {M} {Virtual} {Environments} for the {Training} of {People} with {Visual} {Impairment}},
	isbn = {978-3-030-23560-4},
	doi = {10.1007/978-3-030-23560-4_45},
	abstract = {The lack of vision increases the complexity of performing tasks that require spatial representation. People with visual impairment face challenges in establishing positions of obstacles in a physical space. It is hard for them to discern their current location or a direction to go. Thus, any information about the ambient characteristics tends to be relevant to the decision-making process of people who are blind. This process involves the creation of a mental representation of the physical space, which is responsible for storing, retrieving, and decoding information about the environment. A mental representation of the space is a key factor for their Orientation and Mobility (O \& M) skills. Several studies have investigated the impact of the use of O \& M Virtual Environments in the construction of these mental maps. This paper reports the development of an editor for O \& M Virtual Environment focusing on generating indoor spaces simulations, called E3 Editor. Differently, from other studies that provide solutions specific to a given space, E3 Editor allows O \& M experts to create their virtual indoor maps and applied them in their practical classes with learners with visual impairment, thus favouring the autonomy of these subjects.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Theory}, {Methods} and {Tools}},
	publisher = {Springer International Publishing},
	author = {Rocha Façanha, Agebson and Viana, Windson and Sánchez, Jaime},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2019},
	keywords = {Editor of virtual environments, Orientation and Mobility, People with visual impairment},
	pages = {617--627},
	file = {Full Text PDF:files/651/Rocha Façanha et al. - 2019 - Editor of O & M Virtual Environments for the Training of People with Visual Impairment.pdf:application/pdf},
}

@inproceedings{barber_feasibility_2017,
	address = {Cham},
	title = {Feasibility of {Wearable} {Fitness} {Trackers} for {Adapting} {Multimodal} {Communication}},
	isbn = {978-3-319-58521-5},
	doi = {10.1007/978-3-319-58521-5_39},
	abstract = {In addition to efforts to increase the intelligence and perception capabilities of robots to enable collaboration with human counterparts, there is also a focus towards improving interaction mechanics. Multimodal communication is one such tool under investigation due to its dynamic ability to select explicit and implicit communication modalities with the aim of facilitating robust exchanges of information. Although there is extensive research in the domain of explicit communication using auditory, visual, and tactile interfaces, investigations into systems that incorporate implicit methods, or actually adapt and select appropriate modalities for reporting data from a robot to human is limited. Furthermore, a missing piece is identifying how and when to trigger these changes. A novel strategy to accomplish adaptation is through identification of teammate’s physiological state. From the literature, one can find examples of researchers using high fidelity systems to measure physiological response and predict user workload, but many of these technologies are prohibitively expensive or not suitable for use in many domains of interest for human robot interaction such as dismounted infantry operations. Recent advancements in wearable consumer technologies, specifically fitness trackers supporting integration with third party software, are making it possible for incorporation of low cost systems in a variety of novel applications. A logical extension of these applications being physiological state measurement to drive adaptive automation in the form of multimodal interfaces. This paper describes the results of a study to assess the feasibility of using data from a wearable fitness tracker in an adaptive multimodal interface for squad-level human-robot interaction.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}: {Information}, {Knowledge} and {Interaction} {Design}},
	publisher = {Springer International Publishing},
	author = {Barber, Daniel and Carter, Austin and Harris, Jonathan and Reinerman-Jones, Lauren},
	editor = {Yamamoto, Sakae},
	year = {2017},
	keywords = {Adaptive automation, Fitness trackers, Human-robot interaction, Multimodal communication, Physiological assessment},
	pages = {504--516},
	file = {Full Text PDF:files/674/Barber et al. - 2017 - Feasibility of Wearable Fitness Trackers for Adapting Multimodal Communication.pdf:application/pdf},
}

@inproceedings{sanchez_video_2009,
	address = {Berlin, Heidelberg},
	title = {Video {Gaming} for {Blind} {Learners} {School} {Integration} in {Science} {Classes}},
	isbn = {978-3-642-03655-2},
	doi = {10.1007/978-3-642-03655-2_5},
	abstract = {In this study we evaluate how the use of audio-based technology can facilitate school integration of blind learners through the interaction with a science videogame. This experience consisted of designing and implementing The Natomy’s Journey Game to be played by blind and sighted middle school students. The use of the videogame and its impact on the integration of blind learners into mainstream schools was also evaluated, through the participation of both teachers and learners. In the end, the goal was for blind learners to be able to interact and become socially integrated through active science video gaming and the application of specific science content. The results of this study provide initial data and evidence that the use of video games such as The Natomy’s Journey Game can improve the process for the school integration of learners with visual disabilities.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2009},
	publisher = {Springer},
	author = {Sánchez, Jaime and Sáenz, Mauricio},
	editor = {Gross, Tom and Gulliksen, Jan and Kotzé, Paula and Oestreicher, Lars and Palanque, Philippe and Prates, Raquel Oliveira and Winckler, Marco},
	year = {2009},
	keywords = {school integration, science learning, sound interface, Visual disability},
	pages = {36--49},
	file = {Full Text PDF:files/679/Sánchez e Sáenz - 2009 - Video Gaming for Blind Learners School Integration in Science Classes.pdf:application/pdf},
}

@inproceedings{veinott_leaving_2019,
	address = {Cham},
	title = {Leaving {Hints}: {Using} {Player} {In}-{Game} {Hints} to {Measure} and {Improve} {Learning}},
	isbn = {978-3-030-30712-7},
	shorttitle = {Leaving {Hints}},
	doi = {10.1007/978-3-030-30712-7_29},
	abstract = {Student reflection has been shown to be important for learning in educational domains. In this study, we embedded a student reflection task into a video game to diagnose how players were constructing new knowledge. The game took place in a space station in which odd things had been happening. In order to secure a position on the space station, players had to improve their decision making and solve the mystery. As part of the game narrative, players reflected on each learning opportunity or mini-game by providing hints for future players at the end of each round. A corpus of 674 hints from 41 players, playing a 60-min version of the game were coded independently by two coders. Coding covered four levels of understanding in the hints and ranged from a simple restatement of information to a deeper reflection that integrated ideas and created new knowledge. Analyzing hints provided an in-game learning measure that may complement other measures and a way to understand game play experience that did not interrupt game flow. This study provides some recommendations for the design of embedding user hints into video games.},
	language = {en},
	booktitle = {{HCI} {International} 2019 – {Late} {Breaking} {Posters}},
	publisher = {Springer International Publishing},
	author = {Veinott, Elizabeth S. and Whitaker, Elizabeth},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2019},
	keywords = {Adaptive learning environment, Self-explanation, Video games},
	pages = {222--230},
	file = {Full Text PDF:files/682/Veinott e Whitaker - 2019 - Leaving Hints Using Player In-Game Hints to Measure and Improve Learning.pdf:application/pdf},
}

@inproceedings{ortegon-sarmiento_hand_2016,
	address = {Cham},
	title = {Hand {Tracking} and {Haptic}-{Based} {Jugular} {Neonate} {Central} {Venous} {Access} {Procedure}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_50},
	abstract = {Medical simulators are important because they provide means to teach, learn, train, practice and develop skills necessary during medical practice. Simulation also allows exposing trainees to scenarios not possible during training, thus covering a wide range of life-like situations. Although widely used, simulation still faces challenges due to the high costs associated with the simulation equipment. Current advances in computer graphics and user interfaces provide affordable tools that allow exploring solutions in different medical fields. In this paper, we focus on the jugular central venous access performed on neonates, a procedure commonly practice to save lives through drug, nutrients and other medication administration. Simulation to practice this procedure is scarce and focused on adult simulation, yielding to transfer of knowledge to treat a neonate. Our approach focuses on developing a simulation prototype covering the preparation steps and execution of the procedure. To provide natural interactions, we integrated hand motion capture with haptics within a virtual environment representing the operation room. To study the prototype’s user experience we asked 12 participants from last year of medical school to use the prototype.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Ortegon-Sarmiento, Tatiana and Uribe-Quevedo, Alvaro and Perez-Gutierrez, Byron and Vega-Medina, Lizeth and Tibamoso, Gerardo},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {Central venous access, Haptics, Neonate, Simulator, Tracking, Virtual reality},
	pages = {521--531},
	file = {Full Text PDF:files/706/Ortegon-Sarmiento et al. - 2016 - Hand Tracking and Haptic-Based Jugular Neonate Central Venous Access Procedure.pdf:application/pdf},
}

@inproceedings{kallergis_open_2020,
	address = {Cham},
	title = {Open {City} {Museum}: {Unveiling} the {Cultural} {Heritage} of {Athens} {Through} an -{Augmented} {Reality} {Based}- {Time} {Leap}},
	isbn = {978-3-030-50267-6},
	shorttitle = {Open {City} {Museum}},
	doi = {10.1007/978-3-030-50267-6_13},
	abstract = {A nation’s cultural heritage is of great importance both for indigenous people as well as for foreigners as it is a nation’s contribution to humanity and global civilization. As the spark to discover that treasure has ignited and travelling around the world has become much easier, it is necessary to enhance the way that monuments are exhibited and communicated. Current technology offers the capability to alter the way that information is provided and represented. Augmented reality (AR) is the most characteristic example as it surpasses the limits that exist in other media offering a unique experience to the user. A great challenge for AR is to shed a new light on monuments, especially in cities like Athens, filled with historical monuments. In this paper we describe the methodology that was followed in order to create an AR application that will provide users with a virtual time leap experience in the past depicting the monument’s history in its social context. Thus, the issues emerging during the development of an AR app is discussed, as well as solutions to common problems regarding its utilization in outdoors space. Aiming to shed light on its importance throughout the centuries, an interdisciplinary research has been conducted combining fields like architecture and psychology in order to inform and in parallel, arouse emotions to visitors and thus, intensify the experience. Supporting that, user experience has been enhanced following a UI/UX approach, which provides the appropriate tools between easy-to-use and following a narrative.},
	language = {en},
	booktitle = {Culture and {Computing}},
	publisher = {Springer International Publishing},
	author = {Kallergis, Georgios and Christoulakis, Marios and Diakakis, Aimilios and Ioannidis, Marios and Paterakis, Iasonas and Manoudaki, Nefeli and Liapi, Marianthi and Oungrinis, Konstantinos-Alketas},
	editor = {Rauterberg, Matthias},
	year = {2020},
	keywords = {Augmented reality, Cultural heritage, Mobile application, Navigation},
	pages = {156--171},
	file = {Full Text PDF:files/712/Kallergis et al. - 2020 - Open City Museum Unveiling the Cultural Heritage of Athens Through an -Augmented Reality Based- Tim.pdf:application/pdf},
}

@inproceedings{barneche-naya_ux_2019,
	address = {Cham},
	title = {{UX} {Aspects} of {Kinect}-{Based} {Movement} {Schemes} {Inside} {Virtual} {Environments} for {Museum} {Installations}},
	isbn = {978-3-030-21817-1},
	doi = {10.1007/978-3-030-21817-1_11},
	abstract = {Museum installations, especially those related to the display of virtual archaeology, often make use of natural user interaction (NUI). Those sets require methods of interaction that are intuitive and easy to all users, independent of their previous skills and experience with similar or related technologies. The use of depth cameras such as the Kinect system is a common way to allow visitors to move and interact within the digital replicas of buildings and spaces. This paper presents a study of User Experience (UX) applied to four movement schemes implemented on one such installation. For this research, a mixed method approach is used, using a sample of users segmented into three groups based on their previous skills and experience with video games. The four movement schemes studied combine a user gesture to move forward with another gesture for turning. The quantitative and qualitative data obtained for each movement scheme and user group were analyzed, and several usability metrics were combined to obtain a single UX score, which were then used to compare their performance and suitability for their use in the context of a museum.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Ubiquitous} and {Virtual} {Environments} for {Learning} and {Collaboration}},
	publisher = {Springer International Publishing},
	author = {Barneche-Naya, Viviana and Hernández-Ibáñez, Luis A.},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2019},
	keywords = {Kinect, Museum installation, Natural interaction, User experience, Virtual environments},
	pages = {133--150},
	file = {Full Text PDF:files/714/Barneche-Naya e Hernández-Ibáñez - 2019 - UX Aspects of Kinect-Based Movement Schemes Inside Virtual Environments for Museum Installations.pdf:application/pdf},
}

@inproceedings{cheng_effects_2020,
	address = {Cham},
	title = {The {Effects} of {Body} {Gestures} and {Gender} on {Viewer}’s {Perception} of {Animated} {Pedagogical} {Agent}’s {Emotions}},
	isbn = {978-3-030-49062-1},
	doi = {10.1007/978-3-030-49062-1_11},
	abstract = {The goal of this research is to develop Animated Pedagogical Agents (APA) that can convey clearly perceivable emotions through speech, facial expressions and body gestures. In particular, the two studies reported in the paper investigated the extent to which modifications to the range of movement of 3 beat gestures, e.g., both arms synchronous outward gesture, both arms synchronous forward gesture, and upper body lean, and the agent‘s gender have significant effects on viewer’s perception of the agent’s emotion in terms of valence and arousal. For each gesture the range of movement was varied at 2 discrete levels. The stimuli of the studies were two sets of 12-s animation clips generated using fractional factorial designs; in each clip an animated agent who speaks and gestures, gives a lecture segment on binomial probability. 50\% of the clips featured a female agent and 50\% of the clips featured a male agent. In the first study, which used a within-subject design and metric conjoint analysis, 120 subjects were asked to watch 8 stimuli clips and rank them according to perceived valence and arousal (from highest to lowest). In the second study, which used a between-subject design, 300 participants were assigned to two groups of 150 subjects each. One group watched 8 clips featuring the male agent and one group watched 8 clips featuring the female agent. Each participant was asked to rate perceived valence and arousal for each clip using a 7-point Likert scale. Results from the two studies suggest that the more open and forward the gestures the agent makes, the higher the perceived valence and arousal. Surprisingly, agents who lean their body forward more are not perceived as having higher arousal and valence. Findings also show that female agents’ emotions are perceived as having higher arousal and more positive valence that male agents’ emotions.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Multimodal} and {Natural} {Interaction}},
	publisher = {Springer International Publishing},
	author = {Cheng, Justin and Zhou, Wenbin and Lei, Xingyu and Adamo, Nicoletta and Benes, Bedrich},
	editor = {Kurosu, Masaaki},
	year = {2020},
	keywords = {Affective pedagogical agents, Arousal, Body gestures, Gender, Valence},
	pages = {169--186},
	file = {Full Text PDF:files/718/Cheng et al. - 2020 - The Effects of Body Gestures and Gender on Viewer’s Perception of Animated Pedagogical Agent’s Emoti.pdf:application/pdf},
}

@inproceedings{ito_immersive_2019,
	address = {Cham},
	title = {Immersive {Virtual} {Reality} {Environment} to {Test} {Interface} of {Advanced} {Driver} {Assistance} {Systems} for {Elder} {Driver}},
	isbn = {978-3-030-22649-7},
	doi = {10.1007/978-3-030-22649-7_13},
	abstract = {In Japan, the ratio and the number of elder driver have increased following with the number of traffic accidents which has become a social problem. The problem can easily be solved only if the elders does not need to drive by themselves, although, elders living in the country side or suburban areas still needs to drive for daily activities. Thus, supporting the mobility for the elders and preventing traffic accidents at the same time is very important. Self-driving cars are considered to become one of the solutions to solve this issue, though it is still one of the state-of-art technology still under challenging research. Therefore, this paper proposes a practical approach to test functions and interfaces of the self-driving car for elder drivers. In particular, this paper features use of an immersive virtual reality environment for a human-in-the-loop simulation test.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Information} in {Intelligent} {Systems}},
	publisher = {Springer International Publishing},
	author = {Ito, Kenichiro and Hirose, Michitaka},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2019},
	keywords = {ADAS, CAVE, Driving simulator, Safety, Security and reliability},
	pages = {151--163},
	file = {Full Text PDF:files/743/Ito e Hirose - 2019 - Immersive Virtual Reality Environment to Test Interface of Advanced Driver Assistance Systems for El.pdf:application/pdf},
}

@inproceedings{pallavicini_taking_2019,
	address = {Cham},
	title = {Taking {Neuropsychological} {Test} to the {Next} {Level}: {Commercial} {Virtual} {Reality} {Video} {Games} for the {Assessment} of {Executive} {Functions}},
	isbn = {978-3-030-23563-5},
	shorttitle = {Taking {Neuropsychological} {Test} to the {Next} {Level}},
	doi = {10.1007/978-3-030-23563-5_12},
	abstract = {Virtual reality and video games are increasingly considered as potentially effective tools for the assessment of several cognitive abilities, including executive functions. However, thus far, only non-commercial contents have been tested and virtual reality contents and video games have been investigated separately. Within this context, this study aimed to explore the effectiveness in the assessment of executive functions using a new type of interactive content - commercial virtual reality games - which combines the advantages of virtual reality with that of commercial video games. Thirty-eight participants completed the Trial Making Test as traditional commonly used assessments of executive functions and then played the virtual reality game Audioshield using an HTC Vive systems. Scores on the Trial Making Test (i.e., time to complete part A and B) were compared to scores obtained on Audioshield (i.e., number of orbs hit by the players and technical score). The results showed that: (a) performance on the Trial Making Test correlated significantly with performance on the virtual reality video game; (b) scores on Audioshield can be used as a reliable estimator of the results of Trial Making Test.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Multimodality} and {Assistive} {Environments}},
	publisher = {Springer International Publishing},
	author = {Pallavicini, Federica and Pepe, Alessandro and Minissi, Maria Eleonora},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2019},
	keywords = {Cognitive assessment, Executive functions, Video games, Virtual reality, Virtual reality video games},
	pages = {133--149},
	file = {Full Text PDF:files/746/Pallavicini et al. - 2019 - Taking Neuropsychological Test to the Next Level Commercial Virtual Reality Video Games for the Ass.pdf:application/pdf},
}

@inproceedings{riha_cutscenes_2014,
	address = {Cham},
	title = {Cutscenes in {Computer} {Games} as an {Information} {System}},
	isbn = {978-3-319-07626-3},
	doi = {10.1007/978-3-319-07626-3_62},
	abstract = {While computer game´s cutscenes are predominantly used for a cinematic narrative development, in many computer games cutscenes might serve not only as a basic orientation hint about next level in a gaming environment but might play a role of an information system. This contribution will discuss the eventualities to understand cutscenes and their implementation in a gaming environment as a sort of an information system useful for the interpretation and reconfiguration performed by the player with the respect to the rapid prototyping for serious games. Further, will be briefly discussed the potential to author the cutscenes for a sample concept of a serious game (documentary computer games) in the rapid animated movie production tool Moviestorm.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {User} {Experience} {Design} for {Diverse} {Interaction} {Platforms} and {Environments}},
	publisher = {Springer International Publishing},
	author = {Říha, Daniel},
	editor = {Marcus, Aaron},
	year = {2014},
	keywords = {Computer Games, Creativity, Cut Scenes, Digital Filmmaking, Documentary Computer Games, Gameplay, Machinima, Moviestorm, Visual Communication},
	pages = {661--668},
	file = {Full Text PDF:files/640/Říha - 2014 - Cutscenes in Computer Games as an Information System.pdf:application/pdf},
}

@inproceedings{wahl_user_2016,
	address = {Cham},
	title = {User {Interface} and {Interaction} {Design} in {Future} {Auto}-{Mobility}},
	isbn = {978-3-319-40409-7},
	doi = {10.1007/978-3-319-40409-7_17},
	abstract = {This paper focuses tendencies and developments in HMI design in regard of automotive and human mobility in general. By providing two perspectives, one design theoretical oriented one system development related, we want to contribute to a systematic mapping this particular field of Human Machine Interaction.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Design} {Thinking} and {Methods}},
	publisher = {Springer International Publishing},
	author = {Wahl, Hendrik and Groh, Rainer},
	editor = {Marcus, Aaron},
	year = {2016},
	keywords = {3D look, Adventure of driving, Augmentation, Auto-Mobility, Automotive “toys”, Dashboard, Gesture based interaction, Human-Machine-Interaction, Individual transportation, Layer-Like Boundary, Manual gear shifting, Targeted to a smaller group of enthusiast, Tracking, User Interface Design},
	pages = {161--171},
	file = {Full Text PDF:files/644/Wahl e Groh - 2016 - User Interface and Interaction Design in Future Auto-Mobility.pdf:application/pdf},
}

@inproceedings{baalsrud_hauge_collaborative_2014,
	address = {Berlin, Heidelberg},
	title = {Collaborative {Serious} {Games} for {Awareness} on {Shared} {Resources} in {Supply} {Chain} {Management}},
	isbn = {978-3-662-44736-9},
	doi = {10.1007/978-3-662-44736-9_60},
	abstract = {Today manufacturing is a complex process often resulting in long brittle supply chains with considerable contributions to the global resource demand also posing a negative environmental impact (CO2 emissions, raw material supply, etc.). Reducing the waste of resources and being more sustainable are objectives incentivized by materials in short supply and customer requirements. Thus, the ability to share resources, innovate and to implement emergent ICT will play a key role for companies’ competitiveness and their sustainability. However, sharing resources puts high requirements on trust and gain sharing, amongst others. Although well known in supply chain management, the logistics sector is struggling to increase their shared resources. The authors found serious games (SG) to be a promising tool for awareness rising on shared resource. Existing supply chain games are analysed and their potential and weaknesses for the topic are examined, resulting in an outlook on the research needs in SGs for awareness on shared resources.},
	language = {en},
	booktitle = {Advances in {Production} {Management} {Systems}. {Innovative} and {Knowledge}-{Based} {Production} {Management} in a {Global}-{Local} {World}},
	publisher = {Springer},
	author = {Baalsrud Hauge, Jannicke and Kalverkamp, Matthias and Forcolin, Margherita and Westerheim, Hans and Franke, Marco and Thoben, Klaus-Dieter},
	editor = {Grabot, Bernard and Vallespir, Bruno and Gomes, Samuel and Bouras, Abdelaziz and Kiritsis, Dimitris},
	year = {2014},
	keywords = {CPS, Logistics, Serious Games, Shared resources},
	pages = {491--499},
	file = {Full Text PDF:files/647/Baalsrud Hauge et al. - 2014 - Collaborative Serious Games for Awareness on Shared Resources in Supply Chain Management.pdf:application/pdf},
}

@inproceedings{cardenas-delgado_could_2017,
	address = {Cham},
	title = {Could {People} with {Stereo}-{Deficiencies} {Have} a {Rich} {3D} {Experience} {Using} {HMDs}?},
	isbn = {978-3-319-67744-6},
	doi = {10.1007/978-3-319-67744-6_7},
	abstract = {People with stereo-deficiencies usually have problems for the perception of depth using stereo devices. This paper presents a study that involves participants who did not have stereopsis and participants who had stereopsis. The two groups of participants were exposed to a maze navigation task in a 3D environment in two conditions, using a HMD and a large stereo screen. Fifty-nine adults participated in our study. From the results, there were no statistically significant differences for the performance on the task between the participants with stereopsis and those without stereopsis. We found statistically significant differences between the two conditions in favor of the HMD for the two groups of participants. The participants who did not have stereopsis and could not perceive 3D when looking at the Lang 1 Stereotest did have the illusion of depth perception using the HMD. The study suggests that for the people who did not have stereopsis, the head tracking largely influences the 3D experience.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} - {INTERACT} 2017},
	publisher = {Springer International Publishing},
	author = {Cárdenas-Delgado, Sonia and Juan, M.-Carmen and Méndez-López, Magdalena and Pérez-Hernández, Elena},
	editor = {Bernhaupt, Regina and Dalvi, Girish and Joshi, Anirudha and K. Balkrishan, Devanuj and O'Neill, Jacki and Winckler, Marco},
	year = {2017},
	keywords = {3D experience, Head tracking, HMD, Large stereo screen, Stereo-deficiency, Stereoblindness, Stereopsis, Virtual reality},
	pages = {97--116},
	file = {Full Text PDF:files/650/Cárdenas-Delgado et al. - 2017 - Could People with Stereo-Deficiencies Have a Rich 3D Experience Using HMDs.pdf:application/pdf},
}

@inproceedings{robertson_multiuser_2009,
	address = {Berlin, Heidelberg},
	title = {Multiuser {Collaborative} {Exploration} of {Immersive} {Photorealistic} {Virtual} {Environments} in {Public} {Spaces}},
	isbn = {978-3-642-02771-0},
	doi = {10.1007/978-3-642-02771-0_27},
	abstract = {We have developed and deployed a multimedia museum installation that enables one or several users to interact with and collaboratively explore a 3D virtual environment while simultaneously providing an engaging and educational, theater-like experience for a larger crowd of passive viewers. This interactive theater experience consists of a large, immersive projection display, a touch screen display for gross navigation and three wireless, motion-sensing, hand-held controllers which allow multiple users to collaboratively explore a photorealistic virtual environment of Atlanta, Georgia and learn about Atlanta’s history and the philanthropic legacy of many of Atlanta’s prominent citizens.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality}},
	publisher = {Springer},
	author = {Robertson, Scott and Jones, Brian and O’Quinn, Tiffany and Presti, Peter and Wilson, Jeff and Gandy, Maribeth},
	editor = {Shumaker, Randall},
	year = {2009},
	keywords = {Passive Viewer, Projection Display, Projection Screen, Touch Screen, Virtual Environment},
	pages = {235--243},
	file = {Full Text PDF:files/673/Robertson et al. - 2009 - Multiuser Collaborative Exploration of Immersive Photorealistic Virtual Environments in Public Space.pdf:application/pdf},
}

@inproceedings{scholefield_gamification_2019,
	address = {Cham},
	title = {Gamification {Techniques} for {Raising} {Cyber} {Security} {Awareness}},
	isbn = {978-3-030-22351-9},
	doi = {10.1007/978-3-030-22351-9_13},
	abstract = {Due to the prevalence of online services in modern society, such as internet banking and social media, it is important for users to have an understanding of basic security measures in order to keep themselves safe online. However, users often do not know how to make their online interactions secure, which demonstrates an educational need in this area. Gamification has grown in popularity in recent years and has been used to teach people about a range of subjects. This paper presents an exploratory study investigating the use of gamification techniques to educate average users about password security, with the aim of raising overall security awareness. To explore the impact of such techniques, a role-playing quiz application (RPG) was developed for the Android platform to educate users about password security. Results gained from the work highlighted that users enjoyed learning via the use of the password application, and felt they benefitted from the inclusion of gamification techniques. Future work seeks to expand the prototype into a full solution, covering a range of security awareness issues.},
	language = {en},
	booktitle = {{HCI} for {Cybersecurity}, {Privacy} and {Trust}},
	publisher = {Springer International Publishing},
	author = {Scholefield, Sam and Shepherd, Lynsay A.},
	editor = {Moallem, Abbas},
	year = {2019},
	keywords = {Games-based learning, Gamification, Human-centered cyber security, Security awareness, Usable security},
	pages = {191--203},
	file = {Full Text PDF:files/677/Scholefield e Shepherd - 2019 - Gamification Techniques for Raising Cyber Security Awareness.pdf:application/pdf},
}

@inproceedings{kratky_mixed_2009,
	address = {Berlin, Heidelberg},
	title = {Mixed {Realities} – {Virtual} {Object} {Lessons}},
	isbn = {978-3-642-02580-8},
	doi = {10.1007/978-3-642-02580-8_48},
	abstract = {The question of how to design and implement efficient virtual classroom environments gains a new quality in the light of extensive digital education projects such as the One Laptop Per Child (OLPC) initiative. At the core of this consideration is not only the task of developing content for very different cultural settings but also the necessity to reflect the effects of learning processes that operate exclusively with digitally mediated content. This paper traces the design of the project Venture to the Interior, an interactive experience that presents selected objects from the collections of the Museum of Natural History in Berlin and reflects them as building blocks for the Enlightenment-idea of a building of knowledge. The project investigates the role of objects as a knowledge device and the possibilities for a translation of the didactic effects of experiential learning into virtual environments.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Ambient}, {Ubiquitous} and {Intelligent} {Interaction}},
	publisher = {Springer},
	author = {Kratky, Andreas},
	editor = {Jacko, Julie A.},
	year = {2009},
	keywords = {Distance Learning, Mixed Reality, Photorealism, Virtual Classroom, Virtual Museums, Virtual Reality},
	pages = {440--445},
	file = {Full Text PDF:files/680/Kratky - 2009 - Mixed Realities – Virtual Object Lessons.pdf:application/pdf},
}

@inproceedings{scott-brown_tapology_2014,
	address = {Cham},
	title = {Tapology: {A} {Game}-{Based} {Platform} to {Facilitate} {E}-{Health} and {E}-{Inclusion}},
	isbn = {978-3-319-07446-7},
	shorttitle = {Tapology},
	doi = {10.1007/978-3-319-07446-7_36},
	abstract = {We have developed a tablet computer game app for low vision users that can be used to introduce a platform for gaming, internet and visual rehabilitation to older users who have not had prior experience with information communication technology (ICT). Our target user group is people diagnosed with Age Related Macular Degeneration (AMD). The primary goal of the app is to present a fun and engaging means for participants to engage with Information Communication Technology (ICT). A long-term goal of the project is to build a platform to gather data on current and on-going visual function by creating a suite of games that could generate sufficient regular visual engagement to enable perceptual learning in the preserved peripheral retina that is spared in AMD. The inclusive design process took into consideration the perceptual and cognitive constraints of the user group in. The ‘Tapology©’ app was formally launched at a large computer games festival where we gathered data from a range of users to inform the development of the gameplay. The initial results and feedback inform the ultimate goal of creating a suite of applications that have a wide social and geographic reach to promote and inform e-inclusion and e-health.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Aging} and {Assistive} {Environments}},
	publisher = {Springer International Publishing},
	author = {Scott-Brown, Kenneth C. and Harris, Julie and Simmers, Anita and Thurston, Mhairi and Abbas, Malath and de Majo, Tom and Reynolds, Ian and Robinson, Gareth and Mitchell, Iain and Gilmour, Dan and Martinez, Santiago and Isaacs, John},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {accessible design, co-design, E-Health, E-Inclusion, Games, Mobile HCI},
	pages = {368--377},
	file = {Full Text PDF:files/683/Scott-Brown et al. - 2014 - Tapology A Game-Based Platform to Facilitate E-Health and E-Inclusion.pdf:application/pdf},
}

@inproceedings{tanaka_movement-free_2020,
	address = {Cham},
	title = {Movement-{Free} {Virtual} {Reality} {Interface} {Using} {Kinesthetic} {Illusion} {Induced} by {Tendon} {Vibration}},
	isbn = {978-3-030-58147-3},
	doi = {10.1007/978-3-030-58147-3_35},
	abstract = {In current virtual reality (VR) systems, the physical movement of the body is required, which creates problems of safety, cost, and accessibility. To solve those problems, we propose a system that fixes a user’s body, detects force when a user tries to move, and generates the sensation of movement using kinesthetic illusion caused by tendon vibration. We implemented a system limited to simple motion, and conducted an experiment to evaluate operability, body ownership, and agency. Although we could not statistically verify the effect of kinesthetic illusion, the results suggested that it may be possible that kinesthetic illusion could increase ownership and decrease agency.},
	language = {en},
	booktitle = {Haptics: {Science}, {Technology}, {Applications}},
	publisher = {Springer International Publishing},
	author = {Tanaka, Satoshi and Ushiyama, Keigo and Takahashi, Akifumi and Kajimoto, Hiroyuki},
	editor = {Nisky, Ilana and Hartcher-O’Brien, Jess and Wiertlewski, Michaël and Smeets, Jeroen},
	year = {2020},
	keywords = {Kinesthesia, Tendon vibration, Virtual reality},
	pages = {316--324},
	file = {Full Text PDF:files/708/Tanaka et al. - 2020 - Movement-Free Virtual Reality Interface Using Kinesthetic Illusion Induced by Tendon Vibration.pdf:application/pdf},
}

@inproceedings{vuong_fedgrid_2004,
	address = {Berlin, Heidelberg},
	title = {{FedGrid}: {An} {HLA} {Approach} to {Federating} {Grids}},
	isbn = {978-3-540-24688-6},
	shorttitle = {{FedGrid}},
	doi = {10.1007/978-3-540-24688-6_114},
	abstract = {Research on Grids has received considerable attention in recent years. Whereas existing work on connecting the grids primarily focused on grid services, in our work we propose a unified approach, so-called FederationGrid (or FedGrid for short), which integrates both virtual organization (VO) and grid services, thus enabling formation of virtual communities on top of grids. FedGrid is based on the standard High Level Architecture (HLA) that enjoys the advantages of high-level information modeling, including re-usability of software components and real-time performance of the overall systems. In addition, FedGrid is a fractal grid, comprised of hierarchical HLA federations, e.g. Realms and Lobby, which supports well the concept of virtual communities and scalability.},
	language = {en},
	booktitle = {Computational {Science} - {ICCS} 2004},
	publisher = {Springer},
	author = {Vuong, Son and Cai, Xiaojuan and Li, Juan and Pramanik, Sukanta and Suttles, Duncan and Chen, Reggie},
	editor = {Bubak, Marian and van Albada, Geert Dick and Sloot, Peter M. A. and Dongarra, Jack},
	year = {2004},
	keywords = {Collaborative Computing, Community Grid, Federation, Grid, Grid Access, Grid Computing, Grid Services, HLA, RTI, Virtual Organization},
	pages = {889--896},
	file = {Full Text PDF:files/711/Vuong et al. - 2004 - FedGrid An HLA Approach to Federating Grids.pdf:application/pdf},
}

@inproceedings{almaguer_haptic_2019,
	address = {Cham},
	title = {A {Haptic} {Virtual} {Kitchen} for the {Cognitive} {Empowerment} of {Children} with {Autism} {Spectrum} {Disorder}},
	isbn = {978-3-030-30712-7},
	doi = {10.1007/978-3-030-30712-7_18},
	abstract = {Research works have been carried out on how children with autism spectrum disorder (ASD) react to different virtual environment (VE) stimuli. It is hypothesized by psychologists that children with ASD will be cognitively empowered while interacting in a VE, as it can offer positive reinforcement and a better sense of engagement to keep the child’s interests awake. This research will explore the development and refinement of virtual kitchen applications in an immersive environment by incorporating the sense of touch. A tangible VE lets users touch, grasp, and manipulate virtual objects with the help of a haptic force feedback device. The Unity game engine will be integrated with the existing haptic pipeline to build a full-fledged haptic virtual kitchen. Once developed, the software will be tested by children with and without ASD under a psychologist’s observation. Monitoring will be done by tracking users’ progress while learning different steps of cooking in a haptic VE; transfer of knowledge from a VE to the real world will be closely observed.},
	language = {en},
	booktitle = {{HCI} {International} 2019 – {Late} {Breaking} {Posters}},
	publisher = {Springer International Publishing},
	author = {Almaguer, Erik and Yasmin, Shamima},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2019},
	keywords = {Autism spectrum disorder, Haptic feedback, Virtual environment},
	pages = {137--142},
	file = {Full Text PDF:files/716/Almaguer e Yasmin - 2019 - A Haptic Virtual Kitchen for the Cognitive Empowerment of Children with Autism Spectrum Disorder.pdf:application/pdf},
}

@inproceedings{joung_optimal_2016,
	address = {Cham},
	title = {Optimal {User} {Interface} {Parameters} for {Dual}-{Sided} {Transparent} {Screens} in {Layered} {Window} {Conditions}},
	isbn = {978-3-319-39516-6},
	doi = {10.1007/978-3-319-39516-6_15},
	abstract = {In this research, we assess a set of optimal user interface parameters for a dual-sided transparent display in a collaborative working environment. To provide an experiment setup, we develop a prototype that simulates a dual-sided transparent display using two conventional displays and associated simulation software. The user interface parameters controlled in the experiment include the transparency level and the overlapped (or layered) size of foreground and background user interface windows, where a target marker (being searched by subjects) is presented along with distraction markers. To evaluate the optimal parameter setting, we measure the response time and correct response rate from the subject input to both the foreground and background displays. From the pilot study, we found that appropriate levels of transparency and windows overlapping potentially enhance the visibility of a user interface realized on layered multiple windows. Based on this finding, we propose an extended user research, where a depth factor and a contour effect are employed in addition to the user interface parameters, which may enhance the user response time especially in cases where the windows are highly overlapped.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Interaction} {Platforms} and {Techniques}},
	publisher = {Springer International Publishing},
	author = {Joung, Hae Youn and Kim, Se Young and Im, Seung Hyun and Huh, Bo Kyung and Kim, Heesun and Kwon, Gyu Hyun and Park, Ji-Hyung},
	editor = {Kurosu, Masaaki},
	year = {2016},
	keywords = {Both-sided interaction, Layered interface, Transparency, Transparent display},
	pages = {162--169},
	file = {Full Text PDF:files/720/Joung et al. - 2016 - Optimal User Interface Parameters for Dual-Sided Transparent Screens in Layered Window Conditions.pdf:application/pdf},
}

@inproceedings{kim_sound_2014,
	address = {Cham},
	title = {Sound {Clay}: {An} {Immersive} {Art} {Form} by {Sculpting} {Clay} and {Sound}},
	isbn = {978-3-319-07857-1},
	shorttitle = {Sound {Clay}},
	doi = {10.1007/978-3-319-07857-1_106},
	abstract = {This paper attempts to show the fourth generation of the immersive art forms - Immersive Art - Interactive art platforms that artists and designers serve audience to make their own art through this art platform. The user not only participated in media art, but also they can make their own art. This paper will focus on demonstration all of the above. Therefore we introduce the art form ‘Sound Clay’ on the border of Immersive Art. An audience can deform clay and refine transforming sound simultaneously in the immersive environment. Thus, this study provides an exciting opportunity to advance our knowledge of user experience design in the media art. Our second goal is to develop a range of techniques that support intuitive 3D modeling interactions based on free-form sculpting operations like those used when working with modeling clay.},
	language = {en},
	booktitle = {{HCI} {International} 2014 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Kim, Hyunsoo and Park, Changhoon},
	editor = {Stephanidis, Constantine},
	year = {2014},
	keywords = {augmented reality, hand tracking systems, headmounted display, interactive art, sound generating systems, three-dimensional modeling, Virtual clay},
	pages = {602--606},
	file = {Full Text PDF:files/742/Kim e Park - 2014 - Sound Clay An Immersive Art Form by Sculpting Clay and Sound.pdf:application/pdf},
}

@inproceedings{chodan_safety_2017,
	address = {Cham},
	title = {Safety {Does} {Not} {Happen} by {Accident}, {Can} {Gaming} {Help} {Improve} {Occupational} {Health} and {Safety} in {Organizations}?},
	isbn = {978-3-319-58466-9},
	doi = {10.1007/978-3-319-58466-9_29},
	abstract = {In 2015, the Association of Workers’ Compensation Boards of Canada recorded around quarter-million workplace injuries, a staggering figure which does not include incidents that go undocumented. A lack of health and safety training and/or lack of safety awareness can lead to workplace injuries and in the worst cases a workplace death. It is imperative that organizations make Occupational Health and Safety (OHS) one of their top priorities.},
	language = {en},
	booktitle = {Digital {Human} {Modeling}. {Applications} in {Health}, {Safety}, {Ergonomics}, and {Risk} {Management}: {Health} and {Safety}},
	publisher = {Springer International Publishing},
	author = {Chodan, Cameron and Mirza-Babaei, Pejman and Sankaranarayanan, Karthik},
	editor = {Duffy, Vincent G.},
	year = {2017},
	keywords = {Adaptive learning, Case study, Game development, Serious games, User-centered design},
	pages = {321--332},
	file = {Full Text PDF:files/748/Chodan et al. - 2017 - Safety Does Not Happen by Accident, Can Gaming Help Improve Occupational Health and Safety in Organi.pdf:application/pdf},
}

@inproceedings{jiang_fourier-information_2011,
	address = {Berlin, Heidelberg},
	title = {Fourier-{Information} {Duality} in the {Identity} {Management} {Problem}},
	isbn = {978-3-642-23783-6},
	doi = {10.1007/978-3-642-23783-6_7},
	abstract = {We compare two recently proposed approaches for representing probability distributions over the space of permutations in the context of multi-target tracking. We show that these two representations, the Fourier approximation and the information form approximation can both be viewed as low dimensional projections of a true distribution, but with respect to different metrics. We identify the strengths and weaknesses of each approximation, and propose an algorithm for converting between the two forms, allowing for a hybrid approach that draws on the strengths of both representations. We show experimental evidence that there are situations where hybrid algorithms are favorable.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Jiang, Xiaoye and Huang, Jonathan and Guibas, Leonidas},
	editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
	year = {2011},
	keywords = {Identity Management, Information Form, Loopy Belief Propagation, Simulated Crowd, Tracking Accuracy},
	pages = {97--113},
	file = {Full Text PDF:files/752/Jiang et al. - 2011 - Fourier-Information Duality in the Identity Management Problem.pdf:application/pdf},
}

@inproceedings{bozgeyikli_using_2018,
	address = {Cham},
	title = {Using {Immersive} {Virtual} {Reality} {Serious} {Games} for {Vocational} {Rehabilitation} of {Individuals} with {Physical} {Disabilities}},
	isbn = {978-3-319-92052-8},
	doi = {10.1007/978-3-319-92052-8_5},
	abstract = {This paper presents a system for vocational training and assessment of individuals with severe physical disabilities using immersive virtual reality. The system was developed at the University of South Florida’s Center for Assistive, Rehabilitation, and Robotics Technologies (CARRT). A virtual and physical assistive robot was used for the remote-control skills training. After going through several iterations, the system was tested by a total of 15 participants along with professional job trainers. The results were encouraging in further exploration of virtual reality as a promising tool in vocational training of individuals with severe physical disabilities.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Virtual}, {Augmented}, and {Intelligent} {Environments}},
	publisher = {Springer International Publishing},
	author = {Bozgeyikli, Lal “Lila” and Bozgeyikli, Evren and Aguirrezabal, Andoni and Alqasemi, Redwan and Raij, Andrew and Sundarrao, Stephen and Dubey, Rajiv},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2018},
	keywords = {Physical disabilities, Robotics, Serious games, Virtual reality, Vocational training},
	pages = {48--57},
	file = {Full Text PDF:files/755/Bozgeyikli et al. - 2018 - Using Immersive Virtual Reality Serious Games for Vocational Rehabilitation of Individuals with Phys.pdf:application/pdf},
}

@inproceedings{hu_semantic_2014,
	address = {Cham},
	title = {Semantic {Representation} {Analysis}: {A} {General} {Framework} for {Individualized}, {Domain}-{Specific} and {Context}-{Sensitive} {Semantic} {Processing}},
	isbn = {978-3-319-07527-3},
	shorttitle = {Semantic {Representation} {Analysis}},
	doi = {10.1007/978-3-319-07527-3_4},
	abstract = {Language agnostic methods for semantic extraction, encoding, and applications are an increasingly active research area in computational linguistics. This paper introduces an analytic framework for vector-based semantic representation called semantic representation analysis (SRA). The rationale for this framework is considered, as well as some successes and future challenges that must be addressed. A cloud-based implementation of SRA as a domain-specific semantic processing portal has been developed. Applications of SRA in three different areas are discussed: analysis of online text streams, analysis of the impression formation over time, and a virtual learning environment called V-CAEST that is enhanced by a conversation-based intelligent tutoring system. These use-cases show the flexibility of this approach across domains, applications, and languages.},
	language = {en},
	booktitle = {Foundations of {Augmented} {Cognition}. {Advancing} {Human} {Performance} and {Decision}-{Making} through {Adaptive} {Systems}},
	publisher = {Springer International Publishing},
	author = {Hu, Xiangen and Nye, Benjamin D. and Gao, Chuang and Huang, Xudong and Xie, Jun and Shubeck, Keith},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2014},
	keywords = {domain vocabulary, intelligent tutoring systems, language agnostic, Semantic analysis},
	pages = {35--46},
	file = {Full Text PDF:files/758/Hu et al. - 2014 - Semantic Representation Analysis A General Framework for Individualized, Domain-Specific and Contex.pdf:application/pdf},
}

@inproceedings{lier_tobi_2019,
	address = {Cham},
	title = {{ToBI} - {Team} of {Bielefeld} {Enhancing} the {Robot} {Capabilities} of the {Social} {Standard} {Platform} {Pepper}},
	isbn = {978-3-030-27544-0},
	doi = {10.1007/978-3-030-27544-0_43},
	abstract = {In this paper, we describe the joint effort of the Team of Bielefeld (ToBI) winning the RoboCup@Home Social Standard Platform League (@Home SSPL) at the world cup in Montreal 2018. The @Home competition consists of benchmarking tests that cover multiple skills required for service robotics and human-robot interaction in domestic environments. The @Home SSPL is one of three different sub leagues – two of those focus on specific standard platforms, the third allows open platforms. In the SSPL the standard platform is the Pepper robot by Softbank. In this contribution, we present our approach and the design decisions for enhancing the standard platform Pepper for the competition. This includes the development and testing environment, the preparation process, the integrated software system as well as the components providing enhanced skills for the robot. We further describe the ideas and techniques used to extend the human-robot interaction by a mixed-reality interface and a first approach to bimanual grasping. Both was presented in the final demonstration of the competition.},
	language = {en},
	booktitle = {{RoboCup} 2018: {Robot} {World} {Cup} {XXII}},
	publisher = {Springer International Publishing},
	author = {Lier, Florian and Kummert, Johannes and Renner, Patrick and Wachsmuth, Sven},
	editor = {Holz, Dirk and Genter, Katie and Saad, Maarouf and von Stryk, Oskar},
	year = {2019},
	pages = {524--535},
	file = {Full Text PDF:files/762/Lier et al. - 2019 - ToBI - Team of Bielefeld Enhancing the Robot Capabilities of the Social Standard Platform Pepper.pdf:application/pdf},
}

@inproceedings{kitsuya_proposal_2020,
	address = {Cham},
	title = {Proposal of {Evacuation} {Support} {System} and {Evaluation} by {Multi}-agent {Simulation} in a {Regional} {Disaster}},
	isbn = {978-3-030-48939-7},
	doi = {10.1007/978-3-030-48939-7_5},
	abstract = {In Japan, there are many flood and tsunami disasters caused by typhoons, heavy rains and earthquakes. In this case, the residents have an evacuation time to evacuate to shelters after evacuation alerts from a governmental office. However, the limited capacity of shelters and the delay in the rescue of support required people such as elderly and disabled people, the disaster can cause a serious damage. In order to prevent such a damage, we propose an evacuation support system that enables the information sharing of shelters’ condition, the support required people and the support team. In this paper, we evaluated the effects of the evacuation support system by multi-agent simulation in a regional disaster. As a result of the simulation for a case study in a regional area with three evacuation shelters on Yahaba town in Iwate Prefecture, Japan, we confirmed that if we had enough evacuation time, the evacuation support system had an effect to increase the number of the evacuees.},
	language = {en},
	booktitle = {Information {Technology} in {Disaster} {Risk} {Reduction}},
	publisher = {Springer International Publishing},
	author = {Kitsuya, Makoto and Sasaki, Jun},
	editor = {Murayama, Yuko and Velev, Dimiter and Zlateva, Plamena},
	year = {2020},
	keywords = {Disaster response, Evacuation support, Information sharing},
	pages = {45--54},
	file = {Full Text PDF:files/766/Kitsuya e Sasaki - 2020 - Proposal of Evacuation Support System and Evaluation by Multi-agent Simulation in a Regional Disaste.pdf:application/pdf},
}

@inproceedings{azimi_interactive_2018,
	address = {Cham},
	title = {Interactive {Training} and {Operation} {Ecosystem} for {Surgical} {Tasks} in {Mixed} {Reality}},
	isbn = {978-3-030-01201-4},
	doi = {10.1007/978-3-030-01201-4_3},
	abstract = {Inadequate skill in performing surgical tasks can lead to medical errors and cause avoidable injury or death to the patients. On the other hand, there are situations where a novice surgeon or resident does not have access to an expert while performing a task.},
	language = {en},
	booktitle = {{OR} 2.0 {Context}-{Aware} {Operating} {Theaters}, {Computer} {Assisted} {Robotic} {Endoscopy}, {Clinical} {Image}-{Based} {Procedures}, and {Skin} {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Azimi, Ehsan and Molina, Camilo and Chang, Alexander and Huang, Judy and Huang, Chien-Ming and Kazanzides, Peter},
	editor = {Stoyanov, Danail and Taylor, Zeike and Sarikaya, Duygu and McLeod, Jonathan and González Ballester, Miguel Angel and Codella, Noel C.F. and Martel, Anne and Maier-Hein, Lena and Malpani, Anand and Zenati, Marco A. and De Ribaupierre, Sandrine and Xiongbiao, Luo and Collins, Toby and Reichl, Tobias and Drechsler, Klaus and Erdt, Marius and Linguraru, Marius George and Oyarzun Laura, Cristina and Shekhar, Raj and Wesarg, Stefan and Celebi, M. Emre and Dana, Kristin and Halpern, Allan},
	year = {2018},
	keywords = {Artificial intelligence, Medical augmented reality, Surgical simulation and modeling, Surgical training and assessment},
	pages = {20--29},
	file = {Full Text PDF:files/770/Azimi et al. - 2018 - Interactive Training and Operation Ecosystem for Surgical Tasks in Mixed Reality.pdf:application/pdf},
}

@inproceedings{valakou_framework_2024,
	address = {Cham},
	title = {A {Framework} for {Accessibility} in {XR} {Environments}},
	isbn = {978-3-031-49215-0},
	doi = {10.1007/978-3-031-49215-0_31},
	abstract = {Digital accessibility is vital for ensuring equal access and usability for individuals with disabilities. However, addressing the unique challenges faced by individuals with disabilities in XR environments remains a complex task. This paper presents an ongoing accessibility framework designed to empower developers in creating inclusive XR applications. The framework aims to provide a comprehensive solution, addressing the needs of individuals with disabilities, by incorporating various accessibility features, based on XR accessibility guidelines, best practices, and state of the art approaches. The current version of the framework has focused on the accessibility of XR environments for blind or partially sighted users, enhancing their interaction with text, images, videos, and 3D artefacts. The proposed work lays the foundation for Extended Reality (XR) developers to easily encompass accessible assets. In this respect, it offers customizable text settings, alternative visual content text, and multiple user interaction control mechanisms. Furthermore, it includes features such as edge enhancement, interactive element descriptions with dynamic widgets, scanning for navigation, and foreground positioning of active objects. The framework also supports scene adaptations upon user demand to cater to specific visual needs.},
	language = {en},
	booktitle = {{HCI} {International} 2023 – {Late} {Breaking} {Posters}},
	publisher = {Springer Nature Switzerland},
	author = {Valakou, Aikaterini and Margetis, George and Ntoa, Stavroula and Stephanidis, Constantine},
	editor = {Stephanidis, Constantine and Antona, Margherita and Ntoa, Stavroula and Salvendy, Gavriel},
	year = {2024},
	keywords = {Accessibility, Augmented Reality, Extended Reality, Framework, Inclusion, Virtual Reality},
	pages = {252--263},
	file = {Full Text PDF:files/675/Valakou et al. - 2024 - A Framework for Accessibility in XR Environments.pdf:application/pdf},
}

@inproceedings{ososky_human_2013,
	address = {Berlin, Heidelberg},
	title = {Human {Considerations} in the {Application} of {Cognitive} {Decision} {Models} for {HRI}},
	isbn = {978-3-642-39405-8},
	doi = {10.1007/978-3-642-39405-8_33},
	abstract = {In order for autonomous robots to succeed as useful teammates for humans, it is necessary to examine the lens through which human users view, understand, and predict robotic behavior and abilities. To further study this, we conducted an experiment in which participants viewed video segments of a robot in a task-oriented environment, and were asked to explain what the robot was doing, and would likely do next. Results showed that participants’ perceived knowledge of the robot increased with additional exposures over time; however participant responses to open-ended questions about the robot’s behavior and functions remained divergent over multiple scenarios. A discussion of the implications of apparent differences in human interpretation and prediction of robotic behavior and functionality is presented.},
	language = {en},
	booktitle = {Virtual {Augmented} and {Mixed} {Reality}. {Designing} and {Developing} {Augmented} and {Virtual} {Environments}},
	publisher = {Springer},
	author = {Ososky, Scott and Jentsch, Florian and Phillips, Elizabeth},
	editor = {Shumaker, Randall},
	year = {2013},
	keywords = {human-robot interaction, mental models, perception of behavior},
	pages = {295--303},
	file = {Full Text PDF:files/678/Ososky et al. - 2013 - Human Considerations in the Application of Cognitive Decision Models for HRI.pdf:application/pdf},
}

@inproceedings{bennett_virtual_2018,
	address = {Cham},
	title = {Virtual {Reality} {Based} {Assessment} of {Static} {Object} {Visual} {Search} in {Ocular} {Compared} to {Cerebral} {Visual} {Impairment}},
	isbn = {978-3-319-92052-8},
	doi = {10.1007/978-3-319-92052-8_3},
	abstract = {Virtual reality (VR) can provide robust assessment of cognitive spatial processing skills in individuals with visual impairment. VR combined with objective measures of behavioral performance, such as eye and hand tracking, affords a high degree of experimental control, task flexibility, participant engagement, and enhanced data capture. Individuals with visual impairment typically have difficulties identifying objects in a cluttered environment. Furthermore, these difficulties may differ depending on the type of visual impairment. Specifically, individuals with cortical/cerebral visual impairment (CVI) may show a greater sensitivity to visual task complexity compared to those with ocular based visual impairment (OVI). We have developed a VR environment with integrated eye and hand tracking to simulate exploring a toy box to assess performance on a static object-based visual search task. A grid of toys was displayed for a brief duration while participants found and fixated on a specific toy hidden among others. For a given trial, we manipulated multiple factors: the number of unique distractor toys, a color/theme matched toy, and the background clutter. Results to date show that both visually impaired groups demonstrate increased variability in search patterns and reaction times as compared to controls. Additionally, performance of the CVI group fluctuates greatly as a function of task difficulty. Findings from the current work demonstrate a successful interaction between individuals with visual impairments and VR simulations in assessing high level visual function. Further studies will serve as theoretical foundation for the creation of new assessment and training paradigms for visually impaired individuals.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Virtual}, {Augmented}, and {Intelligent} {Environments}},
	publisher = {Springer International Publishing},
	author = {Bennett, Christopher R. and Bailin, Emma S. and Gottlieb, Timothy K. and Bauer, Corinna M. and Bex, Peter J. and Merabet, Lotfi B.},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2018},
	keywords = {Eye-Tracking, Hand-Tracking, Spatial processing, Virtual reality, Visual impairment, Visual search},
	pages = {28--38},
	file = {Full Text PDF:files/684/Bennett et al. - 2018 - Virtual Reality Based Assessment of Static Object Visual Search in Ocular Compared to Cerebral Visua.pdf:application/pdf},
}

@inproceedings{besz_three_2015,
	address = {Cham},
	title = {Three {Apps} for {Shooting} {Sports}: {The} {Design}, {Development}, and {Deployment}},
	isbn = {978-3-319-24589-8},
	shorttitle = {Three {Apps} for {Shooting} {Sports}},
	doi = {10.1007/978-3-319-24589-8_25},
	abstract = {Video games rarely simulate shooting sports accurately. In this paper, we introduce three mobile applications that try to convey the essence of target shooting and biathlon to the players. We look at the applications from the perspectives of game design, implementation, and marketing. Our analysis provides a basis for developing games that take a real-world sport and help the player to appreciate the nuances of the sport, and maybe even to try it out in reality.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Besz, Agnieszka and Górnicki, Maciej and Heinonen, Toni and Kiikeri, Tapani and Ratamo, Ilkka and Luimula, Mika and Suominen, Taisto and Koponen, Aki and Saarni, Jouni and Suovuo, Tomi “bgt” and Smed, Jouni},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {biathlon, game design, game development, mobile games, mobile marketplaces, shooting sports, sport games},
	pages = {329--342},
	file = {Full Text PDF:files/709/Besz et al. - 2015 - Three Apps for Shooting Sports The Design, Development, and Deployment.pdf:application/pdf},
}

@inproceedings{nakevska_model-driven_2013,
	address = {Berlin, Heidelberg},
	title = {A {Model}-{Driven} {Engineering} {Approach} for {Immersive} {Mixed}-{Reality} {Environments}},
	isbn = {978-3-642-41106-9},
	doi = {10.1007/978-3-642-41106-9_18},
	abstract = {We propose a model-based engineering approach for development of immersive mixed-reality environments based on supervisory control theory that provides for automated software synthesis. The proposed approach greatly improves the consistency of the design process by employing models as means of communication, whereas supervisory control synthesis caters for system flexibility and evolvability.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2013},
	publisher = {Springer},
	author = {Nakevska, Marija and Markovski, Jasen and Rauterberg, Matthias},
	editor = {Anacleto, Junia C. and Clua, Esteban W. G. and da Silva, Flavio S. Correa and Fels, Sidney and Yang, Hyun S.},
	year = {2013},
	pages = {147--150},
	file = {Full Text PDF:files/713/Nakevska et al. - 2013 - A Model-Driven Engineering Approach for Immersive Mixed-Reality Environments.pdf:application/pdf},
}

@inproceedings{giron_eureka_2014,
	address = {Cham},
	title = {Eureka: {Realizing} {That} an {Application} is {Responding} to {Your} {Brainwaves}},
	isbn = {978-3-319-07437-5},
	shorttitle = {Eureka},
	doi = {10.1007/978-3-319-07437-5_47},
	abstract = {We have conducted an experiment in which subjects controlled a brain-computer interface (BCI) without being aware that their brainwaves were responsible for events in the scenario. Ten subjects went through a stage of model training in steady state visually evoked potential (SSVEP)-based BCI, followed by three trials of an immersive experience where stars moved as a response to SSVEP classification. Only then the subjects were explained that they were using a BCI, and this was followed by an additional trial of immersive free choice BCI and a final validation stage. Three out of the ten subjects realized that they controlled the interface, and these subjects had better accuracy than the rest of the subjects and reported a higher sense of agency in a post study questionnaire.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Design} and {Development} {Methods} for {Universal} {Access}},
	publisher = {Springer International Publishing},
	author = {Giron, Jonathan and Friedman, Doron},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2014},
	keywords = {agency, brain computer interface, electroencephalogram, steady state visually evoked potentials},
	pages = {495--502},
	file = {Full Text PDF:files/715/Giron e Friedman - 2014 - Eureka Realizing That an Application is Responding to Your Brainwaves.pdf:application/pdf},
}

@inproceedings{cardenas_testing_2018,
	address = {Cham},
	title = {Testing {Ambient} {Assisted} {Living} {Solutions} with {Simulations}},
	isbn = {978-3-319-99927-2},
	doi = {10.1007/978-3-319-99927-2_5},
	abstract = {The paper introduces a testing solution for evaluating Ambient Assisted Living systems by means of 3D simulations generated with a game engine, complex event processing, and classifiers. The solution aims to ensure that: (1) some key features of the problem appear in the simulation, and (2) the assistive solution interacts with the persons in the right way. A specific testing solution is needed because of the evolving nature of the simulation (each iteration of the requirements specification involves changes in the activities within the simulation) and the assistive solution (it operates in real time and evaluating its performance may require manually inspecting hours of simulation). The approach is illustrated with a proof-of-concept experiment.},
	language = {en},
	booktitle = {Testing {Software} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Cárdenas, Marlon and Sanz, Jorge Gómez and Pavón, Juan},
	editor = {Medina-Bulo, Inmaculada and Merayo, Mercedes G. and Hierons, Robert},
	year = {2018},
	keywords = {3D simulation, Ambient Assisted Living (AAL), Real-time simulation, Requirements gathering},
	pages = {56--61},
	file = {Full Text PDF:files/719/Cárdenas et al. - 2018 - Testing Ambient Assisted Living Solutions with Simulations.pdf:application/pdf},
}

@inproceedings{varisco_designing_2020,
	address = {Cham},
	title = {Designing {Unconscious} and {Enactive} {Interaction} for {Interactive} {Movie} {Experience}},
	isbn = {978-3-030-50344-4},
	doi = {10.1007/978-3-030-50344-4_26},
	abstract = {In a world full of sensors that gather personal data and digital solutions that use these data to provide feedback and personalized experiences, biofeedback is increasingly involved in the definition of new paradigms for tailoring interactions. Companies are collecting and using personal data to propose personalized services. Content providers are pushing users to produce data in order to create personalized storytelling experiences. In this context, the tech market is offering new low-cost solutions able to gather biodata. The paper reports the results of evidence-based explorations aimed at formalizing knowledge regarding the use of passive and unconscious interaction to control the fruition of storytelling artifacts. We investigate a new interaction paradigm that promise to seamlessly enable unconscious and enactive interactions for movie experiences. We propose the use of emotion recognition and eye-tracking as exploratory technologies that promise to be a potential contribution to richer access to the spectators’ emotional involvement. We reflect on disruptive power of non-invasive technologies, given by the possibility to be used for home-cinema experiences. Investigating on emotional states of users in their decision we leverage on the emotive-cognitive data as a matter of creation and enabling of tailored movie experiences. Our research intends to explore the possibility of extracting knowledge from recognition of facial expressions that will contribute to foster its use in real-time passive interaction using emotion recognition as a trigger of enactivity that is not limited to interactive storytelling but opens new scenarios in the design of proactive systems for screens, spaces and environments. Furthermore, we provide suggestions as guidelines for the design of enactive experiences that leverage on emotion recognition and eye-tracking.},
	language = {en},
	booktitle = {Distributed, {Ambient} and {Pervasive} {Interactions}},
	publisher = {Springer International Publishing},
	author = {Varisco, Laura and Interlandi, Giulio},
	editor = {Streitz, Norbert and Konomi, Shin'ichi},
	year = {2020},
	keywords = {Emotion recognition, Enactive interaction, Evidence-based design research, Eye-controlled interfaces, Real-time interaction},
	pages = {362--375},
	file = {Full Text PDF:files/744/Varisco e Interlandi - 2020 - Designing Unconscious and Enactive Interaction for Interactive Movie Experience.pdf:application/pdf},
}

@inproceedings{del_bimbo_natural_2013,
	address = {Berlin, Heidelberg},
	title = {A {Natural} {Interface} for the {Training} of {Medical} {Personnel} in an {Immersive} and {Virtual} {Reality} {System}},
	isbn = {978-3-642-41181-6},
	doi = {10.1007/978-3-642-41181-6_77},
	abstract = {In this paper we present an immersive system, developed for the RIMSI project, designed for the training of medical and para-medical personnel in emergency medicine. Virtual reality systems have been used recently in combination with natural interaction systems for patients’ rehabilitation but are proving especially useful for educational applications in the field of medical training and teaching. The idea is to mimic real three-dimensional environments and to make them available through natural interaction to enable users to explore, interact, collaborate and assist patients in virtual scenarios that otherwise could not be simulated easily in the real world without an high cost (e.g. to prepare the environment for a plane or a car crash) or possible risks of injuries to actors involved in the simulation (e.g. perilous situations like a gas leak). In addition, these simulation scenarios, using a digital environment, are repeatable and can be recorded, easing the process of errors’ highlighting. The RIMSI prototype provides a virtual first aid scenario with interactive 3D graphics which can be controlled and navigated through a natural gesture interface based on KinectTM.},
	language = {en},
	booktitle = {Image {Analysis} and {Processing} – {ICIAP} 2013},
	publisher = {Springer},
	author = {Del Bimbo, Alberto and Ferracani, Andrea and Pezzatini, Daniele and Seidenari, Lorenzo},
	editor = {Petrosino, Alfredo},
	year = {2013},
	keywords = {body tracking, medical training, virtual reality},
	pages = {763--772},
	file = {Full Text PDF:files/749/Del Bimbo et al. - 2013 - A Natural Interface for the Training of Medical Personnel in an Immersive and Virtual Reality System.pdf:application/pdf},
}

@inproceedings{muller_gaming_2012,
	address = {Berlin, Heidelberg},
	title = {Gaming after {Dark}},
	isbn = {978-3-642-33542-6},
	doi = {10.1007/978-3-642-33542-6_2},
	abstract = {Design Patterns help a range of designers, architects, and others. However, there is surprisingly little such guidance for game artists. In this paper, we present our look at late 19th century art works and the emergent set of visual features commonly used to create an atmosphere of horror in visual art. Further, we show how we transformed these features into a set of seven patterns to be used in interactive artistry, based on an analysis of six well known survival horror games. Finally, we provide the full description of one of these patterns, the Visual Contrast.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2012},
	publisher = {Springer},
	author = {Müller, Ivana and Sundström, Petra and Murer, Martin and Tscheligi, Manfred},
	editor = {Herrlich, Marc and Malaka, Rainer and Masuch, Maic},
	year = {2012},
	keywords = {Atmosphere, Design Patterns, Survival Horror Games, Visual Art},
	pages = {16--29},
	file = {Full Text PDF:files/754/Müller et al. - 2012 - Gaming after Dark.pdf:application/pdf},
}

@inproceedings{pfau_give_2019,
	address = {Cham},
	title = {Give {MEANinGS} to {Robots} with {Kitchen} {Clash}: {A} {VR} {Human} {Computation} {Serious} {Game} for {World} {Knowledge} {Accumulation}},
	isbn = {978-3-030-34644-7},
	shorttitle = {Give {MEANinGS} to {Robots} with {Kitchen} {Clash}},
	doi = {10.1007/978-3-030-34644-7_7},
	abstract = {In this paper, we introduce the framework of MEANinGS for the semi-autonomous accumulation of world knowledge for robots. Where manual aggregation is inefficient and prone to incompleteness and autonomous approaches suffer from underspecified information, we deploy the human computation game Kitchen Clash and give evidence of its efficiency, completeness and motivation potential.},
	language = {en},
	booktitle = {Entertainment {Computing} and {Serious} {Games}},
	publisher = {Springer International Publishing},
	author = {Pfau, Johannes and Porzel, Robert and Pomarlan, Mihai and Cangalovic, Vanja Sophie and Grudpan, Supara and Höffner, Sebastian and Bateman, John and Malaka, Rainer},
	editor = {van der Spek, Erik and Göbel, Stefan and Do, Ellen Yi-Luen and Clua, Esteban and Baalsrud Hauge, Jannicke},
	year = {2019},
	keywords = {Framework, Knowledge accumulation, Serious game},
	pages = {85--96},
	file = {Full Text PDF:files/759/Pfau et al. - 2019 - Give MEANinGS to Robots with Kitchen Clash A VR Human Computation Serious Game for World Knowledge.pdf:application/pdf},
}

@inproceedings{zhu_why_2011,
	address = {Berlin, Heidelberg},
	title = {Why {Can}’t a {Virtual} {Character} {Be} {More} {Like} a {Human}: {A} {Mixed}-{Initiative} {Approach} to {Believable} {Agents}},
	isbn = {978-3-642-22024-1},
	shorttitle = {Why {Can}’t a {Virtual} {Character} {Be} {More} {Like} a {Human}},
	doi = {10.1007/978-3-642-22024-1_32},
	abstract = {Believable agents have applications in a wide range of human computer interaction-related domains, such as education, training, arts and entertainment. Autonomous characters that behave in a believable manner have the potential to maintain human users’ suspense of disbelief and fully engage them in the experience. However, how to construct believable agents, especially in a generalizable and cost effective way, is still an open problem. This paper compares the two common approaches for constructing believable agents — human-driven and artificial intelligence-driven interactive characters — and proposes a mixed-initiative approach in the domain of interactive training systems. Our goal is to provide the user with engaging and effective educational experiences through their interaction with our system.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality} - {Systems} and {Applications}},
	publisher = {Springer},
	author = {Zhu, Jichen and Moshell, J. Michael and Ontañón, Santiago and Erbiceanu, Elena and Hughes, Charles E.},
	editor = {Shumaker, Randall},
	year = {2011},
	keywords = {artificial intelligence, character believability, interactive storytelling, interactive virtual environment, Mixed-initiative system},
	pages = {289--296},
	file = {Full Text PDF:files/763/Zhu et al. - 2011 - Why Can’t a Virtual Character Be More Like a Human A Mixed-Initiative Approach to Believable Agents.pdf:application/pdf},
}

@inproceedings{ebrecht_integration_2018,
	address = {Cham},
	title = {Integration of an {Exocentric} {Orthogonal} {Coplanar} 360 {Degree} {Top} {View} in a {Head} {Worn} {See}-{Through} {Display} {Supporting} {Obstacle} {Awareness} for {Helicopter} {Operations}},
	isbn = {978-3-319-92046-7},
	doi = {10.1007/978-3-319-92046-7_32},
	abstract = {The objective was the development of an HMI for helicopter obstacle awareness and warning systems in order to improve the situational and spatial awareness as well as the workload of helicopter pilots. The related work concerning obstacle awareness and warning systems, situational awareness, orthogonal coplanar and perspective representations plus previous work done by DLR was depicted and discussed. The two main aspects of the developed HMI concept were explained, i.e., the combination of the exocentric orthogonal coplanar top view with the egocentric perspective view, and secondly three ways for the integration of the obstacle awareness display inside a head-worn see-through display. The developed HMI concept was applied to two helicopter offshore operations and its specific obstacle situation. The first operation is a hoist operation at the lower access point of an offshore wind turbine. The second regards the landing operation on an offshore platform. From a technical point of view, especially concerning available sensor technologies, helicopter might be fitted with obstacle awareness systems in future. The HMI design is still under investigation in order to support the pilot in a holistic and balanced way.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Information} in {Applications} and {Services}},
	publisher = {Springer International Publishing},
	author = {Ebrecht, Lars and Ernst, Johannes M. and Döhler, Hans-Ullrich and Schmerwitz, Sven},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2018},
	keywords = {2D/3D representations, Augmented reality, Cockpit display systems, Helicopter offshore operations, Human machine interface, Multimodal men machine interaction, Situational awareness, Spatial awareness},
	pages = {369--382},
	file = {Full Text PDF:files/767/Ebrecht et al. - 2018 - Integration of an Exocentric Orthogonal Coplanar 360 Degree Top View in a Head Worn See-Through Disp.pdf:application/pdf},
}

@inproceedings{zhang_employing_2017,
	address = {Cham},
	title = {Employing a {User}-{Centered} {Design} {Process} to {Create} a {Multiplayer} {Online} {Escape} {Game} for {Older} {Adults}},
	isbn = {978-3-319-58536-9},
	doi = {10.1007/978-3-319-58536-9_24},
	abstract = {The purpose of this study was to explore a user-centered design (UCD) process to create a multiplayer online escape game with embedded learning content for older adults. Older adults aged 65 and over were involved in the needs assessment and prototype evaluation. The needs assessment assisted the researchers and developers in understanding older adults’ social interaction in real-life escape rooms, which resulted in a list of design recommendations for the online escape game. The findings of prototype evaluation illustrated that older users enjoyed the theme of classical literary work, crossword puzzles, and the format of dual play. It was also found that our UCD process could not effectively address all design challenges of developing a digital escape game for older adults.},
	language = {en},
	booktitle = {Human {Aspects} of {IT} for the {Aged} {Population}. {Applications}, {Services} and {Contexts}},
	publisher = {Springer International Publishing},
	author = {Zhang, Fan and Doroudian, Amir and Kaufman, David and Hausknecht, Simone and Jeremic, Julija and Owens, Hollis},
	editor = {Zhou, Jia and Salvendy, Gavriel},
	year = {2017},
	keywords = {Escape game, Life-long learning, Older adults, Puzzles, Social interaction},
	pages = {296--307},
	file = {Full Text PDF:files/771/Zhang et al. - 2017 - Employing a User-Centered Design Process to Create a Multiplayer Online Escape Game for Older Adults.pdf:application/pdf},
}

@inproceedings{bozgeyikli_effects_2017,
	address = {Cham},
	title = {Effects of {Instruction} {Methods} on {User} {Experience} in {Virtual} {Reality} {Serious} {Games}},
	isbn = {978-3-319-57987-0},
	doi = {10.1007/978-3-319-57987-0_17},
	abstract = {Instructions are an important aspect of virtual reality serious games since they are crucial to understanding what is expected from the user. User friendly instructions can contribute positively to the user experience, while confusing instructions can degrade it. There are various methods of instruction giving. In this study, we examine the effects of four different instruction methods on user experience in virtual reality serious games. The four instruction methods that were explored in our study are 3D animated, pictograph, written and verbal instructions. Eight simple vocational tasks were designed and implemented to be performed in an immersive virtual warehouse environment. A user study was performed with 15 adult participants. Results revealed that animated instructions provided better user experience among the four methods. Pictograph and written instructions shared similar mid-range rankings. Verbal instructions were the least preferred method. In this paper, we present our experiment design, results and discussions of our implications for future virtual reality serious game studies.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Bozgeyikli, Lal and Raij, Andrew and Katkoori, Srinivas and Alqasemi, Redwan},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Instruction methods, Serious games, Training, Virtual reality},
	pages = {215--226},
	file = {Full Text PDF:files/774/Bozgeyikli et al. - 2017 - Effects of Instruction Methods on User Experience in Virtual Reality Serious Games.pdf:application/pdf},
}

@inproceedings{taucer_addressing_2018,
	address = {Cham},
	title = {On {Addressing} the {Challenges} of {Complex} {Stochastic} {Games} {Using} “{Representative}” {Moves}},
	isbn = {978-3-319-92007-8},
	doi = {10.1007/978-3-319-92007-8_1},
	abstract = {The problem of achieving competitive game play in a board game, against an intelligent opponent, is a well-known and studied field of Artificial Intelligence (AI). This area of research has seen major breakthroughs in recent years, particularly in the game of Go. However, popular hobby board games, and particularly Trading Card Games, have unique qualities that make them very challenging to existing game playing techniques, partly due to enormous branching factors. This remains a largely unexamined domain and is the arena we operate in. To attempt to tackle some of these daunting requirements, we introduce the novel concept of “Representative” Moves (RMs). Rather than examine the complete list of available moves at a given node, we rather propose the strategy of considering only a subset of moves that are determined to be representative of the player’s strategic options. We demonstrate that in the context of a simplified Trading Card Game, the use of RMs leads to a greatly improved search speed and an extremely limited branching factor. This permits the AI player to play more intelligently than the same algorithm that does not employ them.},
	language = {en},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Taucer, Armando H. and Polk, Spencer and Oommen, B. John},
	editor = {Iliadis, Lazaros and Maglogiannis, Ilias and Plagianakos, Vassilis},
	year = {2018},
	pages = {3--13},
	file = {Full Text PDF:files/796/Taucer et al. - 2018 - On Addressing the Challenges of Complex Stochastic Games Using “Representative” Moves.pdf:application/pdf},
}

@inproceedings{dingli_interacting_2019,
	address = {Cham},
	title = {Interacting with {Intelligent} {Digital} {Twins}},
	isbn = {978-3-030-23541-3},
	doi = {10.1007/978-3-030-23541-3_1},
	abstract = {This paper details the Human Computer Interaction (HCI) components of an Intelligent Digital Twin (IDT) system for a semiconductor manufacturing company using gamming visual effects, 3D Computer Aided Design (CAD) and sound effects. The designed digital twin (DT) system will allow users to detect any irregularities (such as equipment failures and defects) in the manufacturing processes, in a timely manner and test some changes without an actual touching of the physical components. The project aims to design an IDT system that enhances the user’s interaction by visualizing big data in such a way which can be easily understood and processed. Thus, allowing the user to intervene and control the entire production processes from the virtual console. The designed IDT system will enhance the UX of the system through the use of new interaction methodologies. The user will have full control of the data flow which is flowing from a data lake through for example, a gazing, a gesturing and a voice recognition interface which will provide contextual information based upon the user’s viewpoint. The current phase of the project investigates the use of Cave VR technology to improve the immersion and the interaction between the user and the virtual system. We seek to develop a virtual environment that makes the users feel they are naturally interacting in a visually-immersed environment irrespective of where they are located. By enhancing the interaction of the user with these new technologies, we will provide a better UX that would create an efficient system which reduces the overall costs of managing the plant and concretely realize the aspirations of Industry 4.0.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {User} {Experience} in {Advanced} {Technological} {Environments}},
	publisher = {Springer International Publishing},
	author = {Dingli, Alexie and Haddod, Foaad},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2019},
	keywords = {Human Computer Interaction, Intelligent Digital Twin, Interaction, Semiconductor manufacturing, User experience},
	pages = {3--15},
	file = {Full Text PDF:files/801/Dingli e Haddod - 2019 - Interacting with Intelligent Digital Twins.pdf:application/pdf},
}

@inproceedings{gieser_real-time_2015,
	address = {Cham},
	title = {Real-{Time} {Static} {Gesture} {Recognition} for {Upper} {Extremity} {Rehabilitation} {Using} the {Leap} {Motion}},
	isbn = {978-3-319-21070-4},
	doi = {10.1007/978-3-319-21070-4_15},
	abstract = {Cerebral Palsy is a motor disability that occurs in early childhood. Conventional therapy methods have proven useful for upper extremity rehabilitation, but can lead to non-compliance due to children getting bored with the repetition of exercises. Virtual reality and game-like simulations of conventional methods have proven to lead to higher rates of compliance, the patient being more engaged during exercising, and yield better performance during exercises. Most games are good at keeping players engaged, but does not focus on exercising fine motor control functions. In this paper, we present an analysis of classification techniques for static hand gestures. We also present a prototype of a game-like simulation of matching static hand gestures in order to increase motor control of the hand.},
	language = {en},
	booktitle = {Digital {Human} {Modeling}. {Applications} in {Health}, {Safety}, {Ergonomics} and {Risk} {Management}: {Ergonomics} and {Health}},
	publisher = {Springer International Publishing},
	author = {Gieser, Shawn N. and Boisselle, Angie and Makedon, Fillia},
	editor = {Duffy, Vincent G.},
	year = {2015},
	keywords = {Cerebral palsy, Gamification, Gesture recognition, Leap motion, Upper extremity rehabilitation},
	pages = {144--154},
	file = {Full Text PDF:files/805/Gieser et al. - 2015 - Real-Time Static Gesture Recognition for Upper Extremity Rehabilitation Using the Leap Motion.pdf:application/pdf},
}

@inproceedings{riha_use_2016,
	address = {Cham},
	title = {Use of {Virtual} {Reality} and {Human}-{Computer} {Interface} for {Diagnostic} and {Treatment} {Purposes} in {Human} {Sexuality} {Research}},
	isbn = {978-3-319-40406-6},
	doi = {10.1007/978-3-319-40406-6_28},
	abstract = {Virtual reality technology allows designing of immersive virtual environment that might be used for diagnostics and treatment of persons with minor sexual preference that is ego-dystonic or is perceived as dangerous to society. We do operate with the theoretical framework that is based on sexual motivational system, which we reintroduce. The eye tracking, electroencephalography and galvanic skin response in combination with phalopletysmography – the change of penile tumescence in male users are data to be collected to distinguish between sexual arousal and stress. The bio-cybernetic loop will consist of combination of positive and negative feedback loop to diverge the scenario. The biofeedback training should allow us to shape the preference by employing biofeedback training of suppression of the arousal to previously created scenario. This procedure might allow better diagnostics and treatment. Although we have in mind individualization of scenario creates problems in comparison during diagnostic phase, and is more important in case of future therapy. The resulting adaptable GNU software will be available to partner and other interested research centers.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Technological} {Contexts}},
	publisher = {Springer International Publishing},
	author = {Říha, Daniel and Bártová, Klára and Binter, Jakub},
	editor = {Marcus, Aaron},
	year = {2016},
	keywords = {Bio-cybernetic loop, Biofeedback, Diagnostics, Sexual paraphilia, Virtual reality},
	pages = {294--305},
	file = {Full Text PDF:files/809/Říha et al. - 2016 - Use of Virtual Reality and Human-Computer Interface for Diagnostic and Treatment Purposes in Human S.pdf:application/pdf},
}

@inproceedings{spitz_you_2018,
	address = {Cham},
	title = {Do {You} {Eat} {This}? {Changing} {Behavior} {Through} {Gamification}, {Crowdsourcing} and {Civic} {Engagement}},
	isbn = {978-3-319-91806-8},
	shorttitle = {Do {You} {Eat} {This}?},
	doi = {10.1007/978-3-319-91806-8_6},
	abstract = {The current excessive use of artificial additives by the food industry, the side effects of these potentially harmful ingredients and their impact on public health should be more widely acknowledged by consumers and further disclosed and discussed by citizens. Governments should develop stricter regulations on food additives, promote better labeling, apply taxes on miscreant food and conduct tighter industry surveillance. In parallel, broader behavior change towards nutrition habits might also be fostered through social innovation and citizen participation. In this paper, we present the design process for creating Dyet (Do you eat this?), a gamified app devised for collecting data and informing on the presence of such additives in commercially available food products. We argue that information on food ingredients and artificial additives should not only be accessible and legible, but also intelligible and personally meaningful to citizens. Through gameplay, we expect to foster the habit of reading ingredients lists, encouraging users to better inform themselves about what they eat and drink. Our overall goal is to change consumer’s potentially unsafe eating habits by bringing visibility to the excessive intake of artificial additives and on harmful food industry practices, making it possible and easier for everyone to make healthier dietary choices.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Users}, {Contexts} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Spitz, Rejane and Queiroz, Francisco and Pereira, Clorisval and Cardarelli Leite, Leonardo and Ferranti, Marcelo P. and Dam, Peter},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2018},
	keywords = {Crowdsourcing, Gamification, Interface design},
	pages = {67--79},
	file = {Full Text PDF:files/815/Spitz et al. - 2018 - Do You Eat This Changing Behavior Through Gamification, Crowdsourcing and Civic Engagement.pdf:application/pdf},
}

@inproceedings{sanchez_video_2010,
	address = {Berlin, Heidelberg},
	title = {Video {Game} {Design} for {Mobile} {Phones}},
	isbn = {978-3-642-15231-3},
	doi = {10.1007/978-3-642-15231-3_20},
	abstract = {This article explores the use of mobile phones for educational purposes through the design, development and evaluation of Role Playing Games (RPG). To accomplish this, we differentiated between the functionality and design of the videogame, developing a videogame engine and videogames prototypes. Thus the engine was responsible for controlling the videogame’s functionalities regarding the prototypes designed. Once designed and developed, a usability evaluation of the videogames with end-users was administered. Results show that the videogames implemented were usable, easy and pleasant to use, and that they fully motivated learners as mobile learning tools.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}},
	publisher = {Springer},
	author = {Sánchez, Jaime and Espinoza, Matías},
	editor = {Forbrig, Peter and Paternó, Fabio and Mark Pejtersen, Annelise},
	year = {2010},
	keywords = {cell phone, mobile, problem solving, RPG, videogame},
	pages = {199--210},
	file = {Full Text PDF:files/707/Sánchez e Espinoza - 2010 - Video Game Design for Mobile Phones.pdf:application/pdf},
}

@inproceedings{nijholt_humor_2015,
	address = {Cham},
	title = {Humor {Techniques}: {From} {Real} {World} and {Game} {Environments} to {Smart} {Environments}},
	isbn = {978-3-319-20804-6},
	shorttitle = {Humor {Techniques}},
	doi = {10.1007/978-3-319-20804-6_60},
	abstract = {In this paper we explore how future smart environments can be given a sense of humor. Humor requires smartness. Entering witty remarks in a conversation requires understanding of the conversation, the conversational partner, the context and the history of the conversation. We can try to model interaction smartness and how to use it in creating not only witty remarks, but also to create humorous events. Smart sensors and actuators, embedded in our environments and our wearables allow us to make changes to a digitally enhanced physical environment. Witty remarks in language can have their counterpart in witty events in digital environments, including social media environments with their own communication characteristics. Sequential and parallel juxtapositions of incongruous and contrasting events invade our communication and, in addition, can be expected to emerge or to be created in digitally enhanced physical environments too, accidentally and intentionally.},
	language = {en},
	booktitle = {Distributed, {Ambient}, and {Pervasive} {Interactions}},
	publisher = {Springer International Publishing},
	author = {Nijholt, Anton},
	editor = {Streitz, Norbert and Markopoulos, Panos},
	year = {2015},
	keywords = {Accidental humor, Ambient intelligence, Computational humor, Incongruity humor, Intended humor, Internet of things, Sense of humor},
	pages = {659--670},
	file = {Full Text PDF:files/710/Nijholt - 2015 - Humor Techniques From Real World and Game Environments to Smart Environments.pdf:application/pdf},
}

@inproceedings{guo_lessons_2015,
	address = {Cham},
	title = {Lessons from {Practicing} an {Adapted} {Model} {Driven} {Approach} in {Game} {Development}},
	isbn = {978-3-319-24589-8},
	doi = {10.1007/978-3-319-24589-8_39},
	abstract = {Various authoring tools have been used to ease the game creation. However, these pre-defined tools may not be suitable for some emerging or special domains. We proposed an approach named Game Creation with Customized Tools (GCCT) to create tools for certain domains first, and then create games using these tools. GCCT is based on the widely applied Model Driven Development (MDD) approach. Despite the apparent appropriateness and benefits, MDD also has drawbacks. Among them, non-trivial cost for tools development is prominent. To address this, some enhancements were made in GCCT, and two case studies were performed to evaluate the cost and the productivity when involving GCCT. In this paper, we reported the results of the case studies as well as practical lessons we have learnt.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Guo, Hong and Trætteberg, Hallvard and Wang, Alf Inge and Gao, Shang and Jaccheri, Maria Letizia},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {Computer Game Development, Cost, Model Driven Development},
	pages = {451--456},
	file = {Full Text PDF:files/717/Guo et al. - 2015 - Lessons from Practicing an Adapted Model Driven Approach in Game Development.pdf:application/pdf},
}

@inproceedings{stockhausen_beats_2013,
	address = {Berlin, Heidelberg},
	title = {Beats {Down}: {Using} {Heart} {Rate} for {Game} {Interaction} in {Mobile} {Settings}},
	isbn = {978-3-642-40498-6},
	shorttitle = {Beats {Down}},
	doi = {10.1007/978-3-642-40498-6_41},
	abstract = {Mobile devices allow integration of different sensors, offering new possibilities for interaction. Integrating heart rate into a mobile game offers several possibilities for enhancing gameplay. In our work we implemented a game prototype on a mobile device with different game modes. Increasing and decreasing heart rate is used for game interaction. The mobile scenario allows involving the environment to influence the heart rate. We conducted a first user experience study for evaluation of the integrated interaction methods in mobile scenarios and conclude with our future work.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2013},
	publisher = {Springer},
	author = {Stockhausen, Claudia and Smyzek, Justine and Krömker, Detlef},
	editor = {Kotzé, Paula and Marsden, Gary and Lindgaard, Gitte and Wesson, Janet and Winckler, Marco},
	year = {2013},
	keywords = {Game Interaction, Mobile Games, Physiological Interaction},
	pages = {523--530},
	file = {Full Text PDF:files/721/Stockhausen et al. - 2013 - Beats Down Using Heart Rate for Game Interaction in Mobile Settings.pdf:application/pdf},
}

@inproceedings{bozzon_introduction_2013,
	address = {Berlin, Heidelberg},
	title = {An {Introduction} to {Human} {Computation} and {Games} with a {Purpose}},
	isbn = {978-3-642-39200-9},
	doi = {10.1007/978-3-642-39200-9_48},
	abstract = {Crowdsourcing and human computation are novel disciplines that enable the design of computation processes that include humans as actors for task execution. In such a context, Games With a Purpose are an effective mean to channel, in a constructive manner, the human brainpower required to perform tasks that computers are unable to perform, through computer games. This tutorial introduces the core research questions in human computation, with a specific focus on the techniques required to manage structured and unstructured data. The second half of the tutorial delves into the field of game design for serious task, with an emphasis on games for human computation purposes. Our goal is to provide participants with a wide, yet complete overview of the research landscape; we aim at giving practitioners a solid understanding of the best practices in designing and running human computation tasks, while providing academics with solid references and, possibly, promising ideas for their future research activities.},
	language = {en},
	booktitle = {Web {Engineering}},
	publisher = {Springer},
	author = {Bozzon, Alessandro and Galli, Luca},
	editor = {Daniel, Florian and Dolog, Peter and Li, Qing},
	year = {2013},
	keywords = {Crowdsourcing, Games With a Purpose, Human Computation},
	pages = {514--517},
	file = {Full Text PDF:files/745/Bozzon e Galli - 2013 - An Introduction to Human Computation and Games with a Purpose.pdf:application/pdf},
}

@inproceedings{mcclinton_immersive_2019,
	address = {Cham},
	title = {An {Immersive} {Brain} {Painting}: {The} {Effects} of {Brain} {Painting} in a {Virtual} {Reality} {Environment}},
	isbn = {978-3-030-22419-6},
	shorttitle = {An {Immersive} {Brain} {Painting}},
	doi = {10.1007/978-3-030-22419-6_31},
	abstract = {Brain Painting is a brain-computer interface (BCI) application that allows users to paint on a virtual canvas without requiring physical movement. Brain Painting has shown to improve the Quality of Life of patients with Amyotrophic lateral sclerosis (ALS), by giving the patients ways of expressing themselves and affecting society through art exhibitions. Although there is currently no known cure for ALS, through such outlets, we can help mitigate the physical and psychological impairments. Therefore, this paper discusses a study where users paint with their brain with a BCI in an immersed Virtual environment, also known as Brain Painting. This study evaluates the cognitive workload, presence, and changes in affect through standardized questionnaires. Also, to explore the validity for users to express themselves creatively can lead to a more positive experience using the application in an immersed environment, in comparison to non-immersive.},
	language = {en},
	booktitle = {Augmented {Cognition}},
	publisher = {Springer International Publishing},
	author = {McClinton, Willie and Garcia, Sarah and Andujar, Marvin},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2019},
	keywords = {Brain Painting, Brain-computer interface, Electroencephalography (EEG), Event-related potentials (ERP), P300, Virtual Reality (VR)},
	pages = {436--445},
	file = {Full Text PDF:files/750/McClinton et al. - 2019 - An Immersive Brain Painting The Effects of Brain Painting in a Virtual Reality Environment.pdf:application/pdf},
}

@inproceedings{huang_involving_2007,
	address = {Berlin, Heidelberg},
	title = {Involving {Engineers} in {User} {Research} and {User} {Experience} {Design} of {ICT} for {China}},
	isbn = {978-3-540-73321-8},
	doi = {10.1007/978-3-540-73321-8_46},
	abstract = {Chinese culture and consumer patterns are dramatically different from those in the US and Europe. That greatly impacts the types of products that are relevant to the Chinese market, the product development life cycle and methods by which these products are defined and developed for information and communication technologies (ICT). To address those unique differences, the User Centered Design (UCD) research team at Intel China has developed and refined techniques for involving engineering staff in the early stages of product design, namely user research and experience design. This early involvement has many advantages and improves the overall effectiveness of the product development team. This article describes the role of the engineers in the early phases of the user centered process, and the learnings and challenges that come from this approach. Real-world case studies are used to illustrate the methodologies.},
	language = {en},
	booktitle = {Digital {Human} {Modeling}},
	publisher = {Springer},
	author = {Huang, Chaoyu and He, Huogao},
	editor = {Duffy, Vincent G.},
	year = {2007},
	keywords = {brainstorming, ethnography, field work, ICT, user centered design, user centered research, user experience design},
	pages = {399--408},
	file = {Full Text PDF:files/751/Huang e He - 2007 - Involving Engineers in User Research and User Experience Design of ICT for China.pdf:application/pdf},
}

@inproceedings{vidakis_ludic_2015,
	address = {Cham},
	title = {Ludic {Educational} {Game} {Creation} {Tool}: {Teaching} {Schoolers} {Road} {Safety}},
	isbn = {978-3-319-20684-4},
	shorttitle = {Ludic {Educational} {Game} {Creation} {Tool}},
	doi = {10.1007/978-3-319-20684-4_55},
	abstract = {This paper presents initial findings and ongoing work of the game creation tool, a core component of the IOLAOS(IOLAOS in ancient Greece was a divine hero famed for helping with some of Heracles’s labors.) platform, a general open authorable framework for educational and training games. The game creation tool features a web editor, where the game narrative can be manipulated, according to specific needs. Moreover, this tool is applied for creating an educational game according to a reference scenario namely teaching schoolers road safety. A ludic approach is used both in game creation and play. Helping children staying safe and preventing serious injury on the roads is crucial. In this context, this work presents an augmented version of the IOLAOS architecture including an enhanced game creation tool and a new multimodality module. In addition presents a case study for creating educational games for teaching road safety, by employing ludic interfaces for both the game creator and the game player, as well as ludic game design.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Access} to {Learning}, {Health} and {Well}-{Being}},
	publisher = {Springer International Publishing},
	author = {Vidakis, Nikolas and Syntychakis, Efthymios and Kalafatis, Kostantinos and Christinaki, Eirini and Triantafyllidis, Georgios},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2015},
	keywords = {Educational game, Ludic game design, Open authorable framework, Road safety},
	pages = {565--576},
	file = {Full Text PDF:files/756/Vidakis et al. - 2015 - Ludic Educational Game Creation Tool Teaching Schoolers Road Safety.pdf:application/pdf},
}

@inproceedings{webb_systems_2016,
	address = {Cham},
	title = {A {Systems} {Approach} for {Augmented} {Reality} {Design}},
	isbn = {978-3-319-39952-2},
	doi = {10.1007/978-3-319-39952-2_37},
	abstract = {Effective ways of presenting digital data are needed to augment a user’s experience in the real world without distracting or overloading them. We propose a system of systems approach for the design, development, and evaluation of information presentation devices, particularly augmented reality devices. We developed an evaluation environment that enables the synchronized presentation of multimodal stimuli and collection of user responses in an immersive environment. We leveraged visual, audio, thermal, and tactile information presentation modalities during a navigation and threat identification task. Twelve participants completed the task while response time and accuracy data were collected. Results indicated variability among devices and pairs of devices, and suggested that information presented by some pairs of devices was more effective and easily acted upon than that presented by others. The results of this work provided important guidance regarding future design decisions and suggest the utility of our system of systems approach. Implications and future directions are discussed.},
	language = {en},
	booktitle = {Foundations of {Augmented} {Cognition}: {Neuroergonomics} and {Operational} {Neuroscience}},
	publisher = {Springer International Publishing},
	author = {Webb, Andrea K. and Vincent, Emily C. and Patnaik, Pooja and Schwartz, Jana L.},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2016},
	keywords = {Augmented reality, Immersive environment, Situation awareness, System of systems},
	pages = {382--389},
	file = {Full Text PDF:files/760/Webb et al. - 2016 - A Systems Approach for Augmented Reality Design.pdf:application/pdf},
}

@inproceedings{schuh_usability_2016,
	address = {Cham},
	title = {Usability {Evaluation} of a {Wheelchair} {Virtual} {Simulator} {Controlled} by a {Brain}-{Computer} {Interface}: {Lessons} {Learned} to the {Design} {Process}},
	isbn = {978-3-319-40244-4},
	shorttitle = {Usability {Evaluation} of a {Wheelchair} {Virtual} {Simulator} {Controlled} by a {Brain}-{Computer} {Interface}},
	doi = {10.1007/978-3-319-40244-4_10},
	abstract = {This paper presents the design, implementation and evaluation of a wheelchair simulator, which is controlled by a noninvasive Brain-Computer Interface device. We use the eye blink to control the control interface. Two experiments were conducted to evaluate the Simulator’s utilization quality. The results showed that it is important to have a training phase or eye blink calibration, and a module for recognition of voluntary and involuntary blinking. The adopted scanning system for the wheelchair driving and the collision system were well accepted by the participants.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Interaction} {Techniques} and {Environments}},
	publisher = {Springer International Publishing},
	author = {Schuh, Anderson and de Borba Campos, Marcia and Bez, Marta and Mossmann, João Batista},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2016},
	keywords = {Brain computer interfaces, Human computer interaction, Usability},
	pages = {92--101},
	file = {Full Text PDF:files/764/Schuh et al. - 2016 - Usability Evaluation of a Wheelchair Virtual Simulator Controlled by a Brain-Computer Interface Les.pdf:application/pdf},
}

@inproceedings{bernin_framework_2012,
	address = {Berlin, Heidelberg},
	title = {A {Framework} {Concept} for {Emotion} {Enriched} {Interfaces}},
	isbn = {978-3-642-33542-6},
	doi = {10.1007/978-3-642-33542-6_59},
	abstract = {The work presented outlines my doctoral research in emotion and action based human computer interfaces, with the approach to create an open source framework. The idea is to find the suitable abstractions and interfaces to integrate different approaches to process input data to interpret user’s emotions and actions. Fusing and comparing alternatives for processing will help to find the appropriate solution for a particular need for the next generation of interactive systems. Widening the sensing abilities of computers has the potential to add new degrees of freedom to the design of human computer interaction and ubiquitous entertainment computing.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2012},
	publisher = {Springer},
	author = {Bernin, Arne},
	editor = {Herrlich, Marc and Malaka, Rainer and Masuch, Maic},
	year = {2012},
	keywords = {Current Emotional State, Emotion Recognition, Emotional Model, Human Computer Interface, Open Source Framework},
	pages = {482--485},
	file = {Full Text PDF:files/768/Bernin - 2012 - A Framework Concept for Emotion Enriched Interfaces.pdf:application/pdf},
}

@inproceedings{boaventura_feature-based_2019,
	address = {Cham},
	title = {A {Feature}-{Based} {Approach} to {Develop} {Digital} {Board} {Games}},
	isbn = {978-3-030-34644-7},
	doi = {10.1007/978-3-030-34644-7_14},
	abstract = {Several types of development strategies are available to provide digital games in a reusable way. However, the idea of a “one-size-fits-all” architecture for digital games can be problematic, being preferable to build dedicated architectures for specific game genres. This paper proposes the development of feature-based artifacts for the production of digital board games. It presents a subdomain game architecture that represents configurable features of core concepts related to board games (the game model and controller), and implements feature artifacts capable of being executed in distinct game clients (the game view). For validation purposes, two types of classic board games, together with a proposed web client for board games, were developed, consolidating as a result a software product line approach to develop classic board games.},
	language = {en},
	booktitle = {Entertainment {Computing} and {Serious} {Games}},
	publisher = {Springer International Publishing},
	author = {Boaventura, Filipe M. B. and Sarinho, Victor T.},
	editor = {van der Spek, Erik and Göbel, Stefan and Do, Ellen Yi-Luen and Clua, Esteban and Baalsrud Hauge, Jannicke},
	year = {2019},
	keywords = {Board games, Feature modeling, Software product line},
	pages = {175--186},
	file = {Full Text PDF:files/772/Boaventura e Sarinho - 2019 - A Feature-Based Approach to Develop Digital Board Games.pdf:application/pdf},
}

@inproceedings{duin_using_2013,
	address = {Berlin, Heidelberg},
	title = {Using {Behavioral} {Indicators} to {Assess} {Competences} in a {Sustainable} {Manufacturing} {Learning} {Scenario}},
	isbn = {978-3-642-40352-1},
	doi = {10.1007/978-3-642-40352-1_73},
	abstract = {This paper introduces a learning scenario created for a serious game to develop competences in the domain of sustainable manufacturing, by applying a Lifecycle Assessment (LCA). A set of behavioral indicators is introduced to assess how particular competences do change while the player is engaged in playing the game scenario. It furthermore presents early evaluation results of the game scenario on a sample of master grade students at the University of Bremen.},
	language = {en},
	booktitle = {Advances in {Production} {Management} {Systems}. {Competitive} {Manufacturing} for {Innovative} {Products} and {Services}},
	publisher = {Springer},
	author = {Duin, Heiko and Cerinsek, Gregor and Oliveira, Manuel and Bedek, Michael and Dolinsek, Slavko},
	editor = {Emmanouilidis, Christos and Taisch, Marco and Kiritsis, Dimitris},
	year = {2013},
	keywords = {Behavioural Indicators, Competence-based Learning, Content Development, Lifecycle Assessment (LCA), Serious Game, Sustainable Manufacturing},
	pages = {582--589},
	file = {Full Text PDF:files/795/Duin et al. - 2013 - Using Behavioral Indicators to Assess Competences in a Sustainable Manufacturing Learning Scenario.pdf:application/pdf},
}

@inproceedings{muller_design_2017,
	address = {Cham},
	title = {Design of a {Robotic} {Workmate}},
	isbn = {978-3-319-58463-8},
	doi = {10.1007/978-3-319-58463-8_37},
	abstract = {In the near future, robots and people will work hand in hand. Through technical development, robots will be able to follow social rules, interact and communicate with people and move freely in the environment. The number of these so-called social robots will increase significantly especially in production spaces forming hybrid human-robot-teams. This expected increasing integration of robots in production environments raises questions on how to design an ideal robot for hybrid collaboration. While most of the research focuses on the technical aspects of human-machine interactions, there is still a strong need for research on the psychological and social aspects that influence the cooperation within hybrid teams.},
	language = {en},
	booktitle = {Digital {Human} {Modeling}. {Applications} in {Health}, {Safety}, {Ergonomics}, and {Risk} {Management}: {Ergonomics} and {Design}},
	publisher = {Springer International Publishing},
	author = {Müller, Sarah Luisa and Schröder, Stefan and Jeschke, Sabina and Richert, Anja},
	editor = {Duffy, Vincent G.},
	year = {2017},
	keywords = {Anthropomorphism, Cooperation, Human-robotic interactions, Hybrid human-robot-teams, Industry 4.0, Reactions, Social robotics},
	pages = {447--456},
	file = {Full Text PDF:files/799/Müller et al. - 2017 - Design of a Robotic Workmate.pdf:application/pdf},
}

@inproceedings{dombrowski_virtual_2018,
	address = {Cham},
	title = {Virtual {Reality} {Training} to {Enhance} {Motor} {Skills}},
	isbn = {978-3-319-91581-4},
	doi = {10.1007/978-3-319-91581-4_29},
	abstract = {The use of Virtual Reality (VR) and Augmented Reality (AR) as a healing aid is a relatively newer concept in the field of rehabilitation and training. Clinicians now have access to virtual worlds and games in which they can immerse their patients into interactive scenarios that would not have been possible in previous years.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Interaction}, {Navigation}, {Visualization}, {Embodiment}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Dombrowski, Matt and Buyssens, Ryan and Smith, Peter A.},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Augmented Reality, Game design, Games, Motor skills, Virtual Reality},
	pages = {393--402},
	file = {Full Text PDF:files/802/Dombrowski et al. - 2018 - Virtual Reality Training to Enhance Motor Skills.pdf:application/pdf},
}

@inproceedings{khatri_optimizing_2020,
	address = {Cham},
	title = {Optimizing {Virtual} {Reality} {Eye} {Tracking} {Fixation} {Algorithm} {Thresholds} {Based} on {Shopper} {Behavior} and {Age}},
	isbn = {978-3-030-50729-9},
	doi = {10.1007/978-3-030-50729-9_9},
	abstract = {Eye tracking (ET) is becoming a popular tool to study the consumer behavior. One of the significant problems that arise with ET integrated into 3D virtual reality is defining fixations and saccades, which are essential part of feature extraction in ET analysis and have a critical impact on higher level analysis. In this study, the ET data from 60 subjects, were recorded. To define the fixations, Dispersion Threshold Identification algorithm was used which requires to define several thresholds. Since there are multiple thresholds and extracted features, a Multi-Objective Reinforcement Learning (MORL) algorithm was implemented to solve this problem. The objective of the study was to optimize these thresholds in order to improve accuracy of classification of the age based on different visual patterns undertaken by the subject during shopping in a virtual store. Regarding the nature of the classification, the objective for this optimization problem was to maximize the differences between the averages of each feature in different classes and minimize the variances of the same feature within each class. For the current study, thresholds optimization has shown an improvement in results for the accuracies of classification between age groups after applying the MORL algorithm. In conclusion, the results suggest that the optimization of thresholds is an important factor to improve feature extraction methods and in turn improve the overall results of an ET study involving consumer behavior inside virtual reality. This method can be used to optimize thresholds in similar studies to provide improved accuracy of classification results.},
	language = {en},
	booktitle = {{HCI} {International} 2020 - {Posters}},
	publisher = {Springer International Publishing},
	author = {Khatri, Jaikishan and Moghaddasi, Masoud and Llanes-Jurado, Jose and Spinella, Luciano and Marín-Morales, Javier and Guixeres, Jaime and Alcañiz, Mariano},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2020},
	keywords = {Dispersion, Eye tracking, Multi-objective optimization, Optimization, Reinforcement learning, Threshold, Virtual reality},
	pages = {64--69},
	file = {Full Text PDF:files/811/Khatri et al. - 2020 - Optimizing Virtual Reality Eye Tracking Fixation Algorithm Thresholds Based on Shopper Behavior and.pdf:application/pdf},
}

@inproceedings{gong_what_2018,
	address = {Cham},
	title = {What {Makes} for {Successful} {Game} {Storytelling}? {A} {Model} for {Evaluating} {Game}-{Adaptability} of {Stories} in {China}},
	isbn = {978-3-319-92252-2},
	shorttitle = {What {Makes} for {Successful} {Game} {Storytelling}?},
	doi = {10.1007/978-3-319-92252-2_3},
	abstract = {Storytelling is important for game flow experience, yet there is no accepted model or theory for game developers or investors to use for evaluating the composition, purchase and adaption of game stories. This paper proposes a concise model, the Game Story model, integrating 12 existing models from the fields of scriptwriting, game playability, and game motivation studies, with empirical findings based on content analysis of over 100 replies from Zhihu on topics related to game storytelling and interviews of 6 experienced game players. Factor extraction conducted afterwards through a survey (N = 516) and exploratory factor analysis (EFA) results in a model of 11 factors, with each factor containing a set of detailed criteria. A validation of the model on stories indicates that GameStory is able to explain the high game-adaptability of stories of many popular video games. This interpretation can be applied to the evaluation, understanding and improvement the game-adaptability of stories.},
	language = {en},
	booktitle = {Cross-{Cultural} {Design}. {Applications} in {Cultural} {Heritage}, {Creativity} and {Social} {Development}},
	publisher = {Springer International Publishing},
	author = {Gong, Yun and Wang, Bingcheng and Rau, Pei-Luen Patrick and Huang, Dinglong},
	editor = {Rau, Pei-Luen Patrick},
	year = {2018},
	keywords = {Digital game, Game-adaptability, Storytelling},
	pages = {30--45},
	file = {Full Text PDF:files/814/Gong et al. - 2018 - What Makes for Successful Game Storytelling A Model for Evaluating Game-Adaptability of Stories in.pdf:application/pdf},
}

@inproceedings{pichiliani_brain_2018,
	address = {Cham},
	title = {Brain {Controlled} {Interface} {Log} {Analysis} in {Real} {Time} {Strategy} {Game} {Matches}},
	isbn = {978-3-319-92049-8},
	doi = {10.1007/978-3-319-92049-8_19},
	abstract = {Emotions are an important aspect that affects human interaction with systems and applications. The correlation of emotional and affective state with game interaction data is a relevant issue since it can explain player behavior and the outcome of a digital game match. In this work, we present an initial exploratory study to analyze interaction log data and its correlation with an off-the-shelf Brain Controlled Interface (BCI) that collected excitement in a RTS (Real Time Strategy game). Our results shown moderate correlations with player’s preferences and amount of interactions. Additionally, we also found in the interaction and game logs that character’s choice significantly impacts the time spent in data-driven levels of excitement. We did not find statistically significant differences of excitement for other factors such as player ranking and game style, map, and opponent character.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Methods}, {Technologies}, and {Users}},
	publisher = {Springer International Publishing},
	author = {Pichiliani, Mauro C.},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2018},
	keywords = {Brain Controlled Interface, Log analysis, Video games},
	pages = {256--272},
	file = {Full Text PDF:files/839/Pichiliani - 2018 - Brain Controlled Interface Log Analysis in Real Time Strategy Game Matches.pdf:application/pdf},
}

@inproceedings{lankes_eye_2017,
	address = {Cham},
	title = {Eye {Contact}: {Gaze} as a {Connector} {Between} {Spectators} and {Players} in {Online} {Games}},
	isbn = {978-3-319-66715-7},
	shorttitle = {Eye {Contact}},
	doi = {10.1007/978-3-319-66715-7_34},
	abstract = {This paper proposes an experimental setting that investigates shared gaze integrations (constant gaze and eye contact) in games and their effects on the social presence perceived by different roles (players and spectators) in a remote scenario. In order to get insights, we conducted a study that is made up of 4 different conditions (2 roles and 2 gaze integrations). Results show, depending on the type of the gaze integration and the role, positive effects of gaze towards an increased awareness and engagement among participants. Through the inclusion of shared gaze information, a new nonverbal communication channel for players and spectators is created that presents an interesting design resource for future approaches of digital play. Designers should receive information on how to design gaze-based interfaces that do not distract players during play, and also give spectators the possibility to experience a game via the player’s eyes.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2017},
	publisher = {Springer International Publishing},
	author = {Lankes, Michael and Rammer, Daniel and Maurer, Bernhard},
	editor = {Munekata, Nagisa and Kunita, Itsuki and Hoshino, Junichi},
	year = {2017},
	keywords = {Gaze-based interaction, Shared gaze, Social presence},
	pages = {310--321},
	file = {Full Text PDF:files/842/Lankes et al. - 2017 - Eye Contact Gaze as a Connector Between Spectators and Players in Online Games.pdf:application/pdf},
}

@inproceedings{bentjen_persistent_2018,
	address = {Cham},
	title = {Persistent {Human} {Control} in a {Reservation}-{Based} {Autonomous} {Intersection} {Protocol}},
	isbn = {978-3-030-04537-1},
	doi = {10.1007/978-3-030-04537-1_11},
	abstract = {Widespread use of fully autonomous vehicles is near. However, the desire of human beings to maintain control of their vehicles – even limited control – is unlikely to ever go away. Several protocols (e.g., AIM, SemiAIM and H-AIM) have been developed to safely and efficiently manage reservation-based intersections with a mixture of fully autonomous, semi-autonomous and non-autonomous vehicles. However, these protocols do not incorporate the dynamic of a human maintaining control of a semi-autonomous vehicle when approaching and crossing an intersection. This chapter lays the foundation for the extensions required for human-control of semi-autonomous vehicles, the ultimate goal being a protocol that maintains the efficiency of a fully autonomous environment while allowing human control of vehicles when navigating an intersection. This chapter also proposes information feedback mechanisms for human response, such as displays that provide the intersection arrival time, goal velocity, lane maintaining assistance and other warnings. Additionally, it describes a synthetic environment that enables the testing of intersection protocols that support human interaction.},
	language = {en},
	booktitle = {Critical {Infrastructure} {Protection} {XII}},
	publisher = {Springer International Publishing},
	author = {Bentjen, Karl and Graham, Scott and Nykl, Scott},
	editor = {Staggs, Jason and Shenoi, Sujeet},
	year = {2018},
	keywords = {human control, intersections, reservations, Semi-autonomous vehicles},
	pages = {197--212},
	file = {Full Text PDF:files/850/Bentjen et al. - 2018 - Persistent Human Control in a Reservation-Based Autonomous Intersection Protocol.pdf:application/pdf},
}

@inproceedings{chen_comparison_2020,
	address = {Cham},
	title = {Comparison of {Pedestrians}’ {Gap} {Acceptance} {Behavior} {Towards} {Automated} and {Human}-{Driven} {Vehicles}},
	isbn = {978-3-030-49183-3},
	doi = {10.1007/978-3-030-49183-3_20},
	abstract = {To protect pedestrian safety, automated vehicles can adopt a conservative strategy by yielding to pedestrians in all interactions and external human-machine interface was suggested to convey vehicle intentions to pedestrians. However, automated vehicles also could convey messages to assist existing pedestrians’ road-crossing decision-making, which is another way to ensure pedestrian safety but has generally been neglected. The current study explored the effect of assistance information on pedestrian gap acceptance behavior by presenting three colors similar to a traffic light to indicate the instant safety to cross road. Forty-eight participants completed the gap acceptance task in a virtual reality environment when interacting with human-driven vehicles or automated vehicles in a mixed or non-mixed traffic environment. The results showed that generally pedestrians had similar gap acceptance trends in rejecting small gaps towards two types of vehicles, but are more likely to accepted a large gap when they interacted with automated vehicles. The assistance information helped pedestrians to make safer road-crossing decisions, but whether the two types of vehicles drove in separately or in mixed condition did not affect pedestrian behavior. The null effect of driving context indicates that pedestrians may rely on their legacy strategy of gap acceptance regardless of vehicle type and the assistance information just only had minor effects.},
	language = {en},
	booktitle = {Engineering {Psychology} and {Cognitive} {Ergonomics}. {Cognition} and {Design}},
	publisher = {Springer International Publishing},
	author = {Chen, Wenxiang and Jiang, Qianni and Zhuang, Xiangling and Ma, Guojie},
	editor = {Harris, Don and Li, Wen-Chin},
	year = {2020},
	keywords = {Automated vehicles, Gap acceptance, Pedestrian safety, Time to arrival},
	pages = {253--261},
	file = {Full Text PDF:files/747/Chen et al. - 2020 - Comparison of Pedestrians’ Gap Acceptance Behavior Towards Automated and Human-Driven Vehicles.pdf:application/pdf},
}

@inproceedings{gonzalez_sanchez_usability_2009,
	address = {Berlin, Heidelberg},
	title = {From {Usability} to {Playability}: {Introduction} to {Player}-{Centred} {Video} {Game} {Development} {Process}},
	isbn = {978-3-642-02806-9},
	shorttitle = {From {Usability} to {Playability}},
	doi = {10.1007/978-3-642-02806-9_9},
	abstract = {While video games have traditionally been considered simple entertainment devices, nowadays they occupy a privileged position in the leisure and entertainment market, representing the fastest-growing industry globally. We regard the video game as a special type of interactive system whose principal aim is to provide the player with fun and entertainment. In this paper we will analyse how, in Video Games context, Usability alone is not sufficient to achieve the optimum Player Experience. It needs broadening and deepening, to embrace further attributes and properties that identify and describe the Player Experience. We present our proposed means of defining Playability. We also introduce the notion of Facets of Playability. Each facet will allow us to characterize the Playability easily, and associate them with the different elements of a video game. To guarantee the optimal Player Experience, Playability needs to be assessed throughout the entire video game development process, taking a Player-Centred Video Game Design approach.},
	language = {en},
	booktitle = {Human {Centered} {Design}},
	publisher = {Springer},
	author = {González Sánchez, Jose Luis and Padilla Zea, Natalia and Gutiérrez, Francisco L.},
	editor = {Kurosu, Masaaki},
	year = {2009},
	keywords = {Interactive Systems, Playability, Usability, User Experience, Video Games},
	pages = {65--74},
	file = {Full Text PDF:files/753/González Sánchez et al. - 2009 - From Usability to Playability Introduction to Player-Centred Video Game Development Process.pdf:application/pdf},
}

@inproceedings{grajewski_examination_2018,
	address = {Cham},
	title = {Examination of {Effectiveness} of a {Performed} {Procedural} {Task} {Using} {Low}-{Cost}  {Peripheral} {Devices} in {VR}},
	isbn = {978-3-319-91581-4},
	doi = {10.1007/978-3-319-91581-4_30},
	abstract = {The paper presents a Virtual Reality (VR) training system dedicated for interactive course focused on acquisition of competences in the field of manual procedural tasks. It was developed as a response for the growing market demand for low-cost VR systems supporting industrial training. A scenario for the implementation of an elementary manual operation (modified peg-in-hole task) was developed. The aim of the test was to show whether the prepared solution (along with peripheral devices) can be an effective tool for training the activities performed at the production site. The procedural task was performed by specific test groups using various peripheral devices. The paper presents preliminary results of tests regarding evaluation of effectiveness of virtual training, depending on specific peripheral devices used.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Interaction}, {Navigation}, {Visualization}, {Embodiment}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Grajewski, Damian and Buń, Paweł and Górski, Filip},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Haptic feedback, Interaction devices, Virtual reality training},
	pages = {403--415},
	file = {Full Text PDF:files/757/Grajewski et al. - 2018 - Examination of Effectiveness of a Performed Procedural Task Using Low-Cost  Peripheral Devices in VR.pdf:application/pdf},
}

@inproceedings{smeddinck_hoverboard_2017,
	address = {Cham},
	title = {Hoverboard: {A} {Leap} to the {Future} of {Locomotion} in {VR}!?},
	isbn = {978-3-319-66715-7},
	shorttitle = {Hoverboard},
	doi = {10.1007/978-3-319-66715-7_24},
	abstract = {Locomotion in virtual reality (VR) remains challenging due to limitations of common input methods. Sedentary input devices may endanger immersion, real-to-virtual world perception dissonance can lead to simulator sickness, and physical input devices such as framed walking dishes are often complex and expensive. We present a low-cost, easy to use, easy to manufacture, and easily portable device for locomotion in VR based on a hoverboard metaphor. Building on related work and our own iterative VR locomotion system designs we hypothesize that hoverboarding can provide a compelling and intuitive method for short- and long-distance locomotion in VR with a potential to reduce simulator sickness due to consistent and stable locomotion that corresponds well to the physical proprioception of the users while navigating VR. We discuss design iterations of our device prototypes, promising results from an early explorative evaluation, as well as ongoing continued work.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2017},
	publisher = {Springer International Publishing},
	author = {Smeddinck, Jan and Alexandrovsky, Dmitry and Wenig, Dirk and Zimmer, Michel and Wegele, Waldemar and Juergens, Sylvia and Malaka, Rainer},
	editor = {Munekata, Nagisa and Kunita, Itsuki and Hoshino, Junichi},
	year = {2017},
	keywords = {Gaming, Hoverboard, Leaning, Locomotion, Motion-based control, Natural user interface, Virtual Reality (VR), Whole-body interface},
	pages = {218--225},
	file = {Full Text PDF:files/761/Smeddinck et al. - 2017 - Hoverboard A Leap to the Future of Locomotion in VR!.pdf:application/pdf},
}

@inproceedings{williams_augmented_2020,
	address = {Cham},
	title = {Augmented {Reality} for {City} {Planning}},
	isbn = {978-3-030-49695-1},
	doi = {10.1007/978-3-030-49695-1_17},
	abstract = {We present an early study designed to analyze how city planning and the health of senior citizens can benefit from the use of augmented reality (AR) with assistance of virtual reality (VR), using Microsoft’s HoloLens and HTC’s Vive headsets. We also explore whether AR and VR can be used to help city planners receive real-time feedback from citizens, such as the elderly, on virtual plans, allowing for informed decisions to be made before any construction begins. In doing so, city planners can more clearly understand what design features would motivate senior citizens to visit or exercise in future parks, for example. The study was conducted on 10 participants 60 years and older who live within 2 miles from the site. They were presented with multiple virtual options for a prospective park, such as different walls for cancelling highway noise, as well as benches, lampposts, bathroom pods, walking and biking lanes, and other street furniture. The headsets allowed the participants to clearly visualize the options and make choices on them. Throughout the study the participants were enthusiastic about using the AR and VR devices, which is noteworthy for a future where city planning is done with these technologies.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Design} and {Interaction}},
	publisher = {Springer International Publishing},
	author = {Williams, Adam Sinclair and Angelini, Catherine and Kress, Mathew and Ramos Vieira, Edgar and D’Souza, Newton and Rishe, Naphtali D. and Medina, Joseph and Özer, Ebru and Ortega, Francisco},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Augmented and virtual reality, Human-centered computing, Visualization design and evaluation methods},
	pages = {256--271},
	file = {Full Text PDF:files/765/Williams et al. - 2020 - Augmented Reality for City Planning.pdf:application/pdf},
}

@inproceedings{shin_steam-x_2020,
	address = {Cham},
	title = {{STEAM}-{X}: {An} {Exploratory} {Study} {Adding} {Interactive} {Physical} {Activity} to the {STEAM} {Model}},
	isbn = {978-3-030-50513-4},
	shorttitle = {{STEAM}-{X}},
	doi = {10.1007/978-3-030-50513-4_14},
	abstract = {The philosophy of holistic education, which emphasizes a balance between physical and cognitive development, has been widely considered important for many years. However, Science, Technology, Engineering, Arts, and Mathematics (STEAM) education, which is currently being promoted worldwide, is moving away from the holistic education model, causing an imbalance between subjects due to its heavy emphasis on science and mathematics. This paper proposes a STEAM-X education model that adds physical activity to the STEAM model for the purpose of providing a modern holistic education that includes physical, cognitive, and mental learning and promotes creative and convergent thinking. In this study, we reviewed the existing literature concerning the benefits of physical activity, student preferences for physical activity, and the link between physical activity and STEAM education. Based on this review, we derived four research questions and conducted experimental research. We selected the appropriate tools for STEAM-X education, collected data on their implementation through observation and user interviews, and received positive responses from students and parents about STEAM-X education. The STEAM-X model appears capable of influencing students’ body reaction speeds, body coordination, concentration, problem-solving abilities, creativity, sequential thinking, strategic thinking, collaboration skills, and ethical attitudes. Implementing the STEAM-X education program would require creating additional plans for safety, developing content for instruction in conjunction with other subjects, and providing support for teachers, such as operational guides and manuals.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Designing}, {Developing} and {Deploying} {Learning} {Experiences}},
	publisher = {Springer International Publishing},
	author = {Shin, Jina and Heo, Jeongyun},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2020},
	keywords = {Interactive game, Mixed reality, STEAM, STEAM-X},
	pages = {179--193},
	file = {Full Text PDF:files/769/Shin e Heo - 2020 - STEAM-X An Exploratory Study Adding Interactive Physical Activity to the STEAM Model.pdf:application/pdf},
}

@inproceedings{tan_multitouch_2019,
	address = {Cham},
	title = {A {Multitouch} {Drawing} {Application} with {Occlusion}-{Free} {Interaction} {Strategies}},
	isbn = {978-3-030-29390-1},
	doi = {10.1007/978-3-030-29390-1_32},
	abstract = {Increasing number of desktop applications are becoming available on smartphones and tablets today with multitouch capabilities, allowing the users’ fingers to perform sophisticated or fine-grained interactivities. However, finger occlusion and imprecision continue to limit the performance of multitouch interactions. Quite a few studies proposed the ways to address this issue, and some of them are now used in commonly encountered situations such as text editing. Many occlusion-avoiding techniques used today focus on initial target acquisition step of touch interaction (e.g. accurately selecting an item or touching a desired starting point in drawing a line), having possible consequences to any further intended action (e.g. dragging the selected item to a different location or drawing a line on the canvas). In order to better understand the influence of finger occlusion-free techniques on other parts of the overall interactions, in this paper we report a full-fledged sketch app that incorporates combinations of basic target acquisition features. As the app is a full-featured, end-to-end tablet prototype, such usability issues can be more readily revealed and discussed in the context of realistic drawing situations.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2019},
	publisher = {Springer International Publishing},
	author = {Tan, Yi Ren and Lee, Hyowon and Ko, Insuk and Zhang, Dongliang and Li, Chunxiao},
	editor = {Lamas, David and Loizides, Fernando and Nacke, Lennart and Petrie, Helen and Winckler, Marco and Zaphiris, Panayiotis},
	year = {2019},
	keywords = {Finger occlusion, Multitouch interaction, Pointing precision},
	pages = {535--539},
	file = {Full Text PDF:files/773/Tan et al. - 2019 - A Multitouch Drawing Application with Occlusion-Free Interaction Strategies.pdf:application/pdf},
}

@inproceedings{mourouzis_touchless_2015,
	address = {Cham},
	title = {Touchless {Text} {Entry} for {All}: {Initial} {Design} {Considerations} and {Prototypes}},
	isbn = {978-3-319-20681-3},
	shorttitle = {Touchless {Text} {Entry} for {All}},
	doi = {10.1007/978-3-319-20681-3_4},
	abstract = {In this paper, the foundations for a new touchless text entry method are set. The method in question is based on hand tracking for interacting with a novel virtual keyboard that has been designed to support adaptive text entry for all, including for users with disabilities or users of various assistive technologies. The virtual keyboard in question, which can be graphical or even imaginary (i.e., implied, but not visualised), implements a hierarchical selection approach for the act of writing, which involves character selection, text navigation and text modification. The proposed structure guarantees minimum keystrokes per character, and allows for user-friendly adaptations for offering accessibility, personalisation and increased performance rates for diverse input settings or display preferences. An implementation of the text entry method in question, using the LEAP, a novel 3D motion controller, is introduced here. Along with the fundamental specifications of the proposed keyboard and the required functionality of a software mechanism for initialising, personalising and interacting with it in any given context, this paper presents two prototypes currently under development for one- and ten-finger touchless text entry with LEAP, which are considered in view of developing a universal text input solution that could overshadow all past technologies.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Access} to {Interaction}},
	publisher = {Springer International Publishing},
	author = {Mourouzis, Alexandros and Arfaras, Giorgos and Kilintzis, Vassilis and Chouvarda, Ioanna and Maglaveras, Nicos},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2015},
	keywords = {3D interaction, LEAP, Soft keyboard, Text entry, Universal access},
	pages = {37--49},
	file = {Full Text PDF:files/797/Mourouzis et al. - 2015 - Touchless Text Entry for All Initial Design Considerations and Prototypes.pdf:application/pdf},
}

@inproceedings{fan_collaborative_2018,
	address = {Cham},
	title = {A {Collaborative} {Virtual} {Game} to {Support} {Activity} and {Social} {Engagement} for {Older} {Adults}},
	isbn = {978-3-319-92049-8},
	doi = {10.1007/978-3-319-92049-8_14},
	abstract = {Many older adults suffer from Alzheimer’s disease or other dementias and have affected cognitive abilities. In general, physical exercise, cognitive stimulation, and social engagement have been found to be beneficial for the physical and mental health of older adults with and without cognitive impairment. In an effort to address these needs, researchers have been developing human-machine interaction (HMI) systems to administer activity-oriented therapies. However, most of these system, while promising, focus on one-on-one interaction with the computer and thus do not support social engagement by involving multiple older adults. In this paper, we present the design and development of a motion-based collaborative virtual environment (CVE) application to support both activity and social engagement. The CVE task is based on a book-sorting activity and has embedded collaborative components to encourage human-human interaction (HHI). The system records quantitative data regarding users’ performance, interaction frequency, and social interaction. A preliminary user study was conducted to validate system usability and test on older adults’ tolerance and acceptance of the motion-based user interface (UI) as well as the CVE task. The results showed the usability of the motion-based UI and system capability to assess HMI and HHI from recorded quantitative data. The results from post-test and analysis of audio files indicated that the system might be potentially useful. More user study and data analysis need to be conducted to further investigate the CVE system.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Methods}, {Technologies}, and {Users}},
	publisher = {Springer International Publishing},
	author = {Fan, Jing and Beuscher, Linda and Newhouse, Paul and Mion, Lorraine C. and Sarkar, Nilanjan},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2018},
	keywords = {Collaborative virtual environment (CVE), Dementia, Elder care, Human-Human interaction (HHI), Human-Machine interaction (HMI)},
	pages = {192--204},
	file = {Full Text PDF:files/804/Fan et al. - 2018 - A Collaborative Virtual Game to Support Activity and Social Engagement for Older Adults.pdf:application/pdf},
}

@inproceedings{chen_immersive_2020,
	address = {Cham},
	title = {An {Immersive} {Virtual} {Reality} {Exergame} for {People} with {Parkinson}’s {Disease}},
	isbn = {978-3-030-58796-3},
	doi = {10.1007/978-3-030-58796-3_18},
	abstract = {Parkinson’s disease is a neurodegenerative disorder that affects primarily motor system. Physical exercise is considered important for people with Parkinson’s disease (PD) to slow down disease progression and maintain abilities and quality of life. However, people with PD often experience barriers to exercises that causes low-level adherence to exercise plans and programs. Virtual Reality (VR) is an innovative and promising technology for motor and cognitive rehabilitation. Immersive VR exergames have potential advantages by allowing for individualized skill practice in a motivating interactive environment without distractions from outside events. This paper presents an immersive virtual reality (VR) exergame aiming at motor training on fingers and hand-and-eye coordination. The results from the usability study indicate that immersive VR exergames have potential to provide motivating and engaging physical exercise for people with PD. Through this research, we hope to contribute to evidence-based design principles for task-specific immersive VR exergames for patients with Parkinson’s Disease.},
	language = {en},
	booktitle = {Computers {Helping} {People} with {Special} {Needs}},
	publisher = {Springer International Publishing},
	author = {Chen, Weiqin and Bang, Martin and Krivonos, Daria and Schimek, Hanna and Naval, Arnau},
	editor = {Miesenberger, Klaus and Manduchi, Roberto and Covarrubias Rodriguez, Mario and Peňáz, Petr},
	year = {2020},
	keywords = {Exergame, Immersive virtual reality, Parkinson’s disease},
	pages = {138--145},
	file = {Full Text PDF:files/807/Chen et al. - 2020 - An Immersive Virtual Reality Exergame for People with Parkinson’s Disease.pdf:application/pdf},
}

@inproceedings{jayender_novel_2018,
	address = {Cham},
	title = {A {Novel} {Mixed} {Reality} {Navigation} {System} for {Laparoscopy} {Surgery}},
	isbn = {978-3-030-00937-3},
	doi = {10.1007/978-3-030-00937-3_9},
	abstract = {OBJECTIVE: To design and validate a novel mixed reality head-mounted display for intraoperative surgical navigation. DESIGN: A mixed reality navigation for laparoscopic surgery (MRNLS) system using a head mounted display (HMD) was developed to integrate the displays from a laparoscope, navigation system, and diagnostic imaging to provide context-specific information to the surgeon. Further, an immersive auditory feedback was also provided to the user. Sixteen surgeons were recruited to quantify the differential improvement in performance based on the mode of guidance provided to the user (laparoscopic navigation with CT guidance (LN-CT) versus mixed reality navigation for laparoscopic surgery (MRNLS)). The users performed three tasks: (1) standard peg transfer, (2) radiolabeled peg identification and transfer, and (3) radiolabeled peg identification and transfer through sensitive wire structures. RESULTS: For the more complex task of peg identification and transfer, significant improvements were observed in time to completion, kinematics such as mean velocity, and task load index subscales of mental demand and effort when using the MRNLS (p {\textless} 0.05) compared to the current standard of LN-CT. For the final task of peg identification and transfer through sensitive structures, time taken to complete the task and frustration were significantly lower for MRNLS compared to the LN-CT approach. CONCLUSIONS: A novel mixed reality navigation for laparoscopic surgery (MRNLS) has been designed and validated. The ergonomics of laparoscopic procedures could be improved while minimizing the necessity of additional monitors in the operating room.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Jayender, Jagadeesan and Xavier, Brian and King, Franklin and Hosny, Ahmed and Black, David and Pieper, Steve and Tavakkoli, Ali},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor},
	year = {2018},
	keywords = {Audio navigation, Ergonomics, Laparoscopy surgery, Mixed-reality, Surgical navigation, Visual navigation},
	pages = {72--80},
	file = {Full Text PDF:files/812/Jayender et al. - 2018 - A Novel Mixed Reality Navigation System for Laparoscopy Surgery.pdf:application/pdf},
}

@inproceedings{bozgeyikli_virtual_2018,
	address = {Cham},
	title = {Virtual {Reality} {Interaction} {Techniques} for {Individuals} with {Autism} {Spectrum} {Disorder}},
	isbn = {978-3-319-92052-8},
	doi = {10.1007/978-3-319-92052-8_6},
	abstract = {Virtual reality (VR) systems are seeing growing use for training individuals with Autism Spectrum Disorder (ASD). Although these systems indicate effective use of VR for training, there is little work in the literature evaluating different VR interaction techniques for this audience. In this paper, different VR interaction techniques are explored in the Virtual Reality for Vocational Rehabilitation (VR4VR) system and additional data analysis on top of our previously published preliminary results [1] was performed via a user study with nine individuals with ASD and ten neurotypical individuals. The participants tried six vocational training modules of the VR4VR system. In these modules, tangible object manipulation, haptic device, touch and snap and touchscreen were tested for object selection and manipulation; real walking and walk-in-place were tested for locomotion; and head mounted display and curtain screen were tested for display. Touchscreen and tangible interaction methods were preferred by the individuals with ASD. The walk-in-place locomotion technique were found frustrating and difficult to perform by the individuals with ASD. Curtain display received higher preference scores from individuals with ASD although they accepted the HMD as well. The observations and findings of the study are expected to give insight into the poorly explored area of experience of individuals with ASD with various interaction techniques in VR.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Virtual}, {Augmented}, and {Intelligent} {Environments}},
	publisher = {Springer International Publishing},
	author = {Bozgeyikli, Evren and Bozgeyikli, Lal “Lila” and Alqasemi, Redwan and Raij, Andrew and Katkoori, Srinivas and Dubey, Rajiv},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2018},
	keywords = {Autism spectrum disorder, Interaction techniques, Virtual reality, Vocational rehabilitation},
	pages = {58--77},
	file = {Full Text PDF:files/838/Bozgeyikli et al. - 2018 - Virtual Reality Interaction Techniques for Individuals with Autism Spectrum Disorder.pdf:application/pdf},
}

@inproceedings{doroudian_creating_2018,
	address = {Cham},
	title = {Creating an {Online} {Escape} {Room} {Game} for {Older} {Adults}: {Needs} {Assessment}, {Design} {Process}, and {Usability} {Testing}},
	isbn = {978-3-319-92037-5},
	shorttitle = {Creating an {Online} {Escape} {Room} {Game} for {Older} {Adults}},
	doi = {10.1007/978-3-319-92037-5_36},
	abstract = {In this project, an online escape game for older adults was developed collaboratively with five novice developers and 12 older adults who tested the prototype game at several stages of the development process. We followed a User-Centered Design (UCD) process to develop the game. During the design process, older adults tested the game and it was refined based on their feedback and the research team’s observation of their play. We found that incorporating older adults into the design process significantly improved the design, as they provided insights that would not have been available otherwise. We also concluded that the older adults’ involvement in the design process could make the game better tailored to their needs. The game is currently being field-tested.},
	language = {en},
	booktitle = {Human {Aspects} of {IT} for the {Aged} {Population}. {Applications} in {Health}, {Assistance}, and {Entertainment}},
	publisher = {Springer International Publishing},
	author = {Doroudian, Amir and Hausknecht, Simone and Kaufman, David},
	editor = {Zhou, Jia and Salvendy, Gavriel},
	year = {2018},
	keywords = {Escape room, Need assessment, Older adults, User-Centered Design},
	pages = {516--525},
	file = {Full Text PDF:files/843/Doroudian et al. - 2018 - Creating an Online Escape Room Game for Older Adults Needs Assessment, Design Process, and Usabilit.pdf:application/pdf},
}

@inproceedings{nunnally_indirect_2011,
	address = {Berlin, Heidelberg},
	title = {An {Indirect} {Measure} of the {Implicit} {Level} of {Presence} in {Virtual} {Environments}},
	isbn = {978-3-642-22021-0},
	doi = {10.1007/978-3-642-22021-0_38},
	abstract = {Virtual Environments (VEs) are a common occurrence for many computer users. Considering their spreading usage and speedy development it is ever more important to develop methods that capture and measure key aspects of a VE, like presence. One of the main problems with measuring the level of presence in VEs is that the users may not be consciously aware of its affect. This is a problem especially for direct measures that rely on questionnaires and only measure the perceived level of presence explicitly. In this paper we develop and validate an indirect measure for the implicit level of presence of users, based on the physical reaction of users to events in the VE. The addition of an implicit measure will enable us to evaluate and compare VEs more effectively, especially with regard to their main function as immersive environments. Our approach is practical, cost-effective and delivers reliable results.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality} - {New} {Trends}},
	publisher = {Springer},
	author = {Nunnally, Steven and Bouchard, Durell},
	editor = {Shumaker, Randall},
	year = {2011},
	keywords = {Indirect Implicit Measure, Presence, Virtual Environments},
	pages = {345--353},
	file = {Full Text PDF:files/846/Nunnally e Bouchard - 2011 - An Indirect Measure of the Implicit Level of Presence in Virtual Environments.pdf:application/pdf},
}

@inproceedings{kitsikidis_game-like_2015,
	address = {Cham},
	title = {A {Game}-like {Application} for {Dance} {Learning} {Using} a {Natural} {Human} {Computer} {Interface}},
	isbn = {978-3-319-20684-4},
	doi = {10.1007/978-3-319-20684-4_46},
	abstract = {Game-based learning and gamification techniques are recently becoming a popular trend in the field of Technology Enhanced Learning. In this paper, we mainly focus on the use of game design elements for the transmission of Intangible Cultural Heritage (ICH) knowledge and, especially, for the learning of traditional dances. More specifically, we present a 3D game environment that employs an enjoyable natural human computer interface, which is based on the fusion of multiple depth sensors data in order to capture the body movements of the user/learner. In addition, the system automatically assesses the learner’s performance by utilizing a combination of Dynamic Time Warping (DTW) with Fuzzy Inference System (FIS) approach and provides feedback in a form of a score as well as instructions from a virtual tutor in order to promote self-learning. As a pilot use case, a Greek traditional dance, namely Tsamiko, has been selected. Preliminary small-scaled experiments with students of the Department of Physical Education and Sports Science at Aristotle University of Thessaloniki have shown the great potential of the proposed application.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Access} to {Learning}, {Health} and {Well}-{Being}},
	publisher = {Springer International Publishing},
	author = {Kitsikidis, Alexandros and Dimitropoulos, Kosmas and Uğurca, Deniz and Bayçay, Can and Yilmaz, Erdal and Tsalakanidou, Filareti and Douka, Stella and Grammalidis, Nikos},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2015},
	keywords = {Dance performance evaluation, Natural human computer interface, Traditional dances},
	pages = {472--482},
	file = {Full Text PDF:files/847/Kitsikidis et al. - 2015 - A Game-like Application for Dance Learning Using a Natural Human Computer Interface.pdf:application/pdf},
}

@inproceedings{huang_method_2018,
	address = {Cham},
	title = {A {Method} of {Evaluating} {User} {Visual} {Attention} to {Moving} {Objects} in {Head} {Mounted} {Virtual} {Reality}},
	isbn = {978-3-319-91797-9},
	doi = {10.1007/978-3-319-91797-9_29},
	abstract = {Virtual reality games/films/applications bring new challenges to conventional film grammar and design principles, due to more spatial freedom available to users in 6-DOF Head-Mounted Display (HMD). This paper introduces a simple model of viewers’ visual attention in environment of virtual reality while watching randomly generated moving objects. The model is based on a dataset collected from 10 users in a 50-seconds-long virtual reality experience on HTC Vive. In this paper, we considered three factors as major parameters affecting audiences’ attention: the distance between object and the viewer, the speed of objects movement, and the direction of object towards. We hope the research result is useful to immersive film directors and VR game designers in the future.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Theory} and {Practice}},
	publisher = {Springer International Publishing},
	author = {Huang, Shi},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2018},
	keywords = {Focus of attention, Immersive film, Virtual reality, VR experience, VR game},
	pages = {406--416},
	file = {Full Text PDF:files/852/Huang - 2018 - A Method of Evaluating User Visual Attention to Moving Objects in Head Mounted Virtual Reality.pdf:application/pdf},
}

@inproceedings{moghaddasi_segmentation_2020,
	address = {Cham},
	title = {Segmentation of {Areas} of {Interest} {Inside} a {Virtual} {Reality} {Store}},
	isbn = {978-3-030-50729-9},
	doi = {10.1007/978-3-030-50729-9_13},
	abstract = {In consumer behavior studies, several signals like head position and eye-tracking, which are mostly unstructured, are recorded. Hence, the first step in these studies is to extract structured features. In feature extraction, segmenting the space into several Areas of Interests (AOI) can be beneficial. In this regard, these features are computed when the shopper is inside a specific zone or interacting with or looking at a specific area. One of the difficulties of this approach is defining AOIs. In this study, positional and eye-tracking data of 57 subjects were recorded in a virtual reality store using a Head Mounted Display. Each subject performed a free navigation task and the objective of the study was to classify the shoppers based on their genders. For this purpose, some AOI-based features were extracted from the behavioral data. The AOIs were cubic and defined with rectangles in zenithal perspective and the shelves levels in virtual store. Sizes of horizontal rectangles were then optimized using Genetic Algorithm (GA). In optimization, a cost function based on Fisher criterion is defined to maximize the linear separability between classes. After optimization, the features extracted with the optimized and several arbitrary AOIs are classified with Support Vector Machine method. The results show that gender classification accuracy with optimized AOIs is 85\% and outperforms that of the other AOIs. Along with the outstanding results in this study, this methodology is capable of tuning other hyperparameters like navigational thresholds in classification problems.},
	language = {en},
	booktitle = {{HCI} {International} 2020 - {Posters}},
	publisher = {Springer International Publishing},
	author = {Moghaddasi, Masoud and Khatri, Jaikishan and Llanes-Jurado, Jose and Spinella, Luciano and Marín-Morales, Javier and Guixeres, Jaime and Alcañiz, Mariano},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2020},
	keywords = {Area of interest, Genetic algorithm, Optimization, Virtual reality, Zone of interest},
	pages = {92--98},
	file = {Full Text PDF:files/853/Moghaddasi et al. - 2020 - Segmentation of Areas of Interest Inside a Virtual Reality Store.pdf:application/pdf},
}

@inproceedings{sanz_aznar_methodology_2020,
	address = {Cham},
	title = {Methodology for {Detection} of {ERD}/{ERS} {EEG} {Patterns} {Produced} by {Cut} {Events} in {Film} {Fragments}},
	isbn = {978-3-030-50353-6},
	doi = {10.1007/978-3-030-50353-6_12},
	abstract = {The goal of this communication is to create a framework to isolate the neural reactions registered through EEG as a consequence of a specific input, among all those caused by an audiovisual. In order to do that, we analysed the neuronal register of the power change reactions related to specific cinematographic techniques, in this case the shot change by cut.},
	language = {en},
	booktitle = {Augmented {Cognition}. {Theoretical} and {Technological} {Approaches}},
	publisher = {Springer International Publishing},
	author = {Sanz Aznar, Javier and Aguilar-Paredes, Carlos and Sánchez-Gómez, Lydia and Bruni, Luis Emilio and Wulff-Abramsson, Andreas},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2020},
	keywords = {Audiovisual, EEG, ERD/ERS, Film, Isolate inputs, Methodology, Multiple inputs, Neurocinematics, Permutation test, Power change, Shot change, Slope analysis, Spearman},
	pages = {157--175},
	file = {Full Text PDF:files/854/Sanz Aznar et al. - 2020 - Methodology for Detection of ERDERS EEG Patterns Produced by Cut Events in Film Fragments.pdf:application/pdf},
}

@inproceedings{miura_designing_2016,
	address = {Cham},
	title = {Designing {Affordances} for {Virtual} {Reality}-{Based} {Services} with {Natural} {User} {Interaction}},
	isbn = {978-3-319-40406-6},
	doi = {10.1007/978-3-319-40406-6_25},
	abstract = {The progress of new technologies makes virtual reality (VR) easier, and inexpensive head-mounted displays (HMDs) accelerate the development of advanced VR services. The advantage of the technologies is to offer the better immersion of a VR world, to use natural user interface (NUI) devices and to operate them with a user’s gesture. The problem to use NUI devices for VR-based services is to take into account various types of NUI devices. We need to consider how a user to use NUI devices in a proper way, in particular, when an HMD limits his/her view of the real world. Our approach to solve the problem is to use an affordance that offers implicit information how to navigate VR-based services. However, there are a little researches to investigate the relationship between different types of NUI devices and the design of proper affordances to navigate VR-based services. The paper provides some insights how respective types of NUI devices influence affordance design to navigate VR-based services. For extracting effective insights, we have developed two VR-based services, where we discuss two types of operating methods and three types of affordances for respective operating methods are examined. Also, we chose two types of NUI devices for navigating the VR-based services. Then, we conducted some experiments to extract some insights as a guideline to develop future VR-based services. The result shows that the differences among respective NUI devices may not significantly influence affordance design, however have strong effects on understanding how to navigate VR-based services. The understanding also affects how each user prefers which NUI devices, because NUI devices require us to use gesture to navigate the services, but the intuition that differs in each individual is important.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Technological} {Contexts}},
	publisher = {Springer International Publishing},
	author = {Miura, Takayuki and Yoshii, Akihito and Nakajima, Tatsuo},
	editor = {Marcus, Aaron},
	year = {2016},
	keywords = {Affordance, Natural user interface, Virtual reality},
	pages = {266--277},
	file = {Full Text PDF:files/857/Miura et al. - 2016 - Designing Affordances for Virtual Reality-Based Services with Natural User Interaction.pdf:application/pdf},
}

@inproceedings{chen_user_2019,
	address = {Cham},
	title = {User {Experience} and {Map} {Design} for {Wayfinding} in a {Virtual} {Environment}},
	isbn = {978-3-030-22649-7},
	doi = {10.1007/978-3-030-22649-7_10},
	abstract = {Virtual environment (VE) has been developed rapidly in recent years. The level of complexity regarding the user interface in VEs has also increased. Users’ performance in VEs can be affected by the field of vision, screen size, operation mode, individual difference, and other factors. While little research has been conducted on the effects of user experience and map design on wayfinding in VEs. The experiment is 2 × 2 between-subject design. Participants needed to complete three wayfinding tasks and fill out questionnaires regarding satisfaction, preference, and System Usability Scale (SUS). Forty participants were invited using convenient sampling method. The results are as follows: (1) In terms of the map design, participants performed significantly better by using the semi-transparent map than the opaque map in a difficult task. (2) In terms of the user experience, the results generated from the SUS questionnaire showed that experienced users had a significantly better subjective evaluation of interface usability than inexperienced users.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Information} in {Intelligent} {Systems}},
	publisher = {Springer International Publishing},
	author = {Chen, Meng-Xi and Chen, Chien-Hsiung},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2019},
	keywords = {Map design, User experience, Virtual environment, Wayfinding},
	pages = {117--126},
	file = {Full Text PDF:files/859/Chen e Chen - 2019 - User Experience and Map Design for Wayfinding in a Virtual Environment.pdf:application/pdf},
}

@inproceedings{sanz_aznar_methodology_2020-1,
	address = {Cham},
	title = {Methodology for {Detection} of {ERD}/{ERS} {EEG} {Patterns} {Produced} by {Cut} {Events} in {Film} {Fragments}},
	isbn = {978-3-030-50353-6},
	doi = {10.1007/978-3-030-50353-6_12},
	abstract = {The goal of this communication is to create a framework to isolate the neural reactions registered through EEG as a consequence of a specific input, among all those caused by an audiovisual. In order to do that, we analysed the neuronal register of the power change reactions related to specific cinematographic techniques, in this case the shot change by cut.},
	language = {en},
	booktitle = {Augmented {Cognition}. {Theoretical} and {Technological} {Approaches}},
	publisher = {Springer International Publishing},
	author = {Sanz Aznar, Javier and Aguilar-Paredes, Carlos and Sánchez-Gómez, Lydia and Bruni, Luis Emilio and Wulff-Abramsson, Andreas},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2020},
	keywords = {Audiovisual, EEG, ERD/ERS, Film, Isolate inputs, Methodology, Multiple inputs, Neurocinematics, Permutation test, Power change, Shot change, Slope analysis, Spearman},
	pages = {157--175},
	file = {Full Text PDF:files/798/Sanz Aznar et al. - 2020 - Methodology for Detection of ERDERS EEG Patterns Produced by Cut Events in Film Fragments.pdf:application/pdf},
}

@inproceedings{gonzalez_sanchez_usability_2009-1,
	address = {Berlin, Heidelberg},
	title = {From {Usability} to {Playability}: {Introduction} to {Player}-{Centred} {Video} {Game} {Development} {Process}},
	isbn = {978-3-642-02806-9},
	shorttitle = {From {Usability} to {Playability}},
	doi = {10.1007/978-3-642-02806-9_9},
	abstract = {While video games have traditionally been considered simple entertainment devices, nowadays they occupy a privileged position in the leisure and entertainment market, representing the fastest-growing industry globally. We regard the video game as a special type of interactive system whose principal aim is to provide the player with fun and entertainment. In this paper we will analyse how, in Video Games context, Usability alone is not sufficient to achieve the optimum Player Experience. It needs broadening and deepening, to embrace further attributes and properties that identify and describe the Player Experience. We present our proposed means of defining Playability. We also introduce the notion of Facets of Playability. Each facet will allow us to characterize the Playability easily, and associate them with the different elements of a video game. To guarantee the optimal Player Experience, Playability needs to be assessed throughout the entire video game development process, taking a Player-Centred Video Game Design approach.},
	language = {en},
	booktitle = {Human {Centered} {Design}},
	publisher = {Springer},
	author = {González Sánchez, Jose Luis and Padilla Zea, Natalia and Gutiérrez, Francisco L.},
	editor = {Kurosu, Masaaki},
	year = {2009},
	keywords = {Interactive Systems, Playability, Usability, User Experience, Video Games},
	pages = {65--74},
	file = {Full Text PDF:files/800/González Sánchez et al. - 2009 - From Usability to Playability Introduction to Player-Centred Video Game Development Process.pdf:application/pdf},
}

@inproceedings{moghaddasi_segmentation_2020-1,
	address = {Cham},
	title = {Segmentation of {Areas} of {Interest} {Inside} a {Virtual} {Reality} {Store}},
	isbn = {978-3-030-50729-9},
	doi = {10.1007/978-3-030-50729-9_13},
	abstract = {In consumer behavior studies, several signals like head position and eye-tracking, which are mostly unstructured, are recorded. Hence, the first step in these studies is to extract structured features. In feature extraction, segmenting the space into several Areas of Interests (AOI) can be beneficial. In this regard, these features are computed when the shopper is inside a specific zone or interacting with or looking at a specific area. One of the difficulties of this approach is defining AOIs. In this study, positional and eye-tracking data of 57 subjects were recorded in a virtual reality store using a Head Mounted Display. Each subject performed a free navigation task and the objective of the study was to classify the shoppers based on their genders. For this purpose, some AOI-based features were extracted from the behavioral data. The AOIs were cubic and defined with rectangles in zenithal perspective and the shelves levels in virtual store. Sizes of horizontal rectangles were then optimized using Genetic Algorithm (GA). In optimization, a cost function based on Fisher criterion is defined to maximize the linear separability between classes. After optimization, the features extracted with the optimized and several arbitrary AOIs are classified with Support Vector Machine method. The results show that gender classification accuracy with optimized AOIs is 85\% and outperforms that of the other AOIs. Along with the outstanding results in this study, this methodology is capable of tuning other hyperparameters like navigational thresholds in classification problems.},
	language = {en},
	booktitle = {{HCI} {International} 2020 - {Posters}},
	publisher = {Springer International Publishing},
	author = {Moghaddasi, Masoud and Khatri, Jaikishan and Llanes-Jurado, Jose and Spinella, Luciano and Marín-Morales, Javier and Guixeres, Jaime and Alcañiz, Mariano},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2020},
	keywords = {Area of interest, Genetic algorithm, Optimization, Virtual reality, Zone of interest},
	pages = {92--98},
	file = {Full Text PDF:files/803/Moghaddasi et al. - 2020 - Segmentation of Areas of Interest Inside a Virtual Reality Store.pdf:application/pdf},
}

@inproceedings{nunnally_indirect_2011-1,
	address = {Berlin, Heidelberg},
	title = {An {Indirect} {Measure} of the {Implicit} {Level} of {Presence} in {Virtual} {Environments}},
	isbn = {978-3-642-22021-0},
	doi = {10.1007/978-3-642-22021-0_38},
	abstract = {Virtual Environments (VEs) are a common occurrence for many computer users. Considering their spreading usage and speedy development it is ever more important to develop methods that capture and measure key aspects of a VE, like presence. One of the main problems with measuring the level of presence in VEs is that the users may not be consciously aware of its affect. This is a problem especially for direct measures that rely on questionnaires and only measure the perceived level of presence explicitly. In this paper we develop and validate an indirect measure for the implicit level of presence of users, based on the physical reaction of users to events in the VE. The addition of an implicit measure will enable us to evaluate and compare VEs more effectively, especially with regard to their main function as immersive environments. Our approach is practical, cost-effective and delivers reliable results.},
	language = {en},
	booktitle = {Virtual and {Mixed} {Reality} - {New} {Trends}},
	publisher = {Springer},
	author = {Nunnally, Steven and Bouchard, Durell},
	editor = {Shumaker, Randall},
	year = {2011},
	keywords = {Indirect Implicit Measure, Presence, Virtual Environments},
	pages = {345--353},
	file = {Full Text PDF:files/806/Nunnally e Bouchard - 2011 - An Indirect Measure of the Implicit Level of Presence in Virtual Environments.pdf:application/pdf},
}

@inproceedings{tan_multitouch_2019-1,
	address = {Cham},
	title = {A {Multitouch} {Drawing} {Application} with {Occlusion}-{Free} {Interaction} {Strategies}},
	isbn = {978-3-030-29390-1},
	doi = {10.1007/978-3-030-29390-1_32},
	abstract = {Increasing number of desktop applications are becoming available on smartphones and tablets today with multitouch capabilities, allowing the users’ fingers to perform sophisticated or fine-grained interactivities. However, finger occlusion and imprecision continue to limit the performance of multitouch interactions. Quite a few studies proposed the ways to address this issue, and some of them are now used in commonly encountered situations such as text editing. Many occlusion-avoiding techniques used today focus on initial target acquisition step of touch interaction (e.g. accurately selecting an item or touching a desired starting point in drawing a line), having possible consequences to any further intended action (e.g. dragging the selected item to a different location or drawing a line on the canvas). In order to better understand the influence of finger occlusion-free techniques on other parts of the overall interactions, in this paper we report a full-fledged sketch app that incorporates combinations of basic target acquisition features. As the app is a full-featured, end-to-end tablet prototype, such usability issues can be more readily revealed and discussed in the context of realistic drawing situations.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2019},
	publisher = {Springer International Publishing},
	author = {Tan, Yi Ren and Lee, Hyowon and Ko, Insuk and Zhang, Dongliang and Li, Chunxiao},
	editor = {Lamas, David and Loizides, Fernando and Nacke, Lennart and Petrie, Helen and Winckler, Marco and Zaphiris, Panayiotis},
	year = {2019},
	keywords = {Finger occlusion, Multitouch interaction, Pointing precision},
	pages = {535--539},
	file = {Full Text PDF:files/808/Tan et al. - 2019 - A Multitouch Drawing Application with Occlusion-Free Interaction Strategies.pdf:application/pdf},
}

@inproceedings{chen_comparison_2020-1,
	address = {Cham},
	title = {Comparison of {Pedestrians}’ {Gap} {Acceptance} {Behavior} {Towards} {Automated} and {Human}-{Driven} {Vehicles}},
	isbn = {978-3-030-49183-3},
	doi = {10.1007/978-3-030-49183-3_20},
	abstract = {To protect pedestrian safety, automated vehicles can adopt a conservative strategy by yielding to pedestrians in all interactions and external human-machine interface was suggested to convey vehicle intentions to pedestrians. However, automated vehicles also could convey messages to assist existing pedestrians’ road-crossing decision-making, which is another way to ensure pedestrian safety but has generally been neglected. The current study explored the effect of assistance information on pedestrian gap acceptance behavior by presenting three colors similar to a traffic light to indicate the instant safety to cross road. Forty-eight participants completed the gap acceptance task in a virtual reality environment when interacting with human-driven vehicles or automated vehicles in a mixed or non-mixed traffic environment. The results showed that generally pedestrians had similar gap acceptance trends in rejecting small gaps towards two types of vehicles, but are more likely to accepted a large gap when they interacted with automated vehicles. The assistance information helped pedestrians to make safer road-crossing decisions, but whether the two types of vehicles drove in separately or in mixed condition did not affect pedestrian behavior. The null effect of driving context indicates that pedestrians may rely on their legacy strategy of gap acceptance regardless of vehicle type and the assistance information just only had minor effects.},
	language = {en},
	booktitle = {Engineering {Psychology} and {Cognitive} {Ergonomics}. {Cognition} and {Design}},
	publisher = {Springer International Publishing},
	author = {Chen, Wenxiang and Jiang, Qianni and Zhuang, Xiangling and Ma, Guojie},
	editor = {Harris, Don and Li, Wen-Chin},
	year = {2020},
	keywords = {Automated vehicles, Gap acceptance, Pedestrian safety, Time to arrival},
	pages = {253--261},
	file = {Full Text PDF:files/810/Chen et al. - 2020 - Comparison of Pedestrians’ Gap Acceptance Behavior Towards Automated and Human-Driven Vehicles.pdf:application/pdf},
}

@inproceedings{kitsikidis_game-like_2015-1,
	address = {Cham},
	title = {A {Game}-like {Application} for {Dance} {Learning} {Using} a {Natural} {Human} {Computer} {Interface}},
	isbn = {978-3-319-20684-4},
	doi = {10.1007/978-3-319-20684-4_46},
	abstract = {Game-based learning and gamification techniques are recently becoming a popular trend in the field of Technology Enhanced Learning. In this paper, we mainly focus on the use of game design elements for the transmission of Intangible Cultural Heritage (ICH) knowledge and, especially, for the learning of traditional dances. More specifically, we present a 3D game environment that employs an enjoyable natural human computer interface, which is based on the fusion of multiple depth sensors data in order to capture the body movements of the user/learner. In addition, the system automatically assesses the learner’s performance by utilizing a combination of Dynamic Time Warping (DTW) with Fuzzy Inference System (FIS) approach and provides feedback in a form of a score as well as instructions from a virtual tutor in order to promote self-learning. As a pilot use case, a Greek traditional dance, namely Tsamiko, has been selected. Preliminary small-scaled experiments with students of the Department of Physical Education and Sports Science at Aristotle University of Thessaloniki have shown the great potential of the proposed application.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Access} to {Learning}, {Health} and {Well}-{Being}},
	publisher = {Springer International Publishing},
	author = {Kitsikidis, Alexandros and Dimitropoulos, Kosmas and Uğurca, Deniz and Bayçay, Can and Yilmaz, Erdal and Tsalakanidou, Filareti and Douka, Stella and Grammalidis, Nikos},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2015},
	keywords = {Dance performance evaluation, Natural human computer interface, Traditional dances},
	pages = {472--482},
	file = {Full Text PDF:files/813/Kitsikidis et al. - 2015 - A Game-like Application for Dance Learning Using a Natural Human Computer Interface.pdf:application/pdf},
}

@inproceedings{jayender_novel_2018-1,
	address = {Cham},
	title = {A {Novel} {Mixed} {Reality} {Navigation} {System} for {Laparoscopy} {Surgery}},
	isbn = {978-3-030-00937-3},
	doi = {10.1007/978-3-030-00937-3_9},
	abstract = {OBJECTIVE: To design and validate a novel mixed reality head-mounted display for intraoperative surgical navigation. DESIGN: A mixed reality navigation for laparoscopic surgery (MRNLS) system using a head mounted display (HMD) was developed to integrate the displays from a laparoscope, navigation system, and diagnostic imaging to provide context-specific information to the surgeon. Further, an immersive auditory feedback was also provided to the user. Sixteen surgeons were recruited to quantify the differential improvement in performance based on the mode of guidance provided to the user (laparoscopic navigation with CT guidance (LN-CT) versus mixed reality navigation for laparoscopic surgery (MRNLS)). The users performed three tasks: (1) standard peg transfer, (2) radiolabeled peg identification and transfer, and (3) radiolabeled peg identification and transfer through sensitive wire structures. RESULTS: For the more complex task of peg identification and transfer, significant improvements were observed in time to completion, kinematics such as mean velocity, and task load index subscales of mental demand and effort when using the MRNLS (p {\textless} 0.05) compared to the current standard of LN-CT. For the final task of peg identification and transfer through sensitive structures, time taken to complete the task and frustration were significantly lower for MRNLS compared to the LN-CT approach. CONCLUSIONS: A novel mixed reality navigation for laparoscopic surgery (MRNLS) has been designed and validated. The ergonomics of laparoscopic procedures could be improved while minimizing the necessity of additional monitors in the operating room.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Jayender, Jagadeesan and Xavier, Brian and King, Franklin and Hosny, Ahmed and Black, David and Pieper, Steve and Tavakkoli, Ali},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor},
	year = {2018},
	keywords = {Audio navigation, Ergonomics, Laparoscopy surgery, Mixed-reality, Surgical navigation, Visual navigation},
	pages = {72--80},
	file = {Full Text PDF:files/816/Jayender et al. - 2018 - A Novel Mixed Reality Navigation System for Laparoscopy Surgery.pdf:application/pdf},
}

@inproceedings{shin_steam-x_2020-1,
	address = {Cham},
	title = {{STEAM}-{X}: {An} {Exploratory} {Study} {Adding} {Interactive} {Physical} {Activity} to the {STEAM} {Model}},
	isbn = {978-3-030-50513-4},
	shorttitle = {{STEAM}-{X}},
	doi = {10.1007/978-3-030-50513-4_14},
	abstract = {The philosophy of holistic education, which emphasizes a balance between physical and cognitive development, has been widely considered important for many years. However, Science, Technology, Engineering, Arts, and Mathematics (STEAM) education, which is currently being promoted worldwide, is moving away from the holistic education model, causing an imbalance between subjects due to its heavy emphasis on science and mathematics. This paper proposes a STEAM-X education model that adds physical activity to the STEAM model for the purpose of providing a modern holistic education that includes physical, cognitive, and mental learning and promotes creative and convergent thinking. In this study, we reviewed the existing literature concerning the benefits of physical activity, student preferences for physical activity, and the link between physical activity and STEAM education. Based on this review, we derived four research questions and conducted experimental research. We selected the appropriate tools for STEAM-X education, collected data on their implementation through observation and user interviews, and received positive responses from students and parents about STEAM-X education. The STEAM-X model appears capable of influencing students’ body reaction speeds, body coordination, concentration, problem-solving abilities, creativity, sequential thinking, strategic thinking, collaboration skills, and ethical attitudes. Implementing the STEAM-X education program would require creating additional plans for safety, developing content for instruction in conjunction with other subjects, and providing support for teachers, such as operational guides and manuals.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Designing}, {Developing} and {Deploying} {Learning} {Experiences}},
	publisher = {Springer International Publishing},
	author = {Shin, Jina and Heo, Jeongyun},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2020},
	keywords = {Interactive game, Mixed reality, STEAM, STEAM-X},
	pages = {179--193},
	file = {Full Text PDF:files/817/Shin e Heo - 2020 - STEAM-X An Exploratory Study Adding Interactive Physical Activity to the STEAM Model.pdf:application/pdf},
}

@inproceedings{williams_augmented_2020-1,
	address = {Cham},
	title = {Augmented {Reality} for {City} {Planning}},
	isbn = {978-3-030-49695-1},
	doi = {10.1007/978-3-030-49695-1_17},
	abstract = {We present an early study designed to analyze how city planning and the health of senior citizens can benefit from the use of augmented reality (AR) with assistance of virtual reality (VR), using Microsoft’s HoloLens and HTC’s Vive headsets. We also explore whether AR and VR can be used to help city planners receive real-time feedback from citizens, such as the elderly, on virtual plans, allowing for informed decisions to be made before any construction begins. In doing so, city planners can more clearly understand what design features would motivate senior citizens to visit or exercise in future parks, for example. The study was conducted on 10 participants 60 years and older who live within 2 miles from the site. They were presented with multiple virtual options for a prospective park, such as different walls for cancelling highway noise, as well as benches, lampposts, bathroom pods, walking and biking lanes, and other street furniture. The headsets allowed the participants to clearly visualize the options and make choices on them. Throughout the study the participants were enthusiastic about using the AR and VR devices, which is noteworthy for a future where city planning is done with these technologies.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Design} and {Interaction}},
	publisher = {Springer International Publishing},
	author = {Williams, Adam Sinclair and Angelini, Catherine and Kress, Mathew and Ramos Vieira, Edgar and D’Souza, Newton and Rishe, Naphtali D. and Medina, Joseph and Özer, Ebru and Ortega, Francisco},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Augmented and virtual reality, Human-centered computing, Visualization design and evaluation methods},
	pages = {256--271},
	file = {Full Text PDF:files/840/Williams et al. - 2020 - Augmented Reality for City Planning.pdf:application/pdf},
}

@inproceedings{doroudian_creating_2018-1,
	address = {Cham},
	title = {Creating an {Online} {Escape} {Room} {Game} for {Older} {Adults}: {Needs} {Assessment}, {Design} {Process}, and {Usability} {Testing}},
	isbn = {978-3-319-92037-5},
	shorttitle = {Creating an {Online} {Escape} {Room} {Game} for {Older} {Adults}},
	doi = {10.1007/978-3-319-92037-5_36},
	abstract = {In this project, an online escape game for older adults was developed collaboratively with five novice developers and 12 older adults who tested the prototype game at several stages of the development process. We followed a User-Centered Design (UCD) process to develop the game. During the design process, older adults tested the game and it was refined based on their feedback and the research team’s observation of their play. We found that incorporating older adults into the design process significantly improved the design, as they provided insights that would not have been available otherwise. We also concluded that the older adults’ involvement in the design process could make the game better tailored to their needs. The game is currently being field-tested.},
	language = {en},
	booktitle = {Human {Aspects} of {IT} for the {Aged} {Population}. {Applications} in {Health}, {Assistance}, and {Entertainment}},
	publisher = {Springer International Publishing},
	author = {Doroudian, Amir and Hausknecht, Simone and Kaufman, David},
	editor = {Zhou, Jia and Salvendy, Gavriel},
	year = {2018},
	keywords = {Escape room, Need assessment, Older adults, User-Centered Design},
	pages = {516--525},
	file = {Full Text PDF:files/844/Doroudian et al. - 2018 - Creating an Online Escape Room Game for Older Adults Needs Assessment, Design Process, and Usabilit.pdf:application/pdf},
}

@inproceedings{chen_user_2019-1,
	address = {Cham},
	title = {User {Experience} and {Map} {Design} for {Wayfinding} in a {Virtual} {Environment}},
	isbn = {978-3-030-22649-7},
	doi = {10.1007/978-3-030-22649-7_10},
	abstract = {Virtual environment (VE) has been developed rapidly in recent years. The level of complexity regarding the user interface in VEs has also increased. Users’ performance in VEs can be affected by the field of vision, screen size, operation mode, individual difference, and other factors. While little research has been conducted on the effects of user experience and map design on wayfinding in VEs. The experiment is 2 × 2 between-subject design. Participants needed to complete three wayfinding tasks and fill out questionnaires regarding satisfaction, preference, and System Usability Scale (SUS). Forty participants were invited using convenient sampling method. The results are as follows: (1) In terms of the map design, participants performed significantly better by using the semi-transparent map than the opaque map in a difficult task. (2) In terms of the user experience, the results generated from the SUS questionnaire showed that experienced users had a significantly better subjective evaluation of interface usability than inexperienced users.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Information} in {Intelligent} {Systems}},
	publisher = {Springer International Publishing},
	author = {Chen, Meng-Xi and Chen, Chien-Hsiung},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2019},
	keywords = {Map design, User experience, Virtual environment, Wayfinding},
	pages = {117--126},
	file = {Full Text PDF:files/848/Chen e Chen - 2019 - User Experience and Map Design for Wayfinding in a Virtual Environment.pdf:application/pdf},
}

@inproceedings{fan_collaborative_2018-1,
	address = {Cham},
	title = {A {Collaborative} {Virtual} {Game} to {Support} {Activity} and {Social} {Engagement} for {Older} {Adults}},
	isbn = {978-3-319-92049-8},
	doi = {10.1007/978-3-319-92049-8_14},
	abstract = {Many older adults suffer from Alzheimer’s disease or other dementias and have affected cognitive abilities. In general, physical exercise, cognitive stimulation, and social engagement have been found to be beneficial for the physical and mental health of older adults with and without cognitive impairment. In an effort to address these needs, researchers have been developing human-machine interaction (HMI) systems to administer activity-oriented therapies. However, most of these system, while promising, focus on one-on-one interaction with the computer and thus do not support social engagement by involving multiple older adults. In this paper, we present the design and development of a motion-based collaborative virtual environment (CVE) application to support both activity and social engagement. The CVE task is based on a book-sorting activity and has embedded collaborative components to encourage human-human interaction (HHI). The system records quantitative data regarding users’ performance, interaction frequency, and social interaction. A preliminary user study was conducted to validate system usability and test on older adults’ tolerance and acceptance of the motion-based user interface (UI) as well as the CVE task. The results showed the usability of the motion-based UI and system capability to assess HMI and HHI from recorded quantitative data. The results from post-test and analysis of audio files indicated that the system might be potentially useful. More user study and data analysis need to be conducted to further investigate the CVE system.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Methods}, {Technologies}, and {Users}},
	publisher = {Springer International Publishing},
	author = {Fan, Jing and Beuscher, Linda and Newhouse, Paul and Mion, Lorraine C. and Sarkar, Nilanjan},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2018},
	keywords = {Collaborative virtual environment (CVE), Dementia, Elder care, Human-Human interaction (HHI), Human-Machine interaction (HMI)},
	pages = {192--204},
	file = {Full Text PDF:files/849/Fan et al. - 2018 - A Collaborative Virtual Game to Support Activity and Social Engagement for Older Adults.pdf:application/pdf},
}

@inproceedings{miura_designing_2016-1,
	address = {Cham},
	title = {Designing {Affordances} for {Virtual} {Reality}-{Based} {Services} with {Natural} {User} {Interaction}},
	isbn = {978-3-319-40406-6},
	doi = {10.1007/978-3-319-40406-6_25},
	abstract = {The progress of new technologies makes virtual reality (VR) easier, and inexpensive head-mounted displays (HMDs) accelerate the development of advanced VR services. The advantage of the technologies is to offer the better immersion of a VR world, to use natural user interface (NUI) devices and to operate them with a user’s gesture. The problem to use NUI devices for VR-based services is to take into account various types of NUI devices. We need to consider how a user to use NUI devices in a proper way, in particular, when an HMD limits his/her view of the real world. Our approach to solve the problem is to use an affordance that offers implicit information how to navigate VR-based services. However, there are a little researches to investigate the relationship between different types of NUI devices and the design of proper affordances to navigate VR-based services. The paper provides some insights how respective types of NUI devices influence affordance design to navigate VR-based services. For extracting effective insights, we have developed two VR-based services, where we discuss two types of operating methods and three types of affordances for respective operating methods are examined. Also, we chose two types of NUI devices for navigating the VR-based services. Then, we conducted some experiments to extract some insights as a guideline to develop future VR-based services. The result shows that the differences among respective NUI devices may not significantly influence affordance design, however have strong effects on understanding how to navigate VR-based services. The understanding also affects how each user prefers which NUI devices, because NUI devices require us to use gesture to navigate the services, but the intuition that differs in each individual is important.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Technological} {Contexts}},
	publisher = {Springer International Publishing},
	author = {Miura, Takayuki and Yoshii, Akihito and Nakajima, Tatsuo},
	editor = {Marcus, Aaron},
	year = {2016},
	keywords = {Affordance, Natural user interface, Virtual reality},
	pages = {266--277},
	file = {Full Text PDF:files/855/Miura et al. - 2016 - Designing Affordances for Virtual Reality-Based Services with Natural User Interaction.pdf:application/pdf},
}

@inproceedings{mourouzis_touchless_2015-1,
	address = {Cham},
	title = {Touchless {Text} {Entry} for {All}: {Initial} {Design} {Considerations} and {Prototypes}},
	isbn = {978-3-319-20681-3},
	shorttitle = {Touchless {Text} {Entry} for {All}},
	doi = {10.1007/978-3-319-20681-3_4},
	abstract = {In this paper, the foundations for a new touchless text entry method are set. The method in question is based on hand tracking for interacting with a novel virtual keyboard that has been designed to support adaptive text entry for all, including for users with disabilities or users of various assistive technologies. The virtual keyboard in question, which can be graphical or even imaginary (i.e., implied, but not visualised), implements a hierarchical selection approach for the act of writing, which involves character selection, text navigation and text modification. The proposed structure guarantees minimum keystrokes per character, and allows for user-friendly adaptations for offering accessibility, personalisation and increased performance rates for diverse input settings or display preferences. An implementation of the text entry method in question, using the LEAP, a novel 3D motion controller, is introduced here. Along with the fundamental specifications of the proposed keyboard and the required functionality of a software mechanism for initialising, personalising and interacting with it in any given context, this paper presents two prototypes currently under development for one- and ten-finger touchless text entry with LEAP, which are considered in view of developing a universal text input solution that could overshadow all past technologies.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Access} to {Interaction}},
	publisher = {Springer International Publishing},
	author = {Mourouzis, Alexandros and Arfaras, Giorgos and Kilintzis, Vassilis and Chouvarda, Ioanna and Maglaveras, Nicos},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2015},
	keywords = {3D interaction, LEAP, Soft keyboard, Text entry, Universal access},
	pages = {37--49},
	file = {Full Text PDF:files/858/Mourouzis et al. - 2015 - Touchless Text Entry for All Initial Design Considerations and Prototypes.pdf:application/pdf},
}

@inproceedings{smeddinck_hoverboard_2017-1,
	address = {Cham},
	title = {Hoverboard: {A} {Leap} to the {Future} of {Locomotion} in {VR}!?},
	isbn = {978-3-319-66715-7},
	shorttitle = {Hoverboard},
	doi = {10.1007/978-3-319-66715-7_24},
	abstract = {Locomotion in virtual reality (VR) remains challenging due to limitations of common input methods. Sedentary input devices may endanger immersion, real-to-virtual world perception dissonance can lead to simulator sickness, and physical input devices such as framed walking dishes are often complex and expensive. We present a low-cost, easy to use, easy to manufacture, and easily portable device for locomotion in VR based on a hoverboard metaphor. Building on related work and our own iterative VR locomotion system designs we hypothesize that hoverboarding can provide a compelling and intuitive method for short- and long-distance locomotion in VR with a potential to reduce simulator sickness due to consistent and stable locomotion that corresponds well to the physical proprioception of the users while navigating VR. We discuss design iterations of our device prototypes, promising results from an early explorative evaluation, as well as ongoing continued work.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2017},
	publisher = {Springer International Publishing},
	author = {Smeddinck, Jan and Alexandrovsky, Dmitry and Wenig, Dirk and Zimmer, Michel and Wegele, Waldemar and Juergens, Sylvia and Malaka, Rainer},
	editor = {Munekata, Nagisa and Kunita, Itsuki and Hoshino, Junichi},
	year = {2017},
	keywords = {Gaming, Hoverboard, Leaning, Locomotion, Motion-based control, Natural user interface, Virtual Reality (VR), Whole-body interface},
	pages = {218--225},
	file = {Full Text PDF:files/880/Smeddinck et al. - 2017 - Hoverboard A Leap to the Future of Locomotion in VR!.pdf:application/pdf},
}

@inproceedings{bozgeyikli_virtual_2018-1,
	address = {Cham},
	title = {Virtual {Reality} {Interaction} {Techniques} for {Individuals} with {Autism} {Spectrum} {Disorder}},
	isbn = {978-3-319-92052-8},
	doi = {10.1007/978-3-319-92052-8_6},
	abstract = {Virtual reality (VR) systems are seeing growing use for training individuals with Autism Spectrum Disorder (ASD). Although these systems indicate effective use of VR for training, there is little work in the literature evaluating different VR interaction techniques for this audience. In this paper, different VR interaction techniques are explored in the Virtual Reality for Vocational Rehabilitation (VR4VR) system and additional data analysis on top of our previously published preliminary results [1] was performed via a user study with nine individuals with ASD and ten neurotypical individuals. The participants tried six vocational training modules of the VR4VR system. In these modules, tangible object manipulation, haptic device, touch and snap and touchscreen were tested for object selection and manipulation; real walking and walk-in-place were tested for locomotion; and head mounted display and curtain screen were tested for display. Touchscreen and tangible interaction methods were preferred by the individuals with ASD. The walk-in-place locomotion technique were found frustrating and difficult to perform by the individuals with ASD. Curtain display received higher preference scores from individuals with ASD although they accepted the HMD as well. The observations and findings of the study are expected to give insight into the poorly explored area of experience of individuals with ASD with various interaction techniques in VR.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Virtual}, {Augmented}, and {Intelligent} {Environments}},
	publisher = {Springer International Publishing},
	author = {Bozgeyikli, Evren and Bozgeyikli, Lal “Lila” and Alqasemi, Redwan and Raij, Andrew and Katkoori, Srinivas and Dubey, Rajiv},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2018},
	keywords = {Autism spectrum disorder, Interaction techniques, Virtual reality, Vocational rehabilitation},
	pages = {58--77},
	file = {Full Text PDF:files/882/Bozgeyikli et al. - 2018 - Virtual Reality Interaction Techniques for Individuals with Autism Spectrum Disorder.pdf:application/pdf},
}

@inproceedings{chen_immersive_2020-1,
	address = {Cham},
	title = {An {Immersive} {Virtual} {Reality} {Exergame} for {People} with {Parkinson}’s {Disease}},
	isbn = {978-3-030-58796-3},
	doi = {10.1007/978-3-030-58796-3_18},
	abstract = {Parkinson’s disease is a neurodegenerative disorder that affects primarily motor system. Physical exercise is considered important for people with Parkinson’s disease (PD) to slow down disease progression and maintain abilities and quality of life. However, people with PD often experience barriers to exercises that causes low-level adherence to exercise plans and programs. Virtual Reality (VR) is an innovative and promising technology for motor and cognitive rehabilitation. Immersive VR exergames have potential advantages by allowing for individualized skill practice in a motivating interactive environment without distractions from outside events. This paper presents an immersive virtual reality (VR) exergame aiming at motor training on fingers and hand-and-eye coordination. The results from the usability study indicate that immersive VR exergames have potential to provide motivating and engaging physical exercise for people with PD. Through this research, we hope to contribute to evidence-based design principles for task-specific immersive VR exergames for patients with Parkinson’s Disease.},
	language = {en},
	booktitle = {Computers {Helping} {People} with {Special} {Needs}},
	publisher = {Springer International Publishing},
	author = {Chen, Weiqin and Bang, Martin and Krivonos, Daria and Schimek, Hanna and Naval, Arnau},
	editor = {Miesenberger, Klaus and Manduchi, Roberto and Covarrubias Rodriguez, Mario and Peňáz, Petr},
	year = {2020},
	keywords = {Exergame, Immersive virtual reality, Parkinson’s disease},
	pages = {138--145},
	file = {Full Text PDF:files/883/Chen et al. - 2020 - An Immersive Virtual Reality Exergame for People with Parkinson’s Disease.pdf:application/pdf},
}

@inproceedings{grajewski_examination_2018-1,
	address = {Cham},
	title = {Examination of {Effectiveness} of a {Performed} {Procedural} {Task} {Using} {Low}-{Cost}  {Peripheral} {Devices} in {VR}},
	isbn = {978-3-319-91581-4},
	doi = {10.1007/978-3-319-91581-4_30},
	abstract = {The paper presents a Virtual Reality (VR) training system dedicated for interactive course focused on acquisition of competences in the field of manual procedural tasks. It was developed as a response for the growing market demand for low-cost VR systems supporting industrial training. A scenario for the implementation of an elementary manual operation (modified peg-in-hole task) was developed. The aim of the test was to show whether the prepared solution (along with peripheral devices) can be an effective tool for training the activities performed at the production site. The procedural task was performed by specific test groups using various peripheral devices. The paper presents preliminary results of tests regarding evaluation of effectiveness of virtual training, depending on specific peripheral devices used.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Interaction}, {Navigation}, {Visualization}, {Embodiment}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Grajewski, Damian and Buń, Paweł and Górski, Filip},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Haptic feedback, Interaction devices, Virtual reality training},
	pages = {403--415},
	file = {Full Text PDF:files/884/Grajewski et al. - 2018 - Examination of Effectiveness of a Performed Procedural Task Using Low-Cost  Peripheral Devices in VR.pdf:application/pdf},
}

@inproceedings{huang_method_2018-1,
	address = {Cham},
	title = {A {Method} of {Evaluating} {User} {Visual} {Attention} to {Moving} {Objects} in {Head} {Mounted} {Virtual} {Reality}},
	isbn = {978-3-319-91797-9},
	doi = {10.1007/978-3-319-91797-9_29},
	abstract = {Virtual reality games/films/applications bring new challenges to conventional film grammar and design principles, due to more spatial freedom available to users in 6-DOF Head-Mounted Display (HMD). This paper introduces a simple model of viewers’ visual attention in environment of virtual reality while watching randomly generated moving objects. The model is based on a dataset collected from 10 users in a 50-seconds-long virtual reality experience on HTC Vive. In this paper, we considered three factors as major parameters affecting audiences’ attention: the distance between object and the viewer, the speed of objects movement, and the direction of object towards. We hope the research result is useful to immersive film directors and VR game designers in the future.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Theory} and {Practice}},
	publisher = {Springer International Publishing},
	author = {Huang, Shi},
	editor = {Marcus, Aaron and Wang, Wentao},
	year = {2018},
	keywords = {Focus of attention, Immersive film, Virtual reality, VR experience, VR game},
	pages = {406--416},
	file = {Full Text PDF:files/887/Huang - 2018 - A Method of Evaluating User Visual Attention to Moving Objects in Head Mounted Virtual Reality.pdf:application/pdf},
}

@inproceedings{brawner_bridging_2020,
	address = {Cham},
	title = {Bridging {Conceptual} {Models} and {Architectural} {Interchange} for {Adaptive} {Instructional} {Systems}},
	isbn = {978-3-030-50788-6},
	doi = {10.1007/978-3-030-50788-6_3},
	abstract = {This paper serves to connect the papers between the AIS conceptual modeling group and the architectural interchange group by deriving requirements for the required components and the information that they need to exchange. It serves as an update to the original work on the subject, prior to the establishment of the conceptual modeling subgroup.},
	language = {en},
	booktitle = {Adaptive {Instructional} {Systems}},
	publisher = {Springer International Publishing},
	author = {Brawner, Keith},
	editor = {Sottilare, Robert A. and Schwarz, Jessica},
	year = {2020},
	keywords = {Adaptive Instructional Systems, Competencies, Interchangeable components, Recommender systems, Standards},
	pages = {34--44},
	file = {Full Text PDF:files/841/Brawner - 2020 - Bridging Conceptual Models and Architectural Interchange for Adaptive Instructional Systems.pdf:application/pdf},
}

@inproceedings{mandeville_remote_2017,
	address = {Cham},
	title = {Remote {Touch}: {Humanizing} {Social} {Interactions} in {Technology} {Through} {Multimodal} {Interfaces}},
	isbn = {978-3-319-57987-0},
	shorttitle = {Remote {Touch}},
	doi = {10.1007/978-3-319-57987-0_10},
	abstract = {Waves, pokes, and tugs are simple social gestures that can benefit from more thoughtful design when translated onto mobile devices and computers. Haptics provide an additional mode of conveyance that is frequently forgotten about in development of mobile technologies, but incorporating it can have significant positive impact on user experience. Combining advanced vibrotactile haptics, location, and multimodally congruent feedback, our prototype creates a simple experience that connects people through non-verbal information to deliver a meaningful gesture and playful interaction.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Mandeville, Alexia and Birnbaum, David and Sampanes, Chad},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Application, Design, Gesture, Haptics, iBeacon, Interaction, Location, Mobile, Multimodal, Social, Tactile, Touch, Touchsense, User experience, UX, Wearable},
	pages = {120--129},
	file = {Full Text PDF:files/845/Mandeville et al. - 2017 - Remote Touch Humanizing Social Interactions in Technology Through Multimodal Interfaces.pdf:application/pdf},
}

@inproceedings{gratch_can_2007,
	address = {Berlin, Heidelberg},
	title = {Can {Virtual} {Humans} {Be} {More} {Engaging} {Than} {Real} {Ones}?},
	isbn = {978-3-540-73110-8},
	doi = {10.1007/978-3-540-73110-8_30},
	abstract = {Emotional bonds don’t arise from a simple exchange of facial displays, but often emerge through the dynamic give and take of face-to-face interactions. This article explores the phenomenon of rapport, a feeling of connectedness that seems to arise from rapid and contingent positive feedback between partners and is often associated with socio-emotional processes. Rapport has been argued to lead to communicative efficiency, better learning outcomes, improved acceptance of medical advice and successful negotiations. We provide experimental evidence that a simple virtual character that provides positive listening feedback can induce stronger rapport-like effects than face-to-face communication between human partners. Specifically, this interaction can be more engaging to storytellers than speaking to a human audience, as measured by the length and content of their stories.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {HCI} {Intelligent} {Multimodal} {Interaction} {Environments}},
	publisher = {Springer},
	author = {Gratch, Jonathan and Wang, Ning and Okhmatovskaia, Anna and Lamothe, Francois and Morales, Mathieu and van der Werf, R. J. and Morency, Louis-Philippe},
	editor = {Jacko, Julie A.},
	year = {2007},
	keywords = {Human Listener, Speech Fluency, Speech Rate, Virtual Character, Virtual Human},
	pages = {286--297},
	file = {Full Text PDF:files/851/Gratch et al. - 2007 - Can Virtual Humans Be More Engaging Than Real Ones.pdf:application/pdf},
}

@inproceedings{matsangidou_dementia_2020,
	address = {Cham},
	title = {Dementia: {I} {Am} {Physically} {Fading}. {Can} {Virtual} {Reality} {Help}? {Physical} {Training} for {People} with {Dementia} in {Confined} {Mental} {Health} {Units}},
	isbn = {978-3-030-49282-3},
	shorttitle = {Dementia},
	doi = {10.1007/978-3-030-49282-3_26},
	abstract = {In recent years, there has been growing interest in designing non-pharmacological interventions to improve the quality of life for People with Dementia (PwD) who face motor impairments. This paper investigates the feasibility of using Virtual Reality (VR) technologies for the rehabilitation of 20 patients with moderate to severe dementia residing in a confined psychiatric hospital and discusses the impact of this interactions on motor training. To accomplish this, we present three interrelated studies that refer to: (1) System requirement analysis carried out through a workshop with experts in dementia care; (2) System interaction method assessment by testing two different types of interaction in Virtual Reality, to identify the most suitable for People with Dementia; and (3) A pilot study with patients performing three upper limb physiotherapy tasks in Virtual Reality. The issues encountered during the design, testing and execution of the experimental tasks are discussed and a set of guidelines and recommendations for the future deployment of VR in healthcare services is provided.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Design} {Approaches} and {Supporting} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Matsangidou, Maria and Schiza, Eirini and Hadjiaros, Marios and Neokleous, Kleanthis C. and Avraamides, Marios and Papayianni, Ersi and Frangoudes, Fotos and Pattichis, Constantinos S.},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2020},
	keywords = {Dementia, Long-term care, Virtual Reality},
	pages = {366--382},
	file = {Full Text PDF:files/856/Matsangidou et al. - 2020 - Dementia I Am Physically Fading. Can Virtual Reality Help Physical Training for People with Dement.pdf:application/pdf},
}

@inproceedings{haefliger_modding_2009,
	address = {Berlin, Heidelberg},
	title = {Modding as {Rating} {Behavior} in {Virtual} {Communities}: {The} {Case} of {Rooster} {Teeth} {Productions}},
	isbn = {978-3-642-02774-1},
	shorttitle = {Modding as {Rating} {Behavior} in {Virtual} {Communities}},
	doi = {10.1007/978-3-642-02774-1_22},
	abstract = {Virtual communities that make use of social network site features blend known applications of virtual communities. These communities can be simultaneously social and commercial, organization sponsored and heavily relying on member interaction. We explore modding behavior that allows members to evaluate other members’ contributions both with numerical value and qualitative rating. We show that approximately half of all members received mods on their comments, that the majority of mods given were positive, and that the amount of mods received for a comment was related to the position of the comment in the community website’s thread. Contributing to the emerging literature of social network sites and virtual communities, we discuss implications for theory, future research and management.},
	language = {en},
	booktitle = {Online {Communities} and {Social} {Computing}},
	publisher = {Springer},
	author = {Haefliger, Stefan and Reichen, Philip and Jäger, Peter M. and von Krogh, Georg},
	editor = {Ozok, A. Ant and Zaphiris, Panayiotis},
	year = {2009},
	keywords = {Communities of Consumption, Machinima, Social Network Sites, Virtual Communities},
	pages = {197--206},
	file = {Full Text PDF:files/881/Haefliger et al. - 2009 - Modding as Rating Behavior in Virtual Communities The Case of Rooster Teeth Productions.pdf:application/pdf},
}

@inproceedings{valls_e-learning_2015,
	address = {Cham},
	title = {E-{Learning} and {Serious} {Games}},
	isbn = {978-3-319-20609-7},
	doi = {10.1007/978-3-319-20609-7_59},
	abstract = {The complexity of urban processes needs professionals trained in understanding and managing the design of its spaces and the implementation of urban policies. This paper discusses an educational methodology to complement the standard Project-Based Learning approach with an experience using serious games with gamification elements to stimulate critical thinking in urban planning and urban design students, to promote designing spaces more adaptable and usable for a wide range of users and situations of public life. The proposed methodology uses five “mini games” that place students in different situations: (1) finding an unknown landmark, (2) reaching goal avoiding obstacles, (3) navigating with artificial lighting, (4) simulating the point of view of a person with a disability, and (5) simulating group behaviour. As a secondary objective the experience will track the participants’ behaviour to extract data to be incorporated into an agent-based model rule set.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Valls, Francesc and Redondo, Ernest and Fonseca, David},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2015},
	keywords = {E-Learning, Gamification, Serious games, Simulation, Urban space},
	pages = {632--643},
	file = {Full Text PDF:files/886/Valls et al. - 2015 - E-Learning and Serious Games.pdf:application/pdf},
}

@inproceedings{bian_novel_2013,
	address = {Berlin, Heidelberg},
	title = {A {Novel} {Virtual} {Reality} {Driving} {Environment} for {Autism} {Intervention}},
	isbn = {978-3-642-39191-0},
	doi = {10.1007/978-3-642-39191-0_52},
	abstract = {Individuals with autism spectrum disorders (ASD) often have difficulty functioning independently and display impairments related to important tasks related to adaptive independence such as driving. Ability to drive is believed to be an important factor of quality of life for individuals with ASD. The presented work describes a novel driving simulator based on a virtual city environment that will be used in the future to impart driving skills to teenagers with ASD as a part of intervention. A physiological data acquisition system, which was used to acquire and process participant’s physiological signals, and an eye tracker, which was utilized to detect eye gaze signals, were each integrated into the driving simulator. These physiological and eye gaze indices were recorded and computed to infer the affective states of the participant in real-time when he/she was driving. Based on the affective states of the participant together with his/her performance, the driving simulator adaptively changes the difficulty level of the task. This VR-based driving simulator will be capable of manipulating the driving task difficulty in response to the physiological and eye gaze indices recorded during the task. The design of this novel driving simulator system and testing data to validate its functionalities are presented in this paper.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {User} and {Context} {Diversity}},
	publisher = {Springer},
	author = {Bian, Dayi and Wade, Joshua W. and Zhang, Lian and Bekele, Esubalew and Swanson, Amy and Crittendon, Julie Ana and Sarkar, Medha and Warren, Zachary and Sarkar, Nilanjan},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2013},
	keywords = {Adaptive task, Autism intervention, Eye gaze, Physiological signals, Virtual Reality},
	pages = {474--483},
	file = {Full Text PDF:files/889/Bian et al. - 2013 - A Novel Virtual Reality Driving Environment for Autism Intervention.pdf:application/pdf},
}

@inproceedings{van_delden_hang_2014,
	address = {Berlin, Heidelberg},
	title = {Hang in {There}: {A} {Novel} {Body}-{Centric} {Interactive} {Playground}},
	isbn = {978-3-642-55143-7},
	shorttitle = {Hang in {There}},
	doi = {10.1007/978-3-642-55143-7_7},
	abstract = {We introduce and evaluate a first version of a novel body-centric playground that aims at increasing bodily exertion and immersion. The concept centers around the player, who is suspended from the ceiling using a rope and climbing harness and stands on a tilted platform. This caused players to assume a body posture that elicits the feeling of flying, which was further enhanced by the flying game that they played. We discuss the choices made in the type of body movements, and how these relate to different aspects such as movement mimicry and exertion. We performed a user study, in which the hanging position was compared to a setting where players stood on the ground. We found no significant differences in the amount of movement and perceived engagement between the two conditions. However, there was a tendency of favoring the hanging position. Moreover, we observed that the placement of game elements affected the movement patterns.},
	language = {en},
	booktitle = {Innovative and {Creative} {Developments} in {Multimodal} {Interaction} {Systems}},
	publisher = {Springer},
	author = {van Delden, Robby and Moreno, Alejandro and Ramos, Carlos and Carrasco, Gonçalo and Reidsma, Dennis and Poppe, Ronald},
	editor = {Rybarczyk, Yves and Cardoso, Tiago and Rosas, João and Camarinha-Matos, Luis M.},
	year = {2014},
	keywords = {Body-centric, engagement, exertion, interactive playground, postures},
	pages = {160--178},
	file = {Full Text PDF:files/891/van Delden et al. - 2014 - Hang in There A Novel Body-Centric Interactive Playground.pdf:application/pdf},
}

@inproceedings{dombrowski_utilizing_2016,
	address = {Cham},
	title = {Utilizing {Digital} {Game} {Environments} for {Training} {Prosthetic} {Use}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_46},
	abstract = {When the children receive their prosthetic, many of them still need to learn or, in many cases, relearn elements such as grabbing, squeezing and other range of movement with their newly fitted prosthetic. In this case, electromyogram or EMG sensors control the arms. The EMG is worn on an existing muscle and detects muscle movement as input. The EMG data feeds into the prosthetic device to stimulate movement in the hand. New users of this device are often confused over how to control the motion at first. Limibitless solutions, the creators of this device, approached the team at the University of Central Florida’s School of Visual Art \& Design, about the possibility of developing videogames to train the use of the prosthetic prior to children receiving them, in hopes of reducing the time it takes to become proficient with the device.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Dombrowski, Matt and Smith, Peter A. and Buyssens, Ryan},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	keywords = {Augmented reality, Game based training, Game design, Games for health, Gamification},
	pages = {481--489},
	file = {Full Text PDF:files/893/Dombrowski et al. - 2016 - Utilizing Digital Game Environments for Training Prosthetic Use.pdf:application/pdf},
}

@inproceedings{harman_virtual_2015,
	address = {Cham},
	title = {Virtual {Business} {Role}-{Play}: {Leveraging} {Familiar} {Environments} to {Prime} {Stakeholder} {Memory} {During} {Process} {Elicitation}},
	isbn = {978-3-319-19069-3},
	shorttitle = {Virtual {Business} {Role}-{Play}},
	doi = {10.1007/978-3-319-19069-3_11},
	abstract = {Business process models have traditionally been an effective way of examining business practices to identify areas for improvement. While common information gathering approaches are generally efficacious, they can be quite time consuming and have the risk of developing inaccuracies when information is forgotten or incorrectly interpreted by analysts. In this study, the potential of a role-playing approach for process elicitation and specification has been examined. This method allows stakeholders to enter a virtual world and role-play actions as they would in reality. As actions are completed, a model is automatically developed, removing the need for stakeholders to learn and understand a modelling grammar. Empirical data obtained in this study suggests that this approach may not only improve both the number of individual process task steps remembered and the correctness of task ordering, but also provide a reduction in the time required for stakeholders to model a process view.},
	language = {en},
	booktitle = {Advanced {Information} {Systems} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Harman, Joel and Brown, Ross and Johnson, Daniel and Rinderle-Ma, Stefanie and Kannengiesser, Udo},
	editor = {Zdravkovic, Jelena and Kirikova, Marite and Johannesson, Paul},
	year = {2015},
	keywords = {3D virtual worlds, Business process management, Human-computer interaction, Process elicitation, Subject-oriented business process management},
	pages = {166--180},
	file = {Full Text PDF:files/915/Harman et al. - 2015 - Virtual Business Role-Play Leveraging Familiar Environments to Prime Stakeholder Memory During Proc.pdf:application/pdf},
}

@inproceedings{harley_design_2011,
	address = {Berlin, Heidelberg},
	title = {The {Design} of an {Interactive} {Stroke} {Rehabilitation} {Gaming} {System}},
	isbn = {978-3-642-21619-0},
	doi = {10.1007/978-3-642-21619-0_22},
	abstract = {There is a compelling need to create an alternative and affordable home based therapy system founded on sound rehabilitative principles, that is readily available, engaging and motivational, and can be remotely monitored by therapists. In the past two years, stroke related medical costs have increased 20\%, while the number of clinical treatment sessions have declined. The purpose of this study was to develop an affordable interactive stroke rehabilitation gaming experience based on therapeutic fundamentals that can easily be used in the clinical setting or the home environment.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Users} and {Applications}},
	publisher = {Springer},
	author = {Harley, Linda and Robertson, Scott and Gandy, Maribeth and Harbert, Simeon and Britton, Douglas},
	editor = {Jacko, Julie A.},
	year = {2011},
	keywords = {gaming, health, rehabilitation, upper extremity},
	pages = {167--173},
	file = {Full Text PDF:files/918/Harley et al. - 2011 - The Design of an Interactive Stroke Rehabilitation Gaming System.pdf:application/pdf},
}

@inproceedings{gushima_deriving_2019,
	address = {Cham},
	title = {Deriving {Features} for {Designing} {Ambient} {Media}},
	isbn = {978-3-030-22419-6},
	doi = {10.1007/978-3-030-22419-6_3},
	abstract = {In this study, we focus on an information presentation method to reduce cognitive load. For example, a typical method of presenting information in a useful manner is Augmented Reality, which overlays information on the real world; however, this approach causes high cognitive load because it is too loud compared to the real world. Thus, we focus on an ambient media approach proposed by Hiroshi Ishii that works on the background of human consciousness to reduce the cognitive load. We aim to find expressions that are midway between information superposition and ambient media. In this study, we focus on the visual and auditory senses, which are mainly used for information presentation, and learn the features necessary to design ambient media through our case studies. We develop two case studies: HoloList and Ambient Table. We implement and evaluate these systems to learn the features required to design the ambient media.},
	language = {en},
	booktitle = {Augmented {Cognition}},
	publisher = {Springer International Publishing},
	author = {Gushima, Kota and Toyama, Shuma and Kinoshita, Yukiko and Nakajima, Tatsuo},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2019},
	keywords = {Ambient media, Augmented reality, Head-mounted display, Mixed reality, Notification},
	pages = {29--40},
	file = {Full Text PDF:files/920/Gushima et al. - 2019 - Deriving Features for Designing Ambient Media.pdf:application/pdf},
}

@inproceedings{larsen_quantitative_2013,
	address = {Berlin, Heidelberg},
	title = {Quantitative {Modal} {Transition} {Systems}},
	isbn = {978-3-642-37635-1},
	doi = {10.1007/978-3-642-37635-1_3},
	abstract = {This extended abstract offers a brief survey presentation of the specification formalism of modal transition systems and its recent extensions to the quantitative setting of timed as well as stochastic systems. Some applications will also be briefly mentioned.},
	language = {en},
	booktitle = {Recent {Trends} in {Algebraic} {Development} {Techniques}},
	publisher = {Springer},
	author = {Larsen, Kim G. and Legay, Axel},
	editor = {Martí-Oliet, Narciso and Palomino, Miguel},
	year = {2013},
	keywords = {Coffee Machine, Contract Theory, Model Check, Parallel Composition, Software Product Line},
	pages = {50--58},
	file = {Full Text PDF:files/923/Larsen e Legay - 2013 - Quantitative Modal Transition Systems.pdf:application/pdf},
}

@inproceedings{banville_validation_2018,
	address = {Cham},
	title = {Validation of a {Sorting} {Task} {Implemented} in the {Virtual} {Multitasking} {Task}-2 and {Effect} of {Aging}},
	isbn = {978-3-319-92046-7},
	doi = {10.1007/978-3-319-92046-7_4},
	abstract = {Normal aging is characterized by cognitive, functional, and neuroanatomic changes. Executive functions (EF), linked to the autonomy in the realization of instrumental Activity of Daily Living (iADL) are particularly sensitive to the effect of aging. To date, the best way to describe the cognitive profile of an individual is using traditional neuropsychological assessment. However, these tasks showed a week ecological validity that compromise the prediction of iADL functioning. Fortunately, virtual reality (VR) seems to be an interesting alternative to assess the functional capacities by reproducing everyday life situations accurately. Nevertheless, we know that the elderly could have some difficulty when interacting with technology (Banville et al. 2017). The aims of this study were (1) to differentiate performance of older adult compared to young people in a sorting task implemented in a virtual environment (VE) and (2) to analyze psychometric properties of that novel task. To do this, 30 participants were recruited in the general population and were divided in 2 groups based on their age. The results indicated that older subjects take more time than younger to compete the sorting task. Then, the scores obtained correlated significantly with some neuropsychological tasks, particularly with those assessing EF. To conclude, the VMT-2 could offer the opportunity to make a valid assessment of functional capacities during aging. Further work could validate the relevance to use it when the individual facing with mild cognitive impairment or dementia.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Information} in {Applications} and {Services}},
	publisher = {Springer International Publishing},
	author = {Banville, Frédéric and Lussier, Claudia and Massicotte, Edith and Verhulst, Eulalie and Couture, Jean-François and Allain, Philippe and Richard, Paul},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2018},
	keywords = {Neuropsychological assessment, Normal aging, Psychometry, Virtual reality},
	pages = {41--54},
	file = {Full Text PDF:files/936/Banville et al. - 2018 - Validation of a Sorting Task Implemented in the Virtual Multitasking Task-2 and Effect of Aging.pdf:application/pdf},
}

@inproceedings{emerson_investigating_2020,
	address = {Cham},
	title = {Investigating {Visitor} {Engagement} in {Interactive} {Science} {Museum} {Exhibits} with {Multimodal} {Bayesian} {Hierarchical} {Models}},
	isbn = {978-3-030-52237-7},
	doi = {10.1007/978-3-030-52237-7_14},
	abstract = {Engagement plays a critical role in visitor learning in museums. Devising computational models of visitor engagement shows significant promise for enabling adaptive support to enhance visitors’ learning experiences and for providing analytic tools for museum educators. A salient feature of science museums is their capacity to attract diverse visitor populations that range broadly in age, interest, prior knowledge, and socio-cultural background, which can significantly affect how visitors interact with museum exhibits. In this paper, we introduce a Bayesian hierarchical modeling framework for predicting learner engagement with Future Worlds, a tabletop science exhibit for environmental sustainability. We utilize multi-channel data (e.g., eye tracking, facial expression, posture, interaction logs) captured from visitor interactions with a fully-instrumented version of Future Worlds to model visitor dwell time with the exhibit in a science museum. We demonstrate that the proposed Bayesian hierarchical modeling approach outperforms competitive baseline techniques. These findings point toward significant opportunities for enriching our understanding of visitor engagement in science museums with multimodal learning analytics.},
	language = {en},
	booktitle = {Artificial {Intelligence} in {Education}},
	publisher = {Springer International Publishing},
	author = {Emerson, Andrew and Henderson, Nathan and Rowe, Jonathan and Min, Wookhee and Lee, Seung and Minogue, James and Lester, James},
	editor = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Millán, Eva},
	year = {2020},
	keywords = {Multimodal learning analytics, Museum-based learning, Visitor modeling},
	pages = {165--176},
	file = {Full Text PDF:files/940/Emerson et al. - 2020 - Investigating Visitor Engagement in Interactive Science Museum Exhibits with Multimodal Bayesian Hie.pdf:application/pdf},
}

@inproceedings{lopomo_reliable_2020,
	address = {Cham},
	title = {A {Reliable} and {Inexpensive} {Integration} of {Virtual} {Reality} and {Digital} {Human} {Modelling} to {Estimate} {Cervical} {Spine} {Function}},
	isbn = {978-3-030-49904-4},
	doi = {10.1007/978-3-030-49904-4_14},
	abstract = {Musculoskeletal disorders present one of the most prominent impact among the work-related diseases. Cervical spine is indeed one of the anatomical regions most affected by these issues; the main impairments concerning the cervical segment inherently limit its ranges of motion (ROMs). In the last years, novel technologies have been developed to support clinicians in assessing and quantifying these limitations, including wearable sensors and Virtual Reality (VR). In this perspective, interest in Digital Human Modeling has been also increasing due to the possibility of using it together with wearable technologies, thus to obtain enhanced information on body dynamics. This study aimed to validate a novel approach, which integrated VR technology and multi-body modelling to reliably estimated the ROMs of the cervical spine during the execution of three specific tasks (i.e. flexion-extension, lateral bending, axial rotation). Comparison with standard optoelectronic system reported strong correlation and good reliability, with an average difference in estimating ROMs of 8.0° and a mean RMSE of 4.7°. Furthermore, a preliminary test in managing different visual cues through VR highlighted interesting trends for future developments. The performed analysis supported the use of the proposed solution for both the clinical settings and telemedicine applications.},
	language = {en},
	booktitle = {Digital {Human} {Modeling} and {Applications} in {Health}, {Safety}, {Ergonomics} and {Risk} {Management}. {Posture}, {Motion} and {Health}},
	publisher = {Springer International Publishing},
	author = {Lopomo, Nicola Francesco and Mosna, Paolo and Lenzi, Stefano Elio and Standoli, Carlo Emilio and Perego, Paolo and Negrini, Stefano and Andreoni, Giuseppe},
	editor = {Duffy, Vincent G.},
	year = {2020},
	keywords = {Cervical spine, Digital Human Model, Functional test, Virtual Reality},
	pages = {178--193},
	file = {Full Text PDF:files/944/Lopomo et al. - 2020 - A Reliable and Inexpensive Integration of Virtual Reality and Digital Human Modelling to Estimate Ce.pdf:application/pdf},
}

@inproceedings{fachechi_i_2019,
	address = {Cham},
	title = {“{I} {Went} to {America} to {See} {Ancient} {Italian} {Paintings}”: {The} {Problem} of the {Re}-contextualization of {Artworks} {Uprooted} from {Their} {Original} {Settings}},
	isbn = {978-3-030-05819-7},
	shorttitle = {“{I} {Went} to {America} to {See} {Ancient} {Italian} {Paintings}”},
	doi = {10.1007/978-3-030-05819-7_15},
	abstract = {The paper aims to explore new methods of re-contextualization of artworks in their original settings, based on 3D reconstruction and 3D Web, through a case-study. It concerns a cycle of Medieval frescoes, detached from the walls of a monastery in Umbria, Central Italy, altered by numerous and some very recent renovations. At the present, the fragments of the cycle of frescoes are preserved in various museums, mostly in the United States.},
	language = {en},
	booktitle = {{VR} {Technologies} in {Cultural} {Heritage}},
	publisher = {Springer International Publishing},
	author = {Fachechi, Grazia Maria and Guidazzoli, Antonella and De Luca, Daniele and Liguori, Maria Chiara and Verri, Luigi and Bellavia, Giovanni},
	editor = {Duguleană, Mihai and Carrozzino, Marcello and Gams, Matjaž and Tanea, Iulian},
	year = {2019},
	keywords = {3D reconstruction, 3DWeb, American museums, Digital Heritage, Italian Medieval frescoes},
	pages = {197--205},
	file = {Full Text PDF:files/950/Fachechi et al. - 2019 - “I Went to America to See Ancient Italian Paintings” The Problem of the Re-contextualization of Art.pdf:application/pdf},
}

@inproceedings{leyer_3d_2019,
	address = {Cham},
	title = {{3D} {Virtual} {World} {BPM} {Training} {Systems}: {Process} {Gateway} {Experimental} {Results}},
	isbn = {978-3-030-21290-2},
	shorttitle = {{3D} {Virtual} {World} {BPM} {Training} {Systems}},
	doi = {10.1007/978-3-030-21290-2_26},
	abstract = {It is important for companies that their operational employees have profound knowledge of the processes in which their work is embedded. 3D virtual world (VW) environments are promising for learning, especially for complex processes that have deviations from the standard flow. We design a 3D VW process training environment to improve process learning, particularly for complex processes with alternative flows, represented with gateways in process models. We adopt the method of loci, which suggests the mental traversal of routines for improving learning. Our experiment with 145 participants compares the level of knowledge acquired for a sample process with our 3D VW environment and a 2D depiction. We found that the 3D VW environment significantly increases the level of process knowledge acquired across the typical gateways in processes. Our results contribute to our understanding of how individuals learn knowledge of processes via 3D environments. With a low initial investment, practitioners are encouraged to invest in 3D training systems for processes, since these can be set up once and reused multiple times for various employees.},
	language = {en},
	booktitle = {Advanced {Information} {Systems} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Leyer, Michael and Brown, Ross and Aysolmaz, Banu and Vanderfeesten, Irene and Turetken, Oktay},
	editor = {Giorgini, Paolo and Weber, Barbara},
	year = {2019},
	keywords = {Experiment, Gateways, Process model, Training, Virtual worlds},
	pages = {415--429},
	file = {Full Text PDF:files/955/Leyer et al. - 2019 - 3D Virtual World BPM Training Systems Process Gateway Experimental Results.pdf:application/pdf},
}

@inproceedings{lopes_interaction_2018,
	address = {Cham},
	title = {Interaction {Techniques} for {Immersive} {CT} {Colonography}: {A} {Professional} {Assessment}},
	isbn = {978-3-030-00934-2},
	shorttitle = {Interaction {Techniques} for {Immersive} {CT} {Colonography}},
	doi = {10.1007/978-3-030-00934-2_70},
	abstract = {CT Colonography (CTC) is considered the leading imaging technique for colorectal cancer (CRC) screening. However, conventional CTC systems rely on clumsy 2D input devices and stationary flat displays that make it hard to perceive the colon structure in 3D. To visualize such anatomically complex data, the immersion and freedom of movement afforded by Virtual Reality (VR) systems bear the promise to assist clinicians to improve 3D reading, hence, enabling more expedite diagnoses. To this end, we propose iCOLONIC, a set of interaction techniques using VR to perform CTC reading. iCOLONIC combines immersive Fly-Through navigation with positional tracking, multi-scale representations and mini-maps to guide radiologists and surgeons while navigating throughout the colon. Contrary to stationary VR solutions, iCOLONIC allows users to freely walk within a work space to analyze both local and global 3D features. To assess whether our non-stationary VR approach can assist clinicians in improving 3D colon reading and 3D perception, we conducted a user study with three senior radiologists, three senior general surgeons and one neuroradiology intern. Results from formal evaluation sessions demonstrate iCOLONIC’s usability and feasibility as the proposed interaction techniques were seen to improve spatial awareness and promote a more fluent navigation. Moreover, participants remarked that our approach shows great potential to speed up the screening process.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Lopes, Daniel Simões and Medeiros, Daniel and Paulo, Soraia Figueiredo and Borges, Pedro Brasil and Nunes, Vitor and Mascarenhas, Vasco and Veiga, Marcos and Jorge, Joaquim Armando},
	editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-López, Carlos and Fichtinger, Gabor},
	year = {2018},
	pages = {629--637},
	file = {Full Text PDF:files/958/Lopes et al. - 2018 - Interaction Techniques for Immersive CT Colonography A Professional Assessment.pdf:application/pdf},
}

@inproceedings{esposito_cooperative_2015,
	address = {Cham},
	title = {Cooperative {Robotic} {Gamma} {Imaging}: {Enhancing} {US}-guided {Needle} {Biopsy}},
	isbn = {978-3-319-24571-3},
	shorttitle = {Cooperative {Robotic} {Gamma} {Imaging}},
	doi = {10.1007/978-3-319-24571-3_73},
	abstract = {Sentinel lymph node (sLN) biopsy mostly requires an invasive surgical intervention to remove sLNs under radioguidance. We present an alternative method where live ultrasound is combined with live robotic gamma imaging to provide real-time anatomical and nuclear guidance of punch biopsies. The robotic arm holding a gamma camera is equipped with a system for inside-out tracking to directly retrieve the relative position of the US transducer with respect to itself. Based on this, the system cooperatively positions the gamma camera parallel to the US imaging plane selected by the physician for real-time multi-modal visualization. We validate the feasibility of this approach with a dedicated gelatine/agar biopsy phantom and show that lymph nodes separated by at least 10 mm can be distinguished.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} -- {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Esposito, Marco and Busam, Benjamin and Hennersperger, Christoph and Rackerseder, Julia and Lu, An and Navab, Nassir and Frisch, Benjamin},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro},
	year = {2015},
	keywords = {Freehand SPECT, Gamma Camera, Sentinel Lymph Node, Sentinel Lymph Node Resection, Visual Servoing},
	pages = {611--618},
	file = {Full Text PDF:files/962/Esposito et al. - 2015 - Cooperative Robotic Gamma Imaging Enhancing US-guided Needle Biopsy.pdf:application/pdf},
}

@inproceedings{voinea_exploring_2019,
	address = {Cham},
	title = {Exploring {Cultural} {Heritage} {Using} {Augmented} {Reality} {Through} {Google}’s {Project} {Tango} and {ARCore}},
	isbn = {978-3-030-05819-7},
	doi = {10.1007/978-3-030-05819-7_8},
	abstract = {This research is focused on the importance of using modern technologies in preserving and exploring Cultural Heritage (CH). Specifically, Augmented Reality (AR) has the potential to enhance the user experience related to cultural heritage. We briefly present the main technological approaches in CH and a state of the art in mobile augmented reality. The latest Software Development Kit (SDK) for building AR applications are reviewed and compared. The 3D object that participants could place in the real environment was obtained using photogrammetry, a popular and relatively easy to use digitization technique. The virtual object represents a fortified church and is part of a group of UNESCO monuments from the historical and ethnographical region called “Țara Bârsei”, located in Brasov, Romania. We also provided some guidelines to ensure an accurate 3D reconstruction of any object. We assess users’ perception regarding two mobile AR applications, one based on Project Tango while the other was developed using ARCore. Results confirm that AR improves user experience and increases the enjoyment of learning about cultural heritage.},
	language = {en},
	booktitle = {{VR} {Technologies} in {Cultural} {Heritage}},
	publisher = {Springer International Publishing},
	author = {Voinea, Gheorghe-Daniel and Girbacia, Florin and Postelnicu, Cristian Cezar and Marto, Anabela},
	editor = {Duguleană, Mihai and Carrozzino, Marcello and Gams, Matjaž and Tanea, Iulian},
	year = {2019},
	keywords = {Augmented reality, Cultural heritage, Project Tango},
	pages = {93--106},
	file = {Full Text PDF:files/885/Voinea et al. - 2019 - Exploring Cultural Heritage Using Augmented Reality Through Google’s Project Tango and ARCore.pdf:application/pdf},
}

@inproceedings{debenham_three_2006,
	address = {Boston, MA},
	title = {Three {Technologies} for {Automated} {Trading}},
	isbn = {978-0-387-34747-9},
	doi = {10.1007/978-0-387-34747-9_42},
	language = {en},
	booktitle = {Artificial {Intelligence} in {Theory} and {Practice}},
	publisher = {Springer US},
	author = {Debenham, John and Simoff, Simeon},
	editor = {Bramer, Max},
	year = {2006},
	keywords = {Epistemic Belief, Multiagent System, Software Agent, Virtual World, World Model},
	pages = {405--414},
	file = {Full Text PDF:files/888/Debenham e Simoff - 2006 - Three Technologies for Automated Trading.pdf:application/pdf},
}

@inproceedings{woo_computerized_2016,
	address = {Cham},
	title = {A {Computerized} {Measurement} of {CROM} ({Cervical} {Range} of {Motion}) by {Using} {Smartphone} {Based} {HMD} ({Head} {Mounted} {Display})},
	isbn = {978-3-319-40548-3},
	doi = {10.1007/978-3-319-40548-3_88},
	abstract = {This paper proposes a computerized measurement of cervical range of motion (CROM) without the help of experts. We aim to develop a reliable and easy-to-use application for CROM by using smartphone based head mounted display (HMD). This healthcare application provides a measuring instrument for 6 cervical motion and tangible visualization for the active cervical ROM data. This computerized approach will increase the accuracy of measurement of CROM by providing real-time feedback for the correct posture during examination.},
	language = {en},
	booktitle = {{HCI} {International} 2016 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Woo, Changgon and Park, Changhoon},
	editor = {Stephanidis, Constantine},
	year = {2016},
	keywords = {Cervical range of motion, Computerized measurement, Head mounted display, Healthcare application},
	pages = {531--535},
	file = {Full Text PDF:files/890/Woo e Park - 2016 - A Computerized Measurement of CROM (Cervical Range of Motion) by Using Smartphone Based HMD (Head Mo.pdf:application/pdf},
}

@inproceedings{ahmed_total_2017,
	address = {Cham},
	title = {Total {Immersion}: {Designing} for {Affective} {Symbiosis}  in a {Virtual} {Reality} {Game} with {Haptics}, {Biosensors},  and {Emotive} {Agents}},
	isbn = {978-3-319-57753-1},
	shorttitle = {Total {Immersion}},
	doi = {10.1007/978-3-319-57753-1_3},
	abstract = {Affective symbiosis for human–computer interaction refers to the dynamic relationship between the user and affective virtual agents. In order to facilitate a true, immersive experience, we believe that it is necessary to adapt the presentation of an affective agent to the user’s affective state. Investigating the experience, behavior, and physiological correlates of affective events, such as winning and losing during a competitive game, therefore might be used to adapt the agent’s emotional states and system events. An experimental virtual reality game environment was designed as a stepping stone toward a system to demonstrate affective symbiosis. Users were invited to play a game of air hockey with affective agents in virtual reality. We collected the electrocardiography, electrodermal activity, and postural data, as well as self-reports, to investigate how emotional events affected physiology, behavior, and experience. The users were found to be engaged in the competition strongly while only paying limited attention to their adversaries’ emotional expressions. We discuss how game events are much stronger causes for affective responses, with the physiological effects of winning and losing becoming more enhanced as the game progresses. We discuss how an affective, symbiotic system could implement both game events and dynamic, affective agents to create a system for total immersion.},
	language = {en},
	booktitle = {Symbiotic {Interaction}},
	publisher = {Springer International Publishing},
	author = {Ahmed, Imtiaj and Harjunen, Ville and Jacucci, Giulio and Ravaja, Niklas and Spapé, Michiel M.},
	editor = {Gamberini, Luciano and Spagnolli, Anna and Jacucci, Giulio and Blankertz, Benjamin and Freeman, Jonathan},
	year = {2017},
	keywords = {3D game, Affective symbiosis, Haptics, HCI, Immersion, Physiology, Symbiotic interaction, Virtual reality},
	pages = {23--37},
	file = {Full Text PDF:files/892/Ahmed et al. - 2017 - Total Immersion Designing for Affective Symbiosis  in a Virtual Reality Game with Haptics, Biosenso.pdf:application/pdf},
}

@inproceedings{novick_paolachat_2018,
	address = {Cham},
	title = {{PaolaChat}: {A} {Virtual} {Agent} with {Naturalistic} {Breathing}},
	isbn = {978-3-319-91581-4},
	shorttitle = {{PaolaChat}},
	doi = {10.1007/978-3-319-91581-4_26},
	abstract = {For embodied conversational agents (ECAs) the relationship between gesture and rapport is an open question. To enable us to learn whether adding breathing behaviors to an agent similar to SimSensei would lead users interacting to perceive the agent as more natural, we built an application, called Paola Chat, in which the ECA could display naturalistic breathing animations. Our study had two phases. In the first phase, we determined the most natural amplitude for the agent’s breathing. In the second phase, we assessed the effect of breathing on the users’ perceptions of rapport and naturalness. The study had a within-subjects design, with breathing/not-breathing as the independent variable. Despite our expectation that increased naturalness from breathing would lead users to report greater rapport in the breathing condition than in the not-breathing condition, the study’s results suggest that the animation of breathing appears to neither increase nor decrease these perceptions.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}: {Interaction}, {Navigation}, {Visualization}, {Embodiment}, and {Simulation}},
	publisher = {Springer International Publishing},
	author = {Novick, David and Afravi, Mahdokht and Camacho, Adriana},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2018},
	keywords = {Dialog system, Embodied conversational agents, Human-agent dialog},
	pages = {351--360},
	file = {Full Text PDF:files/914/Novick et al. - 2018 - PaolaChat A Virtual Agent with Naturalistic Breathing.pdf:application/pdf},
}

@inproceedings{garcia_immersive_2019,
	address = {Cham},
	title = {An {Immersive} {Virtual} {Reality} {Experience} for {Learning} {Spanish}},
	isbn = {978-3-030-21817-1},
	doi = {10.1007/978-3-030-21817-1_12},
	abstract = {While increases in the development of virtual reality (VR) have expanded the possibilities of learning foreign languages, research for computer assisted language learning in VR is lacking [1]. Virtual reality headsets continue to become more accessible to end users, and there is a current lack of educational language learning applications for such platforms. There are many tools and applications for language learning, however most lack the aspect of immersion – a proven method of learning a foreign language as seen in experiments involving overseas language experiences [2]. Therefore, this paper presents an interactive VR experience for learning Spanish through an immersive interaction and game-play, followed by the results from a focus group regarding user experience feedback. Additionally, this paper will detail the design and development of the current prototype. Results indicate that participants found this method of language learning to be more enjoyable than traditional methods because it did not feel like they were studying.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Ubiquitous} and {Virtual} {Environments} for {Learning} and {Collaboration}},
	publisher = {Springer International Publishing},
	author = {Garcia, Sarah and Laesker, Denis and Caprio, Derek and Kauer, Ronald and Nguyen, Jason and Andujar, Marvin},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2019},
	keywords = {Game-based learning, Gaming, Immersion, Language education, Language learning, Virtual reality},
	pages = {151--161},
	file = {Full Text PDF:files/917/Garcia et al. - 2019 - An Immersive Virtual Reality Experience for Learning Spanish.pdf:application/pdf},
}

@inproceedings{larradet_affective_2020,
	address = {Cham},
	title = {Affective {Communication} {Enhancement} {System} for {Locked}-{In} {Syndrome} {Patients}},
	isbn = {978-3-030-49282-3},
	doi = {10.1007/978-3-030-49282-3_10},
	abstract = {Patients with Locked-In Syndrome such as people with Amyotrophic Lateral Sclerosis (ALS) rely on technology for basic communication. However, available Augmentative and Alternative Communication (AAC) tools such as gaze-controlled keyboards have limited abilities. In particular, they do not allow for expression of emotions in addition to words. In this paper we propose a novel gaze-based speaking tool that enable locked-in syndrome patients to express emotions as well as sentences. It also features patient-controlled emotionally modulated speech synthesis. Additionally, an emotional 3D avatar can be controlled by the patient to represent emotional facial-expressions. The systems were tested with 36 people without disabilities separated into an affective group - full control of emotional voice, avatar facial expressions and laugh - and a control group - no emotional tools. The study proved the system’s capacity to enhance communication for both the patient and the interlocutor. The emotions embedded in the synthesized voices were found recognizable at 80\% on the first trial and 90\% on the second trial. The conversation was perceived as more natural when using the affective tool. The subjects felt it was easier to express and identify emotions using this system. The emotional voice and the emotional avatar were found to help the conversation. This highlights the needs for more affective-driven communicative solutions for locked-in patients.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Design} {Approaches} and {Supporting} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Larradet, Fanny and Barresi, Giacinto and Mattos, Leonardo S.},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2020},
	keywords = {Communication, Emotions, Eye-tracking, Gaze},
	pages = {143--156},
	file = {Full Text PDF:files/921/Larradet et al. - 2020 - Affective Communication Enhancement System for Locked-In Syndrome Patients.pdf:application/pdf},
}

@inproceedings{liu_yuri_2019,
	address = {Cham},
	title = {Yuri {Game}: {Romance} and {Characterization} in {Gameplay}},
	isbn = {978-3-030-22602-2},
	shorttitle = {Yuri {Game}},
	doi = {10.1007/978-3-030-22602-2_13},
	abstract = {Yuri games are generally considered as a category of games that the theme of them focus on yuri relationship and stories between characters rather than gameplay. The concept of yuri originates from Japan and refers to the intense emotional connection in female/female relationship regardless of the sexual orientation of participants. This research explores the method of gameplay in yuri games and the way gameplay can influence the characterization of game heroines, thus have an impact on the story-telling of the whole romance as well as the player experience. The analysis focuses on several typical cases of yuri games.},
	language = {en},
	booktitle = {{HCI} in {Games}},
	publisher = {Springer International Publishing},
	author = {Liu, Yueqian},
	editor = {Fang, Xiaowen},
	year = {2019},
	keywords = {Characterization, Gameplay, Interactive movie, Romantic relationship, Visual novel, Yuri game},
	pages = {154--164},
	file = {Full Text PDF:files/924/Liu - 2019 - Yuri Game Romance and Characterization in Gameplay.pdf:application/pdf},
}

@inproceedings{de_albuquerque_modelling_2017,
	address = {Cham},
	title = {Modelling {Playful} {User} {Interfaces} for {Hybrid} {Games}},
	isbn = {978-3-319-58697-7},
	doi = {10.1007/978-3-319-58697-7_48},
	abstract = {Both toy and games industries are investing in hybrid play products. In these scenarios user access the system using toys as input/output, thus, they consist of playful user interfaces. Such systems are complex artifacts since they use real and virtual information. Therefore, they present new challenges to both designers and developers. We supposed both industries could benefit from hybrid design approaches since product concepts. Hence, we applied the model of hybrid gameplay as a practical tool for designing such systems. To achieve it, we included the model in a 16-week class to design hybrid games. In this paper, we detailed model usage in course schedule, and discussed how students experienced it. Besides, we presented student’s six working prototypes, including design cycles, and playtesting sessions. After class, we conducted semi-structured interviews with student’s representatives. Results revealed model usefulness to describe the system setup and interface elements. Furthermore, according to students, the model vocabulary facilitated communication among team members. Finally, we proposed improvements in model nomenclature based on student’s feedback. In addition, we recommend a few topics for a methodological approach to design hybrid games.},
	language = {en},
	booktitle = {Distributed, {Ambient} and {Pervasive} {Interactions}},
	publisher = {Springer International Publishing},
	author = {de Albuquerque, Anna Priscilla and Borba Breyer, Felipe and Kelner, Judith},
	editor = {Streitz, Norbert and Markopoulos, Panos},
	year = {2017},
	keywords = {Hybrid games, Playful user interfaces, Smart toys},
	pages = {640--659},
	file = {Full Text PDF:files/938/de Albuquerque et al. - 2017 - Modelling Playful User Interfaces for Hybrid Games.pdf:application/pdf},
}

@inproceedings{seaborn_evaluating_2016,
	address = {Cham},
	title = {Evaluating {Hedonic} and {Eudaimonic} {Motives} in {Human}-{Computer} {Interaction}},
	isbn = {978-3-319-40397-7},
	doi = {10.1007/978-3-319-40397-7_47},
	abstract = {New measures of well-being are drawing the attention of researchers and practitioner in human factors generally and human-computer interaction (HCI) in particular. Following in the footsteps of previous scholarly endeavours in hedonic well-being (HWB), this paper argues for the adoption of eudaimonic well-being (EWB) in explorations of well-being in HCI. To this end, I report on initial findings from research in which I have evaluated the impact of hedonic and eudaimonic motives on gaming experience using a validated instrument developed by psychologists and adapted for use in HCI contexts.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}: {Applications} and {Services}},
	publisher = {Springer International Publishing},
	author = {Seaborn, Katie},
	editor = {Yamamoto, Sakae},
	year = {2016},
	keywords = {Eudaimonia, Hedonia, Human factors, Human-computer interaction, Interaction design, Motivation, Quality of life, Theory, Well-being},
	pages = {494--500},
	file = {Full Text PDF:files/941/Seaborn - 2016 - Evaluating Hedonic and Eudaimonic Motives in Human-Computer Interaction.pdf:application/pdf},
}

@inproceedings{kato_generating_2018,
	address = {Cham},
	title = {Generating {Training} {Images} {Using} a {3D} {City} {Model} for {Road} {Sign} {Detection}},
	isbn = {978-3-319-92285-0},
	doi = {10.1007/978-3-319-92285-0_51},
	abstract = {In order to prevent traffic accidents due to mistakes in checking road signs, a method for detecting road signs from an image shot by an in-vehicle camera has been developed. On the other hand, Deep Learning which is frequently used in recent years requires preparing a large amount of training data, and it is difficult to photograph road signs from various directions at various places. In this research, we propose a method for generating training images for Deep Learning using 3D urban model simulation for detecting road signs. The appearance of road signs taken in the simulation depends on the distance and direction from the camera and the brightness of the scene. These changes were applied to Japanese road signs, and 303,750 types of sign images and their mask areas were automatically generated and used for training. As a result of training YOLO detectors using these training images, in detection for some road sign class groups, the F values of 66.7\% to 88.9\% could be obtained.},
	language = {en},
	booktitle = {{HCI} {International} 2018 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Kato, Ryuto and Nishiguchi, Satoshi and Hashimoto, Wataru and Mizutani, Yasuharu},
	editor = {Stephanidis, Constantine},
	year = {2018},
	keywords = {3D simulation, Deep Learning, Generating training data, Object detection, Road sign},
	pages = {381--386},
	file = {Full Text PDF:files/945/Kato et al. - 2018 - Generating Training Images Using a 3D City Model for Road Sign Detection.pdf:application/pdf},
}

@inproceedings{novick_pedagogical-agent_2019,
	address = {Cham},
	title = {Pedagogical-{Agent} {Learning} {Companions} in a {Virtual} {Reality} {Educational} {Experience}},
	isbn = {978-3-030-21817-1},
	doi = {10.1007/978-3-030-21817-1_15},
	abstract = {This research studies pedagogical agents as a learning companion (PALs) and aims to find which approach is more effective for learners in terms of learning, engagement and rapport: a PAL that is the same gender as the student, a PAL that is female, or a PAL that is male. We compared results in terms of learning and rapport with respect to the gender of a PAL. To this end, we implemented a multimodal interactive virtual-reality application to teach students about the 1770 Boston Massacre. Our results suggest that (1) both male and female participants will learn well with a female PAL, and (2) differences in rapport do not seem to affect learning.},
	language = {en},
	booktitle = {Learning and {Collaboration} {Technologies}. {Ubiquitous} and {Virtual} {Environments} for {Learning} and {Collaboration}},
	publisher = {Springer International Publishing},
	author = {Novick, David and Afravi, Mahdokht and Camacho, Adriana and Rodriguez, Aaron and Hinojos, Laura},
	editor = {Zaphiris, Panayiotis and Ioannou, Andri},
	year = {2019},
	keywords = {Dialog system, Embodied conversational agents, Human-agent dialog},
	pages = {193--203},
	file = {Full Text PDF:files/948/Novick et al. - 2019 - Pedagogical-Agent Learning Companions in a Virtual Reality Educational Experience.pdf:application/pdf},
}

@inproceedings{vallez_weapon_2019,
	address = {Cham},
	title = {Weapon {Detection} for {Particular} {Scenarios} {Using} {Deep} {Learning}},
	isbn = {978-3-030-31321-0},
	doi = {10.1007/978-3-030-31321-0_32},
	abstract = {The development of object detection systems is normally driven to achieve both high detection and low false positive rates in a certain public dataset. However, when put into a real scenario the result is generally an unacceptable rate of false alarms. In this context we propose to add an additional step that models and filters the typical false alarms of the new scenario while roughly maintaining the ability to detect the objects of interest. We propose to use the false alarms of the new scenario to train a deep autoencoder and to model them. The latter will act as a filter that checks whether the output of the detector is one of its typical false positives or not based on the reconstruction error measured with the Mean Squared Error (MSE) and the Peak Signal-to-Noise Ratio (PSNR). We test the system using an entirely synthetic novel dataset for training and testing the autoencoder generated with Unreal Engine 4. Results show a reduction in the number of FPs of up to 37.9\% in combination with the PSNR error while maintaining the same detection capability.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Vallez, Noelia and Velasco-Mata, Alberto and Corroto, Juan Jose and Deniz, Oscar},
	editor = {Morales, Aythami and Fierrez, Julian and Sánchez, José Salvador and Ribeiro, Bernardete},
	year = {2019},
	keywords = {Autoencoder, False positive ratio, Object detection, One-class classification},
	pages = {371--382},
	file = {Full Text PDF:files/953/Vallez et al. - 2019 - Weapon Detection for Particular Scenarios Using Deep Learning.pdf:application/pdf},
}

@inproceedings{rise_gesture-based_2020,
	address = {Cham},
	title = {Gesture-{Based} Ιnteraction: {Visual} {Gesture} {Mapping}},
	isbn = {978-3-030-49062-1},
	shorttitle = {Gesture-{Based} Ιnteraction},
	doi = {10.1007/978-3-030-49062-1_7},
	abstract = {Gesture-based interaction allows for interacting with computers, machines and robots in an intuitive way without direct physical contact. The challenge is that there are no agreed-upon interaction patterns for gesture-based interaction in VR and AR environments. In this paper we have developed a set of 10 gestures and corresponding visualizations in the following categories of gestures: (1) directional movement, (2) flow control, (3) spatial orientation, (4) multifunctional gestures, and (5) tactile gestures. One of the multifunctional gestures and its visualization were selected for usability testing (N = 18) in a 3D car track simulator. We found that the visualization made it faster and easier to understand the interaction made the interaction more precise. Further, we learned that the visualization worked well as guidance to learn to control the car but could be removed after a while as the user had learned the interaction. By combining gestures from the library, gesture-based interaction can be used to control advanced machines, robots and drones in an intuitive and non-strenuous way.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Multimodal} and {Natural} {Interaction}},
	publisher = {Springer International Publishing},
	author = {Rise, Kasper and Alsos, Ole Andreas},
	editor = {Kurosu, Masaaki},
	year = {2020},
	keywords = {Gesture control, Gesture-based interaction, Virtual reality},
	pages = {106--124},
	file = {Full Text PDF:files/959/Rise e Alsos - 2020 - Gesture-Based Ιnteraction Visual Gesture Mapping.pdf:application/pdf},
}

@inproceedings{schaffer_guidance_2020,
	address = {Cham},
	title = {Guidance {Is} {Good} or {Avoid} {Too} {Much} {Hand}-{Holding}? {Proposing} a {Controlled} {Experiment} on the {Impact} of {Clear} {Proximal} {Goals} on {Digital} {Game} {Enjoyment}},
	isbn = {978-3-030-50164-8},
	shorttitle = {Guidance {Is} {Good} or {Avoid} {Too} {Much} {Hand}-{Holding}?},
	doi = {10.1007/978-3-030-50164-8_12},
	abstract = {Empirical research on what makes digital games enjoyable is critical for practitioners and researchers who want to design for enjoyment. This is true not only for Game Design, but for Gamification of non-game applications, and Serious Games with a purpose beyond enjoyment. But existing theories are incomplete or lacking empirical support. Flow is the psychological state of “getting in the zone”, of enjoying overcoming challenges for the sake of the enjoyment they provide. Flow theory suggests that Clear Proximal Goals, knowing what to do next throughout an activity, is a flow condition, or a factor that leads to the flow state. However, there is a popular notion among game designers and developers that it is better to avoid too much hand-holding, and to allow game players to figure out what to do themselves rather than guiding them each step of the way. These appear to be mutually exclusive assertions that cannot both be true. Does more guidance increase or decrease enjoyment? To resolve this controversy, an online controlled experiment with a 2 × 2 factorial design and a minimum of 280 total participants is proposed to test the impact of Clear Proximal Goals on Task Engagement (flow not including enjoyment) and Enjoyment. The presence or absence of on-screen text prompts and navigational assistance will be used to manipulate clear proximal goals. This study will advance the study of game enjoyment by testing how the clarity of players’ next steps impacts game enjoyment using random assignment to different game designs.},
	language = {en},
	booktitle = {{HCI} in {Games}},
	publisher = {Springer International Publishing},
	author = {Schaffer, Owen},
	editor = {Fang, Xiaowen},
	year = {2020},
	keywords = {Clear proximal goals, Computer games, Controlled experiment, Digital games, Enjoyment, Flow, Game design, Gamification, Intrinsic motivation, Serious games, Task engagement},
	pages = {179--185},
	file = {Full Text PDF:files/963/Schaffer - 2020 - Guidance Is Good or Avoid Too Much Hand-Holding Proposing a Controlled Experiment on the Impact of.pdf:application/pdf},
}

@inproceedings{klamka_charm_2019,
	address = {Cham},
	title = {{CHARM}: {Cord}-{Based} {Haptic} {Augmented} {Reality} {Manipulation}},
	isbn = {978-3-030-21607-8},
	shorttitle = {{CHARM}},
	doi = {10.1007/978-3-030-21607-8_8},
	abstract = {The recent trend of emerging high-quality Augmented Reality (AR) glasses offered the possibility for visually exciting application scenarios. However, the interaction with these devices is often challenging since current input methods most of the time lack haptic feedback and are limited in their user interface controls. With this work, we introduce , a combination of a belt-worn interaction device, utilizing a retractable cord, and a set of interaction techniques to enhance AR input capabilities with physical controls and spatial constraints. Building on our previous research, we created a fully-functional prototype to investigate how body-worn string devices can be used to support generic AR tasks. We contribute a radial widget menu for system control as well as transformation techniques for 3D object manipulation. To validate our interaction concepts for system control, we implemented a mid-air gesture interface as a baseline and evaluated our prototype in two formative user studies. Our results show that our approach provides flexibility regarding possible interaction mappings and was preferred for manipulation tasks compared to mid-air gesture input.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Multimodal} {Interaction}},
	publisher = {Springer International Publishing},
	author = {Klamka, Konstantin and Reipschläger, Patrick and Dachselt, Raimund},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {3D interaction, 3D transformation, Augmented reality, Cord input, Elastic input, Haptic feedback, Radial menu, Wearable computing},
	pages = {96--114},
	file = {Full Text PDF:files/966/Klamka et al. - 2019 - CHARM Cord-Based Haptic Augmented Reality Manipulation.pdf:application/pdf},
}

@inproceedings{kabil_richie_2015,
	address = {Cham},
	title = {{RICHIE}: {A} {Step}-by-step {Navigation} {Widget} to {Enhance} {Broad} {Hierarchy} {Exploration} on {Handheld} {Tactile} {Devices}},
	isbn = {978-3-319-20916-6},
	shorttitle = {{RICHIE}},
	doi = {10.1007/978-3-319-20916-6_19},
	abstract = {Exploring large hierarchies is still a challenging task, especially for handheld tactile devices, due to the lack of visualization space and finger’s occlusion. In this paper, we propose the RICHIE (Radial InCremental HIerarchy Exploration) tool, a new radial widget that allows step-by-step navigation through large hierarchies. We designed it to fit handheld tactile requirements such as target reaching and space optimization. Depth exploration is made by shifting two levels of hierarchy at the same time, for reducing the screen occupation. This widget was implemented in order to adapt a Command and Control (C2) system to mobile tactile devices, as these systems require the on-screen presence of an important unit’s hierarchy (the ORder of BATtle). Nevertheless, we are convinced that RICHIE could be used on several systems that require hierarchical data exploration, such as phylogenetic trees or file browsing.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}: {Interaction} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Kabil, Alexandre and Kubicki, Sébastien},
	editor = {Kurosu, Masaaki},
	year = {2015},
	keywords = {Hierarchy, Information visualization, Multi-touch},
	pages = {196--207},
	file = {Full Text PDF:files/968/Kabil e Kubicki - 2015 - RICHIE A Step-by-step Navigation Widget to Enhance Broad Hierarchy Exploration on Handheld Tactile.pdf:application/pdf},
}

@inproceedings{zielke_serious-game_2015,
	address = {Cham},
	title = {A {Serious}-{Game} {Framework} to {Improve} {Physician}/{Nurse} {Communication}},
	isbn = {978-3-319-21067-4},
	doi = {10.1007/978-3-319-21067-4_35},
	abstract = {This paper focuses on a serious-game framework for a dialogue-driven game called GLIMPSE (A Game to Learn Important Communications Methods for Patient Safety Enhancement). The eight essential components of the framework include: recommended communication behavior; accurate translation; narrative-driven, role-playing episodes that allow practice in different challenging situations; perspective sharing mechanisms; a design paradigm that accommodates time challenges of participants; motivational gameplay rewards; feedback/assessment mechanisms; and curriculum. The paper explores how the framework was developed as well as implementation challenges, lessons learned and opportunities for future research.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Zielke, Marjorie and Houston, Susan and Mancini, Mary Elizabeth and Hardee, Gary and Cole, Louann and Zakhidov, Djakhangir and Fischer, Ute and Lewis, Timothy},
	editor = {Shumaker, Randall and Lackey, Stephanie},
	year = {2015},
	keywords = {Dashboards, Interprofessional communication, Learning portals, Narrative systems, Patient safety, Perspective sharing, Persuasive technology, Physician/nurse communication, Role-playing, SBAR, Serious game framework, Serious games, Team-based communication},
	pages = {337--348},
	file = {Full Text PDF:files/972/Zielke et al. - 2015 - A Serious-Game Framework to Improve PhysicianNurse Communication.pdf:application/pdf},
}

@inproceedings{schweigkofler_development_2018,
	address = {Cham},
	title = {Development of a {Digital} {Platform} {Based} on the {Integration} of {Augmented} {Reality} and {BIM} for the {Management} of {Information} in {Construction} {Processes}},
	isbn = {978-3-030-01614-2},
	doi = {10.1007/978-3-030-01614-2_5},
	abstract = {The construction industry is a project-based industry characterized by heterogeneity, extreme complexity and fragmented supply chain. Its complexity is increased by mutual relationships between different stakeholders involved in the creation, management and efficient exploitation of engineering data. Over the years, productivity and reliability in CI has been struggled by a difficulty in sharing information between construction project participants and in providing accurate information on site, which is a primary cause of poor performances.},
	language = {en},
	booktitle = {Product {Lifecycle} {Management} to {Support} {Industry} 4.0},
	publisher = {Springer International Publishing},
	author = {Schweigkofler, Alice and Monizza, Gabriele Pasetti and Domi, Erdal and Popescu, Andrei and Ratajczak, Julia and Marcher, Carmen and Riedl, Michael and Matt, Dominik},
	editor = {Chiabert, Paolo and Bouras, Abdelaziz and Noël, Frédéric and Ríos, José},
	year = {2018},
	keywords = {3D model-based platform, Augmented reality, BIM metadata, Industry 4.0, Product development},
	pages = {46--55},
	file = {Full Text PDF:files/975/Schweigkofler et al. - 2018 - Development of a Digital Platform Based on the Integration of Augmented Reality and BIM for the Mana.pdf:application/pdf},
}

@inproceedings{li_hampa_2020,
	address = {Cham},
	title = {Hampa: {Solver}-{Aided} {Recency}-{Aware} {Replication}},
	isbn = {978-3-030-53288-8},
	shorttitle = {Hampa},
	doi = {10.1007/978-3-030-53288-8_16},
	abstract = {Replication is a common technique to build reliable and scalable systems. Traditional strong consistency maintains the same total order of operations across replicas. This total order is the source of multiple desirable consistency properties: integrity, convergence and recency. However, maintaining the total order has proven to inhibit availability and performance. Weaker notions exhibit responsiveness and scalability; however, they forfeit the total order and hence its favorable properties. This project revives these properties with as little coordination as possible. It presents a tool called \$\${\textbackslash}textsc \{Hampa\}\$\$that given a sequential object with the declaration of its integrity and recency requirements, automatically synthesizes a correct-by-construction replicated object that simultaneously guarantees the three properties. It features a relational object specification language and a syntax-directed analysis that infers optimum staleness bounds. Further, it defines coordination-avoidance conditions and the operational semantics of replicated systems that provably guarantees the three properties. It characterizes the computational power and presents a protocol for recency-aware objects. \$\${\textbackslash}textsc \{Hampa\}\$\$uses automatic solvers statically and embeds them in the runtime to dynamically decide the validity of coordination-avoidance conditions. The experiments show that recency-aware objects reduce coordination and response time.},
	language = {en},
	booktitle = {Computer {Aided} {Verification}},
	publisher = {Springer International Publishing},
	author = {Li, Xiao and Houshmand, Farzin and Lesani, Mohsen},
	editor = {Lahiri, Shuvendu K. and Wang, Chao},
	year = {2020},
	pages = {324--349},
	file = {Full Text PDF:files/977/Li et al. - 2020 - Hampa Solver-Aided Recency-Aware Replication.pdf:application/pdf},
}

@inproceedings{arrighi_towards_2016,
	address = {Cham},
	title = {Towards {Co}-designing with {Users}: {A} {Mixed} {Reality} {Tool} for {Kansei} {Engineering}},
	isbn = {978-3-319-33111-9},
	shorttitle = {Towards {Co}-designing with {Users}},
	doi = {10.1007/978-3-319-33111-9_68},
	abstract = {Some costly and complex technical products, such as walking assistance devices, require ad-hoc design processes to address the very specific needs of each user. However, the depiction of customer/user requirements in the early stage of design stands difficult due to their subjective nature and the separation between the user and the designer. To bridge these gaps, we introduce the definition of a new modular digital toolbox based upon mixed reality system and kansei engineering techniques. The hardware consists in modular Tangible User Interfaces (TUIs), custom made by 3D printing and powered by a 3D game engine. The interactive content is displayed in mixed reality, simultaneously to the user and the designer. Kansei data are collected through questionnaires and psychophysical measurements, during multiple collaboration phases. The modularity of the system allows the evaluation of various TUIs, 3D content behaviours and the best fitting type of display.},
	language = {en},
	booktitle = {Product {Lifecycle} {Management} in the {Era} of {Internet} of {Things}},
	publisher = {Springer International Publishing},
	author = {Arrighi, Pierre-Antoine and Maurya, Santosh and Mougenot, Céline},
	editor = {Bouras, Abdelaziz and Eynard, Benoit and Foufou, Sebti and Thoben, Klaus-Dieter},
	year = {2016},
	keywords = {Collaborative design, Customer requirements, Kansei engineering, Mixed reality, Tangible user interfaces},
	pages = {751--760},
	file = {Full Text PDF:files/916/Arrighi et al. - 2016 - Towards Co-designing with Users A Mixed Reality Tool for Kansei Engineering.pdf:application/pdf},
}

@inproceedings{gameiro_kinect-sign_2014,
	address = {Berlin, Heidelberg},
	title = {Kinect-{Sign}: {Teaching} {Sign} {Language} to “{Listeners}” through a {Game}},
	isbn = {978-3-642-55143-7},
	shorttitle = {Kinect-{Sign}},
	doi = {10.1007/978-3-642-55143-7_6},
	abstract = {Sign language is the hearing impaired form of communicating with other people, including listeners. Most cases, impaired people have learned sign language form childhood. The problem arises when a listener comes in contact with an impaired person. For instances, if a couple has a child which is impaired, the parents find a challenge to learn the sign language. In this article, a new playful approach to assist the listeners to learn sign language is proposed. This proposal is a serious game composed of two modes: School-mode and Competition-mode. The first offers a virtual school where the user learns to sign letters and the second offers an environment towards applying the learned letters. Behind the scenes, the proposal contains a sign language recognition system, based on three modules: 1 – the standardization of the Kinect depth camera data; 2 – a gesture library relying on the standardized data; and 3 – the real-time recognition of gestures. A prototype was developed – Kinect-Sign – and tested in a Portuguese Sign-Language school and on eNTERFACE’13 resulting in a joyful acceptance of the approach.},
	language = {en},
	booktitle = {Innovative and {Creative} {Developments} in {Multimodal} {Interaction} {Systems}},
	publisher = {Springer},
	author = {Gameiro, João and Cardoso, Tiago and Rybarczyk, Yves},
	editor = {Rybarczyk, Yves and Cardoso, Tiago and Rosas, João and Camarinha-Matos, Luis M.},
	year = {2014},
	keywords = {Gesture Recognition, Kinect Sensor, Serious Game, Sign Language},
	pages = {141--159},
	file = {Full Text PDF:files/919/Gameiro et al. - 2014 - Kinect-Sign Teaching Sign Language to “Listeners” through a Game.pdf:application/pdf},
}

@inproceedings{cagiltay_innovative_2014,
	address = {Cham},
	title = {Innovative {Educational} {Technology} for {Special} {Education} and {Usability} {Issues}},
	isbn = {978-3-319-07635-5},
	doi = {10.1007/978-3-319-07635-5_16},
	abstract = {The purpose of this study is to introduce educational technology project, OZTEK, for special education students and present usability issues related to those developed technologies. With the OZTEK, the researchers intend to develop innovative, technology enhanced learning environments to support the education of children with such special needs and to investigate effectiveness of such learning environments.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {User} {Experience} {Design} for {Everyday} {Life} {Applications} and {Services}},
	publisher = {Springer International Publishing},
	author = {Cagiltay, Kursat and Cicek, Filiz and Karasu, Necdet and Cakir, Hasan and Kaplan Akilli, Goknur},
	editor = {Marcus, Aaron},
	year = {2014},
	keywords = {innovative technology, special needs, students with special needs, technology enhanced learning environments, Usability},
	pages = {155--163},
	file = {Full Text PDF:files/922/Cagiltay et al. - 2014 - Innovative Educational Technology for Special Education and Usability Issues.pdf:application/pdf},
}

@inproceedings{kang_user_2016,
	address = {Cham},
	title = {User {Perceptions} of a {Virtual} {Human} {Over} {Mobile} {Video} {Chat} {Interactions}},
	isbn = {978-3-319-39513-5},
	doi = {10.1007/978-3-319-39513-5_10},
	abstract = {We believe that virtual humans, presented over video chat services, such as Skype, and delivered using smartphones, can be an effective way to deliver innovative applications where social interactions are important, such as counseling and coaching. To explore this subject, we have built a hardware and software apparatus that allows virtual humans to initiate, receive, and interact over video calls using Skype or any similar service. With this platform, we conducted two experiments to investigate the applications and characteristics of virtual humans that interact over mobile video. In Experiment 1, we investigated user reactions to the physical realism of the background scene in which a virtual human was displayed. In Experiment 2, we examined how virtual characters can establish and maintain longer term relationships with users, using ideas from Social Exchange Theory to strengthen bonds between interactants. Experiment 2 involved repeated interactions with a virtual human over a period of time. Both studies used counseling-style interactions with users. The results demonstrated that males were more attracted socially to a virtual human that was presented over a realistic background than a featureless background while females were more socially attracted to a virtual human with a less realistic featureless background. The results further revealed that users felt the virtual human was a compassionate partner when they interacted with the virtual human over multiple calls, rather than just a single call.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Novel} {User} {Experiences}},
	publisher = {Springer International Publishing},
	author = {Kang, Sin-Hwa and Phan, Thai and Bolas, Mark and Krum, David M.},
	editor = {Kurosu, Masaaki},
	year = {2016},
	keywords = {Counseling coaches, Smartphones, Video chats, Virtual humans},
	pages = {107--118},
	file = {Full Text PDF:files/925/Kang et al. - 2016 - User Perceptions of a Virtual Human Over Mobile Video Chat Interactions.pdf:application/pdf},
}

@inproceedings{shikine_game_2017,
	address = {Cham},
	title = {A {Game} {System} for {Learning} {Mathematics} with {Pacing} {Considering} {Individual} {Motivation} and {Feeling}},
	isbn = {978-3-319-66715-7},
	doi = {10.1007/978-3-319-66715-7_17},
	abstract = {In these days, there are many e-learning system for studying by yourself. In these systems, learner can make their own pacing in for each learning section. But many learner needs more shorter term pacing adjusted to individual motivation and feeling in one session. Many video learning system has lack of interaction. On the other hand, novel game type e-learning system can interact to learner. But, previous novel game system can not keep the prosody information. So this paper proposed new novel game system “NOVELICA” which segmented the lesson to conversation size. And you can easily see the head of conversation with no stress by using NOVELICA. We compared with NOVELICA to previous novel game system and video learning system. And we investigate how different in these 3 contents. Finally, we found that NOVELICA is tend to be less stressful and keeping arousal in learning mathematics.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2017},
	publisher = {Springer International Publishing},
	author = {Shikine, Nobumitsu and Yamanaka, Toshimasa and Hoshino, Junichi},
	editor = {Munekata, Nagisa and Kunita, Itsuki and Hoshino, Junichi},
	year = {2017},
	keywords = {e-Learning, Instructional pacing, Math anxiety, Novel game, Remedial education, Video learning},
	pages = {169--176},
	file = {Full Text PDF:files/939/Shikine et al. - 2017 - A Game System for Learning Mathematics with Pacing Considering Individual Motivation and Feeling.pdf:application/pdf},
}

@inproceedings{sourina_eeg-based_2011,
	address = {Berlin, Heidelberg},
	title = {{EEG}-{Based} {Personalized} {Digital} {Experience}},
	isbn = {978-3-642-21663-3},
	doi = {10.1007/978-3-642-21663-3_64},
	abstract = {To make human computer interfaces more immersive and intuitive, a new dimension could be added. Real-time brain state recognition from EEG including emotion recognition and level of concentration recognition would make an access to information more adaptive and personalized. Modern EEG techniques give us an easy and portable way to monitor brain activities by using suitable signal processing and classification methods and algorithms. We proposed a new subject-dependent fractal-based approach to brain state recognition and innovative applications based on EEG-enable user’s interaction. The algorithms of the “inner” brain state quantification including emotion recognition would advance research on human computer interaction bringing the proposed novel objective quantification methods and algorithms as new tools in medical, entertainment, and even digital art methodology applications, and allowing us an integration of the brain state quantification algorithms in the human computer interfaces. In this paper, we describe our fractal-based approach to the brain state recognition and its EEG-enable applications such as serious games, emotional avatar, music therapy, music player, and storytelling.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Users} {Diversity}},
	publisher = {Springer},
	author = {Sourina, Olga and Liu, Yisi and Wang, Qiang and Nguyen, Minh Khoa},
	editor = {Stephanidis, Constantine},
	year = {2011},
	keywords = {BCI, emotion recognition, fractal dimension, HCI, music therapy, serious game, storytelling},
	pages = {591--599},
	file = {Full Text PDF:files/942/Sourina et al. - 2011 - EEG-Based Personalized Digital Experience.pdf:application/pdf},
}

@inproceedings{quinten_exploring_2015,
	address = {Cham},
	title = {Exploring {Deep} {Content} in {Physical} {Rehabilitation} {Games}},
	isbn = {978-3-319-24589-8},
	doi = {10.1007/978-3-319-24589-8_36},
	abstract = {This paper argues that game mechanics are important tools to combine rehabilitation therapy concerns with immersive game play. Through the practical design of a game we describe how properties of game mechanics (actions, attributes, dynamics, rules, space, and skill/ chance) connect to elements of rehabilitation therapy (exercise motion, parameters, therapy context, goals, motion trajectory, and motion constraints). We aim to stimulate rehabilitation game researchers to consider applying the presented approach in their own designs.},
	language = {en},
	booktitle = {Entertainment {Computing} - {ICEC} 2015},
	publisher = {Springer International Publishing},
	author = {Quinten, Niels and Malliet, Steven and Coninx, Karin},
	editor = {Chorianopoulos, Konstantinos and Divitini, Monica and Baalsrud Hauge, Jannicke and Jaccheri, Letizia and Malaka, Rainer},
	year = {2015},
	keywords = {Deep Content, Design Research, Game Conventions, Game Design, Game Mechanics, Physical Rehabilitation},
	pages = {433--438},
	file = {Full Text PDF:files/946/Quinten et al. - 2015 - Exploring Deep Content in Physical Rehabilitation Games.pdf:application/pdf},
}

@inproceedings{pattke_towards_2019,
	address = {Cham},
	title = {Towards a {Mixed} {Reality} {Assistance} {System} for the {Inspection} {After} {Final} {Car} {Assembly}},
	isbn = {978-3-030-21565-1},
	doi = {10.1007/978-3-030-21565-1_37},
	abstract = {Final manual inspection after vehicle assembly is a task that becomes more and more difficult because of the increasing number of customization options. We investigate how an assistance system based on mixed reality can help with this task. To this end we identify four major components that should help guide the worker and document his findings. We implement each component in three versions with increasing implementation complexity and immersion and investigate which version is most suitable in a user study.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Applications} and {Case} {Studies}},
	publisher = {Springer International Publishing},
	author = {Pattke, Marco and Martin, Manuel and Voit, Michael},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {Interaction and navigation in VR and MR: Orientation and navigation, Presence in VR and MR: Design issues},
	pages = {536--546},
	file = {Full Text PDF:files/949/Pattke et al. - 2019 - Towards a Mixed Reality Assistance System for the Inspection After Final Car Assembly.pdf:application/pdf},
}

@inproceedings{estupinan_can_2014,
	address = {Cham},
	title = {Can {Virtual} {Reality} {Increase} {Emotional} {Responses} ({Arousal} and {Valence})? {A} {Pilot} {Study}},
	isbn = {978-3-319-07626-3},
	shorttitle = {Can {Virtual} {Reality} {Increase} {Emotional} {Responses} ({Arousal} and {Valence})?},
	doi = {10.1007/978-3-319-07626-3_51},
	abstract = {Emotions in the context of UX are generally evaluated in regard to product appearance and sensorial experience. The use of virtual reality can be a way to study UX in consumer products. We want to evaluate if we could increase emotional responses using a virtual reality immersive system. For that purpose, we used the GAPED picture database and compared valence and arousal ratings of GAPED and those obtained using virtual reality. Results showed that arousal was higher in virtual reality for all images, and valence was negatively extreme for images of living creatures usually associated with phobias (spiders and snakes). Nonetheless being this is a pilot study, we conclude that there is a tendency for Virtual Reality to increase emotional responses.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {User} {Experience} {Design} for {Diverse} {Interaction} {Platforms} and {Environments}},
	publisher = {Springer International Publishing},
	author = {Estupiñán, Sergio and Rebelo, Francisco and Noriega, Paulo and Ferreira, Carlos and Duarte, Emília},
	editor = {Marcus, Aaron},
	year = {2014},
	keywords = {arousal, Emotions, Head Mounted Display, learning, phobias, User Experience, valence, Virtual Reality},
	pages = {541--549},
	file = {Full Text PDF:files/954/Estupiñán et al. - 2014 - Can Virtual Reality Increase Emotional Responses (Arousal and Valence) A Pilot Study.pdf:application/pdf},
}

@inproceedings{suh_what_2020,
	address = {Cham},
	title = {What to {Be} {Disclosed}? {Attributes} of {Online} {Games} for the {Market} {Transparency} {Policy}},
	isbn = {978-3-030-58141-1},
	shorttitle = {What to {Be} {Disclosed}?},
	doi = {10.1007/978-3-030-58141-1_7},
	abstract = {This paper identifies main features of online games to be disclosed to the market for policy makers. It tests the positive relationships between information disclosure for online games and stock returns using event analysis method. Based on data collected from online game companies between 2004 to 2009 in South Korea, the paper finds positive and significant correlation between game rating information and stock reactions. Specifically, the positive reactions are clear when a company introduces casual games, and a company has development capability. However, we do not find any significant relationships between stock reactions and voluntary information releases after the game rating information released. The findings support the feasibility of introduction of mandatory information disclosure scheme for online game industry.},
	language = {en},
	booktitle = {Electronic {Participation}},
	publisher = {Springer International Publishing},
	author = {Suh, Changwoo and Lee, Byungtae and Lee, Habin and Choi, Youngseok and Ryu, Sunghan},
	editor = {Hofmann, Sara and Csáki, Csaba and Edelmann, Noella and Lampoltshammer, Thomas and Melin, Ulf and Parycek, Peter and Schwabe, Gerhard and Tambouris, Efthimios},
	year = {2020},
	keywords = {Information disclosure, Information spillover, Online game, Stock returns},
	pages = {81--92},
	file = {Full Text PDF:files/957/Suh et al. - 2020 - What to Be Disclosed Attributes of Online Games for the Market Transparency Policy.pdf:application/pdf},
}

@inproceedings{liu_interactive_2018,
	address = {Cham},
	title = {Interactive {Point} {System} {Supporting} {Point} {Classification} and {Spatial} {Visualization}},
	isbn = {978-3-319-92043-6},
	doi = {10.1007/978-3-319-92043-6_7},
	abstract = {Point system is structured marketing strategy offered by retailers to motivate customers to keep buying goods or paying for the services. However, current point system is not enough for reflecting where points come from. In this paper, concept of point classification is put forward. Points are divided into different categories based on source. We introduce mission into point system. Mission content is designed to guide consumption. In our system, point, mission and virtual pet will be spatially visualized using AR technique. The state of virtual pet depends on the evaluation for users. Users need to adjust themselves to keep their pets in a good state. Users can manipulate on the GUI or use gestures to interact with system.},
	language = {en},
	booktitle = {Human {Interface} and the {Management} of {Information}. {Interaction}, {Visualization}, and {Analytics}},
	publisher = {Springer International Publishing},
	author = {Liu, Boyang and Masuko, Soh and Tanaka, Jiro},
	editor = {Yamamoto, Sakae and Mori, Hirohiko},
	year = {2018},
	keywords = {Gamification, Gesture interaction, Interactive system, Spatial visualization, Value creation},
	pages = {78--89},
	file = {Full Text PDF:files/961/Liu et al. - 2018 - Interactive Point System Supporting Point Classification and Spatial Visualization.pdf:application/pdf},
}

@inproceedings{shekar_vreye_2020,
	address = {Cham},
	title = {{VREye}: {Exploring} {Human} {Visual} {Acuity} {Test} {Using} {Virtual} {Reality}},
	isbn = {978-3-030-49698-2},
	shorttitle = {{VREye}},
	doi = {10.1007/978-3-030-49698-2_28},
	abstract = {Human Eye is a complex sense organ that allows vision. Vision problems may arise due to various reasons. Vision tests serve us to determine the levels of vision degradation. Early observations can support us to provide appropriate intervention for vision issues. However, one needs an Optometrist (eye specialist) to identify and validate vision issues. Scheduling an eye check-up can be tedious or unaffordable to people across various parts of the world. Acknowledging the real-world challenges on a scale of affordance to laziness, a simplified solution for vision tests can ease understanding the levels of vision issues. Considering this use case, we attempt to build a virtual reality (VR) based vision testing mechanism for studying vision issues for individuals across all age groups called ‘VREye’. In this paper, we discuss our journey towards developing a VR based solution for detecting myopic vision. We detail our challenges and insights on building the overall VR scene design, developing a virtual distance scale, and using real-world test subjects for initial validation. We also discuss our plans to extend this application to include more vision problems like hypermetropia and color blindness.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Industrial} and {Everyday} {Life} {Applications}},
	publisher = {Springer International Publishing},
	author = {Shekar, Shivang and Pesaladinne, Pranav Reddy and Karre, Sai Anirudh and Reddy, Y. Raghu},
	editor = {Chen, Jessie Y. C. and Fragomeni, Gino},
	year = {2020},
	keywords = {Health care, Human centered computing, Virtual reality, Visual acuity},
	pages = {415--429},
	file = {Full Text PDF:files/964/Shekar et al. - 2020 - VREye Exploring Human Visual Acuity Test Using Virtual Reality.pdf:application/pdf},
}

@inproceedings{suzuki_proposal_2020,
	address = {Cham},
	title = {Proposal of {Perception} {Method} of {Existence} of {Objects} in {3D} {Space} {Using} {Quasi}-electrostatic {Field}},
	isbn = {978-3-030-49760-6},
	doi = {10.1007/978-3-030-49760-6_40},
	abstract = {In real space, it is possible to know if there is a person standing behind you without turning one’s head. This is due to the quasi-electrostatic field, a type of electric fields that can be recognized by simulating body hair. However, a head-mounted display cannot recognize the presence of an object behind the user because it uses only visual and audio to interact with the environment. Therefore, we propose a system that promotes the perception of the existence of an object by generating a quasi-electrostatic field depending on the situation in 3D space.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}. {Design} for {Contemporary} {Interactive} {Environments}},
	publisher = {Springer International Publishing},
	author = {Suzuki, Kenta and Abe, Koya and Sato, Hisashi},
	editor = {Marcus, Aaron and Rosenzweig, Elizabeth},
	year = {2020},
	keywords = {Perception of signs, Quasi-electrostatic field, VirtualReality},
	pages = {561--571},
	file = {Full Text PDF:files/967/Suzuki et al. - 2020 - Proposal of Perception Method of Existence of Objects in 3D Space Using Quasi-electrostatic Field.pdf:application/pdf},
}

@inproceedings{prasad_ar-based_2018,
	address = {Cham},
	title = {{AR}-{Based} {Mobile} {Applications} for {Exposure} {Therapy}},
	isbn = {978-3-319-92279-9},
	doi = {10.1007/978-3-319-92279-9_43},
	abstract = {Facing one’s fears could be a way forward for people whose anxieties debilitate them. Exposure-based treatment is an evidence-based approach to reducing pathological anxiety. As a part of treatment, therapists who practice exposure therapy create environments to provide opportunities for patients to experience (and ultimately, overcome) the anxiety that arises for patients in feared situations. More than 50\% of anxiety patients, however, never receive treatment. We developed an Android application that uses augmented reality (AR) for providing opportunities for exposure to spiders, a relatively common presentation of specific phobia. With this app, users can engage with exposure therapy without visits to a therapist. In the next phase, we will conduct a user study to determine whether mobile applications equipped with augmented reality technologies can be used to reduce the anxiety response to a feared stimulus (i.e., a tarantula), and whether the app is considered easy to use and acceptable. In this poster, we will provide details about the mobile app and describe the study setup.},
	language = {en},
	booktitle = {{HCI} {International} 2018 – {Posters}' {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Prasad, Aarathi and McQuade, Bryan and Schofield, Casey},
	editor = {Stephanidis, Constantine},
	year = {2018},
	keywords = {Anxiety, Augmented reality, Exposure therapy, Mobile, Spider},
	pages = {319--325},
	file = {Full Text PDF:files/970/Prasad et al. - 2018 - AR-Based Mobile Applications for Exposure Therapy.pdf:application/pdf},
}

@inproceedings{nordby_evolution_2014,
	address = {Cham},
	title = {Evolution of a {Laboratory} for {Design} of {Advanced} {Ship} {Bridges}},
	isbn = {978-3-319-07857-1},
	doi = {10.1007/978-3-319-07857-1_21},
	abstract = {In this paper we describe the process of constructing a design laboratory oriented towards designers re-conceptualizing ship bridge interaction. We offer a description of the laboratory itself and the rationale for its form and the current experiences from using it.},
	language = {en},
	booktitle = {{HCI} {International} 2014 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer International Publishing},
	author = {Nordby, Kjetil and Komandur, Sashidharan},
	editor = {Stephanidis, Constantine},
	year = {2014},
	keywords = {Design, design-lab, documentation, H5.m, Information interfaces and presentation, ship bridge interaction},
	pages = {118--122},
	file = {Full Text PDF:files/971/Nordby e Komandur - 2014 - Evolution of a Laboratory for Design of Advanced Ship Bridges.pdf:application/pdf},
}

@inproceedings{dingli_holographic_2016,
	address = {Cham},
	title = {Holographic {Humans}},
	isbn = {978-3-319-39907-2},
	doi = {10.1007/978-3-319-39907-2_28},
	abstract = {Over the last few years, holographic technology has been made readily available. This modern technology may have various applications ranging from medical, cultural, educational and communication industries. Focusing on the three latter industries, the main aim of this project is to create virtual agents to behave in a believable manner and display them within a three dimensional model of local megalithic temple ‘Hagar Qim’ in a museum context. These holographic humans are not only visually appealing with clear animations but also behave in a psychologically sound and autonomous manner, matching our expectations of what life was like in those times. Believability is a cornerstone within Artificial Intelligence and consequently, in order to achieve such a high degree of autonomy and believability, the holographic humans developed in this work are self-determined with their own reactive plan of actions to organise their daily routines. In order to produce such believable behaviour, computational motivation models based on psychological theories from natural intelligence are explored. Furthermore, visitors are able to interact with the holographic humans in order to get a clearer picture of life in prehistoric times and witness the diverse personalities and interests of the humans. Finally, the system was tested empirically by a number of people and questionnaires were filled in order to test the subjective concept of believability of the system as a whole. Highly positive feedback was generated with a 96 \% believability rate and an 80 \% agreement that this platform would be suitable in a museum context. The designed system manifested believable daily routines which visitors were able to relate to as the humans planned their activities just like any ordinary person would having time to be productive and make the most of a day whilst also adhering to biological needs such as thirst and hunger as well as sleeping when dusk falls upon the virtual environment. Therefore, artificially intelligent holographic humans were created to serve as an interactive educational platform.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Dingli, Alexiei and Mifsud, Nicholas},
	editor = {Lackey, Stephanie and Shumaker, Randall},
	year = {2016},
	pages = {299--307},
	file = {Full Text PDF:files/974/Dingli e Mifsud - 2016 - Holographic Humans.pdf:application/pdf},
}

@inproceedings{borsci_early_2015,
	address = {Cham},
	title = {Early {Prototype} {Assessment} of a {New} {Virtual} {System} for {Training} {Procedural} {Skills} of {Automotive} {Service} {Operators}: {LARTE} {Tool}},
	isbn = {978-3-319-21006-3},
	shorttitle = {Early {Prototype} {Assessment} of a {New} {Virtual} {System} for {Training} {Procedural} {Skills} of {Automotive} {Service} {Operators}},
	doi = {10.1007/978-3-319-21006-3_14},
	abstract = {The consortium of the Innovate UK funded Live Augmented Reality Training Environments (LARTE) project, composed of Jaguar Land Rover (JLR), Holovis International Ltd and The University of Nottingham, developed a new concept of a 3D multiplatform training system to train the procedural skills of service maintenance operators. The LARTE tool was designed on the basis of JLR needs and desiderata. This paper presents the functionalities of the initial prototype of LARTE training system, and outcomes of an evaluation study of the usability of the product.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}: {Users} and {Contexts}},
	publisher = {Springer International Publishing},
	author = {Borsci, Simone and Lawson, Glyn and Burgess, Mark and Jha, Bhavna},
	editor = {Kurosu, Masaaki},
	year = {2015},
	keywords = {Automotive, Training, Trust, Usability, Virtual reality},
	pages = {135--143},
	file = {Full Text PDF:files/976/Borsci et al. - 2015 - Early Prototype Assessment of a New Virtual System for Training Procedural Skills of Automotive Serv.pdf:application/pdf},
}

@inproceedings{heidrich_questing_2016,
	address = {Cham},
	title = {Questing {Ruins}: {A} {Game} for a {Digital} {Inclusion}},
	isbn = {978-3-319-40355-7},
	shorttitle = {Questing {Ruins}},
	doi = {10.1007/978-3-319-40355-7_25},
	abstract = {The inclusion of people with cerebral palsy is not an easy path, because many of them preserve their cognitive abilities, despite being unable to speak, walk, or even both. This paper presents a study on the games applied in the education of people with cerebral palsy using Brain Computer Interface (BCI). Children affected by Cerebral Palsies have a disturbance in the control of their postures and body movements as a result of a brain injury. These injuries are the result of several causes. The most frequent is linked to the lack of oxygen flow to the brain, occurring either during or immediately after birth. The objective of the current research is to study the evaluation of the game we developed, called Questing Ruins. The methodology used was that of qualitative approach and case study. At the end we present Questing Ruins, an adventure game for entertainment and environmental awareness.},
	language = {en},
	booktitle = {Design, {User} {Experience}, and {Usability}: {Novel} {User} {Experiences}},
	publisher = {Springer International Publishing},
	author = {Heidrich, Regina and Rebelo, Francisco and Branco, Marsal and Mossmann, João Batista and Schuh, Anderson and Jensen, Emely and Oliveira, Tiago},
	editor = {Marcus, Aaron},
	year = {2016},
	keywords = {Brain computer interface, Digital inclusion, Motor impairments},
	pages = {264--272},
	file = {Full Text PDF:files/978/Heidrich et al. - 2016 - Questing Ruins A Game for a Digital Inclusion.pdf:application/pdf},
}

@inproceedings{ososky_practical_2016,
	address = {Cham},
	title = {Practical {Requirements} for {ITS} {Authoring} {Tools} from a {User} {Experience} {Perspective}},
	isbn = {978-3-319-39952-2},
	doi = {10.1007/978-3-319-39952-2_6},
	abstract = {Intelligent Tutoring Systems (ITS) are not yet widely implemented in learning, despite the general prevalence of digital resources in educational and training environments. ITS have been demonstrated to be effective for learners, but ITS development is not yet efficient for authors. Creating an ITS requires time, resources, and multidisciplinary skills. Authoring tools are intended to reduce the time and skill required to create an ITS, but the current state of those tools is categorized as a series of design tradeoffs between functionality, generalizability, and usability. In practice, the former two factors matter little if potential authors disregard the ITS in favor of other solutions. In this sense, authors, not learners, are the primary users of an ITS; the user experience of authors is critical to greater ITS adoption at an organizational level. With those challenges in mind, ongoing work and lessons learned on the design of authoring tools are described for a specific ITS platform, the Generalized Framework for Intelligent Tutoring (GIFT). User-centered design considerations are examined through the lens of authors’ goals, mental models for authoring, and the definition of authoring sub-roles. Recommendations for authoring tool design and future research directions for design research in authoring tools are discussed.},
	language = {en},
	booktitle = {Foundations of {Augmented} {Cognition}: {Neuroergonomics} and {Operational} {Neuroscience}},
	publisher = {Springer International Publishing},
	author = {Ososky, Scott},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2016},
	keywords = {Adaptive tutoring, Authoring tools, Design research, Intelligent tutoring systems, Mental models, User experience, User-centered design},
	pages = {55--66},
	file = {Full Text PDF:files/979/Ososky - 2016 - Practical Requirements for ITS Authoring Tools from a User Experience Perspective.pdf:application/pdf},
}

@inproceedings{schuh_design_2017,
	address = {Cham},
	title = {Design of a {Decision}-{Making} {Task} for a {Collaborative} {Brain}-{Computer} {Interface} {System} {Based} on {Emotiv} {EEG}},
	isbn = {978-3-319-58475-1},
	doi = {10.1007/978-3-319-58475-1_9},
	abstract = {This article presents lessons learned in the design, implementation and evaluation of a task of computerized decision-making to be used in a non-invasive collaborative and hybrid brain-computer interface, which used the Emotiv EEG for extract neural feature and response time as behavioral feature. The task developed was based on RSVP and has controlled levels of difficulty that can cause uncertainty. It is believed that the participants’ general satisfaction was good, since the majority indicated that they had an easy understanding of the task. The task proved to be efficient for the initial purpose, that is, to generate difficulty to the participants and the experiment can be balanced with respect to the difficulty of executing the task. However, it was not possible to find relationships between the emotions felt by the participants in their subjective answers and in their emotions collected through the Emotiv EEG. It was possible to verify that the participants with less response time tend to answer more correctly, which can indicate their level of confidence, as expected.},
	language = {en},
	booktitle = {Engineering {Psychology} and {Cognitive} {Ergonomics}: {Cognition} and {Design}},
	publisher = {Springer International Publishing},
	author = {Schuh, Ânderson and de Borba Campos, Márcia},
	editor = {Harris, Don},
	year = {2017},
	keywords = {Collaborative BCI, EEG, Hybrid BCI, Making-Decision, Response time, RSVP},
	pages = {115--132},
	file = {Full Text PDF:files/980/Schuh e de Borba Campos - 2017 - Design of a Decision-Making Task for a Collaborative Brain-Computer Interface System Based on Emotiv.pdf:application/pdf},
}

@inproceedings{prpa_pulse_2017,
	address = {Cham},
	title = {The {Pulse} {Breath} {Water} {System}: {Exploring} {Breathing} as an {Embodied} {Interaction} for {Enhancing} the {Affective} {Potential} of {Virtual} {Reality}},
	isbn = {978-3-319-57987-0},
	shorttitle = {The {Pulse} {Breath} {Water} {System}},
	doi = {10.1007/978-3-319-57987-0_13},
	abstract = {We introduce Pulse Breath Water, an immersive virtual environment (VE) with affect estimation in sound. We employ embodied interaction between a user and the system through the user’s breathing frequencies mapped to the system’s behaviour. In this study we investigate how two different mappings (metaphoric, and “reverse”) of embodied interaction design might enhance the affective properties of the presented system. We build on previous work in embodied cognition, embodied interaction, and affect estimation in sound by examining the impact of affective audiovisuals and two kinds of interaction mapping on the user’s engagement, affective states, and overall experience. The insights gained through questionnaires and semi-structured interviews are discussed in the context of participants’ lived experience and the limitations of the system to be addressed in future work.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Prpa, Mirjana and Tatar, Kıvanç and Riecke, Bernhard E. and Pasquier, Philippe},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Affective Property, Audio Sample, Immersive Virtual Environment, Virtual Environment, Virtual Reality},
	pages = {153--172},
	file = {Full Text PDF:files/937/Prpa et al. - 2017 - The Pulse Breath Water System Exploring Breathing as an Embodied Interaction for Enhancing the Affe.pdf:application/pdf},
}

@inproceedings{hansberger_dispelling_2017,
	address = {Cham},
	title = {Dispelling the {Gorilla} {Arm} {Syndrome}: {The} {Viability} of {Prolonged} {Gesture} {Interactions}},
	isbn = {978-3-319-57987-0},
	shorttitle = {Dispelling the {Gorilla} {Arm} {Syndrome}},
	doi = {10.1007/978-3-319-57987-0_41},
	abstract = {The use of gestures as a way to interact with computer systems has shown promise as a natural way to interact and manipulate digital information. However, users performing mid-air gestures for even moderate periods of time experience arm fatigue and discomfort, earning its name of the gorilla arm syndrome. Based on the natural use of hands during communication, a new gesture vocabulary was created that supports the arms while the user performs the gestures. A repeated measures within subject design was conducted where participants interacted with a custom video game using 3 types of input for 30 min each, (1) keyboard, (2) mid-air gestures and (3) supported gestures. Three measures of exertion were collected, (1) time, (2) energy expenditure, and (3) perceived exertion. The newly designed supported gestures required significantly less physical and perceived effort than the mid-air gestures and required similar exertion as the keyboard condition.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}},
	publisher = {Springer International Publishing},
	author = {Hansberger, Jeffrey T. and Peng, Chao and Mathis, Shannon L. and Areyur Shanthakumar, Vaidyanath and Meacham, Sarah C. and Cao, Lizhou and Blakely, Victoria R.},
	editor = {Lackey, Stephanie and Chen, Jessie},
	year = {2017},
	keywords = {Endurance, Fatigue, Gesture interaction, Gorilla arm, Mid-air gestures, Natural user interface},
	pages = {505--520},
	file = {Full Text PDF:files/943/Hansberger et al. - 2017 - Dispelling the Gorilla Arm Syndrome The Viability of Prolonged Gesture Interactions.pdf:application/pdf},
}

@inproceedings{born_masking_2017,
	address = {Cham},
	title = {Masking {Distracting} {Ambient} {Sound} in an {Adaptive} {VR}-{Application} to {Increase} {Presence}},
	isbn = {978-3-319-66715-7},
	doi = {10.1007/978-3-319-66715-7_25},
	abstract = {The perception of disruptive outside sound while being inside a virtual reality simulation can break the experience of presence. Even with noise cancelling headphones external sound cannot be blocked completely. In this paper, we present an acoustic compensation method to sustain the virtual illusion. We developed a testbed VR prototype that allows to classify real-life sound and to adapt the virtual world accordingly by activating pre-defined playable content. The application analyzes and classifies outside sound in real time and triggers a suitable in-game object that matches the outside sound. Our implementation is a first approach, we want to use it to further examine the possibility of adaptive audio to mask external disruptive sounds resulting in an enhanced VR experience.},
	language = {en},
	booktitle = {Entertainment {Computing} – {ICEC} 2017},
	publisher = {Springer International Publishing},
	author = {Born, Felix and Masuch, Maic},
	editor = {Munekata, Nagisa and Kunita, Itsuki and Hoshino, Junichi},
	year = {2017},
	keywords = {Adaptivity, Audio masking, Auditory immersion, Immersion, Presence, Virtual reality gaming},
	pages = {226--232},
	file = {Full Text PDF:files/947/Born e Masuch - 2017 - Masking Distracting Ambient Sound in an Adaptive VR-Application to Increase Presence.pdf:application/pdf},
}

@inproceedings{zheng_design_2015,
	address = {Cham},
	title = {Design of a {Computer}-{Assisted} {System} for {Teaching} {Attentional} {Skills} to {Toddlers} with {ASD}},
	isbn = {978-3-319-20684-4},
	doi = {10.1007/978-3-319-20684-4_69},
	abstract = {Attentional skill, which is considered as one of the fundamental elements of social communication, is among the core areas of impairment among children with Autism Spectrum Disorder (ASD). In recent years, technology-assisted ASD intervention has gained momentum among researchers due its potential advantages in terms of flexibility, accessibility and cost. In this paper, we proposed a computer-assisted system for teaching attentional skills to toddlers with ASD, using the “response to name” skill as a specific example. The system was a fully closed-loop autonomous system capable of both providing name prompting from different locations of a room and detecting the child’s attention in response to his name prompt. A preliminary user study was conducted to validate the proposed system and the protocol. The results showed that the proposed system and the protocol were well tolerated and were engaging for the participants, and were successful in eliciting the desired performance from the participants.},
	language = {en},
	booktitle = {Universal {Access} in {Human}-{Computer} {Interaction}. {Access} to {Learning}, {Health} and {Well}-{Being}},
	publisher = {Springer International Publishing},
	author = {Zheng, Zhi and Fu, Qiang and Zhao, Huan and Swanson, Amy and Weitlauf, Amy and Warren, Zachary and Sarkar, Nilanjan},
	editor = {Antona, Margherita and Stephanidis, Constantine},
	year = {2015},
	keywords = {Computer-mediated attention skills teaching, Toddlers with ASD},
	pages = {721--730},
	file = {Full Text PDF:files/951/Zheng et al. - 2015 - Design of a Computer-Assisted System for Teaching Attentional Skills to Toddlers with ASD.pdf:application/pdf},
}

@inproceedings{szirbik_research_2019,
	address = {Cham},
	title = {Research {Initiative}: {Using} {Games} for {Better} {Career} {Choices}},
	isbn = {978-3-030-30000-5},
	shorttitle = {Research {Initiative}},
	doi = {10.1007/978-3-030-30000-5_54},
	abstract = {This paper presents a novel research initiative, that is, to measure the effectiveness of specific game playing in gaining insight into the nature of various career paths in industrial management and industrial engineering. An experiment to measure the effect of a game is proposed and the complexity of the measuring task is discussed. The paper is a position paper, calling for an open discussion of this subject of research.},
	language = {en},
	booktitle = {Advances in {Production} {Management} {Systems}. {Production} {Management} for the {Factory} of the {Future}},
	publisher = {Springer International Publishing},
	author = {Szirbik, Nick B. and Velthuizen, Vincent R.},
	editor = {Ameri, Farhad and Stecke, Kathryn E. and von Cieminski, Gregor and Kiritsis, Dimitris},
	year = {2019},
	keywords = {Career choices, Industrial engineering and management, Serious gaming},
	pages = {433--439},
	file = {Full Text PDF:files/952/Szirbik e Velthuizen - 2019 - Research Initiative Using Games for Better Career Choices.pdf:application/pdf},
}

@inproceedings{broneder_tactile_2020,
	address = {Cham},
	title = {{TACTILE} – {A} {Novel} {Mixed} {Reality} {System} for {Training} and {Social} {Interaction}},
	isbn = {978-3-030-50732-9},
	doi = {10.1007/978-3-030-50732-9_2},
	abstract = {Elderly people are frequently affected by a decline of mental and physical abilities, which results in anxiety, frailty and reclusiveness. Often, they live alone, spatially separated from their families or friends, unable to meet them on a regular basis. The TACTILE project addresses these challenges and fosters an active lifestyle and well-being of older adults via an enjoyable, innovative, and user-friendly Mixed Reality (MR) solution. The system enables training both the cognitive and physical state using MR technology and maintaining social contacts by connecting seniors with family and friends. Cognitive trainings are provided by conventional and physically available board games - such as Ludo - that are completed with virtual game pieces of a remote partner. Since the system uses real physical game pieces, the user experiences a tactile feedback that enables a more familiar feeling and thus novel way of digital interaction. Physical trainings are provided by a virtual avatar that explains and shows dedicated exercises, adaptable to the individual needs and physical restrictions of the user. Thus, the avatar accompanies the user during the physical training in a more natural way.},
	language = {en},
	booktitle = {{HCI} {International} 2020 - {Posters}},
	publisher = {Springer International Publishing},
	author = {Broneder, Elisabeth and Weiß, Christoph and Puck, Monika and Puck, Stephanie and Sandner, Emanuel and Papp, Adam and Fernández Domínguez, Gustavo and Sili, Miroslav},
	editor = {Stephanidis, Constantine and Antona, Margherita},
	year = {2020},
	keywords = {Cognitive \& physical training, Mixed Reality, Social inclusion, Tangible interaction, User experience, UX},
	pages = {12--20},
	file = {Full Text PDF:files/956/Broneder et al. - 2020 - TACTILE – A Novel Mixed Reality System for Training and Social Interaction.pdf:application/pdf},
}

@inproceedings{polson_blind_2015,
	address = {Cham},
	title = {‘{Blind} {Faith}’. {An} {Experiment} with {Narrative} {Agency} in {Game} {Design}},
	isbn = {978-3-319-20916-6},
	doi = {10.1007/978-3-319-20916-6_57},
	abstract = {This paper reports on the current field of narrative-based game design through case study analysis with a particular focus on balancing high narrative agency with low production resources.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}: {Interaction} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Polson, Deb and Shah, Vidhi},
	editor = {Kurosu, Masaaki},
	year = {2015},
	keywords = {Aesthetics, Case study analysis, Context review, Design-based research, Dynamics, Game design, Mechanics, Narrative agency},
	pages = {618--627},
	file = {Full Text PDF:files/960/Polson e Shah - 2015 - ‘Blind Faith’. An Experiment with Narrative Agency in Game Design.pdf:application/pdf},
}

@inproceedings{lin_cane_2019,
	address = {Cham},
	title = {From “{Cane}” to “{Sugar}”: {Empowering} {Taisugar} {History} of {Digital} {Design} with “{Interactive} {Visual} {Storytelling}”},
	isbn = {978-3-030-22012-9},
	shorttitle = {From “{Cane}” to “{Sugar}”},
	doi = {10.1007/978-3-030-22012-9_31},
	abstract = {Taisugar have developed over 300 years since 17th century in Taiwan, it has been committed even more to research and development of sugarcane varieties than to control of sugarcane pests and diseases prevention. This study aims to discover this Eco-museum by empowering sugarcane pests and diseases as a start point and convert these elements into a task-based treasure-gathering mechanism, then integrate into “Service engineering and knowledge management” within a LBS-AR (Local-Base Service Augments Reality) system. This study hopes that it can help more and more people via experiencing this LBS-AR game to learn more knowledge from sugarcane history and culture of Taisugar. Beside the biodiversity, the process from “cane” to “sugar” is rich in humanity and culture, which is a very important element in the process of digital design. The players will be deepened and improved after the gameplay and experience the local sugar industry. Another cultural key point is Taiwanese “Sugar Mill”, whose role played in the past is similar to foundry manufacturing today. In the field, it can be explored a series of production of modernization from original “Sugar Mill”, modified “Sugar Mill” to mechanized “Sugar Mill”. So that, this study integrated task-level and multiple catching treasures mechanism to improve the player’s adhesion of the game.},
	language = {en},
	booktitle = {Human {Aspects} of {IT} for the {Aged} {Population}. {Design} for the {Elderly} and {Technology} {Acceptance}},
	publisher = {Springer International Publishing},
	author = {Lin, Yang-Chin and Kao, Jui-Yang and Tsai, Wang-Chin},
	editor = {Zhou, Jia and Salvendy, Gavriel},
	year = {2019},
	keywords = {Interactive Visual Storytelling, IP (Intellectual property) character design, Local-Base Service Augments Reality (LBS-AR), Pokémon GO, Taisugar history},
	pages = {431--440},
	file = {Full Text PDF:files/965/Lin et al. - 2019 - From “Cane” to “Sugar” Empowering Taisugar History of Digital Design with “Interactive Visual Story.pdf:application/pdf},
}

@inproceedings{ji_research_2018,
	address = {Cham},
	title = {Research on {Personalized} {Learning} {Pattern} in {Traditional} {Handicraft} {Using} {Augmented} {Reality}: {A} {Case} {Study} of {Cantonese} {Porcelain}},
	isbn = {978-3-319-91244-8},
	shorttitle = {Research on {Personalized} {Learning} {Pattern} in {Traditional} {Handicraft} {Using} {Augmented} {Reality}},
	doi = {10.1007/978-3-319-91244-8_25},
	abstract = {Recently, technology-enhanced learning research has increasingly focused on emergent technologies such as augmented reality. Educational researchers of traditional handicraft have used emergent technologies to inject affective and cognitive learning in teaching. However, numerous studies have highlighted content display of traditional handicraft in teaching based on emergent technologies. As far as we know very little work emphasized personalized learning and experience based on augmented reality in traditional handicraft. In order to address these problems, the researchers present an AR-based experiential learning method for traditional handicraft, with the purpose of shifting the pattern of learning from content-centered to experience-centered. Our approach is Augmented Reality-based Personalized Learning Pattern (ARPLP) for traditional handicraft, and it can be divided into four phases. In addition, a case of Cantonese Porcelain is presented to support this pattern in experience interface of application. The results of research produced an experience interface framework of traditional handicraft based on ARPLP, it contributes to the value of augmented reality as well as experiential learning of students for Cantonese Porcelain.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Interaction} in {Context}},
	publisher = {Springer International Publishing},
	author = {Ji, Yi and Tan, Peng and Duh, Henry Been-Lirn},
	editor = {Kurosu, Masaaki},
	year = {2018},
	keywords = {Augmented reality, Cantonese Porcelain, Experiential learning, Personalized learning pattern, Teaching},
	pages = {304--316},
	file = {Full Text PDF:files/969/Ji et al. - 2018 - Research on Personalized Learning Pattern in Traditional Handicraft Using Augmented Reality A Case.pdf:application/pdf},
}

@inproceedings{gracanin_immersion_2018,
	address = {Cham},
	title = {Immersion {Versus} {Embodiment}: {Embodied} {Cognition} for {Immersive} {Analytics} in {Mixed} {Reality} {Environments}},
	isbn = {978-3-319-91470-1},
	shorttitle = {Immersion {Versus} {Embodiment}},
	doi = {10.1007/978-3-319-91470-1_29},
	abstract = {Visualization techniques are used to analyze and understand data within the context of the underlying conceptual and physical model. We collect and generate data at an increasingly fast rate, but visual analysis capabilities are lagging. The challenges of visual data analysis and exploration are associated with very large data sets, increased dimensionality, and the consideration of data semantics, including features, focus and context. Typically, visual analysis is done using Coordinate Multiple Views (CMV) tools that support linking and brushing (selection) of data in multiple synchronized views. The recent advances in MR technologies provide a great opportunity to support deployment and use of MR applications for visualization and visual analytics. Direct mapping of CMV tools to an MR environment arguably creates more problems than it solves. Embodied interactions and embodied user interfaces lead towards invisible user interfaces and move the visualization and analysis from a computer screen to physical space and place. It is necessary to explore various interaction and visualization modalities in MR environments to identify best practices to leverage embodied cognition and interactions. Such explorations can benefit from a framework and an evaluation testbed for embodied interactive immersive analysis. The framework provides services for data access, data visualization (views), both traditional two-dimensional view and a three-dimensional equivalent, views assembly, gestures, and interaction devices. An Internet of Things based Smart Built Environment example is used to illustrate the proposed approach.},
	language = {en},
	booktitle = {Augmented {Cognition}: {Intelligent} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Gračanin, Denis},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2018},
	keywords = {Immersive analytics, Mixed reality},
	pages = {355--368},
	file = {Full Text PDF:files/973/Gračanin - 2018 - Immersion Versus Embodiment Embodied Cognition for Immersive Analytics in Mixed Reality Environment.pdf:application/pdf},
}

@article{aleem_game_2016,
	title = {Game development software engineering process life cycle: a systematic review},
	volume = {4},
	issn = {2195-1721},
	shorttitle = {Game development software engineering process life cycle},
	url = {https://doi.org/10.1186/s40411-016-0032-7},
	doi = {10.1186/s40411-016-0032-7},
	abstract = {Software game is a kind of application that is used not only for entertainment, but also for serious purposes that can be applicable to different domains such as education, business, and health care. Multidisciplinary nature of the game development processes that combine sound, art, control systems, artificial intelligence (AI), and human factors, makes the software game development practice different from traditional software development. However, the underline software engineering techniques help game development to achieve maintainability, flexibility, lower effort and cost, and better design. The purpose of this study is to assesses the state of the art research on the game development software engineering process and highlight areas that need further consideration by researchers. In the study, we used a systematic literature review methodology based on well-known digital libraries. The largest number of studies have been reported in the production phase of the game development software engineering process life cycle, followed by the pre-production phase. By contrast, the post-production phase has received much less research activity than the pre-production and production phases. The results of this study suggest that the game development software engineering process has many aspects that need further attention from researchers; that especially includes the postproduction phase.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {J Softw Eng Res Dev},
	author = {Aleem, Saiqa and Capretz, Luiz Fernando and Ahmed, Faheem},
	month = nov,
	year = {2016},
	keywords = {Artificial Intelligence, Development software engineerong proces, Online game, Software Game, Systematic review, Video game},
	pages = {6},
	file = {Full Text PDF:files/1001/Aleem et al. - 2016 - Game development software engineering process life cycle a systematic review.pdf:application/pdf},
}

@article{chan_reinforcement_2024,
	title = {Reinforcement learning-based drone simulators: survey, practice, and challenge},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Reinforcement learning-based drone simulators},
	url = {https://doi.org/10.1007/s10462-024-10933-w},
	doi = {10.1007/s10462-024-10933-w},
	abstract = {Recently, machine learning has been very useful in solving diverse tasks with drones, such as autonomous navigation, visual surveillance, communication, disaster management, and agriculture. Among these machine learning, two representative paradigms have been widely utilized in such applications: supervised learning and reinforcement learning. Researchers prefer to use supervised learning, mostly based on convolutional neural networks, because of its robustness and ease of use but yet data labeling is laborious and time-consuming. On the other hand, when traditional reinforcement learning is combined with the deep neural network, it can be a very powerful tool to solve high-dimensional input problems such as image and video. Along with the fast development of reinforcement learning, many researchers utilize reinforcement learning in drone applications, and it often outperforms supervised learning. However, it usually requires the agent to explore the environment on a trial-and-error basis which is high cost and unrealistic in the real environment. Recent advances in simulated environments can allow an agent to learn by itself to overcome these drawbacks, although the gap between the real environment and the simulator has to be minimized in the end. In this sense, a realistic and reliable simulator is essential for reinforcement learning training. This paper investigates various drone simulators that work with diverse reinforcement learning architectures. The characteristics of the reinforcement learning-based drone simulators are analyzed and compared for the researchers who would like to employ them for their projects. Finally, we shed light on some challenges and potential directions for future drone simulators.},
	language = {en},
	number = {10},
	urldate = {2024-09-05},
	journal = {Artif Intell Rev},
	author = {Chan, Jun Hoong and Liu, Kai and Chen, Yu and Sagar, A. S. M. Sharifuzzaman and Kim, Yong-Guk},
	month = sep,
	year = {2024},
	keywords = {Artificial Intelligence, Drone, Drone simulator, Reinforcement learning},
	pages = {281},
	file = {Full Text PDF:files/1002/Chan et al. - 2024 - Reinforcement learning-based drone simulators survey, practice, and challenge.pdf:application/pdf},
}

@article{firat_3d_2022,
	title = {{3D} sound spatialization with game engines: the virtual acoustics performance of a game engine and a middleware for interactive audio design},
	volume = {26},
	issn = {1434-9957},
	shorttitle = {{3D} sound spatialization with game engines},
	url = {https://doi.org/10.1007/s10055-021-00589-0},
	doi = {10.1007/s10055-021-00589-0},
	abstract = {This study analyses one of the most popular game engines and an audio middleware to reproduce sound according to sound propagation physics. The analysis focuses on the transmission path between the sound source and the receiver. Even if there are several ready-to-use real-time auralization platforms and software, game engines' use with this aim is a recent study area for acousticians. However, audio design needs with game engines and the limits of their basic releases require additional tools (plugins and middleware) to improve both the quality and realism of sound in virtual environments. The paper discusses the use of Unreal Engine 4 and Wwise's 3D audio production methods in a set of different test environments. It assesses their performance in regard to a commercial geometrical acoustics software. The results show that the investigated version of the game engine and its sound assets are insufficient to simulate real-world cases and that significant improvements can be achieved with use of the middleware.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Fırat, Hasan Baran and Maffei, Luigi and Masullo, Massimiliano},
	month = jun,
	year = {2022},
	keywords = {3D audio, Artificial Intelligence, Game engines, Physically based rendering, Real-time auralization, Virtual audio reality},
	pages = {539--558},
	file = {Full Text PDF:files/1003/Fırat et al. - 2022 - 3D sound spatialization with game engines the virtual acoustics performance of a game engine and a.pdf:application/pdf},
}

@article{kamienski_empirical_2021,
	title = {An empirical study of {Q}\&{A} websites for game developers},
	volume = {26},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-021-10014-4},
	doi = {10.1007/s10664-021-10014-4},
	abstract = {The game development industry is growing, and training new developers in game development-specific abilities is essential to satisfying its need for skilled game developers. These developers require effective learning resources to acquire the information they need and improve their game development skills. Question and Answer (Q\&A) websites stand out as some of the most used online learning resources in software development. Many studies have investigated how Q\&A websites help software developers become more experienced. However, no studies have explored Q\&A websites aimed at game development, and there is little information about how game developers use and interact with these websites. In this paper, we study four Q\&A communities by analyzing game development data we collected from their websites and the 347 responses received on a survey we ran with game developers. We observe that the communities have declined over the past few years and identify factors that correlate to these changes. Using a Latent Dirichlet Allocation (LDA) model, we characterize the topics discussed in the communities. We also analyze how topics differ across communities and identify the most discussed topics. Furthermore, we find that survey respondents have a mostly negative view of the communities and tended to stop using the websites once they became more experienced. Finally, we provide recommendations on where game developers should post their questions, which can help mitigate the websites’ declines and improve their effectiveness.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Empir Software Eng},
	author = {Kamienski, Arthur and Bezemer, Cor-Paul},
	month = aug,
	year = {2021},
	keywords = {Artificial Intelligence, Game development, Q\&A communities},
	pages = {115},
	file = {Full Text PDF:files/1004/Kamienski e Bezemer - 2021 - An empirical study of Q&A websites for game developers.pdf:application/pdf},
}

@article{gilbert_authoring_2015,
	title = {Authoring {Effective} {Embedded} {Tutors}: {An} {Overview} of the {Extensible} {Problem} {Specific} {Tutor} ({xPST}) {System}},
	volume = {25},
	issn = {1560-4306},
	shorttitle = {Authoring {Effective} {Embedded} {Tutors}},
	url = {https://doi.org/10.1007/s40593-015-0045-0},
	doi = {10.1007/s40593-015-0045-0},
	abstract = {The Extensible Problem Specific Tutor (xPST) allows authors who are not cognitive scientists and not programmers to quickly create an intelligent tutoring system that provides instruction akin to a model-tracing tutor. Furthermore, this instruction is overlaid on existing software, so that the learner’s interface does not have to be made from scratch. The xPST architecture allows for extending its capabilities by the addition of plug-ins that communicate with additional third-party software. After reviewing this general architecture, we describe three major implementations that we have created using the xPST system, each using different third-party software as the learner’s interface. We have conducted three evaluations of authors using xPST to create tutoring content, and these are considered in turn. These evaluations show that xPST authors can quickly learn the system, and can efficiently produce successful embedded instruction.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Int J Artif Intell Educ},
	author = {Gilbert, Stephen B. and Blessing, Stephen B. and Guo, Enruo},
	month = sep,
	year = {2015},
	keywords = {Artificial Intelligence, Authoring tool, Cognitive tutor, Digital Education and Educational Technology, Model-Tracing Tutor, xPST},
	pages = {428--454},
	file = {Full Text PDF:files/1005/Gilbert et al. - 2015 - Authoring Effective Embedded Tutors An Overview of the Extensible Problem Specific Tutor (xPST) Sys.pdf:application/pdf},
}

@article{sorensen_potentials_2022,
	title = {Potentials of game engines for wind power digital twin development: an investigation of the {Unreal} {Engine}},
	volume = {5},
	issn = {2520-8942},
	shorttitle = {Potentials of game engines for wind power digital twin development},
	url = {https://doi.org/10.1186/s42162-022-00227-2},
	doi = {10.1186/s42162-022-00227-2},
	abstract = {Digital twin technologies have become popular in wind energy for monitoring and what-if scenario investigation. However, developing a digital representation of the wind is challenging, especially due to the digital twin platform constraints. Game engines might be possible to solve this issue, especially since game engines have been used for product design, testing, prototyping, and also digital twins. Therefore, this study investigates the potential of developing a digital twin of wind power in the Unreal game engine. A case study of two types of wind turbines (Vestas V164-8 and Enercon E-126 7.580) and one location (Esbjerg, Denmark) is chosen for this study. The digital twin includes the environment with historical wind data and the visual representation of the wind turbine with a wind power production model and the estimated production in the given wind conditions of the area. The results show that game engines are viable for building entire digital twins where a realistic graphical user interface is required. Unreal Engine 5 provides the tools for modelling the landscape, surrounding water, and lighting. In addition, the Unreal Engine ecosystem provides vast amounts of content, such as 3D assets and game logic plugins, easing the digital twin development. The results prove that digital twins built in Unreal Engine 5 have great potential development of digital twins and user interfaces for communicating with a digital twin. The developed digital twin allows for further extension to benefit future digital twins utilizing wind turbines.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Energy Inform},
	author = {Sørensen, Jonas Vedsted and Ma, Zheng and Jørgensen, Bo Nørregaard},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Digital twin, Game engine, Simulation, Unreal Engine, Wind energy},
	pages = {39},
	file = {Full Text PDF:files/1006/Sørensen et al. - 2022 - Potentials of game engines for wind power digital twin development an investigation of the Unreal E.pdf:application/pdf},
}

@article{szirmay-kalos_adapting_2022,
	title = {Adapting {Game} {Engines} to {Curved} {Spaces}},
	volume = {38},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-021-02303-2},
	doi = {10.1007/s00371-021-02303-2},
	abstract = {Curved spaces are very un-intuitive to our eyes trained on Euclidean geometry. Games provide an interesting way to explore these strange worlds. Games are written with the help of modeling tools and game engines based on Euclidean geometry. This paper addresses the problem of adapting 3D game engines to the rules of curved spaces. We consider the conversion of Euclidean objects, geometric calculations, transformation pipeline, lighting and physical simulation. Finally, we identify where existing game engines should be modified.},
	language = {en},
	number = {12},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Szirmay-Kalos, László and Magdics, Milán},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Elliptic geometry, Hyperbolic geometry, Illumination, Transformations},
	pages = {4383--4395},
	file = {Full Text PDF:files/1027/Szirmay-Kalos e Magdics - 2022 - Adapting Game Engines to Curved Spaces.pdf:application/pdf},
}

@article{zhang_high-performance_2022,
	title = {High-performance adaptive texture streaming and rendering of large {3D} cities},
	volume = {38},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-021-02152-z},
	doi = {10.1007/s00371-021-02152-z},
	abstract = {We propose a high-performance texture streaming system for real-time rendering of large 3D cities with millions of textures. Our main contribution is a texture streaming system that automatically adjusts the streaming workload at runtime based on measured frame latencies, specifically addressing the high memory binding costs of hardware virtual texturing which causes frame rate stuttering. Our system streams textures in parallel with prioritization based on GPU computed mesh perceptibility, and these textures are cached in a sparse partially resident image at runtime without the need for a texture preprocessing step. In addition, we improve rendering quality by minimizing texture pop-in artifacts using a color blending scheme based on mipmap levels. We evaluate our texture streaming system using three structurally distinct datasets with many textures and compared it to a baseline, a game engine, and our prior method. Results show an 8X improvement in rendering performance and 7X improvement in rendering quality compared to the baseline.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Zhang, Alex and Chen, Kan and Johan, Henry and Erdt, Marius},
	month = apr,
	year = {2022},
	keywords = {3D cities, Artificial Intelligence, Real-time rendering, Texture streaming, Virtual texturing},
	pages = {1245--1262},
	file = {Full Text PDF:files/1029/Zhang et al. - 2022 - High-performance adaptive texture streaming and rendering of large 3D cities.pdf:application/pdf},
}

@article{flotynski_creating_2021,
	title = {Creating explorable extended reality environments with semantic annotations},
	volume = {80},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-020-09772-y},
	doi = {10.1007/s11042-020-09772-y},
	abstract = {The main element of extended reality (XR) environments is behavior-rich 3D content consisting of objects that act and interact with one another as well as with users. Such actions and interactions constitute the evolution of the content over time. Multiple application domains of XR, e.g., education, training, marketing, merchandising, and design, could benefit from the analysis of 3D content changes based on general or domain knowledge comprehensible to average users or domain experts. Such analysis can be intended, in particular, to monitor, comprehend, examine, and control XR environments as well as users’ skills, experience, interests and preferences, and XR objects’ features. However, it is difficult to achieve as long as XR environments are developed with methods and tools that focus on programming and 3D modeling rather than expressing domain knowledge accompanying content users and objects, and their behavior. The main contribution of this paper is an approach to creating explorable knowledge-based XR environments with semantic annotations. The approach combines description logics with aspect-oriented programming, which enables knowledge representation in an arbitrary domain as well as transformation of available environments with minimal users’ effort. We have implemented the approach using well-established development tools and exemplify it with an explorable immersive car showroom. The approach enables efficient creation of explorable XR environments and knowledge acquisition from XR.},
	language = {en},
	number = {5},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Flotyński, Jakub},
	month = feb,
	year = {2021},
	keywords = {3D Web, Annotations, Artificial Intelligence, Exploration, Extended reality, Ontologies, Queries, Reasoning, Semantic web},
	pages = {6959--6989},
	file = {Full Text PDF:files/1031/Flotyński - 2021 - Creating explorable extended reality environments with semantic annotations.pdf:application/pdf},
}

@article{vilchis_survey_2023,
	title = {A survey on the pipeline evolution of facial capture and tracking for digital humans},
	volume = {29},
	issn = {1432-1882},
	url = {https://doi.org/10.1007/s00530-023-01081-2},
	doi = {10.1007/s00530-023-01081-2},
	abstract = {With the introduction of concepts for virtual interaction and digital doubles, a rich scenario has been created for embodied avatars to strive. These avatars, more recently referred to as digital humans, have become a popular area of research, resulting in various techniques and methods that focus on improving the perception of their realism, fidelity, emphatic response, and interactivity. This survey aims to explore the literature and recent advancements on the key processes behind the creation and animation of digital human faces through the view of a general pipeline. The extensive review carried out in this study explores the usual data collection protocols, the main facial codification paradigms and databases, the approaches for digital human asset creation, facial tracking solutions for performance-driven animation, the solving process, and the final rendering delivery. Different quantitative evaluation methods, visual perception tests, and empathetic response evaluations for digital humans are also included in the survey. Additionally, the paper presents an updated summary of public and private frameworks for digital humans that go through the complete general pipeline presented. Finally, the condensed knowledge is discussed, inquiring into the possible direction of future developments in the field.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Multimedia Systems},
	author = {Vilchis, Carlos and Perez-Guerrero, Carmina and Mendez-Ruiz, Mauricio and Gonzalez-Mendoza, Miguel},
	month = aug,
	year = {2023},
	keywords = {Artificial Intelligence, Deep learning, Digital humans, Empathic response, Facial codification, Facial expressions, Photogrammetry},
	pages = {1917--1940},
	file = {Full Text PDF:files/1033/Vilchis et al. - 2023 - A survey on the pipeline evolution of facial capture and tracking for digital humans.pdf:application/pdf},
}

@article{xiao_toward_2024,
	title = {Toward next generation mixed reality games: a research through design approach},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Toward next generation mixed reality games},
	url = {https://doi.org/10.1007/s10055-024-01041-9},
	doi = {10.1007/s10055-024-01041-9},
	abstract = {Mixed reality (MR) games refer to games that integrate physical entities with digitally mediated contents. Currently, it entails game creators to integrate heterogeneous virtual and physical components, which is often time-consuming and labor-intensive, without the support of a coherent technology stack. The underlying technodiversity manifested by the research corpus suggests a complicated, multi-dimensional design space that goes beyond merely technical concerns. In this research, we adopted a research-through-design approach and proposed an MR game technology stack that facilitates flexible, low-code game development. As design grounding, we first surveyed 34 state-of-the-art studies, and results were synergized into three different spectra of technological affordances, respectively activity range, user interface and feedback control, to inform our next design process. We then went through an iterative prototyping phase and implemented an MR game development toolset. A co-design workshop was conducted, where we invited 15 participants to try the prototype tools and co-ideate the potential use scenarios for the proposed technology stack. First-hand user feedback was collected via questionnaires and semi-structured interviews. As a result, four conceptual game designs with three major design implications were generated, which conjointly reflect a broader understanding on MR gameful experience and contribute fresh insights to this emerging research domain.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Xiao, Ruowei and Zhang, Rongzheng and Buruk, Oğuz and Hamari, Juho and Virkki, Johanna},
	month = jul,
	year = {2024},
	keywords = {Artificial Intelligence, End user development, Extended reality, Game, Gamification, Mixed reality, Research through design, RFID},
	pages = {142},
	file = {Full Text PDF:files/1034/Xiao et al. - 2024 - Toward next generation mixed reality games a research through design approach.pdf:application/pdf},
}

@article{westera_artificial_2020,
	title = {Artificial intelligence moving serious gaming: {Presenting} reusable game {AI} components},
	volume = {25},
	issn = {1573-7608},
	shorttitle = {Artificial intelligence moving serious gaming},
	url = {https://doi.org/10.1007/s10639-019-09968-2},
	doi = {10.1007/s10639-019-09968-2},
	abstract = {This article provides a comprehensive overview of artificial intelligence (AI) for serious games. Reporting about the work of a European flagship project on serious game technologies, it presents a set of advanced game AI components that enable pedagogical affordances and that can be easily reused across a wide diversity of game engines and game platforms. Serious game AI functionalities include player modelling (real-time facial emotion recognition, automated difficulty adaptation, stealth assessment), natural language processing (sentiment analysis and essay scoring on free texts), and believable non-playing characters (emotional and socio-cultural, non-verbal bodily motion, and lip-synchronised speech), respectively. The reuse of these components enables game developers to develop high quality serious games at reduced costs and in shorter periods of time. All these components are open source software and can be freely downloaded from the newly launched portal at gamecomponents.eu. The components come with detailed installation manuals and tutorial videos. All components have been applied and validated in serious games that were tested with real end-users.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Westera, Wim and Prada, Rui and Mascarenhas, Samuel and Santos, Pedro A. and Dias, João and Guimarães, Manuel and Georgiadis, Konstantinos and Nyamsuren, Enkhbold and Bahreini, Kiavash and Yumak, Zerrin and Christyowidiasmoro, Chris and Dascalu, Mihai and Gutu-Robu, Gabriel and Ruseti, Stefan},
	month = jan,
	year = {2020},
	keywords = {Artificial intelligence, Artificial Intelligence, Component-based architecture, Digital Education and Educational Technology, Game development, Intelligent tutoring systems, Serious games, Software reuse},
	pages = {351--380},
	file = {Full Text PDF:files/1036/Westera et al. - 2020 - Artificial intelligence moving serious gaming Presenting reusable game AI components.pdf:application/pdf},
}

@article{horst_virtual_2024,
	title = {Virtual reality content creation based on self-contained components in the e-learning domain: {Re}-using pattern-based vr content in different authoring toolkits},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {Virtual reality content creation based on self-contained components in the e-learning domain},
	url = {https://doi.org/10.1007/s11042-022-13362-5},
	doi = {10.1007/s11042-022-13362-5},
	abstract = {In the context of e-learning, it is challenging to incorporate emerging technologies, such as alternate reality games or Virtual Reality (VR), within current learning trends. Microlearning is such a current trend. It divides large and complex chunks of content into small and elementary learning nuggets. These single self-contained nuggets are then composed to overarching lessons or courses. The concept of VR nuggets dovetails this educational trend. VR nuggets are standalone, self-contained, and rather short VR experiences that can be combined with other learning nuggets. By using initial implementations of VR nuggets, they can be used to let authors create VR earning content, for example, to let learners experience alternate realities. In this paper, we further investigate the VR nugget authoring concept and extent it. We introduce two novel authoring toolkits that rely on VR nuggets – one based on context-related module interaction (CoNMoD) and one based on visual scripting (ViNS Tiles). In two separate user studies, we examine the acceptance of the toolkits and compare them to existing authoring environments that also rely on VR nuggets but utilize different interface techniques. These studies’ results emphasize the importance of exchanging content between different established tools and indicate the acceptance of our tools regarding their hedonic and pragmatic qualities, also compared to existing tools from related work. As a conclusion, we propose an exchange format for VR nuggets that supports their reusability. It enables authors that use different toolkits to work together. They can utilize VR nuggets created with other toolkits and still use their own preferred toolkit. By means of an expert survey, we draw conclusions on technical aspects and a suitable platform to make VR nuggets available to the community. This survey indicates that potential authors would use such an exchange-approach for creating and presenting VR content and that they are willing to share their work and to contribute in a VR nugget authoring community.},
	language = {en},
	number = {15},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Horst, Robin and Gerstmeier, Simon and Naraghi-Taghi-Off, Ramtin and Wagner, Julian and Rau, Linda and Dörner, Ralf},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Authoring, Bite-sized learning, E-Learning, Games engineering, Short virtual reality experiences, Virtual reality, Virtual reality learning nuggets},
	pages = {46557--46594},
	file = {Full Text PDF:files/1037/Horst et al. - 2024 - Virtual reality content creation based on self-contained components in the e-learning domain Re-usi.pdf:application/pdf},
}

@article{flotynski_visual_2022,
	title = {Visual aspect-oriented modeling of explorable extended reality environments},
	volume = {26},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00601-7},
	doi = {10.1007/s10055-021-00601-7},
	abstract = {The availability of various extended reality (XR) systems for tracking users’ and objects’ behavior opens new opportunities for analyzing users’ and objects’ interactions and autonomous actions. Such analysis can be especially useful and attainable to domain experts when it is based on domain knowledge related to a particular application, liberating the analysts from going into technical details of 3D content. Analysis of XR users’ and objects’ behavior can provide knowledge about the users’ experience, interests and preferences, as well as objects’ features, which may be valuable in various domains, e.g., training, design and marketing. However, the available methods and tools for building XR focus on 3D modeling and programming rather than knowledge representation, making them unsuitable for domain-oriented analysis. In this paper, a new visual approach to modeling explorable XR environments is proposed. It is based on a semantic representation of aspects, which extend the primary code of XR environments to register their behavior in a form explorable with reasoning and queries, appropriate for high-level analysis in arbitrary domains. It permits domain experts to comprehend and analyze what happened in an XR environment regarding users’ and objects’ actions and interactions. The approach has been implemented as an extension to MS Visual Studio and demonstrated in an explorable immersive service guide for household appliances. The evaluation results show that the approach enables efficient development of explorable XR and may be useful for people with limited technical skills.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Flotyński, Jakub},
	month = sep,
	year = {2022},
	keywords = {3D web, Artificial Intelligence, Aspect-oriented modeling, Exploration, Extended reality, Ontologies, Semantic web},
	pages = {939--961},
	file = {Full Text PDF:files/1039/Flotyński - 2022 - Visual aspect-oriented modeling of explorable extended reality environments.pdf:application/pdf},
}

@article{clausen_can_2022,
	title = {Can we benefit from game engines to develop digital twins for planning the deployment of photovoltaics?},
	volume = {5},
	issn = {2520-8942},
	url = {https://doi.org/10.1186/s42162-022-00222-7},
	doi = {10.1186/s42162-022-00222-7},
	abstract = {Digital Twins (DTs) have attracted great attention in the energy sector. Game engines have been suggested to model DTs of their physical counterparts because they provide realistic graphics, lighting-, fluid- and physics engines that simulate the real world. However, the application of game engines to develop DTs for photovoltaics (PVs) has not yet been discussed in the literature. Therefore, this paper assesses the built-in game engine features' ability to support the DT development of PVs with Unreal Engine 5. This paper mainly focuses on visual representation because the surrounding environment significantly impacts PV deployment, and the existing software tools do not allow the study of the environmental factors at the early planning phase of a project’s lifecycle. Furthermore, this paper investigates the position of the sun, shadows and reflections from nearby objects that influence the PVs' power output, and if the built-in light engine can be used for planning the deployment of PVs. The result shows that in-game objects in the environment can be used to affect the simulated PV output estimate over a year. It also indicates that applying Unreal Engine 5 to model PV systems that rely on mirroring real-world behaviour is promising if accurate data is used in the modelling. Real data and mathematical PV models are necessary since Unreal Engine 5’s Lumen subsystem cannot provide realistic solar radiance on PVs for a given location on earth.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Energy Inform},
	author = {Clausen, Christian Skafte Beck and Ma, Zheng Grace and Jørgensen, Bo Nørregaard},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Digital twin, Game engine, Photovoltaics, Simulation, Unreal Engine},
	pages = {42},
	file = {Full Text PDF:files/1041/Clausen et al. - 2022 - Can we benefit from game engines to develop digital twins for planning the deployment of photovoltai.pdf:application/pdf},
}

@article{goswami_survey_2021,
	title = {A survey of modeling, rendering and animation of clouds in computer graphics},
	volume = {37},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-020-01953-y},
	doi = {10.1007/s00371-020-01953-y},
	abstract = {Clouds play an important role in enhancing the realism of outdoor scenes in computer graphics (CG). Realistic cloud generation is a challenging task, which entails processes such as modeling, photorealistic rendering and simulation of the clouds. To these ends, several techniques have been proposed within the CG community in the last 4 decades with one or more of the above stated focuses. The growth of modern hardware has also enabled development of techniques that can achieve cloud display and animation at interactive frame rates. In this survey, we review the prominent work in the domain and also summarize the evolution of the research over the time.},
	language = {en},
	number = {7},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Goswami, Prashant},
	month = jul,
	year = {2021},
	keywords = {Animation, Artificial Intelligence, Clouds, Modeling, Rendering},
	pages = {1931--1948},
	file = {Full Text PDF:files/1043/Goswami - 2021 - A survey of modeling, rendering and animation of clouds in computer graphics.pdf:application/pdf},
}

@article{moina-rivera_cloud_2024,
	title = {Cloud media video encoding: review and challenges},
	issn = {1573-7721},
	shorttitle = {Cloud media video encoding},
	url = {https://doi.org/10.1007/s11042-024-18763-2},
	doi = {10.1007/s11042-024-18763-2},
	abstract = {In recent years, Internet traffic patterns have been changing. Most of the traffic demand by end users is multimedia, in particular, video streaming accounts for over 53\%. This demand has led to improved network infrastructures and computing architectures to meet the challenges of delivering these multimedia services while maintaining an adequate quality of experience. Focusing on the preparation and adequacy of multimedia content for broadcasting, Cloud and Edge Computing infrastructures have been and will be crucial to offer high and ultra-high definition multimedia content in live, real-time, or video-on-demand scenarios. For these reasons, this review paper presents a detailed study of research papers related to encoding and transcoding techniques in cloud computing environments. It begins by discussing the evolution of streaming and the importance of the encoding process, with a focus on the latest streaming methods and codecs. Then, it examines the role of cloud systems in multimedia environments and provides details on the cloud infrastructure for media scenarios. After doing a systematic literature review, we have been able to find 49 valid papers that meet the requirements specified in the research questions. Each paper has been analyzed and classified according to several criteria, besides to inspect their relevance. To conclude this review, we have identified and elaborated on several challenges and open research issues associated with the development of video codecs optimized for diverse factors within both cloud and edge architectures. Additionally, we have discussed emerging challenges in designing new cloud/edge architectures aimed at more efficient delivery of media traffic. This involves investigating ways to improve the overall performance, reliability, and resource utilization of architectures that support the transmission of multimedia content over both cloud and edge computing environments ensuring a good quality of experience for the final user.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Moina-Rivera, Wilmer and Garcia-Pineda, Miguel and Gutiérrez-Aguado, Juan and Alcaraz-Calero, Jose M.},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Cloud computing, Coding, Encoding, Multimedia, Review, Survey, Transcoding},
	file = {Full Text PDF:files/1044/Moina-Rivera et al. - 2024 - Cloud media video encoding review and challenges.pdf:application/pdf},
}

@article{rodriguez-garcia_systematic_2024,
	title = {A systematic review of virtual {3D} reconstructions of {Cultural} {Heritage} in immersive {Virtual} {Reality}},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-024-18700-3},
	doi = {10.1007/s11042-024-18700-3},
	abstract = {Immersive Virtual Reality (iVR) devices are increasingly affordable and accessible to consumers. The widespread adoption of this technology for professional training is now finding its way into various other fields. One field that is gaining significant popularity is Cultural Heritage (CH), where iVR enables the reconstruction and exploration of lost heritage. However, an up-to-date systematic review of iVR within this field will be of great benefit. Hence, the present review of 94 papers published between 2013 and 2022 that follows PRISMA methodology on virtual reconstruction of CH for iVR. The aim is to identify the key factors behind the development of these applications and their standards. To do so, a statistical analysis on the following topics was performed: (1) nationality, publication date, and article type; (2) heritage type and its current state of preservation; (3) the area of final application and the features of the reconstructions; (4) the characteristics of the iVR experience; and (5) the assessment of the iVR applications. Finally, a roadmap of best practices is outlined for the virtual reconstruction of CH using iVR and some of the most promising future research lines are outlined.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Rodriguez-Garcia, Bruno and Guillen-Sanz, Henar and Checa, David and Bustillo, Andres},
	month = apr,
	year = {2024},
	keywords = {3D model, Artificial Intelligence, Cultural Heritage, Head Mounted Display, Virtual Reality, Virtual Reconstruction},
	file = {Full Text PDF:files/1046/Rodriguez-Garcia et al. - 2024 - A systematic review of virtual 3D reconstructions of Cultural Heritage in immersive Virtual Reality.pdf:application/pdf},
}

@article{anantrasirichai_artificial_2022,
	title = {Artificial intelligence in the creative industries: a review},
	volume = {55},
	issn = {1573-7462},
	shorttitle = {Artificial intelligence in the creative industries},
	url = {https://doi.org/10.1007/s10462-021-10039-7},
	doi = {10.1007/s10462-021-10039-7},
	abstract = {This paper reviews the current state of the art in artificial intelligence (AI) technologies and applications in the context of the creative industries. A brief background of AI, and specifically machine learning (ML) algorithms, is provided including convolutional neural networks (CNNs), generative adversarial networks (GANs), recurrent neural networks (RNNs) and deep Reinforcement Learning (DRL). We categorize creative applications into five groups, related to how AI technologies are used: (i) content creation, (ii) information analysis, (iii) content enhancement and post production workflows, (iv) information extraction and enhancement, and (v) data compression. We critically examine the successes and limitations of this rapidly advancing technology in each of these areas. We further differentiate between the use of AI as a creative tool and its potential as a creator in its own right. We foresee that, in the near future, ML-based AI will be adopted widely as a tool or collaborative assistant for creativity. In contrast, we observe that the successes of ML in domains with fewer constraints, where AI is the ‘creator’, remain modest. The potential of AI (or its developers) to win awards for its original creations in competition with human creatives is also limited, based on contemporary technologies. We therefore conclude that, in the context of creative industries, maximum benefit from AI will be derived where its focus is human-centric—where it is designed to augment, rather than replace, human creativity.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Artif Intell Rev},
	author = {Anantrasirichai, Nantheera and Bull, David},
	month = jan,
	year = {2022},
	keywords = {Artificial Intelligence, Creative industries, Image and video enhancement, Machine learning},
	pages = {589--656},
	file = {Full Text PDF:files/1047/Anantrasirichai e Bull - 2022 - Artificial intelligence in the creative industries a review.pdf:application/pdf},
}

@article{ma_generative_2024,
	title = {Generative deep learning for data generation in natural hazard analysis: motivations, advances, challenges, and opportunities},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Generative deep learning for data generation in natural hazard analysis},
	url = {https://doi.org/10.1007/s10462-024-10764-9},
	doi = {10.1007/s10462-024-10764-9},
	abstract = {Data mining and analysis are critical for preventing or mitigating natural hazards. However, data availability in natural hazard analysis is experiencing unprecedented challenges due to economic, technical, and environmental constraints. Recently, generative deep learning has become an increasingly attractive solution to these challenges, which can augment, impute, or synthesize data based on these learned complex, high-dimensional probability distributions of data. Over the last several years, much research has demonstrated the remarkable capabilities of generative deep learning for addressing data-related problems in natural hazards analysis. Data processed by deep generative models can be utilized to describe the evolution or occurrence of natural hazards and contribute to subsequent natural hazard modeling. Here we present a comprehensive review concerning generative deep learning for data generation in natural hazard analysis. (1) We summarized the limitations associated with data availability in natural hazards analysis and identified the fundamental motivations for employing generative deep learning as a critical response to these challenges. (2) We discuss several deep generative models that have been applied to overcome the problems caused by limited data availability in natural hazards analysis. (3) We analyze advances in utilizing generative deep learning for data generation in natural hazard analysis. (4) We discuss challenges associated with leveraging generative deep learning in natural hazard analysis. (5) We explore further opportunities for leveraging generative deep learning in natural hazard analysis. This comprehensive review provides a detailed roadmap for scholars interested in applying generative models for data generation in natural hazard analysis.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Artif Intell Rev},
	author = {Ma, Zhengjing and Mei, Gang and Xu, Nengxiong},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Data generation, Downscaling meteorological variables, Generative deep learning, Natural hazard analysis, Seismic data interpolation},
	pages = {160},
	file = {Full Text PDF:files/1049/Ma et al. - 2024 - Generative deep learning for data generation in natural hazard analysis motivations, advances, chal.pdf:application/pdf},
}

@article{sarakatsanos_vr_2024,
	title = {{VR} {Designer}: enhancing fashion showcases through immersive virtual garment fitting},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {{VR} {Designer}},
	url = {https://doi.org/10.1007/s10055-024-00945-w},
	doi = {10.1007/s10055-024-00945-w},
	abstract = {This paper introduces a Virtual Reality (VR) application tailored for fashion designers and retailers, transcending traditional garment design and demonstration boundaries by presenting an immersive digital garment showcase within a captivating VR environment. Simulating a virtual retail store, designers navigate freely, selecting from an array of avatar-garment combinations and exploring garments from diverse perspectives. This immersive experience offers designers a precise representation of the final product’s aesthetics, fit, and functionality on the human body. Our application can be considered as a pre-manufacturing layer, that empowers designers and retailers with a precise understanding of how the actual garment will look and behave. Evaluation involved comprehensive feedback from both professional and undergraduate fashion designers, gathered through usability testing sessions.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Sarakatsanos, Orestis and Papazoglou-Chalikias, Anastasios and Boikou, Machi and Chatzilari, Elisavet and Jauk, Michaela and Hafliger, Ursina and Nikolopoulos, Spiros and Kompatsiaris, Ioannis},
	month = mar,
	year = {2024},
	keywords = {3D fashion design, Artificial Intelligence, Avatar-based garment simulation, Avatar-garment fitting, Digital garment design, Garment visualization, Unity, Virtual garment fitting, Virtual reality, XR},
	pages = {70},
	file = {Full Text PDF:files/1028/Sarakatsanos et al. - 2024 - VR Designer enhancing fashion showcases through immersive virtual garment fitting.pdf:application/pdf},
}

@article{christy_technological_2014,
	title = {Technological {Advancements} in {Affective} {Gaming}: {A} {Historical} {Survey}},
	volume = {3},
	issn = {2010-2283},
	shorttitle = {Technological {Advancements} in {Affective} {Gaming}},
	url = {https://doi.org/10.7603/s40601-013-0038-5},
	doi = {10.7603/s40601-013-0038-5},
	abstract = {Affective gaming (AG) is a cross-disciplinary area drawing upon psychology, physiology, electronic engineering and computer science, among others. This paper presents a historical overview of affective gaming, bringing together psychophysiological system developments, a time-line of video game graphical advancements and industry trends, thereby offering an entry point into affective gaming research. It is proposed that video games may soon reach a peak in perceivable graphical improvements. This opens up the door for innovative game enhancement strategies, such as emotiondriven interactions between the player and the gaming environment.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {GSTF J Comput},
	author = {Christy, Thomas and Kuncheva, Ludmila I.},
	month = may,
	year = {2014},
	keywords = {Affective gaming, Computer graphics, Emotion, Input devices, Physiology, Psychophysiology},
	pages = {38},
	file = {Full Text PDF:files/1030/Christy e Kuncheva - 2014 - Technological Advancements in Affective Gaming A Historical Survey.pdf:application/pdf},
}

@article{egges_presence_2007,
	title = {Presence and interaction in mixed reality environments},
	volume = {23},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-007-0113-z},
	doi = {10.1007/s00371-007-0113-z},
	abstract = {In this paper, we present a simple and robust mixed reality (MR) framework that allows for real-time interaction with virtual humans in mixed reality environments under consistent illumination. We will look at three crucial parts of this system: interaction, animation and global illumination of virtual humans for an integrated and enhanced presence. The interaction system comprises of a dialogue module, which is interfaced with a speech recognition and synthesis system. Next to speech output, the dialogue system generates face and body motions, which are in turn managed by the virtual human animation layer. Our fast animation engine can handle various types of motions, such as normal key-frame animations, or motions that are generated on-the-fly by adapting previously recorded clips. Real-time idle motions are an example of the latter category. All these different motions are generated and blended on-line, resulting in a flexible and realistic animation. Our robust rendering method operates in accordance with the previous animation layer, based on an extended for virtual humans precomputed radiance transfer (PRT) illumination model, resulting in a realistic rendition of such interactive virtual characters in mixed reality environments. Finally, we present a scenario that illustrates the interplay and application of our methods, glued under a unique framework for presence and interaction in MR.},
	language = {en},
	number = {5},
	urldate = {2024-09-05},
	journal = {Visual Comput},
	author = {Egges, Arjan and Papagiannakis, George and Magnenat-Thalmann, Nadia},
	month = may,
	year = {2007},
	keywords = {Animation, Artificial Intelligence, Interaction, Mixed reality, Presence, Real-time rendering},
	pages = {317--333},
	file = {Full Text PDF:files/1032/Egges et al. - 2007 - Presence and interaction in mixed reality environments.pdf:application/pdf},
}

@article{maik_knowledge-based_2024,
	title = {Knowledge-based approach to adaptive {XR} interface design for non-programmers},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-024-03472-6},
	doi = {10.1007/s00371-024-03472-6},
	abstract = {Customizing extended reality (XR) interfaces presents a significant challenge, especially for users with limited programming expertise. This paper introduces the method for adaptation of XR interfaces (MAXI-XR), a novel approach to simplify the customization process of XR user interfaces through knowledge technologies. MAXI-XR offers a user-friendly solution for interface design, supporting users with varying levels of technical skills. The basis of MAXI-XR is its Semantic Knowledge Base, which facilitates intelligent adaptations through advanced querying and reasoning, enabling the extraction of user-specific information for context-based XR interface adaptation. The functionality of MAXI-XR is demonstrated by its application in a VR stock market data visualization system. This system demonstrates MAXI-XR’s ability to adapt to complex and data-intensive environments according to user requirements, improving the interaction experience. Furthermore, the method’s scalability and ease of maintenance make it a versatile tool for a wide range of applications beyond stock market visualization, suggesting its potential for broader adoption in various XR domains.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Maik, Mikołaj and Flotyński, Jakub and Walczak, Krzysztof},
	month = jun,
	year = {2024},
	keywords = {Adaptive 3D user interfaces, Artificial Intelligence, Extended reality, HCI, Knowledge representation, Stock exchange visualization},
	file = {Full Text PDF:files/1035/Maik et al. - 2024 - Knowledge-based approach to adaptive XR interface design for non-programmers.pdf:application/pdf},
}

@article{engstrom_prototyping_2018,
	title = {Prototyping {Tools} for {Game} {Writers}},
	volume = {7},
	issn = {2052-773X},
	url = {https://doi.org/10.1007/s40869-018-0062-y},
	doi = {10.1007/s40869-018-0062-y},
	abstract = {A game is best evaluated by playing it and prototyping is therefore an important activity in game development. Game writers and narrative designers are responsible for the narrative structure of a game, which may have a varying degree of interactivity to it. The aim of this paper is to analyse the role of prototyping tools for game writers. There is a limited range of such tools available, of which Twine is one of the most established. Most of these tools have a text-based programming interface for modelling of game mechanics. This paper presents Deig—a prototyping tool for creating point-and-click adventure games. In Deig, game mechanics is modelled graphically using nodes from a set of primitives. We present an interview study where game writing students reflect on their experience of using Deig and Twine as prototyping tools. The result shows that both tools have their merits and complement each other. Deig was found to be intuitive for modelling of game mechanics, which lead students to create interactive narratives. Twine was found to be more useful for experimental writing. The conclusion of this work is that there is a need for a diverse set of prototyping tools to support game writing.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Comput Game J},
	author = {Engström, Henrik and Brusk, Jenny and Erlandsson, Patrik},
	month = sep,
	year = {2018},
	keywords = {Artificial Intelligence, Computer game, Game writing, Narrative design, Pototyping, Tools},
	pages = {153--172},
	file = {Full Text PDF:files/1038/Engström et al. - 2018 - Prototyping Tools for Game Writers.pdf:application/pdf},
}

@article{thomas_real-time_2023,
	title = {Real-time fracturing in video games},
	volume = {82},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-022-13049-x},
	doi = {10.1007/s11042-022-13049-x},
	abstract = {Destruction in video games traditionally use pre-fracturing techniques to efficiently swap out game objects at run-time, which requires artists to create multiple models in various states of destruction. Real-time methods require less design time and can produce more realistic results but often come at a cost of performance. If real-time methods can perform at similar levels as tradition pre-fracturing means, this could increase the current standards of realistic destruction in video games. This paper explores and implements real-time fracturing techniques, comparing these to traditional pre-fracturing methods in terms of visual realism and performance. The results of the implementation were then distributed in an online questionnaire, to view how participants perceived the differences of both techniques and whether the visual or performance aspects affected their preferences of one method over the other.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Thomas, Rachel and Zhang, Wenshu},
	month = jan,
	year = {2023},
	keywords = {Real-time mesh fracturing, Video game destruction, Voronoi diagrams},
	pages = {4709--4734},
	file = {Full Text PDF:files/1040/Thomas e Zhang - 2023 - Real-time fracturing in video games.pdf:application/pdf},
}

@article{gupta_deep_2023,
	title = {Deep learning model based multimedia retrieval and its optimization in augmented reality applications},
	volume = {82},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-022-13555-y},
	doi = {10.1007/s11042-022-13555-y},
	abstract = {With the uproar of touchless technology, the Virtual Continuum has seen some spark in the upcoming products. Today numerous gadgets support the use of Mixed Reality / Augmented Reality (AR)/ Virtual Reality. The Head Mounted Displays (HMDs) like that of Hololens, Google Lens, Jio Glass manifested reality into virtuality. Other than the HMDs many organizations tend to develop mobile AR applications to support umpteen number of industries like medicine, education, construction. Currently, the major issue lies in the performance parameters of these applications, while deploying for mobile application’s graphics performance, latency, and CPU functioning. Many industries pose real-time computation requirements in AR but do not implement an efficient algorithm in their frameworks. Offloading the computation of deep learning models involved in the application to the cloud servers will highly affect the processing parameters. For our use case, we will be using Multi-Task Cascaded Convolutional Neural Network (MTCNN) which is a modern tool for face detection, using a 3-stage neural network detector. Therefore, the optimization of communication between local application and cloud computing frameworks needs to be optimized. The proposed framework defines how the parameters involving the complete deployment of a mobile AR application can be optimized in terms of retrieval of multimedia, its processing, and augmentation of graphics, eventually enhancing the performance. To implement the proposed algorithm a mobile application is created in Unity3D. The mobile application virtually augments a 3D model of a skeleton on a target face. After the mentioned experimentation, it is found that average Media Retrieval Time (1.1471 μ s) and Client Time (1.1207 μ s) in the local application are extremely low than the average API process time (288.934ms). The highest time latency is achieved at the frame rate higher than 80fps.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Gupta, Yash Prakash and {Mukul} and Gupta, Nitin},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, Augmented reality, Cloud computation, Deep learning, Latency, Media retrieval, Medical augmented reality, MTCNN, OffLoading},
	pages = {8447--8466},
	file = {Full Text PDF:files/1042/Gupta et al. - 2023 - Deep learning model based multimedia retrieval and its optimization in augmented reality application.pdf:application/pdf},
}

@article{periyasamy_towards_2023,
	title = {Towards {3D} {Scene} {Understanding} {Using} {Differentiable} {Rendering}},
	volume = {4},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-022-01663-3},
	doi = {10.1007/s42979-022-01663-3},
	abstract = {Deep learning methods have achieved significant results in many 2D computer vision tasks. To realize similar results in 3D tasks, equipping deep learning pipelines with components that incorporate knowledge about 2D image generation from the 3D scene description is a promising research direction. Rasterization, the standard formulation of the image generation process is not differentiable, and thus not compatible with the deep learning models trained using gradient-based optimization schemes. In recent years, many new approximate differentiable renderers have been proposed to enable compatibility between deep learning methods and image rendering techniques. Differentiable renderers fit naturally into the render-and-compare framework where the 3D scene parameters are estimated iteratively by minimizing the error between the observed image and the image rendered according to the current scene parameter estimate. In this article, we present StilllebenDR, a light-weight, scalable differentiable renderer built as an extension to the openly available Stillleben library. We demonstrate the usability of the proposed differentiable renderer for the task of iterative 3D deformable registration using a latent shape-space model and occluded object pose refinement using order-independent transparency based on analytical gradients and learned scene aggregation.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {SN COMPUT. SCI.},
	author = {Periyasamy, Arul Selvam and Behnke, Sven},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, Deformable registration, Differentiable rendering, Order independent transparency},
	pages = {245},
	file = {Full Text PDF:files/1045/Periyasamy e Behnke - 2023 - Towards 3D Scene Understanding Using Differentiable Rendering.pdf:application/pdf},
}

@article{mu_user_2024,
	title = {User attention and behaviour in virtual reality art encounter},
	volume = {83},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-022-13365-2},
	doi = {10.1007/s11042-022-13365-2},
	abstract = {With the proliferation of consumer virtual reality (VR) headsets and creative tools, content creators are experimenting with new forms of interactive audience experience using immersive media. Understanding user attention and behaviours in virtual environment can greatly inform the creative processes in VR. We developed an abstract VR painting and an experimentation system to study audience art encounters through eye gaze and movement tracking. The data from a user experiment with 35 participants reveal a range of user activity patterns in art exploration. Deep learning models are used to study the connections between the behavioural data and the audience’s background. The work also introduced new integrated methods to visualise user attention for content creators.},
	language = {en},
	number = {15},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Mu, Mu and Dohan, Murtada and Goodyear, Alison and Hill, Gary and Johns, Cleyon and Mauthe, Andreas},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Eye-tracking, Machine learning, User attention, Virtual reality, VR abstract painting},
	pages = {46595--46624},
	file = {Full Text PDF:files/1048/Mu et al. - 2024 - User attention and behaviour in virtual reality art encounter.pdf:application/pdf},
}

@article{howie_virtual_2021,
	title = {Virtual {Observations}: a software tool for contextual observation and assessment of user’s actions in virtual reality},
	volume = {25},
	issn = {1434-9957},
	shorttitle = {Virtual {Observations}},
	url = {https://doi.org/10.1007/s10055-020-00463-5},
	doi = {10.1007/s10055-020-00463-5},
	abstract = {In this paper, we present ‘Virtual Observation’ (VO) a software tool for contextual observation and assessment of user’s directly from within the virtual reality (VR) simulation framework. Unlike other recording systems, the VO system described in this paper focuses on recording and reconstructing VR user’s positional, rotational and input data to recreate the same experience the user had with a VR simulation. Different from animation-based approaches, VO records user inputs and reconstructs the simulation from them and the user positional data. Moreover, the system allows the broadcast of this information to a remote machine enabling remote live observation of the simulation. Datasets recorded by the system can be shared by exporting them as XML files or, optionally, into a standalone online application, such as browser WebGL, allowing researchers, developers and educators to share and review a VR user simulation through a free-moving camera using a web browser. In this paper, the consistency of the data generated from the software by the client, server and reconstructed datasets acquired during real-time live observations was evaluated. We conclude that this Virtual Observation software offers detailed reconstruction of low-level information and visual information of user actions during simulations for both live and offline observations. We envision that our system will be of benefit for researchers, developers and educators that work with VR applications.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Howie, Scott and Gilardi, Marco},
	month = jun,
	year = {2021},
	keywords = {Artificial Intelligence, Observing, Replaying, Reviewing, Simulations, Virtual Observation, Virtual reality},
	pages = {447--460},
	file = {Full Text PDF:files/1050/Howie e Gilardi - 2021 - Virtual Observations a software tool for contextual observation and assessment of user’s actions in.pdf:application/pdf},
}

@article{shum_personalised_2023,
	title = {Personalised {Learning} through {Context}-{Based} {Adaptation} in the {Serious} {Games} with {Gating} {Mechanism}},
	volume = {28},
	issn = {1573-7608},
	url = {https://doi.org/10.1007/s10639-023-11695-8},
	doi = {10.1007/s10639-023-11695-8},
	abstract = {When the traditional "one size fits all" approach is used in designing educational games, the game context is usually arranged in a fixed sequence. However, the designated content may not effectively support the diversity of players. The player's ability and characteristics should be considered and supported with an appropriate learning context embedded in the game to facilitate personalised experiences. Adapting game scenarios to a player's characteristics can boost motivation and ultimately improve learning outcomes. This research applies a context-aware design approach and the Learner-Centered Design approach to establish a personalised adaptation framework for designing educational serious games and enhancing personalised knowledge delivery. The proposed framework decouples the game logic implementation and adaptation mechanism. It dynamically adapts the designed game objects and activities to personal learning objectives, learning levels and learning progress to achieve a non-linear learning sequence. Through synchronous real-time xAPI message exchange mechanisms, system components and learning content adaptation are enabled. The adaptation aims to fit personal learning objectives and provide a non-linear learning sequence in a game environment. The framework provides students with personalised learning experiences. A game named GhostCoder is implemented and used to evaluate the framework. Based on the externalised adaptive mechanism, the game content is adapted to the player's performance by adjusting the difficulty of the learning content within the game. Testing of the game in the lab environment has been performed. At the next stage, an evaluation will be conducted with the target groups of students.},
	language = {en},
	number = {10},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Shum, Lok Cheung and Rosunally, Yasmine and Scarle, Simon and Munir, Kamran},
	month = oct,
	year = {2023},
	keywords = {Artificial Intelligence, Context-Based Adaptation, Digital Education and Educational Technology, Educational Games, Games Based Learning, Personalised Learning, Serious Games},
	pages = {13077--13108},
	file = {Full Text PDF:files/1051/Shum et al. - 2023 - Personalised Learning through Context-Based Adaptation in the Serious Games with Gating Mechanism.pdf:application/pdf},
}

@article{hossain_nccollab_2023,
	title = {{NCCollab}: collaborative behavior tree authoring in game development},
	volume = {82},
	issn = {1573-7721},
	shorttitle = {{NCCollab}},
	url = {https://doi.org/10.1007/s11042-022-12307-2},
	doi = {10.1007/s11042-022-12307-2},
	abstract = {Game development is a collective process in which a variety of different professionals from different backgrounds collaborate together not only by means of conversational interaction but also collaborative participation, one of which is programming. While collaborative and pair programming solutions exist for text-based programming languages, visual programming has not enjoyed as much attention. These solutions would not only address advanced forms of business communication among team members but could find their use in distance learning, which would have been useful during the pandemic. In our work, we propose a solution for collaborative behavioral animation of NPCs using behavior trees through synchronous and asynchronous modes of collaboration. We conducted a user study with 12 moderately skilled game development university students who were placed in groups of two and engaged in joint fixed behavior tree development tasks using the synchronous and asynchronous modes and auxiliary features of live preview, access and restoration of previous states from behavior tree history, conflict resolution, and instant messaging. Participants also completed a control task where no collaboration was involved and auxiliary features were not available. Feedback form Creativity Support Index, a self-developed questionnaire, and a semi-structured interview were collected. Additionally, task completion times were logged. The results indicate that the two collaborative modes provide expected improvement over the control condition. No significant differences were found between the two collaborative modes. However, the semi-structed interview revealed that the synchronous mode could be useful for quick prototyping, while the asynchronous mode – for most other situations.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Hossain, Md. Yousuf and Zaman, Loutfouz},
	month = jan,
	year = {2023},
	keywords = {Artificial Intelligence, Behavior tree, Collaboration, Game AI, Game development, Unity engine},
	pages = {4671--4708},
	file = {Full Text PDF:files/1052/Hossain e Zaman - 2023 - NCCollab collaborative behavior tree authoring in game development.pdf:application/pdf},
}

@article{krishnan_air_2021,
	title = {Air {Learning}: a deep reinforcement learning gym for autonomous aerial robot visual navigation},
	volume = {110},
	issn = {1573-0565},
	shorttitle = {Air {Learning}},
	url = {https://doi.org/10.1007/s10994-021-06006-6},
	doi = {10.1007/s10994-021-06006-6},
	abstract = {We introduce Air Learning, an open-source simulator, and a gym environment for deep reinforcement learning research on resource-constrained aerial robots. Equipped with domain randomization, Air Learning exposes a UAV agent to a diverse set of challenging scenarios. We seed the toolset with point-to-point obstacle avoidance tasks in three different environments and Deep Q Networks (DQN) and Proximal Policy Optimization (PPO) trainers. Air Learning assesses the policies’ performance under various quality-of-flight (QoF) metrics, such as the energy consumed, endurance, and the average trajectory length, on resource-constrained embedded platforms like a Raspberry Pi. We find that the trajectories on an embedded Ras-Pi are vastly different from those predicted on a high-end desktop system, resulting in up to \$\$40{\textbackslash}\%\$\$longer trajectories in one of the environments. To understand the source of such discrepancies, we use Air Learning to artificially degrade high-end desktop performance to mimic what happens on a low-end embedded system. We then propose a mitigation technique that uses the hardware-in-the-loop to determine the latency distribution of running the policy on the target platform (onboard compute on aerial robot). A randomly sampled latency from the latency distribution is then added as an artificial delay within the training loop. Training the policy with artificial delays allows us to minimize the hardware gap (discrepancy in the flight time metric reduced from 37.73\% to 0.5\%). Thus, Air Learning with hardware-in-the-loop characterizes those differences and exposes how the onboard compute’s choice affects the aerial robot’s performance. We also conduct reliability studies to assess the effect of sensor failures on the learned policies. All put together, Air Learning enables a broad class of deep RL research on UAVs. The source code is available at: https://github.com/harvard-edge/AirLearning.},
	language = {en},
	number = {9},
	urldate = {2024-09-05},
	journal = {Mach Learn},
	author = {Krishnan, Srivatsan and Boroujerdian, Behzad and Fu, William and Faust, Aleksandra and Reddi, Vijay Janapa},
	month = sep,
	year = {2021},
	keywords = {Artificial Intelligence, Autonomous aerial robots, Deep reinforcement learning, Deep RL challenges, Real life RL, Resource-constrained deep RL, Robotics, Sim2Real},
	pages = {2501--2540},
	file = {Full Text PDF:files/1053/Krishnan et al. - 2021 - Air Learning a deep reinforcement learning gym for autonomous aerial robot visual navigation.pdf:application/pdf},
}

@article{checa_review_2020,
	title = {A review of immersive virtual reality serious games to enhance learning and training},
	volume = {79},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-019-08348-9},
	doi = {10.1007/s11042-019-08348-9},
	abstract = {The merger of game-based approaches and Virtual Reality (VR) environments that can enhance learning and training methodologies have a very promising future, reinforced by the widespread market-availability of affordable software and hardware tools for VR-environments. Rather than passive observers, users engage in those learning environments as active participants, permitting the development of exploration-based learning paradigms. There are separate reviews of VR technologies and serious games for educational and training purposes with a focus on only one knowledge area. However, this review covers 135 proposals for serious games in immersive VR-environments that are combinations of both VR and serious games and that offer end-user validation. First, an analysis of the forum, nationality, and date of publication of the articles is conducted. Then, the application domains, the target audience, the design of the game and its technological implementation, the performance evaluation procedure, and the results are analyzed. The aim here is to identify the factual standards of the proposed solutions and the differences between training and learning applications. Finally, the study lays the basis for future research lines that will develop serious games in immersive VR-environments, providing recommendations for the improvement of these tools and their successful application for the enhancement of both learning and training tasks.},
	language = {en},
	number = {9},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Checa, David and Bustillo, Andres},
	month = mar,
	year = {2020},
	keywords = {Artificial Intelligence, Evaluation, Learning, Serious game, Systematic literature review, Virtual reality},
	pages = {5501--5527},
	file = {Full Text PDF:files/1054/Checa e Bustillo - 2020 - A review of immersive virtual reality serious games to enhance learning and training.pdf:application/pdf},
}

@article{wang_scene_2021,
	title = {Scene {Walk}: a non-photorealistic viewing tool for first-person video},
	volume = {25},
	issn = {1434-9957},
	shorttitle = {Scene {Walk}},
	url = {https://doi.org/10.1007/s10055-021-00523-4},
	doi = {10.1007/s10055-021-00523-4},
	abstract = {Scene Walk is a video viewing technique suited to first-person video recorded from wearable cameras. It integrates a 2D video player and visualisation of the camera trajectory into a non-photorealistic partial rendering of the 3D environment as reconstructed from image content. Applications include forensic analysis of first-person video archives, for example as recorded by emergency response teams. The Scene Walk method is designed to support the viewer’s construction and application of a cognitive map of the context in which first-person video was captured. We use methods from wayfinding research to assess the effectiveness of this non-photorealistic approach in comparison to actual physical experience of the scene. We find that Scene Walk does allow viewers to create a more accurate and effective cognitive map of first-person video than is achieved using a conventional video browsing interface and that this model is comparable to actually walking through the original environment.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Wang, Xiaomeng and Blackwell, Alan F. and Jones, Richard and Nguyen, Hieu T.},
	month = dec,
	year = {2021},
	keywords = {3D scene reconstruction, Artificial Intelligence, Body-worn camera, Camera trajectory, Cognitive Map, First-person video, Video viewing},
	pages = {1171--1191},
	file = {Full Text PDF:files/1055/Wang et al. - 2021 - Scene Walk a non-photorealistic viewing tool for first-person video.pdf:application/pdf},
}

@article{gao_review_2023,
	title = {A {Review} and {Outlook} on {Predictive} {Cruise} {Control} of {Vehicles} and {Typical} {Applications} {Under} {Cloud} {Control} {System}},
	volume = {20},
	issn = {2731-5398},
	url = {https://doi.org/10.1007/s11633-022-1395-3},
	doi = {10.1007/s11633-022-1395-3},
	abstract = {With the application of mobile communication technology in the automotive industry, intelligent connected vehicles equipped with communication and sensing devices have been rapidly promoted. The road and traffic information perceived by intelligent vehicles has important potential application value, especially for improving the energy-saving and safe-driving of vehicles as well as the efficient operation of traffic. Therefore, a type of vehicle control technology called predictive cruise control (PCC) has become a hot research topic. It fully taps the perceived or predicted environmental information to carry out predictive cruise control of vehicles and improves the comprehensive performance of the vehicle-road system. Most existing reviews focus on the economical driving of vehicles, but few scholars have conducted a comprehensive survey of PCC from theory to the status quo. In this paper, the methods and advances of PCC technologies are reviewed comprehensively by investigating the global literature, and typical applications under a cloud control system (CCS) are proposed. Firstly, the methodology of PCC is generally introduced. Then according to typical scenarios, the PCC-related research is deeply surveyed, including freeway and urban traffic scenarios involving traditional vehicles, new energy vehicles, intelligent vehicles, and multi-vehicle platoons. Finally, the general architecture and three typical applications of the cloud control system (CCS) on PCC are briefly introduced, and the prospect and future trends of PCC are proposed.},
	language = {en},
	number = {5},
	urldate = {2024-09-05},
	journal = {Mach. Intell. Res.},
	author = {Gao, Bolin and Wan, Keke and Chen, Qien and Wang, Zhou and Li, Rui and Jiang, Yu and Mei, Run and Luo, Yinghui and Li, Keqiang},
	month = oct,
	year = {2023},
	keywords = {Artificial Intelligence, Automotive Engineering, cloud control system (CCS), cooperative control, efficient operation, intelligent connected vehicle, Predictive cruise control (PCC)},
	pages = {614--639},
	file = {Full Text PDF:files/1056/Gao et al. - 2023 - A Review and Outlook on Predictive Cruise Control of Vehicles and Typical Applications Under Cloud C.pdf:application/pdf},
}

@article{cowley_behavlets_2016,
	title = {Behavlets: a method for practical player modelling using psychology-based player traits and domain specific features},
	volume = {26},
	issn = {1573-1391},
	shorttitle = {Behavlets},
	url = {https://doi.org/10.1007/s11257-016-9170-1},
	doi = {10.1007/s11257-016-9170-1},
	abstract = {As player demographics broaden it has become important to understand variation in player types. Improved player models can help game designers create games that accommodate a range of playing styles, and may also facilitate the design of systems that detect the currently-expressed player type and adapt dynamically in real-time. Existing approaches can model players, but most focus on tracking and classifying behaviour based on simple functional metrics such as deaths, specific choices, player avatar attributes, and completion times. We describe a novel approach which seeks to leverage expert domain knowledge using a theoretical framework linking behaviour and game design patterns. The aim is to derive features of play from sequences of actions which are intrinsically informative about behaviour—which, because they are directly interpretable with respect to psychological theory of behaviour, we name ‘Behavlets’. We present the theoretical underpinning of this approach from research areas including psychology, temperament theory, player modelling, and game composition. The Behavlet creation process is described in detail; illustrated using a clone of the well-known game Pac-Man, with data gathered from 100 participants. A workshop-based evaluation study is also presented, where nine game design expert participants were briefed on the Behavlet concepts and requisite models, and then attempted to apply the method to games of the well-known first/third-person shooter genres, exemplified by ‘Gears of War’, (Microsoft). The participants found 139 Behavlet concepts mapping from behavioural preferences of the temperament types, to design patterns of the shooter genre games. We conclude that the Behavlet approach has significant promise, is complementary to existing methods and can improve theoretical validity of player models.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {User Model User-Adap Inter},
	author = {Cowley, Benjamin and Charles, Darryl},
	month = jun,
	year = {2016},
	keywords = {Artificial Intelligence, Behavlet, Game design patterns, Machine learning, Player modelling, Psychology, Temperament theory},
	pages = {257--306},
	file = {Full Text PDF:files/1057/Cowley e Charles - 2016 - Behavlets a method for practical player modelling using psychology-based player traits and domain s.pdf:application/pdf},
}

@article{clausen_scoping_2024,
	title = {A scoping review of {In}-the-loop paradigms in the energy sector focusing on software-in-the-loop},
	volume = {7},
	issn = {2520-8942},
	url = {https://doi.org/10.1186/s42162-024-00312-8},
	doi = {10.1186/s42162-024-00312-8},
	abstract = {Software-in-the-Loop (SIL) testing is an approach used for verification and validation in the energy sector. However, there is no comprehensive overview of the application, potential, and challenges of SIL within this sector. Therefore, this paper conducts a thorough scoping review of the existing literature within the scope of SIL and related in-the-loop approaches in the energy sector. A total of 88 full-text articles from four significant databases ACM, IEEE Xplore, Scopus, and Web of Science are analyzed and categorized to map the purpose, methods, architecture, interoperability and protocols, technologies, challenges, and limitations. The results present a grand perspective of in-the-loop across several domains followed by an analysis of SIL in the energy sector. The application domains carry characteristics from complex systems, systems-of-systems, cyber-physical systems, critical systems, real-time systems, and sociotechnical systems. The energy sector and the automotive industry are amongst the most applied domains. Within energy- and electricity systems, hardware-based in-the-loop paradigms are mostly applied for testing low-level signaling, and SIL is used for control strategy testing, optimization, dispatching, and experimentation. The examined SIL architectures have distributed-, real-time, and closed-loop properties, and are constrained by specialized simulation power hardware. Future research should address how to systematically develop SIL testing environments with guiding principles to support application development for the future digitalized energy system.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Energy Inform},
	author = {Clausen, Christian Skafte Beck and Jørgensen, Bo Nørregaard and Ma, Zheng Grace},
	month = feb,
	year = {2024},
	keywords = {Artificial Intelligence, Electricity systems, Energy sector, Energy systems, In-the-loop, Scoping review, Software-in-the-loop},
	pages = {12},
	file = {Full Text PDF:files/1058/Clausen et al. - 2024 - A scoping review of In-the-loop paradigms in the energy sector focusing on software-in-the-loop.pdf:application/pdf},
}

@article{mustafa_temporally_2021,
	title = {Temporally {Coherent} {General} {Dynamic} {Scene} {Reconstruction}},
	volume = {129},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-020-01367-2},
	doi = {10.1007/s11263-020-01367-2},
	abstract = {Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: an automatic method for initial coarse reconstruction to initialize joint estimation; sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to various applications such as free-view rendering and virtual reality.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Int J Comput Vis},
	author = {Mustafa, Armin and Volino, Marco and Kim, Hansung and Guillemaut, Jean-Yves and Hilton, Adrian},
	month = jan,
	year = {2021},
	keywords = {3D, Artificial Intelligence, Dynamic 4D reconstruction, Dynamic scenes, Reconstruction, Segmentation, Temporal coherence},
	pages = {123--141},
	file = {Full Text PDF:files/1059/Mustafa et al. - 2021 - Temporally Coherent General Dynamic Scene Reconstruction.pdf:application/pdf},
}

@article{el-agamy_comprehensive_2024,
	title = {Comprehensive analysis of digital twins in smart cities: a 4200-paper bibliometric study},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Comprehensive analysis of digital twins in smart cities},
	url = {https://doi.org/10.1007/s10462-024-10781-8},
	doi = {10.1007/s10462-024-10781-8},
	abstract = {This survey paper comprehensively reviews Digital Twin (DT) technology, a virtual representation of a physical object or system, pivotal in Smart Cities for enhanced urban management. It explores DT's integration with Machine Learning for predictive analysis, IoT for real-time data, and its significant role in Smart City development. Addressing the gap in existing literature, this survey analyzes over 4,220 articles from the Web of Science, focusing on unique aspects like datasets, platforms, and performance metrics. Unlike other studies in the field, this research paper distinguishes itself through its comprehensive and bibliometric approach, analyzing over 4,220 articles and focusing on unique aspects like datasets, platforms, and performance metrics. This approach offers an unparalleled depth of analysis, enhancing the understanding of Digital Twin technology in Smart City development and setting a new benchmark in scholarly research in this domain. The study systematically identifies emerging trends and thematic topics, utilizing tools like VOSviewer for data visualization. Key findings include publication trends, prolific authors, and thematic clusters in research. The paper highlights the importance of DT in various urban applications, discusses challenges and limitations, and presents case studies showcasing successful implementations. Distinguishing from prior studies, it offers detailed insights into emerging trends, future research directions, and the evolving role of policy and governance in DT development, thereby making a substantial contribution to the field.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Artif Intell Rev},
	author = {El-Agamy, Rasha F. and Sayed, Hanaa A. and AL Akhatatneh, Arwa M. and Aljohani, Mansourah and Elhosseini, Mostafa},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Bibliometric study, Datasets, Digital twin, IoT integration, Smart city development},
	pages = {154},
	file = {Full Text PDF:files/1060/El-Agamy et al. - 2024 - Comprehensive analysis of digital twins in smart cities a 4200-paper bibliometric study.pdf:application/pdf},
}

@article{goswami_survey_2021-1,
	title = {A survey of modeling, rendering and animation of clouds in computer graphics},
	volume = {37},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-020-01953-y},
	doi = {10.1007/s00371-020-01953-y},
	abstract = {Clouds play an important role in enhancing the realism of outdoor scenes in computer graphics (CG). Realistic cloud generation is a challenging task, which entails processes such as modeling, photorealistic rendering and simulation of the clouds. To these ends, several techniques have been proposed within the CG community in the last 4 decades with one or more of the above stated focuses. The growth of modern hardware has also enabled development of techniques that can achieve cloud display and animation at interactive frame rates. In this survey, we review the prominent work in the domain and also summarize the evolution of the research over the time.},
	language = {en},
	number = {7},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Goswami, Prashant},
	month = jul,
	year = {2021},
	keywords = {Animation, Artificial Intelligence, Clouds, Modeling, Rendering},
	pages = {1931--1948},
	file = {Full Text PDF:files/1081/Goswami - 2021 - A survey of modeling, rendering and animation of clouds in computer graphics.pdf:application/pdf},
}

@article{horst_virtual_2024-1,
	title = {Virtual reality content creation based on self-contained components in the e-learning domain: {Re}-using pattern-based vr content in different authoring toolkits},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {Virtual reality content creation based on self-contained components in the e-learning domain},
	url = {https://doi.org/10.1007/s11042-022-13362-5},
	doi = {10.1007/s11042-022-13362-5},
	abstract = {In the context of e-learning, it is challenging to incorporate emerging technologies, such as alternate reality games or Virtual Reality (VR), within current learning trends. Microlearning is such a current trend. It divides large and complex chunks of content into small and elementary learning nuggets. These single self-contained nuggets are then composed to overarching lessons or courses. The concept of VR nuggets dovetails this educational trend. VR nuggets are standalone, self-contained, and rather short VR experiences that can be combined with other learning nuggets. By using initial implementations of VR nuggets, they can be used to let authors create VR earning content, for example, to let learners experience alternate realities. In this paper, we further investigate the VR nugget authoring concept and extent it. We introduce two novel authoring toolkits that rely on VR nuggets – one based on context-related module interaction (CoNMoD) and one based on visual scripting (ViNS Tiles). In two separate user studies, we examine the acceptance of the toolkits and compare them to existing authoring environments that also rely on VR nuggets but utilize different interface techniques. These studies’ results emphasize the importance of exchanging content between different established tools and indicate the acceptance of our tools regarding their hedonic and pragmatic qualities, also compared to existing tools from related work. As a conclusion, we propose an exchange format for VR nuggets that supports their reusability. It enables authors that use different toolkits to work together. They can utilize VR nuggets created with other toolkits and still use their own preferred toolkit. By means of an expert survey, we draw conclusions on technical aspects and a suitable platform to make VR nuggets available to the community. This survey indicates that potential authors would use such an exchange-approach for creating and presenting VR content and that they are willing to share their work and to contribute in a VR nugget authoring community.},
	language = {en},
	number = {15},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Horst, Robin and Gerstmeier, Simon and Naraghi-Taghi-Off, Ramtin and Wagner, Julian and Rau, Linda and Dörner, Ralf},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Authoring, Bite-sized learning, E-Learning, Games engineering, Short virtual reality experiences, Virtual reality, Virtual reality learning nuggets},
	pages = {46557--46594},
	file = {Full Text PDF:files/1082/Horst et al. - 2024 - Virtual reality content creation based on self-contained components in the e-learning domain Re-usi.pdf:application/pdf},
}

@article{chan_reinforcement_2024-1,
	title = {Reinforcement learning-based drone simulators: survey, practice, and challenge},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Reinforcement learning-based drone simulators},
	url = {https://doi.org/10.1007/s10462-024-10933-w},
	doi = {10.1007/s10462-024-10933-w},
	abstract = {Recently, machine learning has been very useful in solving diverse tasks with drones, such as autonomous navigation, visual surveillance, communication, disaster management, and agriculture. Among these machine learning, two representative paradigms have been widely utilized in such applications: supervised learning and reinforcement learning. Researchers prefer to use supervised learning, mostly based on convolutional neural networks, because of its robustness and ease of use but yet data labeling is laborious and time-consuming. On the other hand, when traditional reinforcement learning is combined with the deep neural network, it can be a very powerful tool to solve high-dimensional input problems such as image and video. Along with the fast development of reinforcement learning, many researchers utilize reinforcement learning in drone applications, and it often outperforms supervised learning. However, it usually requires the agent to explore the environment on a trial-and-error basis which is high cost and unrealistic in the real environment. Recent advances in simulated environments can allow an agent to learn by itself to overcome these drawbacks, although the gap between the real environment and the simulator has to be minimized in the end. In this sense, a realistic and reliable simulator is essential for reinforcement learning training. This paper investigates various drone simulators that work with diverse reinforcement learning architectures. The characteristics of the reinforcement learning-based drone simulators are analyzed and compared for the researchers who would like to employ them for their projects. Finally, we shed light on some challenges and potential directions for future drone simulators.},
	language = {en},
	number = {10},
	urldate = {2024-09-05},
	journal = {Artif Intell Rev},
	author = {Chan, Jun Hoong and Liu, Kai and Chen, Yu and Sagar, A. S. M. Sharifuzzaman and Kim, Yong-Guk},
	month = sep,
	year = {2024},
	keywords = {Artificial Intelligence, Drone, Drone simulator, Reinforcement learning},
	pages = {281},
	file = {Full Text PDF:files/1083/Chan et al. - 2024 - Reinforcement learning-based drone simulators survey, practice, and challenge.pdf:application/pdf},
}

@article{firat_3d_2022-1,
	title = {{3D} sound spatialization with game engines: the virtual acoustics performance of a game engine and a middleware for interactive audio design},
	volume = {26},
	issn = {1434-9957},
	shorttitle = {{3D} sound spatialization with game engines},
	url = {https://doi.org/10.1007/s10055-021-00589-0},
	doi = {10.1007/s10055-021-00589-0},
	abstract = {This study analyses one of the most popular game engines and an audio middleware to reproduce sound according to sound propagation physics. The analysis focuses on the transmission path between the sound source and the receiver. Even if there are several ready-to-use real-time auralization platforms and software, game engines' use with this aim is a recent study area for acousticians. However, audio design needs with game engines and the limits of their basic releases require additional tools (plugins and middleware) to improve both the quality and realism of sound in virtual environments. The paper discusses the use of Unreal Engine 4 and Wwise's 3D audio production methods in a set of different test environments. It assesses their performance in regard to a commercial geometrical acoustics software. The results show that the investigated version of the game engine and its sound assets are insufficient to simulate real-world cases and that significant improvements can be achieved with use of the middleware.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Fırat, Hasan Baran and Maffei, Luigi and Masullo, Massimiliano},
	month = jun,
	year = {2022},
	keywords = {3D audio, Artificial Intelligence, Game engines, Physically based rendering, Real-time auralization, Virtual audio reality},
	pages = {539--558},
	file = {Full Text PDF:files/1084/Fırat et al. - 2022 - 3D sound spatialization with game engines the virtual acoustics performance of a game engine and a.pdf:application/pdf},
}

@article{clausen_can_2022-1,
	title = {Can we benefit from game engines to develop digital twins for planning the deployment of photovoltaics?},
	volume = {5},
	issn = {2520-8942},
	url = {https://doi.org/10.1186/s42162-022-00222-7},
	doi = {10.1186/s42162-022-00222-7},
	abstract = {Digital Twins (DTs) have attracted great attention in the energy sector. Game engines have been suggested to model DTs of their physical counterparts because they provide realistic graphics, lighting-, fluid- and physics engines that simulate the real world. However, the application of game engines to develop DTs for photovoltaics (PVs) has not yet been discussed in the literature. Therefore, this paper assesses the built-in game engine features' ability to support the DT development of PVs with Unreal Engine 5. This paper mainly focuses on visual representation because the surrounding environment significantly impacts PV deployment, and the existing software tools do not allow the study of the environmental factors at the early planning phase of a project’s lifecycle. Furthermore, this paper investigates the position of the sun, shadows and reflections from nearby objects that influence the PVs' power output, and if the built-in light engine can be used for planning the deployment of PVs. The result shows that in-game objects in the environment can be used to affect the simulated PV output estimate over a year. It also indicates that applying Unreal Engine 5 to model PV systems that rely on mirroring real-world behaviour is promising if accurate data is used in the modelling. Real data and mathematical PV models are necessary since Unreal Engine 5’s Lumen subsystem cannot provide realistic solar radiance on PVs for a given location on earth.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Energy Inform},
	author = {Clausen, Christian Skafte Beck and Ma, Zheng Grace and Jørgensen, Bo Nørregaard},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Digital twin, Game engine, Photovoltaics, Simulation, Unreal Engine},
	pages = {42},
	file = {Full Text PDF:files/1085/Clausen et al. - 2022 - Can we benefit from game engines to develop digital twins for planning the deployment of photovoltai.pdf:application/pdf},
}

@article{vilchis_survey_2023-1,
	title = {A survey on the pipeline evolution of facial capture and tracking for digital humans},
	volume = {29},
	issn = {1432-1882},
	url = {https://doi.org/10.1007/s00530-023-01081-2},
	doi = {10.1007/s00530-023-01081-2},
	abstract = {With the introduction of concepts for virtual interaction and digital doubles, a rich scenario has been created for embodied avatars to strive. These avatars, more recently referred to as digital humans, have become a popular area of research, resulting in various techniques and methods that focus on improving the perception of their realism, fidelity, emphatic response, and interactivity. This survey aims to explore the literature and recent advancements on the key processes behind the creation and animation of digital human faces through the view of a general pipeline. The extensive review carried out in this study explores the usual data collection protocols, the main facial codification paradigms and databases, the approaches for digital human asset creation, facial tracking solutions for performance-driven animation, the solving process, and the final rendering delivery. Different quantitative evaluation methods, visual perception tests, and empathetic response evaluations for digital humans are also included in the survey. Additionally, the paper presents an updated summary of public and private frameworks for digital humans that go through the complete general pipeline presented. Finally, the condensed knowledge is discussed, inquiring into the possible direction of future developments in the field.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Multimedia Systems},
	author = {Vilchis, Carlos and Perez-Guerrero, Carmina and Mendez-Ruiz, Mauricio and Gonzalez-Mendoza, Miguel},
	month = aug,
	year = {2023},
	keywords = {Artificial Intelligence, Deep learning, Digital humans, Empathic response, Facial codification, Facial expressions, Photogrammetry},
	pages = {1917--1940},
	file = {Full Text PDF:files/1086/Vilchis et al. - 2023 - A survey on the pipeline evolution of facial capture and tracking for digital humans.pdf:application/pdf},
}

@article{aleem_game_2016-1,
	title = {Game development software engineering process life cycle: a systematic review},
	volume = {4},
	issn = {2195-1721},
	shorttitle = {Game development software engineering process life cycle},
	url = {https://doi.org/10.1186/s40411-016-0032-7},
	doi = {10.1186/s40411-016-0032-7},
	abstract = {Software game is a kind of application that is used not only for entertainment, but also for serious purposes that can be applicable to different domains such as education, business, and health care. Multidisciplinary nature of the game development processes that combine sound, art, control systems, artificial intelligence (AI), and human factors, makes the software game development practice different from traditional software development. However, the underline software engineering techniques help game development to achieve maintainability, flexibility, lower effort and cost, and better design. The purpose of this study is to assesses the state of the art research on the game development software engineering process and highlight areas that need further consideration by researchers. In the study, we used a systematic literature review methodology based on well-known digital libraries. The largest number of studies have been reported in the production phase of the game development software engineering process life cycle, followed by the pre-production phase. By contrast, the post-production phase has received much less research activity than the pre-production and production phases. The results of this study suggest that the game development software engineering process has many aspects that need further attention from researchers; that especially includes the postproduction phase.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {J Softw Eng Res Dev},
	author = {Aleem, Saiqa and Capretz, Luiz Fernando and Ahmed, Faheem},
	month = nov,
	year = {2016},
	keywords = {Artificial Intelligence, Development software engineerong proces, Online game, Software Game, Systematic review, Video game},
	pages = {6},
	file = {Full Text PDF:files/1087/Aleem et al. - 2016 - Game development software engineering process life cycle a systematic review.pdf:application/pdf},
}

@article{zhang_high-performance_2022-1,
	title = {High-performance adaptive texture streaming and rendering of large {3D} cities},
	volume = {38},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-021-02152-z},
	doi = {10.1007/s00371-021-02152-z},
	abstract = {We propose a high-performance texture streaming system for real-time rendering of large 3D cities with millions of textures. Our main contribution is a texture streaming system that automatically adjusts the streaming workload at runtime based on measured frame latencies, specifically addressing the high memory binding costs of hardware virtual texturing which causes frame rate stuttering. Our system streams textures in parallel with prioritization based on GPU computed mesh perceptibility, and these textures are cached in a sparse partially resident image at runtime without the need for a texture preprocessing step. In addition, we improve rendering quality by minimizing texture pop-in artifacts using a color blending scheme based on mipmap levels. We evaluate our texture streaming system using three structurally distinct datasets with many textures and compared it to a baseline, a game engine, and our prior method. Results show an 8X improvement in rendering performance and 7X improvement in rendering quality compared to the baseline.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Zhang, Alex and Chen, Kan and Johan, Henry and Erdt, Marius},
	month = apr,
	year = {2022},
	keywords = {3D cities, Artificial Intelligence, Real-time rendering, Texture streaming, Virtual texturing},
	pages = {1245--1262},
	file = {Full Text PDF:files/1088/Zhang et al. - 2022 - High-performance adaptive texture streaming and rendering of large 3D cities.pdf:application/pdf},
}

@article{kamienski_empirical_2021-1,
	title = {An empirical study of {Q}\&{A} websites for game developers},
	volume = {26},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-021-10014-4},
	doi = {10.1007/s10664-021-10014-4},
	abstract = {The game development industry is growing, and training new developers in game development-specific abilities is essential to satisfying its need for skilled game developers. These developers require effective learning resources to acquire the information they need and improve their game development skills. Question and Answer (Q\&A) websites stand out as some of the most used online learning resources in software development. Many studies have investigated how Q\&A websites help software developers become more experienced. However, no studies have explored Q\&A websites aimed at game development, and there is little information about how game developers use and interact with these websites. In this paper, we study four Q\&A communities by analyzing game development data we collected from their websites and the 347 responses received on a survey we ran with game developers. We observe that the communities have declined over the past few years and identify factors that correlate to these changes. Using a Latent Dirichlet Allocation (LDA) model, we characterize the topics discussed in the communities. We also analyze how topics differ across communities and identify the most discussed topics. Furthermore, we find that survey respondents have a mostly negative view of the communities and tended to stop using the websites once they became more experienced. Finally, we provide recommendations on where game developers should post their questions, which can help mitigate the websites’ declines and improve their effectiveness.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Empir Software Eng},
	author = {Kamienski, Arthur and Bezemer, Cor-Paul},
	month = aug,
	year = {2021},
	keywords = {Artificial Intelligence, Game development, Q\&A communities},
	pages = {115},
	file = {Full Text PDF:files/1089/Kamienski e Bezemer - 2021 - An empirical study of Q&A websites for game developers.pdf:application/pdf},
}

@article{anantrasirichai_artificial_2022-1,
	title = {Artificial intelligence in the creative industries: a review},
	volume = {55},
	issn = {1573-7462},
	shorttitle = {Artificial intelligence in the creative industries},
	url = {https://doi.org/10.1007/s10462-021-10039-7},
	doi = {10.1007/s10462-021-10039-7},
	abstract = {This paper reviews the current state of the art in artificial intelligence (AI) technologies and applications in the context of the creative industries. A brief background of AI, and specifically machine learning (ML) algorithms, is provided including convolutional neural networks (CNNs), generative adversarial networks (GANs), recurrent neural networks (RNNs) and deep Reinforcement Learning (DRL). We categorize creative applications into five groups, related to how AI technologies are used: (i) content creation, (ii) information analysis, (iii) content enhancement and post production workflows, (iv) information extraction and enhancement, and (v) data compression. We critically examine the successes and limitations of this rapidly advancing technology in each of these areas. We further differentiate between the use of AI as a creative tool and its potential as a creator in its own right. We foresee that, in the near future, ML-based AI will be adopted widely as a tool or collaborative assistant for creativity. In contrast, we observe that the successes of ML in domains with fewer constraints, where AI is the ‘creator’, remain modest. The potential of AI (or its developers) to win awards for its original creations in competition with human creatives is also limited, based on contemporary technologies. We therefore conclude that, in the context of creative industries, maximum benefit from AI will be derived where its focus is human-centric—where it is designed to augment, rather than replace, human creativity.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Artif Intell Rev},
	author = {Anantrasirichai, Nantheera and Bull, David},
	month = jan,
	year = {2022},
	keywords = {Artificial Intelligence, Creative industries, Image and video enhancement, Machine learning},
	pages = {589--656},
	file = {Full Text PDF:files/1090/Anantrasirichai e Bull - 2022 - Artificial intelligence in the creative industries a review.pdf:application/pdf},
}

@article{ma_generative_2024-1,
	title = {Generative deep learning for data generation in natural hazard analysis: motivations, advances, challenges, and opportunities},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Generative deep learning for data generation in natural hazard analysis},
	url = {https://doi.org/10.1007/s10462-024-10764-9},
	doi = {10.1007/s10462-024-10764-9},
	abstract = {Data mining and analysis are critical for preventing or mitigating natural hazards. However, data availability in natural hazard analysis is experiencing unprecedented challenges due to economic, technical, and environmental constraints. Recently, generative deep learning has become an increasingly attractive solution to these challenges, which can augment, impute, or synthesize data based on these learned complex, high-dimensional probability distributions of data. Over the last several years, much research has demonstrated the remarkable capabilities of generative deep learning for addressing data-related problems in natural hazards analysis. Data processed by deep generative models can be utilized to describe the evolution or occurrence of natural hazards and contribute to subsequent natural hazard modeling. Here we present a comprehensive review concerning generative deep learning for data generation in natural hazard analysis. (1) We summarized the limitations associated with data availability in natural hazards analysis and identified the fundamental motivations for employing generative deep learning as a critical response to these challenges. (2) We discuss several deep generative models that have been applied to overcome the problems caused by limited data availability in natural hazards analysis. (3) We analyze advances in utilizing generative deep learning for data generation in natural hazard analysis. (4) We discuss challenges associated with leveraging generative deep learning in natural hazard analysis. (5) We explore further opportunities for leveraging generative deep learning in natural hazard analysis. This comprehensive review provides a detailed roadmap for scholars interested in applying generative models for data generation in natural hazard analysis.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Artif Intell Rev},
	author = {Ma, Zhengjing and Mei, Gang and Xu, Nengxiong},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Data generation, Downscaling meteorological variables, Generative deep learning, Natural hazard analysis, Seismic data interpolation},
	pages = {160},
	file = {Full Text PDF:files/1091/Ma et al. - 2024 - Generative deep learning for data generation in natural hazard analysis motivations, advances, chal.pdf:application/pdf},
}

@article{szirmay-kalos_adapting_2022-1,
	title = {Adapting {Game} {Engines} to {Curved} {Spaces}},
	volume = {38},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-021-02303-2},
	doi = {10.1007/s00371-021-02303-2},
	abstract = {Curved spaces are very un-intuitive to our eyes trained on Euclidean geometry. Games provide an interesting way to explore these strange worlds. Games are written with the help of modeling tools and game engines based on Euclidean geometry. This paper addresses the problem of adapting 3D game engines to the rules of curved spaces. We consider the conversion of Euclidean objects, geometric calculations, transformation pipeline, lighting and physical simulation. Finally, we identify where existing game engines should be modified.},
	language = {en},
	number = {12},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Szirmay-Kalos, László and Magdics, Milán},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Elliptic geometry, Hyperbolic geometry, Illumination, Transformations},
	pages = {4383--4395},
	file = {Full Text PDF:files/1092/Szirmay-Kalos e Magdics - 2022 - Adapting Game Engines to Curved Spaces.pdf:application/pdf},
}

@article{flotynski_creating_2021-1,
	title = {Creating explorable extended reality environments with semantic annotations},
	volume = {80},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-020-09772-y},
	doi = {10.1007/s11042-020-09772-y},
	abstract = {The main element of extended reality (XR) environments is behavior-rich 3D content consisting of objects that act and interact with one another as well as with users. Such actions and interactions constitute the evolution of the content over time. Multiple application domains of XR, e.g., education, training, marketing, merchandising, and design, could benefit from the analysis of 3D content changes based on general or domain knowledge comprehensible to average users or domain experts. Such analysis can be intended, in particular, to monitor, comprehend, examine, and control XR environments as well as users’ skills, experience, interests and preferences, and XR objects’ features. However, it is difficult to achieve as long as XR environments are developed with methods and tools that focus on programming and 3D modeling rather than expressing domain knowledge accompanying content users and objects, and their behavior. The main contribution of this paper is an approach to creating explorable knowledge-based XR environments with semantic annotations. The approach combines description logics with aspect-oriented programming, which enables knowledge representation in an arbitrary domain as well as transformation of available environments with minimal users’ effort. We have implemented the approach using well-established development tools and exemplify it with an explorable immersive car showroom. The approach enables efficient creation of explorable XR environments and knowledge acquisition from XR.},
	language = {en},
	number = {5},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Flotyński, Jakub},
	month = feb,
	year = {2021},
	keywords = {3D Web, Annotations, Artificial Intelligence, Exploration, Extended reality, Ontologies, Queries, Reasoning, Semantic web},
	pages = {6959--6989},
	file = {Full Text PDF:files/1093/Flotyński - 2021 - Creating explorable extended reality environments with semantic annotations.pdf:application/pdf},
}

@article{gilbert_authoring_2015-1,
	title = {Authoring {Effective} {Embedded} {Tutors}: {An} {Overview} of the {Extensible} {Problem} {Specific} {Tutor} ({xPST}) {System}},
	volume = {25},
	issn = {1560-4306},
	shorttitle = {Authoring {Effective} {Embedded} {Tutors}},
	url = {https://doi.org/10.1007/s40593-015-0045-0},
	doi = {10.1007/s40593-015-0045-0},
	abstract = {The Extensible Problem Specific Tutor (xPST) allows authors who are not cognitive scientists and not programmers to quickly create an intelligent tutoring system that provides instruction akin to a model-tracing tutor. Furthermore, this instruction is overlaid on existing software, so that the learner’s interface does not have to be made from scratch. The xPST architecture allows for extending its capabilities by the addition of plug-ins that communicate with additional third-party software. After reviewing this general architecture, we describe three major implementations that we have created using the xPST system, each using different third-party software as the learner’s interface. We have conducted three evaluations of authors using xPST to create tutoring content, and these are considered in turn. These evaluations show that xPST authors can quickly learn the system, and can efficiently produce successful embedded instruction.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Int J Artif Intell Educ},
	author = {Gilbert, Stephen B. and Blessing, Stephen B. and Guo, Enruo},
	month = sep,
	year = {2015},
	keywords = {Artificial Intelligence, Authoring tool, Cognitive tutor, Digital Education and Educational Technology, Model-Tracing Tutor, xPST},
	pages = {428--454},
	file = {Full Text PDF:files/1094/Gilbert et al. - 2015 - Authoring Effective Embedded Tutors An Overview of the Extensible Problem Specific Tutor (xPST) Sys.pdf:application/pdf},
}

@article{sorensen_potentials_2022-1,
	title = {Potentials of game engines for wind power digital twin development: an investigation of the {Unreal} {Engine}},
	volume = {5},
	issn = {2520-8942},
	shorttitle = {Potentials of game engines for wind power digital twin development},
	url = {https://doi.org/10.1186/s42162-022-00227-2},
	doi = {10.1186/s42162-022-00227-2},
	abstract = {Digital twin technologies have become popular in wind energy for monitoring and what-if scenario investigation. However, developing a digital representation of the wind is challenging, especially due to the digital twin platform constraints. Game engines might be possible to solve this issue, especially since game engines have been used for product design, testing, prototyping, and also digital twins. Therefore, this study investigates the potential of developing a digital twin of wind power in the Unreal game engine. A case study of two types of wind turbines (Vestas V164-8 and Enercon E-126 7.580) and one location (Esbjerg, Denmark) is chosen for this study. The digital twin includes the environment with historical wind data and the visual representation of the wind turbine with a wind power production model and the estimated production in the given wind conditions of the area. The results show that game engines are viable for building entire digital twins where a realistic graphical user interface is required. Unreal Engine 5 provides the tools for modelling the landscape, surrounding water, and lighting. In addition, the Unreal Engine ecosystem provides vast amounts of content, such as 3D assets and game logic plugins, easing the digital twin development. The results prove that digital twins built in Unreal Engine 5 have great potential development of digital twins and user interfaces for communicating with a digital twin. The developed digital twin allows for further extension to benefit future digital twins utilizing wind turbines.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Energy Inform},
	author = {Sørensen, Jonas Vedsted and Ma, Zheng and Jørgensen, Bo Nørregaard},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Digital twin, Game engine, Simulation, Unreal Engine, Wind energy},
	pages = {39},
	file = {Full Text PDF:files/1095/Sørensen et al. - 2022 - Potentials of game engines for wind power digital twin development an investigation of the Unreal E.pdf:application/pdf},
}

@article{flotynski_visual_2022-1,
	title = {Visual aspect-oriented modeling of explorable extended reality environments},
	volume = {26},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00601-7},
	doi = {10.1007/s10055-021-00601-7},
	abstract = {The availability of various extended reality (XR) systems for tracking users’ and objects’ behavior opens new opportunities for analyzing users’ and objects’ interactions and autonomous actions. Such analysis can be especially useful and attainable to domain experts when it is based on domain knowledge related to a particular application, liberating the analysts from going into technical details of 3D content. Analysis of XR users’ and objects’ behavior can provide knowledge about the users’ experience, interests and preferences, as well as objects’ features, which may be valuable in various domains, e.g., training, design and marketing. However, the available methods and tools for building XR focus on 3D modeling and programming rather than knowledge representation, making them unsuitable for domain-oriented analysis. In this paper, a new visual approach to modeling explorable XR environments is proposed. It is based on a semantic representation of aspects, which extend the primary code of XR environments to register their behavior in a form explorable with reasoning and queries, appropriate for high-level analysis in arbitrary domains. It permits domain experts to comprehend and analyze what happened in an XR environment regarding users’ and objects’ actions and interactions. The approach has been implemented as an extension to MS Visual Studio and demonstrated in an explorable immersive service guide for household appliances. The evaluation results show that the approach enables efficient development of explorable XR and may be useful for people with limited technical skills.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Flotyński, Jakub},
	month = sep,
	year = {2022},
	keywords = {3D web, Artificial Intelligence, Aspect-oriented modeling, Exploration, Extended reality, Ontologies, Semantic web},
	pages = {939--961},
	file = {Full Text PDF:files/1096/Flotyński - 2022 - Visual aspect-oriented modeling of explorable extended reality environments.pdf:application/pdf},
}

@article{xiao_toward_2024-1,
	title = {Toward next generation mixed reality games: a research through design approach},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Toward next generation mixed reality games},
	url = {https://doi.org/10.1007/s10055-024-01041-9},
	doi = {10.1007/s10055-024-01041-9},
	abstract = {Mixed reality (MR) games refer to games that integrate physical entities with digitally mediated contents. Currently, it entails game creators to integrate heterogeneous virtual and physical components, which is often time-consuming and labor-intensive, without the support of a coherent technology stack. The underlying technodiversity manifested by the research corpus suggests a complicated, multi-dimensional design space that goes beyond merely technical concerns. In this research, we adopted a research-through-design approach and proposed an MR game technology stack that facilitates flexible, low-code game development. As design grounding, we first surveyed 34 state-of-the-art studies, and results were synergized into three different spectra of technological affordances, respectively activity range, user interface and feedback control, to inform our next design process. We then went through an iterative prototyping phase and implemented an MR game development toolset. A co-design workshop was conducted, where we invited 15 participants to try the prototype tools and co-ideate the potential use scenarios for the proposed technology stack. First-hand user feedback was collected via questionnaires and semi-structured interviews. As a result, four conceptual game designs with three major design implications were generated, which conjointly reflect a broader understanding on MR gameful experience and contribute fresh insights to this emerging research domain.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Xiao, Ruowei and Zhang, Rongzheng and Buruk, Oğuz and Hamari, Juho and Virkki, Johanna},
	month = jul,
	year = {2024},
	keywords = {Artificial Intelligence, End user development, Extended reality, Game, Gamification, Mixed reality, Research through design, RFID},
	pages = {142},
	file = {Full Text PDF:files/1097/Xiao et al. - 2024 - Toward next generation mixed reality games a research through design approach.pdf:application/pdf},
}

@article{westera_artificial_2020-1,
	title = {Artificial intelligence moving serious gaming: {Presenting} reusable game {AI} components},
	volume = {25},
	issn = {1573-7608},
	shorttitle = {Artificial intelligence moving serious gaming},
	url = {https://doi.org/10.1007/s10639-019-09968-2},
	doi = {10.1007/s10639-019-09968-2},
	abstract = {This article provides a comprehensive overview of artificial intelligence (AI) for serious games. Reporting about the work of a European flagship project on serious game technologies, it presents a set of advanced game AI components that enable pedagogical affordances and that can be easily reused across a wide diversity of game engines and game platforms. Serious game AI functionalities include player modelling (real-time facial emotion recognition, automated difficulty adaptation, stealth assessment), natural language processing (sentiment analysis and essay scoring on free texts), and believable non-playing characters (emotional and socio-cultural, non-verbal bodily motion, and lip-synchronised speech), respectively. The reuse of these components enables game developers to develop high quality serious games at reduced costs and in shorter periods of time. All these components are open source software and can be freely downloaded from the newly launched portal at gamecomponents.eu. The components come with detailed installation manuals and tutorial videos. All components have been applied and validated in serious games that were tested with real end-users.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Westera, Wim and Prada, Rui and Mascarenhas, Samuel and Santos, Pedro A. and Dias, João and Guimarães, Manuel and Georgiadis, Konstantinos and Nyamsuren, Enkhbold and Bahreini, Kiavash and Yumak, Zerrin and Christyowidiasmoro, Chris and Dascalu, Mihai and Gutu-Robu, Gabriel and Ruseti, Stefan},
	month = jan,
	year = {2020},
	keywords = {Artificial intelligence, Artificial Intelligence, Component-based architecture, Digital Education and Educational Technology, Game development, Intelligent tutoring systems, Serious games, Software reuse},
	pages = {351--380},
	file = {Full Text PDF:files/1098/Westera et al. - 2020 - Artificial intelligence moving serious gaming Presenting reusable game AI components.pdf:application/pdf},
}

@article{moina-rivera_cloud_2024-1,
	title = {Cloud media video encoding: review and challenges},
	issn = {1573-7721},
	shorttitle = {Cloud media video encoding},
	url = {https://doi.org/10.1007/s11042-024-18763-2},
	doi = {10.1007/s11042-024-18763-2},
	abstract = {In recent years, Internet traffic patterns have been changing. Most of the traffic demand by end users is multimedia, in particular, video streaming accounts for over 53\%. This demand has led to improved network infrastructures and computing architectures to meet the challenges of delivering these multimedia services while maintaining an adequate quality of experience. Focusing on the preparation and adequacy of multimedia content for broadcasting, Cloud and Edge Computing infrastructures have been and will be crucial to offer high and ultra-high definition multimedia content in live, real-time, or video-on-demand scenarios. For these reasons, this review paper presents a detailed study of research papers related to encoding and transcoding techniques in cloud computing environments. It begins by discussing the evolution of streaming and the importance of the encoding process, with a focus on the latest streaming methods and codecs. Then, it examines the role of cloud systems in multimedia environments and provides details on the cloud infrastructure for media scenarios. After doing a systematic literature review, we have been able to find 49 valid papers that meet the requirements specified in the research questions. Each paper has been analyzed and classified according to several criteria, besides to inspect their relevance. To conclude this review, we have identified and elaborated on several challenges and open research issues associated with the development of video codecs optimized for diverse factors within both cloud and edge architectures. Additionally, we have discussed emerging challenges in designing new cloud/edge architectures aimed at more efficient delivery of media traffic. This involves investigating ways to improve the overall performance, reliability, and resource utilization of architectures that support the transmission of multimedia content over both cloud and edge computing environments ensuring a good quality of experience for the final user.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Moina-Rivera, Wilmer and Garcia-Pineda, Miguel and Gutiérrez-Aguado, Juan and Alcaraz-Calero, Jose M.},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Cloud computing, Coding, Encoding, Multimedia, Review, Survey, Transcoding},
	file = {Full Text PDF:files/1099/Moina-Rivera et al. - 2024 - Cloud media video encoding review and challenges.pdf:application/pdf},
}

@article{rodriguez-garcia_systematic_2024-1,
	title = {A systematic review of virtual {3D} reconstructions of {Cultural} {Heritage} in immersive {Virtual} {Reality}},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-024-18700-3},
	doi = {10.1007/s11042-024-18700-3},
	abstract = {Immersive Virtual Reality (iVR) devices are increasingly affordable and accessible to consumers. The widespread adoption of this technology for professional training is now finding its way into various other fields. One field that is gaining significant popularity is Cultural Heritage (CH), where iVR enables the reconstruction and exploration of lost heritage. However, an up-to-date systematic review of iVR within this field will be of great benefit. Hence, the present review of 94 papers published between 2013 and 2022 that follows PRISMA methodology on virtual reconstruction of CH for iVR. The aim is to identify the key factors behind the development of these applications and their standards. To do so, a statistical analysis on the following topics was performed: (1) nationality, publication date, and article type; (2) heritage type and its current state of preservation; (3) the area of final application and the features of the reconstructions; (4) the characteristics of the iVR experience; and (5) the assessment of the iVR applications. Finally, a roadmap of best practices is outlined for the virtual reconstruction of CH using iVR and some of the most promising future research lines are outlined.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Rodriguez-Garcia, Bruno and Guillen-Sanz, Henar and Checa, David and Bustillo, Andres},
	month = apr,
	year = {2024},
	keywords = {3D model, Artificial Intelligence, Cultural Heritage, Head Mounted Display, Virtual Reality, Virtual Reconstruction},
	file = {Full Text PDF:files/1100/Rodriguez-Garcia et al. - 2024 - A systematic review of virtual 3D reconstructions of Cultural Heritage in immersive Virtual Reality.pdf:application/pdf},
}

@article{checa_immersive_2023,
	title = {Immersive virtual-reality computer-assembly serious game to enhance autonomous learning},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00607-1},
	doi = {10.1007/s10055-021-00607-1},
	abstract = {Immersive virtual reality (VR) environments create a very strong sense of presence and immersion. Nowadays, especially when student isolation and online autonomous learning is required, such sensations can provide higher satisfaction and learning rates than conventional teaching. However, up until the present, learning outcomes with VR tools have yet to prove their advantageous aspects over conventional teaching. The project presents a VR serious game for teaching concepts associated with computer hardware assembly. These concepts are often included in any undergraduate’s introduction to Computer Science. The learning outcomes are evaluated using a pre-test of previous knowledge, a satisfaction/usability test, and a post-test on knowledge acquisition, structured with questions on different knowledge areas. The results of the VR serious game are compared with another two learning methodologies adapted to online learning: (1) an online conventional lecture; and (2) playing the same serious game on a desktop PC. An extensive sample of students (n = 77) was formed for this purpose. The results showed the strong potential of VR serious games to improve student well-being during spells of confinement, due to higher learning satisfaction. Besides, ease of usability and the use of in-game tutorials are directly related with game-user satisfaction and performance. The main novelty of this research is related to academic performance. Although a very limited effect was noted for learning theoretical knowledge with the VR application in comparison with the other methodologies, this effect was significantly improved through visual knowledge, understanding and making connections between different concepts. It can therefore be concluded that the proposed VR serious game has the potential to increase student learning and therefore student satisfaction, by imparting a deeper understanding of the subject matter to students.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Checa, David and Miguel-Alonso, Ines and Bustillo, Andres},
	month = dec,
	year = {2023},
	keywords = {Active learning, Artificial Intelligence, Computer science, e-Learning, Educational game, Game engine, Head mounted display, Virtual reality},
	pages = {3301--3318},
	file = {Full Text PDF:files/1121/Checa et al. - 2023 - Immersive virtual-reality computer-assembly serious game to enhance autonomous learning.pdf:application/pdf},
}

@article{baldassarre_phydslk_2021,
	title = {{PhyDSLK}: a model-driven framework for generating exergames},
	volume = {80},
	issn = {1573-7721},
	shorttitle = {{PhyDSLK}},
	url = {https://doi.org/10.1007/s11042-021-10980-3},
	doi = {10.1007/s11042-021-10980-3},
	abstract = {In recent years, we have been witnessing a rapid increase of research on exergames—i.e., computer games that require users to move during gameplay as a form of physical activity and rehabilitation. Properly balancing the need to develop an effective exercise activity with the requirements for a smooth interaction with the software system and an engaging game experience is a challenge. Model-driven software engineering enables the fast prototyping of multiple system variants, which can be very useful for exergame development. In this paper, we propose a framework, PhyDSLK, which eases the development process of personalized and engaging Kinect-based exergames for rehabilitation purposes, providing high-level tools that abstract the technical details of using the Kinect sensor and allows developers to focus on the game design and user experience. The system relies on model-driven software engineering technologies and is made of two main components: (i) an authoring environment relying on a domain-specific language to define the exergame model encapsulating the gameplay that the exergame designer has envisioned and (ii) a code generator that transforms the exergame model into executable code. To validate our approach, we performed a preliminary empirical evaluation addressing development effort and usability of the PhyDSLK framework. The results are promising and provide evidence that people with no experience in game development are able to create exergames with different complexity levels in one hour, after a less-than-two-hour training on PhyDSLK. Also, they consider PhyDSLK usable regardless of the exergame complexity.},
	language = {en},
	number = {18},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Baldassarre, Maria Teresa and Caivano, Danilo and Romano, Simone and Cagnetta, Francesco and Fernandez-Cervantes, Victor and Stroulia, Eleni},
	month = jul,
	year = {2021},
	keywords = {Artificial Intelligence, Model-driven game development, Model-driven software engineering, Rehabilitation exergames},
	pages = {27947--27971},
	file = {Full Text PDF:files/1122/Baldassarre et al. - 2021 - PhyDSLK a model-driven framework for generating exergames.pdf:application/pdf},
}

@article{gil_rodriguez_colour_2021,
	title = {Colour {Calibration} of a {Head} {Mounted} {Display} for {Colour} {Vision} {Research} {Using} {Virtual} {Reality}},
	volume = {3},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-021-00855-7},
	doi = {10.1007/s42979-021-00855-7},
	abstract = {Virtual reality (VR) technology offers vision researchers the opportunity to conduct immersive studies in simulated real-world scenes. However, an accurate colour calibration of the VR head mounted display (HMD), both in terms of luminance and chromaticity, is required to precisely control the presented stimuli. Such a calibration presents significant new challenges, for example, due to the large field of view of the HMD, or the software implementation used for scene rendering, which might alter the colour appearance of objects. Here, we propose a framework for calibrating an HMD using an imaging colorimeter, the I29 (Radiant Vision Systems, Redmond, WA, USA). We examine two scenarios, both with and without using a rendering software for visualisation. In addition, we present a colour constancy experiment design for VR through a gaming engine software, Unreal Engine 4. The colours of the objects of study are chosen according to the previously defined calibration. Results show a high-colour constancy performance among participants, in agreement with recent studies performed on real-world scenarios. Our studies show that our methodology allows us to control and measure the colours presented in the HMD, effectively enabling the use of VR technology for colour vision research.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {SN COMPUT. SCI.},
	author = {Gil Rodríguez, Raquel and Bayer, Florian and Toscani, Matteo and Guarnera, Dar’ya and Guarnera, Giuseppe Claudio and Gegenfurtner, Karl R.},
	month = oct,
	year = {2021},
	keywords = {Colorimeter, Colour calibration, Colour constancy, Colour vision, Head mounted display, Spectroradiometer, Virtual reality},
	pages = {22},
	file = {Full Text PDF:files/1123/Gil Rodríguez et al. - 2021 - Colour Calibration of a Head Mounted Display for Colour Vision Research Using Virtual Reality.pdf:application/pdf},
}

@article{cai_rgb-d_2017,
	title = {{RGB}-{D} datasets using microsoft kinect or similar sensors: a survey},
	volume = {76},
	issn = {1573-7721},
	shorttitle = {{RGB}-{D} datasets using microsoft kinect or similar sensors},
	url = {https://doi.org/10.1007/s11042-016-3374-6},
	doi = {10.1007/s11042-016-3374-6},
	abstract = {RGB-D data has turned out to be a very useful representation of an indoor scene for solving fundamental computer vision problems. It takes the advantages of the color image that provides appearance information of an object and also the depth image that is immune to the variations in color, illumination, rotation angle and scale. With the invention of the low-cost Microsoft Kinect sensor, which was initially used for gaming and later became a popular device for computer vision, high quality RGB-D data can be acquired easily. In recent years, more and more RGB-D image/video datasets dedicated to various applications have become available, which are of great importance to benchmark the state-of-the-art. In this paper, we systematically survey popular RGB-D datasets for different applications including object recognition, scene classification, hand gesture recognition, 3D-simultaneous localization and mapping, and pose estimation. We provide the insights into the characteristics of each important dataset, and compare the popularity and the difficulty of those datasets. Overall, the main goal of this survey is to give a comprehensive description about the available RGB-D datasets and thus to guide researchers in the selection of suitable datasets for evaluating their algorithms.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Cai, Ziyun and Han, Jungong and Liu, Li and Shao, Ling},
	month = feb,
	year = {2017},
	keywords = {Artificial Intelligence, Computer vision, Database, Microsoft Kinect sensor or similar devices, RGB-D dataset, Survey},
	pages = {4313--4355},
	file = {Full Text PDF:files/1124/Cai et al. - 2017 - RGB-D datasets using microsoft kinect or similar sensors a survey.pdf:application/pdf},
}

@article{ritter_three-dimensional_2022,
	title = {Three-dimensional modeled environments versus 360 degree panoramas for mobile virtual reality training},
	volume = {26},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00502-9},
	doi = {10.1007/s10055-021-00502-9},
	abstract = {Virtual field trip is a way of providing users with some knowledge and exposure of a facility without requiring them to physically visit the location. Due to the high computational costs that are necessary to produce virtual environments (VEs), the potential for photorealism is sacrificed. Often these three-dimensional (3D) modeled applications use an unrealistic VE and, therefore, do not provide a full depiction of real-world environments. Panoramas can be used to showcase complex scenarios that are difficult to model and are computationally expensive to view in virtual reality (VR). Utilizing 360° panoramas can provide a low-cost and quick-to-capture alternative with photorealistic representations of the actual environment. The advantages of photorealism over 3D models for training and education are not clearly defined. This paper initially summarizes the development of a VR training application and initial pilot study. Quantitative and qualitative study then was conducted to compare the effectiveness of a 360° panorama VR training application and a 3D modeled one. Switching to a mobile VR headset saves money, increases mobility, decreases set-up and breakdown time, and has less spatial requirements. Testing results of the 3D modeled VE group had an average normalized gain of 0.03 and the 360° panorama group, 0.43. Although the 3D modeled group had slightly higher realism according to the presence questionnaire and had slightly higher averages in the comparative analysis questionnaire, the 360° panorama application has shown to be the most effective for training and the quickest to develop.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Ritter, K. A. and Chambers, Terrence L.},
	month = jun,
	year = {2022},
	keywords = {Artificial Intelligence, Virtual field trip, Virtual laboratories, Virtual reality, Virtual training},
	pages = {571--581},
	file = {Full Text PDF:files/1125/Ritter e Chambers - 2022 - Three-dimensional modeled environments versus 360 degree panoramas for mobile virtual reality traini.pdf:application/pdf},
}

@article{cooke_rorsim_2013,
	title = {{RORSIM}: a warship collision avoidance {3D} simulation designed to complement existing {Junior} {Warfare} {Officer} training},
	volume = {17},
	issn = {1434-9957},
	shorttitle = {{RORSIM}},
	url = {https://doi.org/10.1007/s10055-013-0223-z},
	doi = {10.1007/s10055-013-0223-z},
	abstract = {Royal Navy Junior Warfare Officers (JWO) undergo a comprehensive training package in order to prepare them to be officers of the watch. One aspect of this training relates to their knowledge of the ‘Rules of the Road’ or ‘COLREGS’; the rules for the manoeuvring and signalling that approaching vessels make in order to avoid collision. The training and assessment exercises undertaken predominantly use non-interactive static materials. These do not exercise the required skill in reconciling information from maritime charts, radar displays and ‘out-of-the-window’ monitoring. Consequently, performance during assessment on the VR-based bridge simulator falls short. This paper describes The Rules of the Road SIMulator (RORSIM)—a proof of concept interactive 3D (i3D) simulator developed to bridge the training gap between classroom teaching and VR bridge simulator assessment. RORSIM’s differentiation and its key functionality in terms of visualisaton, physics/interaction and game mechanics are influenced by the consideration of pedagogical learning models during requirements capture. This capture is formalised by a ‘Training Gap Use Case’—a graphical viewpoint using the Universal Modelling Language which can assist developers in requirements capture and development of i3D tools for existing training programmes. Key functionality, initial JWO feedback and a planned pilot study design are reported.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Cooke, Neil and Stone, Robert},
	month = sep,
	year = {2013},
	keywords = {Artificial Intelligence, Defence, Game-based training, Serious games, Simulation, Virtual environments},
	pages = {169--179},
	file = {Full Text PDF:files/1126/Cooke e Stone - 2013 - RORSIM a warship collision avoidance 3D simulation designed to complement existing Junior Warfare O.pdf:application/pdf},
}

@article{linse_walk_2022,
	title = {A walk in the black-box: {3D} visualization of large neural networks in virtual reality},
	volume = {34},
	issn = {1433-3058},
	shorttitle = {A walk in the black-box},
	url = {https://doi.org/10.1007/s00521-022-07608-4},
	doi = {10.1007/s00521-022-07608-4},
	abstract = {Within the last decade Deep Learning has become a tool for solving challenging problems like image recognition. Still, Convolutional Neural Networks (CNNs) are considered black-boxes, which are difficult to understand by humans. Hence, there is an urge to visualize CNN architectures, their internal processes and what they actually learn. Previously, virtual realityhas been successfully applied to display small CNNs in immersive 3D environments. In this work, we address the problem how to feasibly render large-scale CNNs, thereby enabling the visualization of popular architectures with ten thousands of feature maps and branches in the computational graph in 3D. Our software ”DeepVisionVR” enables the user to freely walk through the layered network, pick up and place images, move/scale layers for better readability, perform feature visualization and export the results. We also provide a novel Pytorch module to dynamically link PyTorch with Unity, which gives developers and researchers a convenient interface to visualize their own architectures. The visualization is directly created from the PyTorch class that defines the Pytorch model used for training and testing. This approach allows full access to the network’s internals and direct control over what exactly is visualized. In a use-case study, we apply the module to analyze models with different generalization abilities in order to understand how networks memorize images. We train two recent architectures, CovidResNet and CovidDenseNet on the Caltech101 and the SARS-CoV-2 datasets and find that bad generalization is driven by high-frequency features and the susceptibility to specific pixel arrangements, leading to implications for the practical application of CNNs. The code is available on Github https://github.com/Criscraft/DeepVisionVR.},
	language = {en},
	number = {23},
	urldate = {2024-09-05},
	journal = {Neural Comput \& Applic},
	author = {Linse, Christoph and Alshazly, Hammam and Martinetz, Thomas},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Deep convolutional neural network visualization, Explainable artificial intelligence, Human-understandable AI systems, Virtual reality},
	pages = {21237--21252},
	file = {Full Text PDF:files/1127/Linse et al. - 2022 - A walk in the black-box 3D visualization of large neural networks in virtual reality.pdf:application/pdf},
}

@article{sciacca_scientific_2022,
	title = {Scientific {Visualization} on the {Cloud}: the {NEANIAS} {Services} towards {EOSC} {Integration}},
	volume = {20},
	issn = {1572-9184},
	shorttitle = {Scientific {Visualization} on the {Cloud}},
	url = {https://doi.org/10.1007/s10723-022-09598-y},
	doi = {10.1007/s10723-022-09598-y},
	abstract = {NEANIAS is a research and innovation action project funded by the European Union under the Horizon 2020 program. The project addresses the challenge of prototyping novel solutions for the underwater, atmospheric and space research communities, creating a collaborative research ecosystem, and contributing to the effective materialization of the European Open Science Cloud (EOSC). NEANIAS drives the co-design, implementation, delivery, and integration into EOSC of innovative thematic and core services, derived from state-of-the-art assets and practices in the target scientific communities. We present the overall NEANIAS ecosystem architecture, with an emphasis on its core visualization services, detailing their specifications and software development plan, and focusing on the underpinning service-oriented architecture for their delivery. We report on the underlying ideas and guiding principles for designing such visualization services, outlining their current release status and future development roadmaps towards Technological Readiness Level (TRL) 8 maturity and EOSC integration.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {J Grid Computing},
	author = {Sciacca, Eva and Krokos, Mel and Bordiu, Cristobal and Brandt, Carlos and Vitello, Fabio and Bufano, Filomena and Becciani, Ugo and Raciti, Mario and Tudisco, Giuseppe and Riggi, Simone and Topa, Eugenio and Azzi, Sami and Kyd, Benjamin and Mantovani, Simone and Vettorello, Laura and Tan, Jiacheng and Quintana, Josep and Campos, Ricard and Pina, Noela},
	month = mar,
	year = {2022},
	keywords = {Big data, Cloud services, European Open Science Cloud (EOSC), Scientific visualization, Service oriented architectures},
	pages = {7},
	file = {Full Text PDF:files/1128/Sciacca et al. - 2022 - Scientific Visualization on the Cloud the NEANIAS Services towards EOSC Integration.pdf:application/pdf},
}

@article{albeedan_designing_2024,
	title = {Designing and evaluation of a mixed reality system for crime scene investigation training: a hybrid approach},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Designing and evaluation of a mixed reality system for crime scene investigation training},
	url = {https://doi.org/10.1007/s10055-024-01018-8},
	doi = {10.1007/s10055-024-01018-8},
	abstract = {Police investigation in real-life crime scenes is an essential aspect of forensic science education. However, the practicality of bringing young investigators to actual crime scenes is often hindered by the costs and challenges involved. In order to overcome these obstacles, new technologies such as mixed reality (MR) are being explored as potential solutions. MR technology offers an interactive and cost-effective way to simulate real-life crime scenes, providing a valuable training experience for young investigators. This paper presents a novel design of a MR system using Microsoft HoloLens 2.0, which is tailored to work in a spatial 3D scanned and reconstructed crime scene using FARO point cloud 3D scanner X130 blended with photogrammetry techniques. The system was developed through the lens of Experiential Learning Theory and designed using a participatory approach, providing a cost-effective solution to help trained Kuwaiti police officers enhance their investigative skills. In order to evaluate the system’s user experience and user interaction, the Questionnaire of User Interaction Satisfaction and User Experience Questionnaire were utilised. Forty-four young police officers evaluated the system. Police students showed positive levels of satisfaction with user interaction and overall user experience with minimal negative feedback. Female students showed higher satisfaction with the overall impression compared to male students. Based on the positive feedback regarding the system expansion, the system will be taken into the commercialisation stage in the future to be provided as an essential tool for crime scene education and investigation practices.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Albeedan, Meshal and Kolivanda, Hoshang and Hammady, Ramy},
	month = jun,
	year = {2024},
	keywords = {3D scanning, Artificial Intelligence, Crime scene, Investigation training, Mixed reality, Photogrammetry, User experience, User interaction},
	pages = {127},
	file = {Full Text PDF:files/1129/Albeedan et al. - 2024 - Designing and evaluation of a mixed reality system for crime scene investigation training a hybrid.pdf:application/pdf},
}

@article{yigitbas_design_2023,
	title = {Design and evaluation of a collaborative {UML} modeling environment in virtual reality},
	volume = {22},
	issn = {1619-1374},
	url = {https://doi.org/10.1007/s10270-022-01065-2},
	doi = {10.1007/s10270-022-01065-2},
	abstract = {Modeling is a key activity in conceptual design and system design. Through collaborative modeling, end-users, stakeholders, experts, and entrepreneurs are able to create a shared understanding of a system representation. While the Unified Modeling Language (UML) is one of the major conceptual modeling languages in object-oriented software engineering, more and more concerns arise from the modeling quality of UML and its tool-support. Among them, the limitation of the two-dimensional presentation of its notations and lack of natural collaborative modeling tools are reported to be significant. In this paper, we explore the potential of using virtual reality (VR) technology for collaborative UML software design by comparing it with classical collaborative software design using conventional devices (desktop PC/laptop). For this purpose, we have developed a VR modeling environment that offers a natural collaborative modeling experience for UML Class Diagrams. Based on a user study with 24 participants, we have compared collaborative VR modeling with conventional modeling with regard to efficiency, effectiveness, and user satisfaction. Results show that the use of VR has some disadvantages concerning efficiency and effectiveness, but the user’s fun, the feeling of being in the same room with a remote collaborator, and the naturalness of collaboration were increased.},
	language = {en},
	number = {5},
	urldate = {2024-09-05},
	journal = {Softw Syst Model},
	author = {Yigitbas, Enes and Gorissen, Simon and Weidmann, Nils and Engels, Gregor},
	month = oct,
	year = {2023},
	keywords = {Artificial Intelligence, Collaborative modeling, UML, Virtual reality},
	pages = {1397--1425},
	file = {Full Text PDF:files/1130/Yigitbas et al. - 2023 - Design and evaluation of a collaborative UML modeling environment in virtual reality.pdf:application/pdf},
}

@article{guillen-sanz_systematic_2024,
	title = {A systematic review of wearable biosensor usage in immersive virtual reality experiences},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-00970-9},
	doi = {10.1007/s10055-024-00970-9},
	abstract = {Wearable biosensors are increasingly incorporated in immersive Virtual Reality (iVR) applications. A trend that is attributed to the availability of better quality, less costly, and easier-to-use devices. However, consensus is yet to emerge over the most optimal combinations. In this review, the aim is to clarify the best examples of biosensor usage in combination with iVR applications. The high number of papers in the review (560) were classified into the following seven fields of application: psychology, medicine, sports, education, ergonomics, military, and tourism and marketing. The use of each type of wearable biosensor and Head-Mounted Display was analyzed for each field of application. Then, the development of the iVR application is analyzed according to its goals, user interaction levels, and the possibility of adapting the iVR environment to biosensor feedback. Finally, the evaluation of the iVR experience was studied, considering such issues as sample size, the presence of a control group, and post-assessment routines. A working method through which the most common solutions, the best practices, and the most promising trends in biofeedback-based iVR applications were identified for each field of application. Besides, guidelines oriented towards good practice are proposed for the development of future iVR with biofeedback applications. The results of this review suggest that the use of biosensors within iVR environments need to be standardized in some fields of application, especially when considering the adaptation of the iVR experience to real-time biosignals to improve user performance.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Guillen-Sanz, Henar and Checa, David and Miguel-Alonso, Ines and Bustillo, Andres},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Biofeedback, Biosensors, Head-mounted displays, Heart rate, Physiology, Virtual reality},
	pages = {74},
	file = {Full Text PDF:files/1131/Guillen-Sanz et al. - 2024 - A systematic review of wearable biosensor usage in immersive virtual reality experiences.pdf:application/pdf},
}

@article{guner_sahan_survey_2024,
	title = {A survey of appearance-based approaches for human gait recognition: techniques, challenges, and future directions},
	volume = {80},
	issn = {1573-0484},
	shorttitle = {A survey of appearance-based approaches for human gait recognition},
	url = {https://doi.org/10.1007/s11227-024-06172-z},
	doi = {10.1007/s11227-024-06172-z},
	abstract = {Gait recognition has become an important biometric feature for human identification, in addition to data such as face, iris, and fingerprint. The goal of human gait recognition is to identify people based on walking images. Artificial intelligence technologies have revolutionized the field of gait recognition by enabling computers to automatically learn and extract intricate patterns. These techniques examine video recordings to determine key features in an individual's gait, and these features are used to identify the person. This paper examines the existing appearance-based gait recognition methods that have been published in recent years. The primary objective of this paper is to provide an informative survey of the state-of-the-art in appearance-based gait recognition techniques, highlighting their applications, strengths, and limitations. Through our analysis, we aim to highlight the significant advance that has been made in this field, draw attention to the challenges that have been faced, and identify areas of prospective future research and advances in technology. Furthermore, we comprehensively examine common datasets used in gait recognition research. By analyzing the latest developments in appearance-based gait recognition, our study aims to be a helpful resource for researchers, providing an extensive overview of current methods and guiding future attempts in this dynamic field.},
	language = {en},
	number = {13},
	urldate = {2024-09-05},
	journal = {J Supercomput},
	author = {Güner Şahan, Pınar and Şahin, Suhap and Kaya Gülağız, Fidan},
	month = sep,
	year = {2024},
	keywords = {Artificial intelligence, Artificial Intelligence, Gait datasets, Gait recognition, Neural networks},
	pages = {18392--18429},
	file = {Full Text PDF:files/1132/Güner Şahan et al. - 2024 - A survey of appearance-based approaches for human gait recognition techniques, challenges, and futu.pdf:application/pdf},
}

@article{baslamisli_shadingnet_2021,
	title = {{ShadingNet}: {Image} {Intrinsics} by {Fine}-{Grained} {Shading} {Decomposition}},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {{ShadingNet}},
	url = {https://doi.org/10.1007/s11263-021-01477-5},
	doi = {10.1007/s11263-021-01477-5},
	abstract = {In general, intrinsic image decomposition algorithms interpret shading as one unified component including all photometric effects. As shading transitions are generally smoother than reflectance (albedo) changes, these methods may fail in distinguishing strong photometric effects from reflectance variations. Therefore, in this paper, we propose to decompose the shading component into direct (illumination) and indirect shading (ambient light and shadows) subcomponents. The aim is to distinguish strong photometric effects from reflectance variations. An end-to-end deep convolutional neural network (ShadingNet) is proposed that operates in a fine-to-coarse manner with a specialized fusion and refinement unit exploiting the fine-grained shading model. It is designed to learn specific reflectance cues separated from specific photometric effects to analyze the disentanglement capability. A large-scale dataset of scene-level synthetic images of outdoor natural environments is provided with fine-grained intrinsic image ground-truths. Large scale experiments show that our approach using fine-grained shading decompositions outperforms state-of-the-art algorithms utilizing unified shading on NED, MPI Sintel, GTA V, IIW, MIT Intrinsic Images, 3DRMS and SRD datasets.},
	language = {en},
	number = {8},
	urldate = {2024-09-05},
	journal = {Int J Comput Vis},
	author = {Baslamisli, Anil S. and Das, Partha and Le, Hoang-An and Karaoglu, Sezer and Gevers, Theo},
	month = aug,
	year = {2021},
	keywords = {Albedo, Artificial Intelligence, Intrinsic image decomposition, Photometric effects, Reflectance, Shadow},
	pages = {2445--2473},
	file = {Full Text PDF:files/1133/Baslamisli et al. - 2021 - ShadingNet Image Intrinsics by Fine-Grained Shading Decomposition.pdf:application/pdf},
}

@article{cauz_text_2024,
	title = {Text readability in augmented reality: a multivocal literature review},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Text readability in augmented reality},
	url = {https://doi.org/10.1007/s10055-024-00949-6},
	doi = {10.1007/s10055-024-00949-6},
	abstract = {Augmented reality (AR) is making its way into many sectors. Its rapid evolution in recent years has led to the development of prototypes demonstrating its effectiveness. However, to be able to push these prototypes to the scale of fully usable applications, it is important to ensure the readability of the texts they include. To this end, we conducted a multivocal literature review (MLR) to determine the text parameters a designer can tune, as well as the contextual constraints they need to pay attention to, in relation to Optical See-Through (OST) and Video See-Through (VST) displays. We also included guidelines from device manufacturing and game engines sites to compare the current state of research in the academic and industrial worlds. The results show that parameters pertaining more to letter legibility have been extensively studied (e.g., color and size), while those pertaining to the whole text still require further research (e.g., alignment or space between lines). The former group of parameters, and their associated constraints, were assembled in the form of two decision trees to facilitate implementation of AR applications. Finally, we also concluded that there was a lack of alignment between academic and industrial recommendations.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Cauz, Maxime and Clarinval, Antoine and Dumas, Bruno},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Augmented Reality, Legibility, Mixed Reality, Readability, Text},
	pages = {59},
	file = {Full Text PDF:files/1134/Cauz et al. - 2024 - Text readability in augmented reality a multivocal literature review.pdf:application/pdf},
}

@article{lukezic_new_2024,
	title = {A {New} {Dataset} and a {Distractor}-{Aware} {Architecture} for {Transparent} {Object} {Tracking}},
	volume = {132},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-024-02010-0},
	doi = {10.1007/s11263-024-02010-0},
	abstract = {Performance of modern trackers degrades substantially on transparent objects compared to opaque objects. This is largely due to two distinct reasons. Transparent objects are unique in that their appearance is directly affected by the background. Furthermore, transparent object scenes often contain many visually similar objects (distractors), which often lead to tracking failure. However, development of modern tracking architectures requires large training sets, which do not exist in transparent object tracking. We present two contributions addressing the aforementioned issues. We propose the first transparent object tracking training dataset Trans2k that consists of over 2k sequences with 104,343 images overall, annotated by bounding boxes and segmentation masks. Standard trackers trained on this dataset consistently improve by up to 16\%. Our second contribution is a new distractor-aware transparent object tracker (DiTra) that treats localization accuracy and target identification as separate tasks and implements them by a novel architecture. DiTra sets a new state-of-the-art in transparent object tracking and generalizes well to opaque objects.},
	language = {en},
	number = {8},
	urldate = {2024-09-05},
	journal = {Int J Comput Vis},
	author = {Lukežič, Alan and Trojer, Žiga and Matas, Jiří and Kristan, Matej},
	month = aug,
	year = {2024},
	keywords = {Artificial Intelligence, Distractors, Transparent object tracking, Visual object tracking},
	pages = {2729--2742},
	file = {Full Text PDF:files/1135/Lukežič et al. - 2024 - A New Dataset and a Distractor-Aware Architecture for Transparent Object Tracking.pdf:application/pdf},
}

@article{mazier_sonics_2024,
	title = {Sonics: develop intuition on biomechanical systems through interactive error controlled simulations},
	volume = {40},
	issn = {1435-5663},
	shorttitle = {Sonics},
	url = {https://doi.org/10.1007/s00366-023-01877-w},
	doi = {10.1007/s00366-023-01877-w},
	abstract = {We describe the SOniCS (SOFA + FEniCS) plugin to help develop an intuitive understanding of complex biomechanics systems. This new approach allows the user to experiment with model choices easily and quickly without requiring in-depth expertise. Constitutive models can be modified by one line of code only. This ease in building new models makes SOniCS ideal to develop surrogate, reduced order models and to train machine-learning algorithms for enabling real-time patient-specific simulations. SOniCS is thus not only a tool that facilitates the development of surgical training simulations but also, and perhaps more importantly, paves the way to increase the intuition of users or otherwise non-intuitive behaviors of (bio)mechanical systems. The plugin uses new developments of the FEniCSx project enabling automatic generation with FFCx of finite-element tensors, such as the local residual vector and Jacobian matrix. We verify our approach with numerical simulations, such as manufactured solutions, cantilever beams, and benchmarks provided by FEBio. We reach machine precision accuracy and demonstrate the use of the plugin for a real-time haptic simulation involving a surgical tool controlled by the user in contact with a hyperelastic liver. We include complete examples showing the use of our plugin for simulations involving Saint Venant–Kirchhoff, Neo-Hookean, Mooney–Rivlin, and Holzapfel Ogden anisotropic models as supplementary material.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Engineering with Computers},
	author = {Mazier, Arnaud and El Hadramy, Sidaty and Brunet, Jean-Nicolas and Hale, Jack S. and Cotin, Stéphane and Bordas, Stéphane P. A.},
	month = jun,
	year = {2024},
	keywords = {Automatic differentiation, Benchmark, Biomechanics, FEniCS, Hyperelasticity, SOFA},
	pages = {1857--1876},
	file = {Full Text PDF:files/1136/Mazier et al. - 2024 - Sonics develop intuition on biomechanical systems through interactive error controlled simulations.pdf:application/pdf},
}

@article{solmaz_immersive_2024,
	title = {An immersive virtual reality learning environment with {CFD} simulations: {Unveiling} the {Virtual} {Garage} concept},
	volume = {29},
	issn = {1573-7608},
	shorttitle = {An immersive virtual reality learning environment with {CFD} simulations},
	url = {https://doi.org/10.1007/s10639-023-11747-z},
	doi = {10.1007/s10639-023-11747-z},
	abstract = {Virtual reality has become a significant asset to diversify the existing toolkit supporting engineering education and training. The cognitive and behavioral advantages of virtual reality (VR) can help lecturers reduce entry barriers to concepts that students struggle with. Computational fluid dynamics (CFD) simulations are imperative tools intensively utilized in the design and analysis of chemical engineering problems. Although CFD simulation tools can be directly applied in engineering education, they bring several challenges in the implementation and operation for both students and lecturers. In this study, we develop the “Virtual Garage” as a task-centered educational VR application with CFD simulations to tackle these challenges. The Virtual Garage is composed of a holistic immersive virtual reality experience to educate students with a real-life engineering problem solved by CFD simulation data. The prototype is tested by graduate students (n = 24) assessing usability, user experience, task load and simulator sickness via standardized questionnaires together with self-reported metrics and a semi-structured interview. Results show that the Virtual Garage is well-received by participants. We identify features that can further leverage the quality of the VR experience with CFD simulations. Implications are incorporated throughout the study to provide practical guidance for developers and practitioners.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Solmaz, Serkan and Kester, Liesbeth and Van Gerven, Tom},
	month = feb,
	year = {2024},
	keywords = {Artificial Intelligence, Computational fluid dynamics, Digital Education and Educational Technology, Engineering education, Immersive learning, Virtual reality},
	pages = {1455--1488},
	file = {Full Text PDF:files/1137/Solmaz et al. - 2024 - An immersive virtual reality learning environment with CFD simulations Unveiling the Virtual Garage.pdf:application/pdf},
}

@article{pastel_application_2023,
	title = {Application of eye-tracking systems integrated into immersive virtual reality and possible transfer to the sports sector - {A} systematic review},
	volume = {82},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-022-13474-y},
	doi = {10.1007/s11042-022-13474-y},
	abstract = {In recent years, Virtual Reality (VR) has become a valuable tool in rehabilitation and sports training applications. New technologies offer opportunities to combine various systems and use them for sports-related scientific purposes. For instance, examining the visual perception of athletes within a standardized environment could be helpful to understand the differences between novices and experts in their visual behavior and could further reveal possible training applications for enhancing athletes’ visual attention. The current systematic literature review thematizes the importance of eye-tracking (ET) systems’ usage integrated into head-mounted displays (HMDs) in virtual environments for further inclusion in sports-related usage. An overview of possible implementations is given, and additional recommendations for using the combined technic regarding sports are made. Although only one study examined gaze behavior during sports activity within a standardized virtual environment, 38 relevant papers were identified using the ET systems integrated into the HMDs, which ideas can be transferred to the sports sector. The increased usability and fidelity in the virtual environment enabled through the combined technology were illustrated, and different approaches were listed in using and calculating gaze parameters. This literature review examines the possibility of integrating ET in VR, which can be further used to improve usability, interaction methods, image presentation, and visual perception analyses within future physical training scenarios. The compiled studies have shown that the existing methods are feasible due to the performance of the integrated ET systems but still need to be improved for practical use.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Pastel, Stefan and Marlok, Josua and Bandow, Nicole and Witte, Kerstin},
	month = jan,
	year = {2023},
	keywords = {Artificial Intelligence, Eye-tracking, Gaze behavior, Head-mounted display, Virtual reality, Visual perception},
	pages = {4181--4208},
	file = {Full Text PDF:files/1138/Pastel et al. - 2023 - Application of eye-tracking systems integrated into immersive virtual reality and possible transfer.pdf:application/pdf},
}

@article{pisani_digital_2024,
	title = {Digital modes of interpretation of {Pictish} sculpture},
	volume = {29},
	issn = {1573-7608},
	url = {https://doi.org/10.1007/s10639-023-12151-3},
	doi = {10.1007/s10639-023-12151-3},
	abstract = {Cultural heritage is no longer something that can only be experienced in a museum exhibition. Digital tools have facilitated the distribution of material relating to artefacts, both in its representation and in presenting its context. This paper describes how digital modelling techniques can be synthesised with 3D scanning to digitally restore artefacts and create authentic replicas of their original states. The digital artefacts can then be used to assist the process of interpreting these artefacts in diverse forms, both in the museum and outside the museum. The study looks at Pictish sculpture as a case-study, restoring 3D models of two stones, and creating varying opportunities for their interpretation. As part of this study, new interactive tools, a virtual reality environment, and a virtual tour are built to assist immersive interpretation of the Pictish sculpture. The application of these digitised objects serves as an opportunity for informal learning. These applications were evaluated during a drop-in session. Findings show that all participants enjoyed the immersive mode of learning with 89\% also showing a willingness to learn more about the topic.},
	language = {en},
	number = {8},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Pisani, Sharon and Miller, Alan and Hall, Mark},
	month = jun,
	year = {2024},
	keywords = {3D modelling, Artificial Intelligence, Digital Education and Educational Technology, Digital heritage, Digitisation, Heritage interpretation, Immersive learning, Virtual reality},
	pages = {10009--10042},
	file = {Full Text PDF:files/1139/Pisani et al. - 2024 - Digital modes of interpretation of Pictish sculpture.pdf:application/pdf},
}

@article{aurava_expectations_2022,
	title = {Expectations and realities: {Examining} adolescent students’ game jam experiences},
	volume = {27},
	issn = {1573-7608},
	shorttitle = {Expectations and realities},
	url = {https://doi.org/10.1007/s10639-021-10782-y},
	doi = {10.1007/s10639-021-10782-y},
	abstract = {This article describes the expectations and experiences of young (16 to 19 year old) digital game jam participants (N = 34) who attend Finnish general upper secondary schools. Game jams are a form of game creation: events where games are made in co-operation. They are widely used in game design education and in addition, when participated voluntarily, learning has been reported as an important motivation. The existing literature mostly concentrates on game jams for adults, and informal or non-formal learning. This article is adding to the literature by examining learning in formal education for adolescents. As part of our research, we have organised game jams in formal general education, and this article is based on the pre-event and post-event surveys of three game jam events. The article maps 1) the motivations to attend a school related game jam, 2) the expectations and apprehensions the would-be participants have, 3) what kind of learning game jams promote, and 4) how does attending a game jam affect participants’ attitudes and apprehensions regarding learning, STEAM and information technology, and their own skills. Our results indicate the creative side of digital game making, desire to learn new skills and make new friends to be the main motivations for participation, and the lack of confidence in technical skills to cause most anxiety before the jam event. The effects of attending a game jam are mostly positive, with the participants reporting learning experiences in several soft and technical skills and increased motivation to take part in creative and co-creative projects. A gendered result can be seen in the participants’ altered stance on technology and programming: girls and non-binary students report technology and programming being easier and more fun than they had thought before attending a game jam event, which is well in line with previous research on STEM/STEAM education and gender.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Aurava, Riikka and Meriläinen, Mikko},
	month = apr,
	year = {2022},
	keywords = {Adolescents, Artificial Intelligence, Co-creation, Creativity, Digital Education and Educational Technology, Game jams, Gender inclusivity, STEAM learning},
	pages = {4399--4426},
	file = {Full Text PDF:files/1140/Aurava e Meriläinen - 2022 - Expectations and realities Examining adolescent students’ game jam experiences.pdf:application/pdf},
}

@article{mossel_immersive_2021,
	title = {Immersive training of first responder squad leaders in untethered virtual reality},
	volume = {25},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-020-00487-x},
	doi = {10.1007/s10055-020-00487-x},
	abstract = {We present the VROnSite platform that supports immersive training of first responder units’ on-site squad leaders. Our training platform is fully immersive, entirely untethered to ease use and provides two means of navigation—abstract and natural walking—to simulate stress and exhaustion, two important factors for decision making. With the platform’s capabilities, we close a gap in prior art for first responder training. Our research is closely interlocked with stakeholders from multiple fire brigades to gather early feedback in an iterative design process. In this paper, we present the system’s design rationale, provide insight into the process of training scenario development and present results of a user study with 41 squad leaders from the firefighting domain. Virtual disaster environments with two different navigation types were evaluated using quantitative and qualitative measures. Participants considered our platform highly suitable for training of decision making in complex first responder scenarios and results show the importance of the provided navigation technologies in this context.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Mossel, Annette and Schoenauer, Christian and Froeschl, Mario and Peer, Andreas and Goellner, Johannes and Kaufmann, Hannes},
	month = sep,
	year = {2021},
	keywords = {Artificial Intelligence},
	pages = {745--759},
	file = {Full Text PDF:files/1161/Mossel et al. - 2021 - Immersive training of first responder squad leaders in untethered virtual reality.pdf:application/pdf},
}

@article{montagud_towards_2022,
	title = {Towards {socialVR}: evaluating a novel technology for watching videos together},
	volume = {26},
	issn = {1434-9957},
	shorttitle = {Towards {socialVR}},
	url = {https://doi.org/10.1007/s10055-022-00651-5},
	doi = {10.1007/s10055-022-00651-5},
	abstract = {Social VR enables people to interact over distance with others in real-time. It allows remote people, typically represented as avatars, to communicate and perform activities together in a shared virtual environment, extending the capabilities of traditional social platforms like Facebook and Netflix. This paper explores the benefits and drawbacks provided by a lightweight and low-cost Social VR platform (SocialVR), in which users are captured by several cameras and reconstructed in real-time. In particular, the paper contributes with (1) the design and evaluation of an experimental protocol for Social VR experiences; (2) the report of a production workflow for this new type of media experiences; and (3) the results of experiments with both end-users (N = 15 pairs) and professionals (N = 22 companies) to evaluate the potential of the SocialVR platform. Results from the questionnaires and semi-structured interviews show that end-users rated positively towards the experiences provided by the SocialVR platform, which enabled them to sense emotions and communicate effortlessly. End-users perceived the photo-realistic experience of SocialVR similar to face-to-face scenarios and appreciated this new creative medium. From a commercial perspective, professionals confirmed the potential of this communication medium and encourage further research for the adoption of the platform in the commercial landscape.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Montagud, Mario and Li, Jie and Cernigliaro, Gianluca and El Ali, Abdallah and Fernández, Sergi and Cesar, Pablo},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Evaluation Protocol, Presence, Social VR, Togetherness, Virtual Reality, Volumetric Media},
	pages = {1593--1613},
	file = {Full Text PDF:files/1162/Montagud et al. - 2022 - Towards socialVR evaluating a novel technology for watching videos together.pdf:application/pdf},
}

@article{bowling_machine_2006,
	title = {Machine learning and games},
	volume = {63},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-006-8919-x},
	doi = {10.1007/s10994-006-8919-x},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Mach Learn},
	author = {Bowling, Michael and Fürnkranz, Johannes and Graepel, Thore and Musick, Ron},
	month = jun,
	year = {2006},
	keywords = {Artificial Intelligence},
	pages = {211--215},
	file = {Full Text PDF:files/1163/Bowling et al. - 2006 - Machine learning and games.pdf:application/pdf},
}

@article{ruiz_bridging_2024,
	title = {Bridging realities: training visuo-haptic object recognition models for robots using {3D} virtual simulations},
	volume = {40},
	issn = {1432-2315},
	shorttitle = {Bridging realities},
	url = {https://doi.org/10.1007/s00371-024-03455-7},
	doi = {10.1007/s00371-024-03455-7},
	abstract = {This paper proposes an approach for training visuo-haptic object recognition models for robots using synthetic datasets generated by 3D virtual simulations. In robotics, where visual object recognition has witnessed considerable progress due to an abundance of image datasets, the scarcity of diverse haptic samples has resulted in a noticeable gap in research on machine learning incorporating the haptic sense. Our proposed methodology addresses this challenge by utilizing 3D virtual simulations to create realistic synthetic datasets, offering a scalable and cost-effective solution to integrate haptic and visual cues for object recognition seamlessly. Acknowledging the importance of multimodal perception, particularly in robotic applications, our research not only closes the existing gap but envisions a future where intelligent agents possess a holistic understanding of their environment derived from both visual and haptic senses. Our experiments show that synthetic datasets can be used for training object recognition in haptic and visual modes by incorporating noise, performing some preprocessing, data augmentation, or domain adaptation. This work contributes to the advancement of multimodal machine learning toward a more nuanced and comprehensive robotic perception.},
	language = {en},
	number = {7},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Ruiz, Conrado and de Jesús, Òscar and Serrano, Claudia and González, Alejandro and Nonell, Pau and Metaute, Arnau and Miralles, David},
	month = jul,
	year = {2024},
	keywords = {3D virtual simulations, Artificial Intelligence, Multimodal machine learning, Robotics, Synthetic datasets, Visuo-haptic object recognition},
	pages = {4661--4673},
	file = {Full Text PDF:files/1164/Ruiz et al. - 2024 - Bridging realities training visuo-haptic object recognition models for robots using 3D virtual simu.pdf:application/pdf},
}

@article{cavedoni_virtual_2022,
	title = {Virtual reality for the assessment and rehabilitation of neglect: where are we now? {A} 6-year review update},
	volume = {26},
	issn = {1434-9957},
	shorttitle = {Virtual reality for the assessment and rehabilitation of neglect},
	url = {https://doi.org/10.1007/s10055-022-00648-0},
	doi = {10.1007/s10055-022-00648-0},
	abstract = {Unilateral spatial neglect (USN) is a frequent repercussion of a cerebrovascular accident, typically a stroke. USN patients fail to orient their attention to the contralesional side to detect auditory, visual, and somatosensory stimuli, as well as to collect and purposely use this information. Traditional methods for USN assessment and rehabilitation include paper-and-pencil procedures, which address cognitive functions as isolated from other aspects of patients’ functioning within a real-life context. This might compromise the ecological validity of these procedures and limit their generalizability; moreover, USN evaluation and treatment currently lacks a gold standard. The field of technology has provided several promising tools that have been integrated within the clinical practice; over the years, a “first wave” has promoted computerized methods, which cannot provide an ecological and realistic environment and tasks. Thus, a “second wave” has fostered the implementation of virtual reality (VR) devices that, with different degrees of immersiveness, induce a sense of presence and allow patients to actively interact within the life-like setting. The present paper provides an updated, comprehensive picture of VR devices in the assessment and rehabilitation of USN, building on the review of Pedroli et al. (2015). The present paper analyzes the methodological and technological aspects of the studies selected, considering the issue of usability and ecological validity of virtual environments and tasks. Despite the technological advancement, the studies in this field lack methodological rigor as well as a proper evaluation of VR usability and should improve the ecological validity of VR-based assessment and rehabilitation of USN.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Cavedoni, S. and Cipresso, P. and Mancuso, V. and Bruni, F. and Pedroli, E.},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Assessment, Neglect, Rehabilitation, Technology, Virtual reality},
	pages = {1663--1704},
	file = {Full Text PDF:files/1165/Cavedoni et al. - 2022 - Virtual reality for the assessment and rehabilitation of neglect where are we now A 6-year review.pdf:application/pdf},
}

@article{laine_immersive_2024,
	title = {Immersive virtual reality for complex skills training: content analysis of experienced challenges},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Immersive virtual reality for complex skills training},
	url = {https://doi.org/10.1007/s10055-024-00955-8},
	doi = {10.1007/s10055-024-00955-8},
	abstract = {This study aimed to examine the challenges that adult participants experienced in immersive virtual reality (I-VR). Practitioners have indicated that some challenges persist from trainee to trainee and scholars have called for the design and development of virtual reality (VR) applications based on learning theories. Thus, we examined challenges immersed learners experienced during self-discovery of game mechanics and assembly task within an early-development I-VR program. We clarified the immersive learning phenomenon by studying the self-reported problem statements from 168 university students and staff. They used an HTC Vive Pro Eye device and a custom-built software. Through an iterative content analysis of post-survey and video-stimulated recall interviews, we retrieved 481 problem statements from the participants. As a result, we derived and detailed 89 challenges, 22 component features, 11 components, and 5 principal factors of immersive learning. The most cited components that the participants found challenging were the use of controllers and functions, reciprocal software interaction, spatial and navigational constraints, relevance realisation, and learner capabilities. Closer inspection of the quantified data revealed that the participants without digital gaming experience reported relatively more hardware-related problem statements. The findings regarding the constraints of immersive learning helped clarify the various actants involved in immersive learning. In this paper, we provide a design implication summary for VR application developers. Further research on theory-based development and design implications in various immersive training settings is needed.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Laine, Joakim and Rastas, Elisa and Seitamaa, Aino and Hakkarainen, Kai and Korhonen, Tiina},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Autonomous training, Challenge, Complex skill, Design implication, Immersive learning theory, Virtual reality},
	pages = {61},
	file = {Full Text PDF:files/1166/Laine et al. - 2024 - Immersive virtual reality for complex skills training content analysis of experienced challenges.pdf:application/pdf},
}

@article{jin_comparison_2022,
	title = {A comparison of natural user interface and graphical user interface for narrative in {HMD}-based augmented reality},
	volume = {81},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-021-11723-0},
	doi = {10.1007/s11042-021-11723-0},
	abstract = {Over the years, the various mediums available for storytelling have progressively expanded, from spoken to written word, then to film, and now to Virtual Reality (VR) and Augmented Reality (AR). In 2016, the cutting-edge Head-Mounted Display (HMD) AR Microsoft HoloLens was released. However, though it has been several years, the quality of the user experience with narration using HMD-based AR technology has been rarely discussed. The present study explored interactive narrative in HMD-based AR regarding different user interfaces and their influence on users’ presence, narrative engagement and reflection. Inspired by an existing exhibition at the National Holocaust Centre and Museum in the U.K., a HoloLens narrative application, entitled The AR Journey, was developed by the authors using two different interaction methods, Natural User Interface (NUI) and Graphical User Interface (GUI), which were used to perform an empirical study. As revealed from the results of the between-subject design experiment, NUI exhibited statistically significant advantages in creating presence for users without 3D Role Playing Game (RPG) experience, and GUI was superior in creating presence and increasing narrative engagement for users with 3D RPG experience. As indicated by the results of the interviews, the overall narrative experience in HMD-based AR was acceptable, and the branching narrative design was engaging. However, HoloLens hardware issues, as well as virtuality and reality mismatch, adversely affected user experience. Design guidelines were proposed according to the qualitative results.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Jin, Yunshui and Ma, Minhua and Zhu, Yongning},
	month = feb,
	year = {2022},
	keywords = {Artificial Intelligence, Augmented Reality (AR), Game experience, Head Mounted Display (HMD), HoloLens, Interactive narrative, Narrative engagement, Natural User Interface (NUI), Presence},
	pages = {5795--5826},
	file = {Full Text PDF:files/1167/Jin et al. - 2022 - A comparison of natural user interface and graphical user interface for narrative in HMD-based augme.pdf:application/pdf},
}

@article{montero_designing_2019,
	title = {Designing and implementing interactive and realistic augmented reality experiences},
	volume = {18},
	issn = {1615-5297},
	url = {https://doi.org/10.1007/s10209-017-0584-2},
	doi = {10.1007/s10209-017-0584-2},
	abstract = {In this paper, we propose an approach for supporting the design and implementation of interactive and realistic Augmented Reality (AR). Despite the advances in AR technology, most software applications still fail to support AR experiences where virtual objects appear as merged into the real setting. To alleviate this situation, we propose to combine the use of model-based AR techniques with the advantages of current game engines to develop AR scenes in which the virtual objects collide, are occluded, project shadows and, in general, are integrated into the augmented environment more realistically. To evaluate the feasibility of the proposed approach, we extended an existing game platform named GREP to enhance it with AR capacities. The realism of the AR experiences produced with the software was assessed in an event in which more than 100 people played two AR games simultaneously.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Univ Access Inf Soc},
	author = {Montero, Alvaro and Zarraonandia, Telmo and Diaz, Paloma and Aedo, Ignacio},
	month = mar,
	year = {2019},
	keywords = {Artificial Intelligence, Augmented reality, Authoring tools, Interactive environments, Mixed reality},
	pages = {49--61},
	file = {Full Text PDF:files/1168/Montero et al. - 2019 - Designing and implementing interactive and realistic augmented reality experiences.pdf:application/pdf},
}

@article{muctadir_current_2024,
	title = {Current trends in digital twin development, maintenance, and operation: an interview study},
	issn = {1619-1374},
	shorttitle = {Current trends in digital twin development, maintenance, and operation},
	url = {https://doi.org/10.1007/s10270-024-01167-z},
	doi = {10.1007/s10270-024-01167-z},
	abstract = {Digital twins (DTs) are often defined as a pairing of a physical entity and a corresponding virtual entity (VE), mimicking certain aspects of the former depending on the use-case. In recent years, this concept has facilitated numerous use-cases ranging from design to validation and predictive maintenance of large and small high-tech systems. Various heterogeneous cross-domain models are essential for such systems, and model-driven engineering plays a pivotal role in the design, development, and maintenance of these models. We believe models and model-driven engineering play a similarly crucial role in the context of a VE of a DT. Due to the rapidly growing popularity of DTs and their use in diverse domains and use-cases, the methodologies, tools, and practices for designing, developing, and maintaining the corresponding VEs differ vastly. To better understand these differences and similarities, we performed a semi-structured interview research with 19 professionals from industry and academia who are closely associated with different lifecycle stages of digital twins. In this paper, we present our analysis and findings from this study, which is based on seven research questions. In general, we identified an overall lack of uniformity in terms of the understanding of digital twins and used tools, techniques, and methodologies for the development and maintenance of the corresponding VEs. Furthermore, considering that digital twins are software intensive systems, we recognize a significant growth potential for adopting more software engineering practices, processes, and expertise in various stages of a digital twin’s lifecycle.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Softw Syst Model},
	author = {Muctadir, Hossain Muhammad and Manrique Negrin, David A. and Gunasekaran, Raghavendran and Cleophas, Loek and van den Brand, Mark and Haverkort, Boudewijn R.},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Digital twin, Digital twin trends, Semi-structured interview},
	file = {Full Text PDF:files/1169/Muctadir et al. - 2024 - Current trends in digital twin development, maintenance, and operation an interview study.pdf:application/pdf},
}

@article{sonkoly_edge_2024,
	title = {An {Edge} {Cloud} {Based} {Coordination} {Platform} for {Multi}-user {AR} {Applications}},
	volume = {32},
	issn = {1573-7705},
	url = {https://doi.org/10.1007/s10922-024-09809-9},
	doi = {10.1007/s10922-024-09809-9},
	abstract = {Augmented Reality (AR) applications can reshape our society enabling novel ways of interactions and immersive experiences in many fields. However, multi-user and collaborative AR applications pose several challenges. The expected user experience requires accurate position and orientation information for each device and precise synchronization of the respective coordinate systems in real-time. Unlike mobile phones or AR glasses running on battery with constrained resource capacity, cloud and edge platforms can provide the computing power for the core functions under the hood. In this paper, we propose a novel edge cloud based platform for multi-user AR applications realizing an essential coordination service among the users. The latency critical, computation intensive Simultaneous Localization And Mapping (SLAM) function is offloaded from the device to the edge cloud infrastructure. Our solution is built on open-source SLAM libraries and the Robot Operating System (ROS). Our contribution is threefold. First, we propose an extensible, edge cloud based AR architecture. Second, we develop a proof-of-concept prototype supporting multiple devices and building on an AI-based SLAM selection component. Third, a dedicated measurement methodology is described, including energy consumption aspects as well, and the overall performance of the system is evaluated via real experiments.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {J Netw Syst Manage},
	author = {Sonkoly, Balázs and Nagy, Bálint György and Dóka, János and Kecskés-Solymosi, Zsófia and Czentye, János and Formanek, Bence and Jocha, Dávid and Gerő, Balázs Péter},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Augmented reality, Edge computing, SLAM},
	pages = {40},
	file = {Full Text PDF:files/1170/Sonkoly et al. - 2024 - An Edge Cloud Based Coordination Platform for Multi-user AR Applications.pdf:application/pdf},
}

@article{liu_study_2023,
	title = {A {Study} of {Using} {Synthetic} {Data} for {Effective} {Association} {Knowledge} {Learning}},
	volume = {20},
	issn = {2731-5398},
	url = {https://doi.org/10.1007/s11633-022-1380-x},
	doi = {10.1007/s11633-022-1380-x},
	abstract = {Association, aiming to link bounding boxes of the same identity in a video sequence, is a central component in multi-object tracking (MOT). To train association modules, e.g., parametric networks, real video data are usually used. However, annotating person tracks in consecutive video frames is expensive, and such real data, due to its inflexibility, offer us limited opportunities to evaluate the system performance w.r.t. changing tracking scenarios. In this paper, we study whether 3D synthetic data can replace real-world videos for association training. Specifically, we introduce a large-scale synthetic data engine named MOTX, where the motion characteristics of cameras and objects are manually configured to be similar to those of real-world datasets. We show that, compared with real data, association knowledge obtained from synthetic data can achieve very similar performance on real-world test sets without domain adaption techniques. Our intriguing observation is credited to two factors. First and foremost, 3D engines can well simulate motion factors such as camera movement, camera view, and object movement so that the simulated videos can provide association modules with effective motion features. Second, the experimental results show that the appearance domain gap hardly harms the learning of association knowledge. In addition, the strong customization ability of MOTX allows us to quantitatively assess the impact of motion factors on MOT, which brings new insights to the community.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Mach. Intell. Res.},
	author = {Liu, Yuchi and Wang, Zhongdao and Zhou, Xiangxin and Zheng, Liang},
	month = apr,
	year = {2023},
	keywords = {Artificial Intelligence, association knowledge learning, data association, motion simulation, Multi-object tracking (MOT), synthetic data},
	pages = {194--206},
	file = {Full Text PDF:files/1171/Liu et al. - 2023 - A Study of Using Synthetic Data for Effective Association Knowledge Learning.pdf:application/pdf},
}

@article{hajirasouli_augmented_2022,
	title = {Augmented reality in architecture and construction education: state of the field and opportunities},
	volume = {19},
	issn = {2365-9440},
	shorttitle = {Augmented reality in architecture and construction education},
	url = {https://doi.org/10.1186/s41239-022-00343-9},
	doi = {10.1186/s41239-022-00343-9},
	abstract = {Over the past decade, the architecture and construction (AC) industries have been evolving from traditional practices into more current, interdisciplinary and technology integrated methods. Complex and intricate digital technologies and mobile computing such as simulation, computational design and immersive technologies, have been exploited for different purposes such as reducing cost and time, improving design and enhancing overall project efficiency. Immersive technologies and augmented reality (AR), in particular, have proven to be extremely beneficial in this field. However, the application and usage of these technologies and devices in higher education teaching and learning environments are yet to be fully explored and still scarce. More importantly, there is still a significant gap in developing pedagogies and teaching methods that embrace the usage of such technologies in the AC curricula. This study, therefore, aims to critically analyse the current state-of-the-art and present the developed and improved AR approaches in teaching and learning methods of AC, addressing the identified gap in the extant literature, while developing transformational frameworks to link the gaps to their future research agenda. The conducted analysis incorporates the critical role of the AR implications on the AC students’ skillsets, pedagogical philosophies in AC curricula, techno-educational aspects and content domains in the design and implementation of AR environments for AC learning. The outcomes of this comprehensive study prepare trainers, instructors, and the future generation of AC workers for the rapid advancements in this industry.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Int J Educ Technol High Educ},
	author = {Hajirasouli, Aso and Banihashemi, Saeed},
	month = jul,
	year = {2022},
	keywords = {Artificial Intelligence, Augmented reality, Construction pedagogy, Design and architecture pedagogy, Digital Education and Educational Technology, Digital pedagogy, Digital technology, Emerging technology, Immersive pedagogy},
	pages = {39},
	file = {Full Text PDF:files/1172/Hajirasouli e Banihashemi - 2022 - Augmented reality in architecture and construction education state of the field and opportunities.pdf:application/pdf},
}

@article{kloiber_immersive_2020,
	title = {Immersive analysis of user motion in {VR} applications},
	volume = {36},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-020-01942-1},
	doi = {10.1007/s00371-020-01942-1},
	abstract = {With the rise of virtual reality experiences for applications in entertainment, industry, science and medicine, the evaluation of human motion in immersive environments is becoming more important. By analysing the motion of virtual reality users, design choices and training progress in the virtual environment can be understood and improved. Since the motion is captured in a virtual environment, performing the analysis in the same environment provides a valuable context and guidance for the analysis. We have created a visual analysis system that is designed for immersive visualisation and exploration of human motion data. By combining suitable data mining algorithms with immersive visualisation techniques, we facilitate the reasoning and understanding of the underlying motion. We apply and evaluate this novel approach on a relevant VR application domain to identify and interpret motion patterns in a meaningful way.},
	language = {en},
	number = {10},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Kloiber, Simon and Settgast, Volker and Schinko, Christoph and Weinzerl, Martin and Fritz, Johannes and Schreck, Tobias and Preiner, Reinhold},
	month = oct,
	year = {2020},
	keywords = {Artificial Intelligence, Immersive analytics, Movement analysis, Trajectory visualisation, Virtual reality, Virtual training evaluation},
	pages = {1937--1949},
	file = {Full Text PDF:files/1173/Kloiber et al. - 2020 - Immersive analysis of user motion in VR applications.pdf:application/pdf},
}

@article{biagiola_two_2024,
	title = {Two is better than one: digital siblings to improve autonomous driving testing},
	volume = {29},
	issn = {1573-7616},
	shorttitle = {Two is better than one},
	url = {https://doi.org/10.1007/s10664-024-10458-4},
	doi = {10.1007/s10664-024-10458-4},
	abstract = {Simulation-based testing represents an important step to ensure the reliability of autonomous driving software. In practice, when companies rely on third-party general-purpose simulators, either for in-house or outsourced testing, the generalizability of testing results to real autonomous vehicles is at stake. In this paper, we enhance simulation-based testing by introducing the notion of digital siblings—a multi-simulator approach that tests a given autonomous vehicle on multiple general-purpose simulators built with different technologies, that operate collectively as an ensemble in the testing process. We exemplify our approach on a case study focused on testing the lane-keeping component of an autonomous vehicle. We use two open-source simulators as digital siblings, and we empirically compare such a multi-simulator approach against a digital twin of a physical scaled autonomous vehicle on a large set of test cases. Our approach requires generating and running test cases for each individual simulator, in the form of sequences of road points. Then, test cases are migrated between simulators, using feature maps to characterize the exercised driving conditions. Finally, the joint predicted failure probability is computed, and a failure is reported only in cases of agreement among the siblings. Our empirical evaluation shows that the ensemble failure predictor by the digital siblings is superior to each individual simulator at predicting the failures of the digital twin. We discuss the findings of our case study and detail how our approach can help researchers interested in automated testing of autonomous driving software.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Empir Software Eng},
	author = {Biagiola, Matteo and Stocco, Andrea and Riccio, Vincenzo and Tonella, Paolo},
	month = may,
	year = {2024},
	keywords = {AI testing, Artificial Intelligence, Autonomous vehicles., Deep neural networks, Digital twins, Self-driving cars, Simulation-based testing},
	pages = {72},
	file = {Full Text PDF:files/1174/Biagiola et al. - 2024 - Two is better than one digital siblings to improve autonomous driving testing.pdf:application/pdf},
}

@article{kaur_deep_2023,
	title = {Deep learning: survey of environmental and camera impacts on internet of things images},
	volume = {56},
	issn = {1573-7462},
	shorttitle = {Deep learning},
	url = {https://doi.org/10.1007/s10462-023-10405-7},
	doi = {10.1007/s10462-023-10405-7},
	abstract = {Internet of Things (IoT) images are captivating growing attention because of their wide range of applications which requires visual analysis to drive automation. However, IoT images are predominantly captured from outdoor environments and thus are inherently impacted by the camera and environmental parameters which can adversely affect corresponding applications. Deep Learning (DL) has been widely adopted in the field of image processing and computer vision and can reduce the impact of these parameters on IoT images. Albeit, there are many DL-based techniques available in the current literature for analyzing and reducing the environmental and camera impacts on IoT images. However, to the best of our knowledge, no survey paper presents state-of-the-art DL-based approaches for this purpose. Motivated by this, for the first time, we present a Systematic Literature Review (SLR) of existing DL techniques available for analyzing and reducing environmental and camera lens impacts on IoT images. As part of this SLR, firstly, we reiterate and highlight the significance of IoT images in their respective applications. Secondly, we describe the DL techniques employed for assessing the environmental and camera lens distortion impacts on IoT images. Thirdly, we illustrate how DL can be effective in reducing the impact of environmental and camera lens distortion in IoT images. Finally, along with the critical reflection on the advantages and limitations of the techniques, we also present ways to address the research challenges of existing techniques and identify some further researches to advance the relevant research areas.},
	language = {en},
	number = {9},
	urldate = {2024-09-05},
	journal = {Artif Intell Rev},
	author = {Kaur, Roopdeep and Karmakar, Gour and Xia, Feng and Imran, Muhammad},
	month = sep,
	year = {2023},
	keywords = {Artificial Intelligence, Camera lens distortions, Deep learning, Environmental impacts, Image quality, Internet of things},
	pages = {9605--9638},
	file = {Full Text PDF:files/1175/Kaur et al. - 2023 - Deep learning survey of environmental and camera impacts on internet of things images.pdf:application/pdf},
}

@article{wheeler_prop-oriented_2024,
	title = {Prop-oriented world rotation: enabling passive haptic feedback by aligning real and virtual objects in virtual reality},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {Prop-oriented world rotation},
	url = {https://doi.org/10.1007/s11042-024-18200-4},
	doi = {10.1007/s11042-024-18200-4},
	abstract = {Passive haptics have long been used to enhance the user’s experience in virtual reality (VR). However, creating props to be used in a virtual environment can be a complicated and lengthy process. Current research looks to create passive haptic props based on the layout of, or objects in, the user’s real environment. However, we identify three key limitations of current research. Firstly, procedural generation introduces many unknown variables into the design process, which complicates applying such techniques to scenarios requiring knowledge of the virtual environment’s layout ahead of time. Furthermore, such techniques limit the size and dimensions of the virtual space to that of the real space. Lastly, current research necessitates pre-scanning or real-time scanning of the user’s real environment, often requiring specialist equipment and expertise, thus limiting its generalisability. This research proposes Prop Oriented World Rotation, a technique that attempts to answer the aforementioned limitations and simplify the process of adding haptic feedback to VR applications. We implemented this technique in a demonstration game and give an overview of the steps taken to apply the technique in a real context. We analysed the demonstration system’s performance and conducted an initial user evaluation in three different physical environments. While our stress test of the system’s performance highlights the necessity for certain optimisations in complex environments, our initial user feedback suggests that users experienced a stronger sense of presence and feelings of safety in our passive haptics-enhanced environment. Hence, we conclude that our proposal has the potential to enhance experiences in VR with haptic feedback.},
	language = {en},
	number = {25},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Wheeler, Steven G. and Hoermann, Simon and Lindeman, Robert W. and Ghinea, George and Covaci, Alexandra},
	month = jul,
	year = {2024},
	keywords = {Artificial Intelligence, Human-computer interaction, Passive haptics, Presence, Virtual reality},
	pages = {66013--66032},
	file = {Full Text PDF:files/1176/Wheeler et al. - 2024 - Prop-oriented world rotation enabling passive haptic feedback by aligning real and virtual objects.pdf:application/pdf},
}

@article{li_gpu_2015,
	title = {{GPU} based real-time simulation of massive falling leaves},
	volume = {1},
	issn = {2096-0662},
	url = {https://doi.org/10.1007/s41095-015-0025-1},
	doi = {10.1007/s41095-015-0025-1},
	abstract = {As an important autumn feature, scenes with large numbers of falling leaves are common in movies and games. However, it is a challenge for computer graphics to simulate such scenes in an authentic and efficient manner. This paper proposes a GPU based approach for simulating the falling motion of many leaves in real time. Firstly, we use a motionsynthesis based method to analyze the falling motion of the leaves, which enables us to describe complex falling trajectories using low-dimensional features. Secondly, we transmit a primitive-motion trajectory dataset together with the low-dimensional features of the falling leaves to video memory, allowing us to execute the appropriate calculations on the GPU.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Comp. Visual Media},
	author = {Li, Chengyang and Qian, Jingye and Tong, Ruofeng and Chang, Jian and Zhang, Jianjun},
	month = dec,
	year = {2015},
	keywords = {Artificial Intelligence, falling leaves, GPU acceleration, real-time simulation},
	pages = {351--358},
	file = {Full Text PDF:files/1177/Li et al. - 2015 - GPU based real-time simulation of massive falling leaves.pdf:application/pdf},
}

@article{munoz-pandiella_digital_2024,
	title = {Digital {3D} models for medieval heritage: diachronic analysis and documentation of its architecture and paintings},
	issn = {1617-4917},
	shorttitle = {Digital {3D} models for medieval heritage},
	url = {https://doi.org/10.1007/s00779-024-01816-6},
	doi = {10.1007/s00779-024-01816-6},
	abstract = {In this paper, we discuss the requirements and technical challenges within the EHEM project, Enhancement of Heritage Experiences: The Middle Ages, an ongoing research program for the acquisition, analysis, documentation, interpretation, digital restoration, and communication of medieval artistic heritage. The project involves multidisciplinary teams comprising art historians and visual computing experts. Despite the vast literature on digital 3D models in support of Cultural Heritage, the field is so rich and diverse that specific projects often imply distinct, unique requirements which often challenge the computational technologies and suggest new research opportunities. As good representatives of such diversity, we describe the three monuments that serve as test cases for the project, all of them with a rich history of architecture and paintings. We discuss the art historians’ view of how digital models can support their research, the expertise and technological solutions adopted so far, as well as the technical challenges in multiple areas spanning geometry and appearance acquisition, color analysis and digital restitution, as well as the representation of the profound transformations due to the alterations suffered over the centuries.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Pers Ubiquit Comput},
	author = {Munoz-Pandiella, Imanol and Bosch, Carles and Guardia, Milagros and Cayuela, Begoña and Pogliani, Paola and Bordi, Giulia and Paschali, Maria and Andujar, Carlos and Charalambous, Panayiotis},
	month = jun,
	year = {2024},
	keywords = {3D reconstruction, Architecture, Artificial Intelligence, Color analysis and restoration, Laser scanning, Medieval artistic heritage, Mural paintings},
	file = {Full Text PDF:files/1178/Munoz-Pandiella et al. - 2024 - Digital 3D models for medieval heritage diachronic analysis and documentation of its architecture a.pdf:application/pdf},
}

@article{su_comprehensive_2021,
	title = {Comprehensive review and classification of game analytics},
	volume = {15},
	issn = {1863-2394},
	url = {https://doi.org/10.1007/s11761-020-00303-z},
	doi = {10.1007/s11761-020-00303-z},
	abstract = {As a business model, the essence of games is to provide a service to satisfy the player experience. From a business perspective, development in the game industry has led to the application of Business Intelligence (BI) becoming more and more extensive. However, related research lacks systematic examination and precise classification. This paper provides a comprehensive literature review of BI used in the game industry, focusing primarily on game analytics. This research mainly studies and discusses five aspects. First, we explore game analytics aspects in the available literature based on the traditional game value chain. Second, we find out the main purposes of using analytics in the game industry. Third, we present the problems or challenges in the game area, which can be addressed by using game analytics. Fourth, we also list different algorithms that have been used in game analytics for prediction. Finally, we summarize the research areas that have already been covered in literature but need further development. Based on the categories established after the mapping and the review findings, we also discuss the limitations of game analytics and propose potential research points for future research.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {SOCA},
	author = {Su, Yanhui and Backlund, Per and Engström, Henrik},
	month = jun,
	year = {2021},
	keywords = {Artificial Intelligence, Business intelligence, Distribution channels, Game analytics, Game metrics, Game publishing, Prediction},
	pages = {141--156},
	file = {Full Text PDF:files/1179/Su et al. - 2021 - Comprehensive review and classification of game analytics.pdf:application/pdf},
}

@article{hind_using_2024,
	title = {Using a {NEAT} approach with curriculums for dynamic content generation in video games},
	issn = {1617-4917},
	url = {https://doi.org/10.1007/s00779-024-01801-z},
	doi = {10.1007/s00779-024-01801-z},
	abstract = {This paper presents a novel exploration of the use of an evolving neural network approach to generate dynamic content for video games, specifically for a tower defence game. The objective is to employ the NeuroEvolution of Augmenting Topologies (NEAT) technique to train a NEAT neural network as a wave manager to generate enemy waves that challenge the player’s defences. The approach is extended to incorporate NEAT-generated curriculums for tower deployments to gradually increase the difficulty for the generated enemy waves, allowing the neural network to learn incrementally. The approach dynamically adapts to changes in the player’s skill level, providing a more personalised and engaging gaming experience. The quality of the machine-generated waves is evaluated through a blind A/B test with the Games Experience Questionnaire (GEQ), and results are compared with manually designed human waves. The study finds no discernible difference in the reported player experience between AI and human-designed waves. The approach can significantly reduce the time and resources required to design game content while maintaining the quality of the player experience. The approach has the potential to be applied to a range of video game genres and within the design and development process, providing a more personalised and engaging gaming experience for players.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Pers Ubiquit Comput},
	author = {Hind, Daniel and Harvey, Carlo},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Dynamic content generation, Game experience, NEAT},
	file = {Full Text PDF:files/1180/Hind e Harvey - 2024 - Using a NEAT approach with curriculums for dynamic content generation in video games.pdf:application/pdf},
}

@article{berg_marklund_what_2019,
	title = {What {Empirically} {Based} {Research} {Tells} {Us} {About} {Game} {Development}},
	volume = {8},
	issn = {2052-773X},
	url = {https://doi.org/10.1007/s40869-019-00085-1},
	doi = {10.1007/s40869-019-00085-1},
	abstract = {This paper reviews empirically grounded research on practices in game development with the intent to give a comprehensive overview of contemporary development practices used in the video game industry. While there are many intangible elements that inform game development processes, this review specifically covers the more immediate practical challenges. The review covers a total of 48 papers published between 2006 and 2016, which were all subjected to thematic analysis by three reviewers. The results of the review show that an almost universal characteristic of game development is that it is almost impossible to accurately plan a development project in detail, largely due to the soft requirements inherent in game production which emerge mid-process during development projects, during when testing is coupled with continuous ideation and refinement. Practicing game developers have created their own frameworks that accommodate for this lack of planning. They include flat hierarchies, democratic decision-making, creative autonomy, and informal communication, which are designed to create an environment that maintains creativity and openness to product changes long into the production process. These frameworks vary significantly between studios and often between individual projects. This review also shows that the term ‘Agile’, while often used by both researchers and developers to characterize the process of game development, is not an apt descriptor of how game developers actually work. Agile is used as shorthand for unstructured and flexible development, rather than serving as a descriptor of a definable or unified work method. Finally, as companies develop more complicated hierarchies of stakeholders and staff, the desired flexibility and autonomy of game development becomes increasingly complicated to maintain, and often necessitates more formalized management processes and company structures. In these cases, inherent tensions of game development become more pronounced, and continuous creativity is hard to maintain due to a growing need to formalize processes.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Comput Game J},
	author = {Berg Marklund, Björn and Engström, Henrik and Hellkvist, Marcus and Backlund, Per},
	month = dec,
	year = {2019},
	keywords = {Artificial Intelligence, Development processes, Empirics, Game development, Game industry, Literature review, Management},
	pages = {179--198},
	file = {Full Text PDF:files/1201/Berg Marklund et al. - 2019 - What Empirically Based Research Tells Us About Game Development.pdf:application/pdf},
}

@article{si_virtual_2015,
	title = {A {Virtual} {Space} for {Children} to {Meet} and {Practice} {Chinese}},
	volume = {25},
	issn = {1560-4306},
	url = {https://doi.org/10.1007/s40593-014-0035-7},
	doi = {10.1007/s40593-014-0035-7},
	abstract = {Second language acquisition after the students have learned their first language is a unique process. One major difference between learning a foreign language and one’s mother tongue is that second language learning is often facilitated with digital media, and in particular, through interacting with computers. This project is aimed at leveraging computer game technologies and Microsoft Kinect camera to create virtual learning environments suitable for children to practice their language and culture skills. We present a unique virtual environment that contextualizes the practice and engages the learners with narratives, encourages group work and leverages the power of embodied cognition in language learning. Our system has been deployed in an afterschool program for children from 6 to 8 years old. We report our evaluation results and reflections on the deployment process, followed by discussion and future work.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Int J Artif Intell Educ},
	author = {Si, Mei},
	month = jun,
	year = {2015},
	keywords = {Artificial Intelligence, Digital Education and Educational Technology, Embodied cognition, Kinect, Language and culture learning, Virtual environment},
	pages = {271--290},
	file = {Full Text PDF:files/1202/Si - 2015 - A Virtual Space for Children to Meet and Practice Chinese.pdf:application/pdf},
}

@article{catal_evaluation_2020,
	title = {Evaluation of augmented reality technology for the design of an evacuation training game},
	volume = {24},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-019-00410-z},
	doi = {10.1007/s10055-019-00410-z},
	abstract = {Building evacuation training systems and training employees in an organization have a vital role in emergency cases in which people need to know what to do exactly. In every building, procedures, rules, and actions are attractively shown on the walls, but most of the people living in that building are not aware of these procedures and do not have any experience what to do in these dangerous situations. In order to be able to apply these procedures properly in an emergency situation, community members should be trained with the state-of-the-art equipment and technologies, but to do so, up-front investment and development of such a system are necessary. In this study, augmented reality (AR) technology was applied to realize a game-based evacuation training system that implements gamification practices. The architectural plans of a university were used to model the floors and the relevant environment. Employees are trained to learn how to reach the nearest exit location in the event of a fire or earthquake, and also, the system provides the shortest path for the evacuation. In addition to these features, our training game has educational animations about the fire, chemical attack, and earthquake events. A mobile application was implemented to train employees working in the building and inform them to know how to escape in an emergency situation. The technology acceptance model and the related questionnaire form were applied, and the response of 36 participants was analyzed. It was demonstrated that AR and relevant tools provide a flexible environment to develop evacuation systems in a university, our mobile application enabled participants to be trained in a realistic environment, and trainees were highly satisfied with the system. Educational animations were also another benefit for the trainees.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Catal, Cagatay and Akbulut, Akhan and Tunali, Berkay and Ulug, Erol and Ozturk, Eren},
	month = sep,
	year = {2020},
	keywords = {Animation, ARKit framework, Artificial Intelligence, Augmented reality, Evacuation training system, Game engine, Software, Training, Unity3D},
	pages = {359--368},
	file = {Full Text PDF:files/1203/Catal et al. - 2020 - Evaluation of augmented reality technology for the design of an evacuation training game.pdf:application/pdf},
}

@article{valmorisco_enabling_2024,
	title = {Enabling personalized {VR} experiences: a framework for real-time adaptation and recommendations in {VR} environments},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Enabling personalized {VR} experiences},
	url = {https://doi.org/10.1007/s10055-024-01020-0},
	doi = {10.1007/s10055-024-01020-0},
	abstract = {The personalization of user experiences through recommendation systems has been extensively explored in Internet applications, but this has yet to be fully addressed in Virtual Reality (VR) environments. The complexity of managing geometric 3D data, computational load, and natural interactions poses significant challenges in real-time adaptation in these immersive experiences. However, tailoring VR environments to individual user needs and interests holds promise for enhancing user experiences. In this paper, we present Virtual Reality Environment Adaptation through Recommendations (VR-EAR), a framework designed to address this challenge. VR-EAR employs customizable object metadata and a hybrid recommendation system modeling implicit user feedback in VR environments. We utilize VR optimization techniques to ensure efficient performance. To evaluate our framework, we designed a virtual store where product locations dynamically adjust based on user interactions. Our results demonstrate the effectiveness of VR-EAR in adapting and personalizing VR environments in real time. domains.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Valmorisco, Sergio and Raya, Laura and Sanchez, Alberto},
	month = jun,
	year = {2024},
	keywords = {Artificial Intelligence, E-commerce, Recommendation systems, Virtual reality, Virtual shopping},
	pages = {128},
	file = {Full Text PDF:files/1204/Valmorisco et al. - 2024 - Enabling personalized VR experiences a framework for real-time adaptation and recommendations in VR.pdf:application/pdf},
}

@article{shooter_sydog-video_2024,
	title = {{SyDog}-{Video}: {A} {Synthetic} {Dog} {Video} {Dataset} for {Temporal} {Pose} {Estimation}},
	volume = {132},
	issn = {1573-1405},
	shorttitle = {{SyDog}-{Video}},
	url = {https://doi.org/10.1007/s11263-023-01946-z},
	doi = {10.1007/s11263-023-01946-z},
	abstract = {We aim to estimate the pose of dogs from videos using a temporal deep learning model as this can result in more accurate pose predictions when temporary occlusions or substantial movements occur. Generally, deep learning models require a lot of data to perform well. To our knowledge, public pose datasets containing videos of dogs are non existent. To solve this problem, and avoid manually labelling videos as it can take a lot of time, we generated a synthetic dataset containing 500 videos of dogs performing different actions using Unity3D. Diversity is achieved by randomising parameters such as lighting, backgrounds, camera parameters and the dog’s appearance and pose. We evaluate the quality of our synthetic dataset by assessing the model’s capacity to generalise to real data. Usually, networks trained on synthetic data perform poorly when evaluated on real data, this is due to the domain gap. As there was still a domain gap after improving the quality of the synthetic dataset and inserting diversity, we bridged the domain gap by applying 2 different methods: fine-tuning and using a mixed dataset to train the network. Additionally, we compare the model pre-trained on synthetic data with models pre-trained on a real-world animal pose datasets. We demonstrate that using the synthetic dataset is beneficial for training models with (small) real-world datasets. Furthermore, we show that pre-training the model with the synthetic dataset is the go to choice rather than pre-training on real-world datasets for solving the pose estimation task from videos of dogs.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Int J Comput Vis},
	author = {Shooter, Moira and Malleson, Charles and Hilton, Adrian},
	month = jun,
	year = {2024},
	keywords = {Animal pose estimation, Artificial Intelligence, Deep learning, Domain adaptation, Synthetic data, Temporal},
	pages = {1986--2002},
	file = {Full Text PDF:files/1205/Shooter et al. - 2024 - SyDog-Video A Synthetic Dog Video Dataset for Temporal Pose Estimation.pdf:application/pdf},
}

@article{filippas_elementium_2023,
	title = {Elementium: design and pilot evaluation of a serious game for familiarizing players with basic chemistry},
	volume = {28},
	issn = {1573-7608},
	shorttitle = {Elementium},
	url = {https://doi.org/10.1007/s10639-023-11791-9},
	doi = {10.1007/s10639-023-11791-9},
	abstract = {Serious games (SGs) about Chemistry have the potential to cope with challenges, such as students’ low performance and lack of motivation for the subject. However, the majority of existing SGs for Chemistry have the form of educational applications infused with some elements of entertaining games. The aim of the study presented was to design and evaluate a new SG with rich game mechanics for Chemistry. The game is called Elementium and revolves around basic topics of Chemistry, such as chemical elements and compound terminology, creation and everyday usage of such elements. The main goal of the game is to familiarize junior high school students with the aforementioned subjects. The design of Elementium was carried out implementing the dimensions described in the Four-Dimensional framework, as proposed by de Freitas and Jarvis in 2006. After the development process, Elementium was evaluated by people in the field of education that are currently teaching or have taught Chemistry in the past. The participants play-tested the game at leisure in their homes and evaluated it based on the key criteria for SGs design proposed by Sanchez in 2011, as well as other quality indicators established in the literature. Elementium was positively evaluated by Chemistry teachers in terms of its acceptance, usability, didactic utility, and game environment. The positive results concluded from this evaluation show that Elementium is fulfilling its main purpose and can be used as a supplementary tool in the teaching process. However, its true didactical effectiveness has to be confirmed through a study with high school students.},
	language = {en},
	number = {11},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Filippas, Alexandros and Xinogalos, Stelios},
	month = nov,
	year = {2023},
	keywords = {Artificial Intelligence, Chemistry, Digital Education and Educational Technology, Game design, Game evaluation, Serious games, Survey},
	pages = {14721--14746},
	file = {Full Text PDF:files/1206/Filippas e Xinogalos - 2023 - Elementium design and pilot evaluation of a serious game for familiarizing players with basic chemi.pdf:application/pdf},
}

@article{kim_exploring_2024,
	title = {Exploring students’ perspectives on {Generative} {AI}-assisted academic writing},
	issn = {1573-7608},
	url = {https://doi.org/10.1007/s10639-024-12878-7},
	doi = {10.1007/s10639-024-12878-7},
	abstract = {The rapid development of generative artificial intelligence (GenAI), including large language models (LLM), has merged to support students in their academic writing process. Keeping pace with the technical and educational landscape requires careful consideration of the opportunities and challenges that GenAI-assisted systems create within education. This serves as a useful and necessary starting point for fully leveraging its potential for learning and teaching. Hence, it is crucial to gather insights from diverse perspectives and use cases from actual users, particularly the unique voices and needs of student-users. Therefore, this study explored and examined students' perceptions and experiences about  GenAI-assisted academic writing by conducting in-depth interviews with 20 Chinese students in higher education after completing academic writing tasks using a ChatGPT4-embedded writing system developed by the research team. The study found that students expected AI to serve multiple roles, including multi-tasking writing assistant, virtual tutor, and digital peer to support multifaceted writing processes and performance. Students perceived that GenAI-assisted writing could benefit them in three areas including the writing process, performance, and their affective domain. Meanwhile, they also identified AI-related, student-related, and task-related challenges that were experienced during the GenAI-assisted writing activity. These findings contribute to a more nuanced understanding of GenAI's impact on academic writing that is inclusive of student perspectives, offering implications for educational AI design and instructional design.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Kim, Jinhee and Yu, Seongryeong and Detrick, Rita and Li, Na},
	month = jul,
	year = {2024},
	keywords = {academic writing, AI in education, AI literacy, AI-assisted writing, Artificial Intelligence, Digital Education and Educational Technology, generative Artificial Intelligence, student-AI Interaction},
	file = {Full Text PDF:files/1207/Kim et al. - 2024 - Exploring students’ perspectives on Generative AI-assisted academic writing.pdf:application/pdf},
}

@article{sielis_archreco_2017,
	title = {{ArchReco}: a software tool to assist software design based on context aware recommendations of design patterns},
	volume = {5},
	issn = {2195-1721},
	shorttitle = {{ArchReco}},
	url = {https://doi.org/10.1186/s40411-017-0036-y},
	doi = {10.1186/s40411-017-0036-y},
	abstract = {This work describes the design, development and evaluation of a software Prototype, named ArchReco, an educational tool that employs two types of Context-aware Recommendations of Design Patterns, to support users (CS students or professionals) who want to improve their design skills when it comes to training for High Level Software models. The tool’s underlying algorithms take advantage of Semantic Web technologies, and the usage of Content based analysis for the computation of non-personalized recommendations for Design Patterns. The recommendations’ objective is to support users in functions such as finding the most suitable Design Pattern to use according to the working context, learn the meaning, objectives and usages of each Design Pattern. The current work presents the Semantic Modeling of the Software Design process through the definition of the context that defines the Software Design process and in particular the representation of the Design Patterns as Ontology model, the implemented Context Aware Recommendation Algorithms and the evaluation results extracted from a user based testing for the ArchReco prototype.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {J Softw Eng Res Dev},
	author = {Sielis, George A. and Tzanavari, Aimilia and Papadopoulos, George A.},
	month = apr,
	year = {2017},
	keywords = {Artificial Intelligence, Context awareness, Design patterns learning, Recommendation algorithms, Semantic web, Software design, Software engineering educational tools},
	pages = {2},
	file = {Full Text PDF:files/1208/Sielis et al. - 2017 - ArchReco a software tool to assist software design based on context aware recommendations of design.pdf:application/pdf},
}

@article{kolivand_relishaft_2019,
	title = {{ReLiShaft}: realistic real-time light shaft generation taking sky illumination into account},
	volume = {78},
	issn = {1573-7721},
	shorttitle = {{ReLiShaft}},
	url = {https://doi.org/10.1007/s11042-018-6296-7},
	doi = {10.1007/s11042-018-6296-7},
	abstract = {Rendering atmospheric phenomena is known to have its basis in the fields of atmospheric optics and meteorology and is increasingly used in games and movies. Although many researchers have focused on generating and enhancing realistic light shafts, there is still room for improvement in terms of both qualification and quantification. In this paper, a new technique, called ReLiShaft, is presented to generate realistic light shafts for outdoor rendering. In the first step, a realistic light shaft with respect to the sun position and sky colour in any specific location, date and time is constructed in real-time. Then, Hemicube visibility-test radiosity is employed to reveal the effect of a generated sky colour on environments. Two different methods are considered for indoor and outdoor rendering, ray marching based on epipolar sampling for indoor environments, and filtering on regular epipolar of z-partitioning for outdoor environments. Shadow maps and shadow volumes are integrated to consider the computational costs. Through this technique, the light shaft colour is adjusted according to the sky colour in any specific location, date and time. The results show different light shaft colours in different times of day in real-time.},
	language = {en},
	number = {5},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Kolivand, Hoshang and Sunar, Mohd Shahrizal and Saba, Tanzila and Ali, Hatem},
	month = mar,
	year = {2019},
	keywords = {Light shaft, Outdoor rendering, Real-time light shaft generation, Sky illumination, Virtual reality},
	pages = {6073--6092},
	file = {Full Text PDF:files/1209/Kolivand et al. - 2019 - ReLiShaft realistic real-time light shaft generation taking sky illumination into account.pdf:application/pdf},
}

@article{wang_geometry_2024,
	title = {The geometry of the vergence-accommodation conflict in mixed reality systems},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-00991-4},
	doi = {10.1007/s10055-024-00991-4},
	abstract = {Mixed reality technologies, such as virtual (VR) and augmented (AR) reality, present promising opportunities to advance education and professional training due to their adaptability to diverse contexts. Distortions in the perceived distance in such mediated conditions, however, are well documented and have imposed nontrivial challenges that complicate and limit transferring task performance in a virtual setting to the unmediated reality (UR). One potential source of the distance distortion is the vergence-accommodation conflict—the discrepancy between the depth specified by the eyes’ accommodative state and the angle at which the eyes converge to fixate on a target. The present study involved the use of a manual pointing task in UR, VR, and AR to quantify the magnitude of the potential depth distortion in each modality. Conceptualizing the effect of vergence-accommodation offset as a constant offset to the vergence angle, a model was developed based on the stereoscopic viewing geometry. Different versions of the model were used to fit and predict the behavioral data for all modalities. Results confirmed the validity of the conceptualization of vergence-accommodation as a device-specific vergence offset, which predicted up to 66\% of the variance in the data. The fitted parameters indicate that, due to the vergence-accommodation conflict, participants’ vergence angle was driven outwards by approximately 0.2°, which disrupted the stereoscopic viewing geometry and produced distance distortion in VR and AR. The implications of this finding are discussed in the context of developing virtual environments that minimize the effect of depth distortion.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Wang, Xiaoye Michael and Southwick, Daniel and Robinson, Ian and Nitsche, Michael and Resch, Gabby and Mazalek, Ali and Welsh, Timothy N.},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Augmented reality, Depth perception, Manual pointing, Vergence-accommodation conflict, Virtual reality},
	pages = {95},
	file = {Full Text PDF:files/1210/Wang et al. - 2024 - The geometry of the vergence-accommodation conflict in mixed reality systems.pdf:application/pdf},
}

@article{neugebauer_simulating_2024,
	title = {Simulating vision impairment in virtual reality: a comparison of visual task performance with real and simulated tunnel vision},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Simulating vision impairment in virtual reality},
	url = {https://doi.org/10.1007/s10055-024-00987-0},
	doi = {10.1007/s10055-024-00987-0},
	abstract = {In this work, we explore the potential and limitations of simulating gaze-contingent tunnel vision conditions using Virtual Reality (VR) with built-in eye tracking technology. This approach promises an easy and accessible way of expanding study populations and test groups for visual training, visual aids, or accessibility evaluations. However, it is crucial to assess the validity and reliability of simulating these types of visual impairments and evaluate the extend to which participants with simulated tunnel vision can represent real patients. Two age-matched participant groups were acquired: The first group (n = 8, aged 20–60, average 49.1 ± 13.2) consisted of patients diagnosed with Retinitis pigmentosa (RP). The second group (n = 8, aged 27–59, average 46.5 ± 10.8) consisted of visually healthy participants with simulated tunnel vision. Both groups carried out different visual tasks in a virtual environment for 30 min per day over the course of four weeks. Task performances as well as gaze characteristics were evaluated in both groups over the course of the study. Using the ’two one-sided tests for equivalence’ method, the two groups were found to perform similar in all three visual tasks. Significant differences between groups were found in different aspects of their gaze behavior, though most of these aspects seem to converge over time. Our study evaluates the potential and limitations of using Virtual Reality technology to simulate the effects of tunnel vision within controlled virtual environments. We find that the simulation accurately represents performance of RP patients in the context of group averages, but fails to fully replicate effects on gaze behavior.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Neugebauer, Alexander and Castner, Nora and Severitt, Björn and Stingl, Katarina and Ivanov, Iliya and Wahl, Siegfried},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Disability, Retinitis pigmentosa, Tunnel vision, Virtual reality, Vision impairment simulation},
	pages = {97},
	file = {Full Text PDF:files/1211/Neugebauer et al. - 2024 - Simulating vision impairment in virtual reality a comparison of visual task performance with real a.pdf:application/pdf},
}

@article{moncada_virtual_2023,
	title = {Virtual reality and machine learning in the automatic photoparoxysmal response detection},
	volume = {35},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-022-06940-z},
	doi = {10.1007/s00521-022-06940-z},
	abstract = {Photosensitivity, in relation to epilepsy, is a genetically determined condition in which patients have epileptic seizures of different severity provoked by visual stimuli. It can be diagnosed by detecting epileptiform discharges in their electroencephalogram (EEG), known as photoparoxysmal responses (PPR). The most accepted PPR detection method—a manual method—considered as the standard one, consists in submitting the subject to intermittent photic stimulation (IPS), i.e. a flashing light stimulation at increasing and decreasing flickering frequencies in a hospital room under controlled ambient conditions, while at the same time recording her/his brain response by means of EEG signals. This research focuses on introducing virtual reality (VR) in this context, adding, to the conventional infrastructure a more flexible one that can be programmed and that will allow developing a much wider and richer set of experiments in order to detect neurological illnesses, and to study subjects’ behaviours automatically. The loop includes the subject, the VR device, the EEG infrastructure and a computer to analyse and monitor the EEG signal and, in some cases, provide feedback to the VR. As will be shown, AI modelling will be needed in the automatic detection of PPR, but it would also be used in extending the functionality of this system with more advanced features. This system is currently in study with subjects at Burgos University Hospital, Spain.},
	language = {en},
	number = {8},
	urldate = {2024-09-05},
	journal = {Neural Comput \& Applic},
	author = {Moncada, Fernando and Martín, Sofía and González, Víctor M. and Álvarez, Víctor M. and García-López, Beatriz and Gómez-Menéndez, Ana Isabel and Villar, José R.},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, Electroencefalogram, Machine learning, Photoparoxysmal response, Virtual reality},
	pages = {5643--5659},
	file = {Full Text PDF:files/1212/Moncada et al. - 2023 - Virtual reality and machine learning in the automatic photoparoxysmal response detection.pdf:application/pdf},
}

@article{marques_exploring_2024,
	title = {Exploring different content creation and display methods for remote collaboration supported by {eXtended} reality: comparative analysis of distinct task scenarios},
	issn = {1573-7721},
	shorttitle = {Exploring different content creation and display methods for remote collaboration supported by {eXtended} reality},
	url = {https://doi.org/10.1007/s11042-024-19836-y},
	doi = {10.1007/s11042-024-19836-y},
	abstract = {Remote collaboration using eXtended Reality (XR) has been explored to establish a common ground between physically distributed individuals. To achieve usable and impactful solutions, it is paramount to understand how different content creation and display methods contribute to the work effort of each member (remote and on-site). Additionally, explore how various task scenarios influence the collaborative process, specifically examining how activities with unique characteristics and complexities affect remote work. Hence, the question arises, ’How do different display and interaction methods impact the collaborative process within specific task domains?’ In this paper, two user studies with 30 distinct participants each are described, focusing on different content creation support (Laptop Computer; Video Wall \& Keyboard; Interactive Projector) and display methods (Hand-Held Device (HHD); HHD \& Articulated Support; Head-Mounted Display (HMD)) for remote and on-site collaborators respectively, during scenarios of remote guidance. Plus, different physical tasks were considered for each study (Lego pieces assembly; Tangram puzzle assembly; Maintenance procedures; Resource management in a map; Training activity in a laboratory classroom), to understand which method stands out according to the characteristics of the said tasks. We report the results obtained, suggesting that for the remote part of the collaborative process, using a Video Wall \& Keyboard appears to be more suitable for tasks associated with maintenance procedures, learning, and training activities. As for the on-site part, using HMD was considered the better option for maintenance, and learning tasks, closely followed by HHD \& Articulated Support.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Marques, Bernardo and Ferreira, Carlos and Silva, Samuel and Santos, André and Santos, Andreia and Dias, Paulo and Santos, Beatriz Sousa},
	month = jul,
	year = {2024},
	keywords = {Artificial Intelligence, Creation and display methods, eXtended reality, Remote collaboration, Task complexity, User study},
	file = {Full Text PDF:files/1213/Marques et al. - 2024 - Exploring different content creation and display methods for remote collaboration supported by eXten.pdf:application/pdf},
}

@article{eidenberger_smell_2018,
	title = {Smell and touch in the {Virtual} {Jumpcube}},
	volume = {24},
	issn = {1432-1882},
	url = {https://doi.org/10.1007/s00530-018-0592-y},
	doi = {10.1007/s00530-018-0592-y},
	abstract = {The Virtual Jumpcube is a virtual reality setup from 2015 that allows for jumping and flying in audiovisual virtual environments. Recently, we have included several haptic and olfactory stimuli that should further increase the degree of immersion in the experienced virtuality. These additional media channels were tested by the participants of several events and the feedback of 196 jumpers was gathered in a questionnaire. In this paper, we describe the stimulation hardware and software as well as the performed experiment and we present the major findings of the evaluation. It shows that if employed correctly, haptic and olfactory stimuli can enhance immersion and user experience significantly. Major success factors appear to be the amplitude and frequency of stimulation as well as the temporal synchronization with the other media channels, in particular the visual stimuli.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Multimedia Systems},
	author = {Eidenberger, Horst},
	month = nov,
	year = {2018},
	keywords = {Artificial Intelligence, Force simulation, Haptic stimuli, Olfactory stimuli, User-based evaluation, Virtual reality},
	pages = {695--709},
	file = {Full Text PDF:files/1214/Eidenberger - 2018 - Smell and touch in the Virtual Jumpcube.pdf:application/pdf},
}

@article{gaidon_reasonable_2018,
	title = {The {Reasonable} {Effectiveness} of {Synthetic} {Visual} {Data}},
	volume = {126},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-018-1108-0},
	doi = {10.1007/s11263-018-1108-0},
	language = {en},
	number = {9},
	urldate = {2024-09-05},
	journal = {Int J Comput Vis},
	author = {Gaidon, Adrien and Lopez, Antonio and Perronnin, Florent},
	month = sep,
	year = {2018},
	pages = {899--901},
	file = {Full Text PDF:files/1215/Gaidon et al. - 2018 - The Reasonable Effectiveness of Synthetic Visual Data.pdf:application/pdf},
}

@article{dekker_adoption_2024,
	title = {Adoption of immersive-virtual reality as an intrinsically motivating learning tool in parasitology},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-01016-w},
	doi = {10.1007/s10055-024-01016-w},
	abstract = {Veterinary parasitology is study of parasitic diseases, treatment and prevention. It is a major component of animal health courses due to impacts parasites have on production and companion animals. Extant tertiary education in parasitology typically involves theory sessions coupled with practical experience. In this study we propose tertiary parasitology teaching would be enhanced through adoption of immersive Virtual Reality (I-VR) as an intrinsically motivating learning tool to complement their studies. To evaluate this adoption, a custom I-VR parasitology game was developed that tertiary veterinary science students experienced (n = 109), with feedback assessed using the Hedonic-Motivation System Adoption Model (HMSAM). HMSAM proved appropriate for measuring student’s hedonistic and utilitarian perspectives of I-VR experience with perceived ease of use, perceived usefulness, joy, ability to control, immersion levels and intention to use displaying significant positive relationships in derived model. However, in a departure from similar studies, the curiosity construct was not a useful predictor of intention to use in this context of a scaffolded, instructional application. This study highlights suitability of I-VR and provides a statistically robust evaluation method using a modified HMSAM to evaluate acceptance, usefulness, and ease of use of I-VR in tertiary education.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Dekker, Evan and Whitburn, Damien and Preston, Sarah},
	month = jun,
	year = {2024},
	keywords = {Artificial Intelligence, HMSAM, Immersive, Immersive virtual reality, Motivation, Parasitology, Quantitative, SEM},
	pages = {123},
	file = {Full Text PDF:files/1216/Dekker et al. - 2024 - Adoption of immersive-virtual reality as an intrinsically motivating learning tool in parasitology.pdf:application/pdf},
}

@article{jiang_game_2018,
	title = {A game prototype for understanding the safety issues of a lifeboat launch},
	volume = {22},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-018-0334-7},
	doi = {10.1007/s10055-018-0334-7},
	abstract = {Novel, advanced game techniques provide us with new possibilities to mimic a complicated training process, with the added benefit of enhanced safety. In this paper, we design and implement a 3D game with the support of virtual reality equipment which imitates the process of a lifeboat launch, involving both tractor manoeuvres and boat operations. It is a complex but vital process which can save lives at sea but also has many potential hazards. The primary objective of the game is to allow novices to better understand the sequence of the operations and manage the potential risks which may occur during the launch process. Additionally, the game has been promoted to the general public for educational purposes and to raise awareness of the safety issues involved. The key modules of the game are designed based on physical simulations to give the players enhanced plausible cognition and enjoyable interaction. We conducted two case studies for the two purposes of the games: one for training with volunteers without launching experience and the other for public awareness of the potential hazards with young children. The game is proven to be very promising for future professional training, and it serves the educational purpose of awareness of the safety issues for general public while being entertaining.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Jiang, Min and Lan, Wei and Chang, Jian and Dodwell, Mark and Jekins, Jeremy and Yang, Hui Jun and Tong, Ruo Feng and Zhang, Jian Jun},
	month = jun,
	year = {2018},
	keywords = {Artificial Intelligence, Game-based training, General public, Lifeboat launch, Serious games, Virtual reality},
	pages = {137--148},
	file = {Full Text PDF:files/1217/Jiang et al. - 2018 - A game prototype for understanding the safety issues of a lifeboat launch.pdf:application/pdf},
}

@article{soltiyeva_my_2023,
	title = {My {Lovely} {Granny}’s {Farm}: {An} immersive virtual reality training system for children with autism spectrum disorder},
	volume = {28},
	issn = {1573-7608},
	shorttitle = {My {Lovely} {Granny}’s {Farm}},
	url = {https://doi.org/10.1007/s10639-023-11862-x},
	doi = {10.1007/s10639-023-11862-x},
	abstract = {One of the biggest difficulties faced by children with Autism Spectrum Disorder during their learning process and general life, is communication and social interaction. In recent years, researchers and practitioners have invested in different approaches to improving aspects of their communication and learning. However, there is still no consolidated approach and the community is still looking for new approaches that can meet this need. Addressing this challenge, in this article we propose a novelty approach (i.e., an Adaptive Immersive Virtual Reality Training System), aiming to enrich social interaction and communication skills for children with Autism Spectrum Disorder. In this adaptive system (called My Lovely Granny’s Farm), the behavior of the virtual trainer changes depending on the mood and actions of the users (i.e., patients/learners). Additionally, we conducted an initial observational study by monitoring the behavior of children with autism in a virtual environment. In the initial study, the system was offered to users with a high degree of interactivity so that they might practice various social situations in a safe and controlled environment. The results demonstrate that the use of the system can allow patients who needed treatment to receive therapy without leaving home. Our approach is the first experience of treating children with autism in Kazakhstan and can contribute to improving the communication and social interaction of children with Autism Spectrum Disorder. We contribute to the community of educational technologies and mental health by providing a system that can improve communication among children with autism and providing insights on how to design this kind of system.},
	language = {en},
	number = {12},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Soltiyeva, Aiganym and Oliveira, Wilk and Madina, Alimanova and Adilkhan, Shyngys and Urmanov, Marat and Hamari, Juho},
	month = dec,
	year = {2023},
	keywords = {Artificial Intelligence, Autism spectrum disorder, Communicational skills, Digital Education and Educational Technology, Immersive systems, Social interaction, Virtual reality},
	pages = {16887--16907},
	file = {Full Text PDF:files/1218/Soltiyeva et al. - 2023 - My Lovely Granny’s Farm An immersive virtual reality training system for children with autism spect.pdf:application/pdf},
}

@article{skjermo_evaluation_2022,
	title = {Evaluation of {Road} {Safety} {Education} {Program} with {Virtual} {Reality} {Eye} {Tracking}},
	volume = {3},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-022-01036-w},
	doi = {10.1007/s42979-022-01036-w},
	abstract = {Traffic rules are essential and part of today’s road safety education programs at school. However, learning how to properly use attention in complex traffic situations has never before been a focus when teaching pupils. A new road safety education program based on latest findings in Neuro-Education has been developed. In the program, pupil learned three new concepts: risk, orientation and attention. This to stimulate the pupil’s reflection about own behavior on school roads, before attended a session at a traffic center, where teachers encouraged pupil’s reflection about how to practically use their new knowledge about the three concepts. This new program design was evaluated in a Virtual Reality laboratory. This paper presents the approach for evaluation using VR and eye-tracking, including scenario building, measurement approach, and procedure for measurements in addition to a look at the actual evaluation results. The results showed that pupils managed to cycle in a safer way by putting their attention at the right places after the course compared to student following existing curriculum. As such the selected validation approach utilizing VR gave clear support for the proposed education program.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {SN COMPUT. SCI.},
	author = {Skjermo, Jo and Roche-Cerasi, Isabelle and Moe, Dagfinn and Opland, Robert},
	month = feb,
	year = {2022},
	keywords = {Artificial Intelligence, Cycling, Education, Virtual reality},
	pages = {149},
	file = {Full Text PDF:files/1219/Skjermo et al. - 2022 - Evaluation of Road Safety Education Program with Virtual Reality Eye Tracking.pdf:application/pdf},
}

@article{parekh_systematic_2020,
	title = {Systematic review and meta-analysis of augmented reality in medicine, retail, and games},
	volume = {3},
	issn = {2524-4442},
	url = {https://doi.org/10.1186/s42492-020-00057-7},
	doi = {10.1186/s42492-020-00057-7},
	abstract = {This paper presents a detailed review of the applications of augmented reality (AR) in three important fields where AR use is currently increasing. The objective of this study is to highlight how AR improves and enhances the user experience in entertainment, medicine, and retail. The authors briefly introduce the topic of AR and discuss its differences from virtual reality. They also explain the software and hardware technologies required for implementing an AR system and the different types of displays required for enhancing the user experience. The growth of AR in markets is also briefly discussed. In the three sections of the paper, the applications of AR are discussed. The use of AR in multiplayer gaming, computer games, broadcasting, and multimedia videos, as an aspect of entertainment and gaming is highlighted. AR in medicine involves the use of AR in medical healing, medical training, medical teaching, surgery, and post-medical treatment. AR in retail was discussed in terms of its uses in advertisement, marketing, fashion retail, and online shopping. The authors concluded the paper by detailing the future use of AR and its advantages and disadvantages in the current scenario.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Vis. Comput. Ind. Biomed. Art},
	author = {Parekh, Pranav and Patel, Shireen and Patel, Nivedita and Shah, Manan},
	month = sep,
	year = {2020},
	keywords = {Artificial Intelligence, Augmented reality, Gaming, Medical Ethics, Medicine, Software},
	pages = {21},
	file = {Full Text PDF:files/1220/Parekh et al. - 2020 - Systematic review and meta-analysis of augmented reality in medicine, retail, and games.pdf:application/pdf},
}

@article{bialecki_determinants_2023,
	title = {Determinants of victory in {Esports} - {StarCraft} {II}},
	volume = {82},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-022-13373-2},
	doi = {10.1007/s11042-022-13373-2},
	abstract = {Esports offer a unique opportunity to conduct human performance studies, as they use modern hardware and software as an operation platform. Insights on gameplay and underlying processes may push the development of new and optimal practice methods. The aim of this study was to investigate performance indicators from in-game data to predict the outcome of the matches in StarCraft II: Legacy of The Void.},
	language = {en},
	number = {7},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Białecki, Andrzej and Gajewski, Jan and Białecki, Piotr and Phatak, Ashwin and Memmert, Daniel},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, Esports, Gaming, Performance analysis, Real-time strategy, Starcraft},
	pages = {11099--11115},
	file = {Full Text PDF:files/1241/Białecki et al. - 2023 - Determinants of victory in Esports - StarCraft II.pdf:application/pdf},
}

@article{hepperle_aspects_2022,
	title = {Aspects of visual avatar appearance: self-representation, display type, and uncanny valley},
	volume = {38},
	issn = {1432-2315},
	shorttitle = {Aspects of visual avatar appearance},
	url = {https://doi.org/10.1007/s00371-021-02151-0},
	doi = {10.1007/s00371-021-02151-0},
	abstract = {The visual representation of human-like entities in virtual worlds is becoming a very important aspect as virtual reality becomes more and more “social”. The visual representation of a character’s resemblance to a real person and the emotional response to it, as well as the expectations raised, have been a topic of discussion for several decades and have been debated by scientists from different disciplines. But as with any new technology, the findings may need to be reevaluated and adapted to new modalities. In this context, we make two contributions which may have implications for how avatars should be represented in social virtual reality applications. First, we determine how default and customized characters of current social virtual reality platforms appear in terms of human likeness, eeriness, and likability, and whether there is a clear resemblance to a given person. It can be concluded that the investigated platforms vary strongly in their representation of avatars. Common to all is that a clear resemblance does not exist. Second, we show that the uncanny valley effect is also present in head-mounted displays, but—compared to 2D monitors—even more pronounced.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Hepperle, Daniel and Purps, Christian Felix and Deuchler, Jonas and Wölfel, Matthias},
	month = apr,
	year = {2022},
	keywords = {Artificial Intelligence, Avatar customization, Head-mounted displays, Nonverbal communication, Social networks, Virtual reality},
	pages = {1227--1244},
	file = {Full Text PDF:files/1242/Hepperle et al. - 2022 - Aspects of visual avatar appearance self-representation, display type, and uncanny valley.pdf:application/pdf},
}

@article{sattler_embedded_2024,
	title = {Embedded {3D} reconstruction of dynamic objects in real time for maritime situational awareness pictures},
	volume = {40},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-023-02802-4},
	doi = {10.1007/s00371-023-02802-4},
	abstract = {Assessing the security status of maritime infrastructures is a key factor for maritime safety and security. Facilities such as ports and harbors are highly active traffic zones with many different agents and infrastructures present, like containers, trucks or vessels. Conveying security-related information in a concise and easily understandable format can support the decision-making process of stakeholders, such as port authorities, law enforcement agencies and emergency services. In this work, we propose a novel real-time 3D reconstruction framework for enhancing maritime situational awareness pictures by joining temporal 2D video data into a single consistent display. We introduce and verify a pipeline prototype for dynamic 3D reconstruction of maritime objects using a static observer and stereoscopic cameras on an GPU-accelerated embedded device. A simulated dataset of a harbor basin was created and used for real-time processing. Usage of a simulated setup allowed verification against synthetic ground-truth data. The presented pipeline runs entirely on a remote, low-power embedded system with \$\${\textbackslash}sim \$\$6 Hz. A Nvidia Jetson Xavier AGX module was used, featuring 512 CUDA-cores, 16 GB memory and an ARMv8 64-bit octa-core CPU.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Sattler, Felix and Carrillo-Perez, Borja and Barnes, Sarah and Stebner, Karsten and Stephan, Maurice and Lux, Gregor},
	month = feb,
	year = {2024},
	keywords = {Artificial Intelligence, Dynamic 3D reconstruction, Maritime safety and security, Real-time monitoring, Situtational awareness},
	pages = {571--584},
	file = {Full Text PDF:files/1243/Sattler et al. - 2024 - Embedded 3D reconstruction of dynamic objects in real time for maritime situational awareness pictur.pdf:application/pdf},
}

@article{gomez-huelamo_train_2022,
	title = {Train here, drive there: {ROS} based end-to-end autonomous-driving pipeline validation in {CARLA} simulator using the {NHTSA} typology},
	volume = {81},
	issn = {1573-7721},
	shorttitle = {Train here, drive there},
	url = {https://doi.org/10.1007/s11042-021-11681-7},
	doi = {10.1007/s11042-021-11681-7},
	abstract = {Urban complex scenarios are the most challenging situations in the field of Autonomous Driving (AD). In that sense, an AD pipeline should be tested in countless environments and scenarios, escalating the cost and development time exponentially with a physical approach. In this paper we present a validation of our fully-autonomous driving architecture using the NHTSA (National Highway Traffic Safety Administration) protocol in the CARLA simulator, focusing on the analysis of our decision-making module, based on Hierarchical Interpreted Binary Petri Nets (HIBPN). First, the paper states the importance of using hyper-realistic simulators, as a preliminary help to real test, as well as an appropriate design of the traffic scenarios as the two current keys to build safe and robust AD technology. Second, our pipeline is introduced, which exploits the concepts of standard communication in robotics using the Robot Operating System (ROS) and the Docker approach to provide the system with isolation, flexibility and portability, describing the main modules and approaches to perform the navigation. Third, the CARLA simulator is described, outlining the steps carried out to merge our architecture with the simulator and the advantages to create ad-hoc driving scenarios for use cases validation instead of just modular evaluation. Finally, the architecture is validated using some challenging driving scenarios such as Pedestrian Crossing, Stop, Adaptive Cruise Control (ACC) and Unexpected Pedestrian. Some qualitative (video files: Simulation Use Cases) and quantitative (linear velocity and trajectory splitted in the corresponding HIBPN states) results are presented for each use case, as well as an analysis of the temporal graphs associated to the Vulnerable Road Users (VRU) cases, validating our architecture in simulation as a preliminary stage before implementing it in our real autonomous electric car.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Gómez-Huélamo, Carlos and Del Egido, Javier and Bergasa, Luis M. and Barea, Rafael and López-Guillén, Elena and Arango, Felipe and Araluce, Javier and López, Joaquín},
	month = jan,
	year = {2022},
	keywords = {Artificial Intelligence, Autonomous Driving, CARLA, Decision-Making, NHTSA typology, ROS, Simulation},
	pages = {4213--4240},
	file = {Full Text PDF:files/1244/Gómez-Huélamo et al. - 2022 - Train here, drive there ROS based end-to-end autonomous-driving pipeline validation in CARLA simula.pdf:application/pdf},
}

@article{ayedoun_adding_2019,
	title = {Adding {Communicative} and {Affective} {Strategies} to an {Embodied} {Conversational} {Agent} to {Enhance} {Second} {Language} {Learners}’ {Willingness} to {Communicate}},
	volume = {29},
	issn = {1560-4306},
	url = {https://doi.org/10.1007/s40593-018-0171-6},
	doi = {10.1007/s40593-018-0171-6},
	abstract = {This paper describes an embodied conversational agent enhanced with specific conversational strategies aiming to foster learners’ readiness towards communication in a second language (L2). Willingness to communicate (WTC) in a second language is believed to have a direct and sustained influence on learners’ actual usage frequency of the target language. To help overcome the lack of suitable environments for increasing L2 learners’ WTC, our approach is to build embodied conversational agents that can help learners surmount their apprehension towards communication in L2. Here, we focus on the dialogue management aspects of our approach and propose a model based on a set of communication strategies (CS) and affective backchannels (AB) to foster such agents’ ability to carry on natural and WTC-friendly conversations with learners. We examined learners’ expected WTC after interacting with one of the following versions of the system: an agent featuring both CS and AB; an agent featuring only CS; and an agent featuring only AB. The results suggested that combining CS and AB empowers the conversational agent and leads to higher expected WTC among L2 learners. We also found that even the AB-only version of the system had the potential to enhance WTC to some extent. These findings are evidence of the feasibility of enhancing L2 learners’ engagement towards communication using a computer-based environment coupled with appropriate conversational strategies.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Int J Artif Intell Educ},
	author = {Ayedoun, Emmanuel and Hayashi, Yuki and Seta, Kazuhisa},
	month = mar,
	year = {2019},
	keywords = {Affective backchannels, Artificial Intelligence, Communication strategies, Digital Education and Educational Technology, Embodied conversational agents, Willingness to communicate in L2},
	pages = {29--57},
	file = {Full Text PDF:files/1245/Ayedoun et al. - 2019 - Adding Communicative and Affective Strategies to an Embodied Conversational Agent to Enhance Second.pdf:application/pdf},
}

@article{pastel_comparison_2021,
	title = {Comparison of gaze accuracy and precision in real-world and virtual reality},
	volume = {25},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-020-00449-3},
	doi = {10.1007/s10055-020-00449-3},
	abstract = {Virtual reality (VR) is popular across many fields and is increasingly used in sports as a training tool. The reason, therefore, is recently improved display technologies, more powerful computation capacity, and lower costs of head-mounted displays for VR. As in the real-world (R), visual effects are the most important stimulus provided by VR. However, it has not been demonstrated whether the gaze behavior would achieve the same level in VR as in R. This information will be important for the development of applications or software in VR. Therefore, several tasks were designed to analyze the gaze accuracy and gaze precision using eye-tracking devices in R and VR. 21 participants conducted three eye-movement tasks in sequence: gaze at static targets, tracking a moving target, and gaze at targets at different distances. To analyze the data, an averaged distance with root mean square was calculated between the coordinates of each target and the recorded gaze points for each task. In gaze accuracy, the results showed no significant differences between R and VR in gaze at static targets (1 m distance, p {\textgreater} 0.05) and small significant differences at targets placed at different distances (p {\textless} 0.05), as well as large differences in tracking the moving target (p {\textless} 0.05). The precision in VR is significantly worse compared to R in all tasks with static gaze targets (p {\textless} 0.05). On the whole, this study gives a first insight into comparing foveal vision, especially gaze accuracy and precision between R and VR, and can, therefore, serve as a reference for the development of VR applications in the future.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Pastel, Stefan and Chen, Chien-Hsi and Martin, Luca and Naujoks, Mats and Petri, Katharina and Witte, Kerstin},
	month = mar,
	year = {2021},
	keywords = {Accuracy, Artificial Intelligence, Eye-tracking, Gaze behavior, Head-mounted display, Precision, Virtual reality},
	pages = {175--189},
	file = {Full Text PDF:files/1246/Pastel et al. - 2021 - Comparison of gaze accuracy and precision in real-world and virtual reality.pdf:application/pdf},
}

@article{hadjipanayi_cultivating_2024,
	title = {Cultivating empathy through narratives in virtual reality: a review},
	issn = {1617-4917},
	shorttitle = {Cultivating empathy through narratives in virtual reality},
	url = {https://doi.org/10.1007/s00779-024-01812-w},
	doi = {10.1007/s00779-024-01812-w},
	abstract = {In recent years, there has been a growing interest in the potential of virtual reality (VR) as a powerful tool for storytelling and as a means of promoting empathy. This systematic review examines 20 research papers that were deemed relevant based on inclusion and exclusion criteria from a database of a total of 661 papers to investigate the use of VR for empathy-building through immersive storytelling. Thematic analysis of the interventions revealed that most of the narratives focused on the experiences of victims of abuse, social minorities, and individuals affected by medical conditions or political ramifications. These fall under three types of digital narratives identified as (a) personal, (b) historical, and (c) educational. Changes in empathy are identified either through comparisons with non-VR narratives or pre- and post-interventions. Interaction techniques, VR affordances, and methods to measure empathy are further identified. The review concludes that while VR shows promise as a tool for promoting empathy, more research is needed to fully understand its potential and limitations.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Pers Ubiquit Comput},
	author = {Hadjipanayi, Christos and Christofi, Maria and Banakou, Domna and Michael-Grigoriou, Despina},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Empathy, Immersive, Narrative, Storytelling, Virtual reality},
	file = {Full Text PDF:files/1247/Hadjipanayi et al. - 2024 - Cultivating empathy through narratives in virtual reality a review.pdf:application/pdf},
}

@article{pesek_enhancing_2024,
	title = {Enhancing music rhythmic perception and performance with a {VR} game},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-01014-y},
	doi = {10.1007/s10055-024-01014-y},
	abstract = {This study analyzes the effect of using a virtual reality (VR) game as a complementary tool to improve users’ rhythmic performance and perception in a remote and self-learning environment. In recent years, remote learning has gained importance due to various everyday situations; however, the effects of using VR in such situations for individual and self-learning have yet to be evaluated. In music education, learning processes are usually heavily dependent on face-to-face communication with a teacher and are based on a formal or informal curriculum. The aim of this study is to investigate the potential of gamified VR learning and its influence on users’ rhythmic sensory and perceptual abilities. We developed a drum-playing game based on a tower defense scenario designed to improve four aspects of rhythmic perceptual skills in elementary school children with various levels of music learning experience. In this study, 14 elementary school children received Meta Quest 2 headsets for individual use in a 14-day individual training session. The results showed a significant increase in their rhythmical skills through an analysis of their rhythmic performance before and after the training sessions. In addition, the experience of playing the VR game and using the HMD setup was also assessed, highlighting some of the challenges of currently available affordable headsets for gamified learning scenarios.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Pesek, Matevž and Hirci, Nejc and Žnideršič, Klara and Marolt, Matija},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, E-learning, Music theory, Virtual reality},
	pages = {118},
	file = {Full Text PDF:files/1248/Pesek et al. - 2024 - Enhancing music rhythmic perception and performance with a VR game.pdf:application/pdf},
}

@article{tatnall_editorial_2023,
	title = {Editorial for {EAIT} issue 7, 2023},
	volume = {28},
	issn = {1573-7608},
	url = {https://doi.org/10.1007/s10639-023-11979-z},
	doi = {10.1007/s10639-023-11979-z},
	language = {en},
	number = {7},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Tatnall, Arthur},
	month = jul,
	year = {2023},
	pages = {7725--7736},
	file = {Full Text PDF:files/1249/Tatnall - 2023 - Editorial for EAIT issue 7, 2023.pdf:application/pdf},
}

@article{malleson_real-time_2020,
	title = {Real-{Time} {Multi}-person {Motion} {Capture} from {Multi}-view {Video} and {IMUs}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-019-01270-5},
	doi = {10.1007/s11263-019-01270-5},
	abstract = {A real-time motion capture system is presented which uses input from multiple standard video cameras and inertial measurement units (IMUs). The system is able to track multiple people simultaneously and requires no optical markers, specialized infra-red cameras or foreground/background segmentation, making it applicable to general indoor and outdoor scenarios with dynamic backgrounds and lighting. To overcome limitations of prior video or IMU-only approaches, we propose to use flexible combinations of multiple-view, calibrated video and IMU input along with a pose prior in an online optimization-based framework, which allows the full 6-DoF motion to be recovered including axial rotation of limbs and drift-free global position. A method for sorting and assigning raw input 2D keypoint detections into corresponding subjects is presented which facilitates multi-person tracking and rejection of any bystanders in the scene. The approach is evaluated on data from several indoor and outdoor capture environments with one or more subjects and the trade-off between input sparsity and tracking performance is discussed. State-of-the-art pose estimation performance is obtained on the Total Capture (mutli-view video and IMU) and Human 3.6M (multi-view video) datasets. Finally, a live demonstrator for the approach is presented showing real-time capture, solving and character animation using a light-weight, commodity hardware setup.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Int J Comput Vis},
	author = {Malleson, Charles and Collomosse, John and Hilton, Adrian},
	month = jun,
	year = {2020},
	keywords = {Artificial Intelligence, IMU, Motion capture, Multi-person, Multi-view video, Pose estimation, Real-time},
	pages = {1594--1611},
	file = {Full Text PDF:files/1250/Malleson et al. - 2020 - Real-Time Multi-person Motion Capture from Multi-view Video and IMUs.pdf:application/pdf},
}

@article{georgiou_replicating_2024,
	title = {Replicating outdoor environments using {VR} and ambisonics: a methodology for accurate audio-visual recording, processing and reproduction},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Replicating outdoor environments using {VR} and ambisonics},
	url = {https://doi.org/10.1007/s10055-024-01003-1},
	doi = {10.1007/s10055-024-01003-1},
	abstract = {This paper introduces a methodology tailored to capture, post-process, and replicate audio-visual data of outdoor environments (urban or natural) for VR experiments carried out within a controlled laboratory environment. The methodology consists of 360\$\${\textasciicircum}{\textbackslash}circ\$\$video and higher order ambisonic (HOA) field recordings and subsequent calibrated spatial sound reproduction with a spherical loudspeaker array and video played back via a head-mounted display using a game engine and a graphical user interface for a perceptual experimental questionnaire. Attention was given to the equalisation and calibration of the ambisonic microphone and to the design of different ambisonic decoders. A listening experiment was conducted to evaluate four different decoders (one 2D first-order ambisonic decoder and three 3D third-order decoders) by asking participants to rate the relative (perceived) realism of recorded outdoor soundscapes reproduced with these decoders. The results showed that the third-order decoders were ranked as more realistic.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Georgiou, Fotis and Kawai, Claudia and Schäffer, Beat and Pieren, Reto},
	month = may,
	year = {2024},
	keywords = {Ambisonics, Artificial Intelligence, Decoder equalisation, Perceptual evaluation, Virtual reality},
	pages = {111},
	file = {Full Text PDF:files/1251/Georgiou et al. - 2024 - Replicating outdoor environments using VR and ambisonics a methodology for accurate audio-visual re.pdf:application/pdf},
}

@article{argyriou_design_2020,
	title = {Design methodology for 360° immersive video applications: the case study of a cultural heritage virtual tour},
	volume = {24},
	issn = {1617-4917},
	shorttitle = {Design methodology for 360° immersive video applications},
	url = {https://doi.org/10.1007/s00779-020-01373-8},
	doi = {10.1007/s00779-020-01373-8},
	abstract = {Three hundred sixty–degree (360°) immersive video applications for Head Mounted Display (HMD) devices offer great potential in providing engaging forms of experiential media solutions especially in Cultural Heritage education. Design challenges emerge though by this new kind of immersive media due to the 2D form of resources used for their construction, the lack of depth, the limited interaction and the need to address the sense of presence. In addition, the use of Virtual Reality (VR) headsets often causes nausea, or motion sickness effects imposing further implications in moderate motion design tasks. This paper introduces a methodological categorisation of tasks and techniques for the design of 360° immersive video applications. Following the design approach presented, a testbed application has been created as an immersive interactive virtual tour at the historical centre of the city of Rethymno in Crete, Greece, which has undergone user trials. Based on the analysis of the results of this study, a set of design guidelines for the implementation of 360° immersive video virtual tours is proposed.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Pers Ubiquit Comput},
	author = {Argyriou, Lemonia and Economou, Daphne and Bouki, Vassiliki},
	month = dec,
	year = {2020},
	keywords = {Artificial Intelligence, Design guidelines, Experiential media, Immersion, Immersive storytelling, Immersive video, Virtual cultural heritage},
	pages = {843--859},
	file = {Full Text PDF:files/1252/Argyriou et al. - 2020 - Design methodology for 360° immersive video applications the case study of a cultural heritage virt.pdf:application/pdf},
}

@article{ladakis_virtual_2024,
	title = {Virtual reality environments for stress reduction and management: a scoping review},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Virtual reality environments for stress reduction and management},
	url = {https://doi.org/10.1007/s10055-024-00943-y},
	doi = {10.1007/s10055-024-00943-y},
	abstract = {Virtual reality, a cutting-edge innovation in the realm of digital experiences, though more frequently employed for entertainment and education, can also serve as a tool for immersing users in therapeutic settings that promote relaxation and mindfulness. An increasing number of research attempts investigate its usability and impact on stress evaluation, management and reduction. This scoping review aims to depict the current role of virtual reality in stress reduction and identify common methods and practice, technology patterns as well as gaps. Results depict the emerging research interest in the domain of VR-based stress reduction systems. The developed systems included in this review were basically addressed to the general public (59\%) for daily life stress reduction utilizing a commercial VR headset often combined with supportive sensors. Guided imagery emerged as the most implemented method, but it is also noteworthy that almost all studies implicitly used this method. According to the analysis, most studies performed evaluation of the proposed VR system including both subjective and objective measurements to provide evidence on its efficiency and its actual impact on stress levels. Finally, validation methodologies attempt to point out the potential of VR technology in the direction of providing an efficient solution for the alleviation of stress burdens. Even though numerous studies report the usefulness and efficiency of VR technology regarding stress reduction, several challenges still need to be addressed, mainly because of the difficult definition, detection and evaluation of stress. An approach integrating the existing knowledge regarding signals that can act as biomarkers of stress and qualitative measurements could open new pathways toward the development of more impactful VR-based stress reduction systems.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Ladakis, Ioannis and Filos, Dimitrios and Chouvarda, Ioanna},
	month = feb,
	year = {2024},
	keywords = {Artificial Intelligence, Stress reduction, Virtual environments, Virtual reality, VR},
	pages = {50},
	file = {Full Text PDF:files/1253/Ladakis et al. - 2024 - Virtual reality environments for stress reduction and management a scoping review.pdf:application/pdf},
}

@article{li_ctsn_2024,
	title = {{CTSN}: {Predicting} cloth deformation for skeleton-based characters with a two-stream skinning network},
	volume = {10},
	issn = {2096-0662},
	shorttitle = {{CTSN}},
	url = {https://doi.org/10.1007/s41095-023-0344-6},
	doi = {10.1007/s41095-023-0344-6},
	abstract = {We present a novel learning method using a two-stream network to predict cloth deformation for skeleton-based characters. The characters processed in our approach are not limited to humans, and can be other targets with skeleton-based representations such as fish or pets. We use a novel network architecture which consists of skeleton-based and mesh-based residual networks to learn the coarse features and wrinkle features forming the overall residual from the template cloth mesh. Our network may be used to predict the deformation for loose or tight-fitting clothing. The memory footprint of our network is low, thereby resulting in reduced computational requirements. In practice, a prediction for a single cloth mesh for a skeleton-based character takes about 7 ms on an nVidia GeForce RTX 3090 GPU. Compared to prior methods, our network can generate finer deformation results with details and wrinkles.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Comp. Visual Media},
	author = {Li, Yudi and Tang, Min and Yang, Yun and Tong, Ruofeng and Yang, Shuangcai and Li, Yao and An, Bailin and Kou, Qilong},
	month = jun,
	year = {2024},
	keywords = {Artificial Intelligence, cloth deformation, learning network, skinning},
	pages = {471--485},
	file = {Full Text PDF:files/1254/Li et al. - 2024 - CTSN Predicting cloth deformation for skeleton-based characters with a two-stream skinning network.pdf:application/pdf},
}

@article{ludwig_share-it_2021,
	title = {{shARe}-{IT}: {Ad} hoc {Remote} {Troubleshooting} through {Augmented} {Reality}},
	volume = {30},
	issn = {1573-7551},
	shorttitle = {{shARe}-{IT}},
	url = {https://doi.org/10.1007/s10606-021-09393-5},
	doi = {10.1007/s10606-021-09393-5},
	abstract = {10 years ago, Castellani et al. (Journal of Computer Supported Cooperative Work, vol. 18, no. 2–3, 2009, pp. 199–227, 2009) showed that using just an audio channel for remote troubleshooting can lead to a range of problems and already envisioned a future in which augmented reality (AR) could solve many of these issues. In the meantime, AR technologies have found their way into our everyday lives and using such technologies to support remote collaboration has been widely studied within the fields of Human-Computer Interaction and Computer-Supported Cooperative Work. In this paper, we contribute to this body of research by reporting on an extensive empirical study within a Fab Lab of troubleshooting and expertise sharing and the potential relevance of articulation work to their realization. Based on the findings of this study, we derived design challenges that led to an AR-based concept, implemented as a HoloLens application, called shARe-it. This application is designed to support remote troubleshooting and expertise sharing through different communication channels and AR-based interaction modalities. Early testing of the application revealed that novel interaction modalities such as AR-based markers and drawings play only a minor role in remote collaboration due to various limiting factors. Instead, the transmission of a shared view and especially arriving at a shared understanding of the situation as a prerequisite for articulation work continue to be the decisive factors in remote troubleshooting.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Comput Supported Coop Work},
	author = {Ludwig, Thomas and Stickel, Oliver and Tolmie, Peter and Sellmer, Malte},
	month = feb,
	year = {2021},
	keywords = {Artificial Intelligence, Augmented reality, CSCW, Expertise sharing, Fab lab, Remote troubleshooting},
	pages = {119--167},
	file = {Full Text PDF:files/1255/Ludwig et al. - 2021 - shARe-IT Ad hoc Remote Troubleshooting through Augmented Reality.pdf:application/pdf},
}

@article{holler_automatic_2021,
	title = {Automatic object annotation in streamed and remotely explored large {3D} reconstructions},
	volume = {7},
	issn = {2096-0662},
	url = {https://doi.org/10.1007/s41095-020-0194-4},
	doi = {10.1007/s41095-020-0194-4},
	abstract = {We introduce a novel framework for 3D scene reconstruction with simultaneous object annotation, using a pre-trained 2D convolutional neural network (CNN), incremental data streaming, and remote exploration, with a virtual reality setup. It enables versatile integration of any 2D box detection or segmentation network. We integrate new approaches to (i) asynchronously perform dense 3D-reconstruction and object annotation at interactive frame rates, (ii) efficiently optimize CNN results in terms of object prediction and spatial accuracy, and (iii) generate computationally-efficient colliders in large triangulated 3D-reconstructions at run-time for 3D scene interaction. Our method is novel in combining CNNs with long and varying inference time with live 3D-reconstruction from RGB-D camera input. We further propose a lightweight data structure to store the 3D-reconstruction data and object annotations to enable fast incremental data transmission for real-time exploration with a remote client, which has not been presented before. Our framework achieves update rates of 22 fps (SSD Mobile Net) and 19 fps (Mask RCNN) for indoor environments up to 800 m3. We evaluated the accuracy of 3D-object detection. Our work provides a versatile foundation for semantic scene understanding of large streamed 3D-reconstructions, while being independent from the CNN’s processing time. Source code is available for non-commercial use.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Comp. Visual Media},
	author = {Höller, Benjamin and Mossel, Annette and Kaufmann, Hannes},
	month = mar,
	year = {2021},
	keywords = {Artificial Intelligence, CNN, dense 3D reconstruction, distributed virtual reality, object detection},
	pages = {71--86},
	file = {Full Text PDF:files/1256/Höller et al. - 2021 - Automatic object annotation in streamed and remotely explored large 3D reconstructions.pdf:application/pdf},
}

@article{galani_augmented_2024,
	title = {An augmented reality approach for communicating intangible and architectural heritage through digital characters and scale models},
	issn = {1617-4917},
	url = {https://doi.org/10.1007/s00779-024-01792-x},
	doi = {10.1007/s00779-024-01792-x},
	abstract = {Intangible cultural heritage (ICH) represents living cultural expressions and practices that are part of the heritage of a community, and their preservation and transmission are considered highly important. Various methods and tools have been applied so far for the digitization and dissemination of ICH content including a wide range of technologies. Mobile augmented reality is a promising solution along this path that enables the overlap of digital and real-world information in an engaging and efficient manner. Despite the widespread use of AR in cultural heritage, there are not many studies regarding the user experience, the learning outcomes, and the way in which users observe and interact with the virtual content. This paper presents a mobile augmented reality installation that re-enacts the stages of leather tanning process, adopting a novel approach that augments 3D content upon a physical scale model of an old tannery. This approach pursues to transmit the cultural value of traditional craftmanship to visitors of the building and associate its architectural elements to its history and use. A user evaluation was conducted aiming to measure the users’ engagement, learning, and experience using the installation. The encouraging results led to a follow-up study about the impact of the physical scale model on the experience. Two variations of the experience have been studied, one with a physical scale model and one with a digital-only version in a between-subject design. The results of the two studies provide evidence that the proposed approach generated a positive user experience and evident learning gain and was considered easy to use, highlighting its potential to be widely adopted in buildings with architectural value.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Pers Ubiquit Comput},
	author = {Galani, Sophie and Vosinakis, Spyros},
	month = mar,
	year = {2024},
	keywords = {Architectural heritage, Artificial Intelligence, Human–computer interaction, Intangible Cultural heritage, Mobile augmented reality},
	file = {Full Text PDF:files/1257/Galani e Vosinakis - 2024 - An augmented reality approach for communicating intangible and architectural heritage through digita.pdf:application/pdf},
}

@article{liu_precision_2017,
	title = {On the precision of third person perspective augmented reality for target designation tasks},
	volume = {76},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-016-3817-0},
	doi = {10.1007/s11042-016-3817-0},
	abstract = {The availability of powerful consumer-level smart devices and off-the-shelf software frameworks has tremendously popularized augmented reality (AR) applications. However, since the built-in cameras typically have rather limited field of view, it is usually preferable to position AR tools built upon these devices at a distance when large objects need to be tracked for augmentation. This arrangement makes it difficult or even impossible to physically interact with the augmented object. One solution is to adopt third person perspective (TPP) with which the smart device shows in real time the object to be interacted with, the AR information and the user herself, all captured by a remote camera. Through mental transformation between the user-centric coordinate space and the coordinate system of the remote camera, the user can directly interact with objects in the real world. To evaluate user performance under this cognitively demanding situation, we developed such an experimental TPP AR system and conducted experiments which required subjects to make markings on a whiteboard according to virtual marks displayed by the AR system. The same markings were also made manually with a ruler. We measured the precision of the markings as well as the time to accomplish the task. Our results show that although the AR approach was on average around half a centimeter less precise than the manual measurement, it was approximately three times as fast as the manual counterpart. Additionally, we also found that subjects could quickly adapt to the mental transformation between the two coordinate systems.},
	language = {en},
	number = {14},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Liu, Fei and Seipel, Stefan},
	month = jul,
	year = {2017},
	keywords = {Artificial Intelligence, Augmented reality, Experiment, Precision study, Target designation, Third person perspective},
	pages = {15279--15296},
	file = {Full Text PDF:files/1258/Liu e Seipel - 2017 - On the precision of third person perspective augmented reality for target designation tasks.pdf:application/pdf},
}

@article{wang_using_2023,
	title = {Using visual feedback to improve hand movement accuracy in confined-occluded spaces in virtual reality},
	volume = {39},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-022-02424-2},
	doi = {10.1007/s00371-022-02424-2},
	abstract = {Accurate and informative hand-object collision feedback is of vital importance for hand manipulation in virtual reality (VR). However, to our best knowledge, the hand movement performance in fully-occluded and confined VR spaces under visual collision feedback is still under investigation. In this paper, we firstly studied the effects of several popular visual feedback of hand-object collision on hand movement performance. To test the effects, we conducted a within-subject user study (n=18) using a target-reaching task in a confined box. Results indicated that users had the best task performance with see-through visualization, and the most accurate movement with the hybrid of proximity-based gradation and deformation. By further analysis, we concluded that the integration of see-through visualization and proximity-based visual cue could be the best compromise between the speed and accuracy for hand movement in the enclosed VR space. On the basis, we designed a visual collision feedback based on projector decal,which incorporates the advantages of see-through and color gradation. In the end, we present demos of potential usage of the proposed visual cue.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Wang, Yu and Hu, Ziran and Yao, Shouwen and Liu, Hui},
	month = apr,
	year = {2023},
	keywords = {3D occlusion management, Artificial Intelligence, Hand movement performance, Occluded interaction, See-through visualization, Visual collision feedback},
	pages = {1485--1501},
	file = {Full Text PDF:files/1259/Wang et al. - 2023 - Using visual feedback to improve hand movement accuracy in confined-occluded spaces in virtual reali.pdf:application/pdf},
}

@article{vescan_genetic_2021,
	title = {Genetic programming for feature model synthesis: a replication study},
	volume = {26},
	issn = {1573-7616},
	shorttitle = {Genetic programming for feature model synthesis},
	url = {https://doi.org/10.1007/s10664-021-09947-7},
	doi = {10.1007/s10664-021-09947-7},
	abstract = {Software Product Lines (SPLs) make it possible to configure a single system based on features in order to create many different variants and cater to a wide range of customers with varying requirements. This configuration space is often modeled using Feature Models (FMs). However, in practice, the SPL (and consequently the FM) is often created after a set of variants has already been created manually. Automating the task of reverse engineering a feature model that describes a set of variants makes the process of adopting an SPL easier. The genetic programming pipeline is a good fit for feature models and has been shown to produce good reverse engineering results. In this paper, we replicate the results of such an existing approach with a larger set of feature models and investigate the effects of various genetic programming parameters and operators on the results. The design of our replication experiments employs three perspectives: duplicate the exact conditions using various features models, study the interaction of two parameters of the genetic programming approach, and optimize the values for the population and generation parameters and for the mutation and crossover operators. Results reinforce the previously obtained outcome, the original study being confirmed. The relations between the number of features and number of generations, respectively number of features and size of populations were also investigated and best values based on obtained results are provided. The current study also aimed to optimize various parameters of the genetic programming approach, the interpretation of those experiments discovering concrete values.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Empir Software Eng},
	author = {Vescan, Andreea and Pintea, Adrian and Linsbauer, Lukas and Egyed, Alexander},
	month = apr,
	year = {2021},
	keywords = {Artificial Intelligence, Feature models, Replication study, Reverse engineering, Software product lines},
	pages = {58},
	file = {Full Text PDF:files/1260/Vescan et al. - 2021 - Genetic programming for feature model synthesis a replication study.pdf:application/pdf},
}

@article{berthiaume_evaluation_2024,
	title = {Evaluation of a virtual reality training tool for firefighters responding to transportation incidents with dangerous goods},
	issn = {1573-7608},
	url = {https://doi.org/10.1007/s10639-023-12357-5},
	doi = {10.1007/s10639-023-12357-5},
	abstract = {Access to dangerous goods training for firefighters in remote areas is limited for financial and logistical reasons. Virtual reality (VR) is a promising solution for this challenge as it is cost-effective, safe, and allows to simulate realistic scenarios that would be dangerous or difficult to implement in the real world. However, rigorous evaluations of VR training tools for first responders are still scarce. In this exploratory user study, a simple VR training tool involving two dangerous goods scenarios was developed. In each scenario, trainees learned how to safely approach a jackknifed truck with a trailer and how to collect and communicate information about the transported materials. The tool was tested with a group of 24 professional firefighter trainees (n = 22) and instructors (n = 2), who each completed the two training scenarios. The main goal of the study was to assess the usability of the VR tool in the given scenarios. Participants provided feedback on cybersickness, perceived workload, and usability. They also filled out a knowledge test before and after the VR training and gave feedback at the end of the study. The VR tool recorded task completion duration and participants’ navigation and use of tools events. Overall, the tool provided good usability, acceptance, and satisfaction. However, a wide range in individuals’ responses was observed. In addition, no post-training improvement in participants' knowledge was found, likely due to the already high level of knowledge pre-training. Future directions for improving the VR tool, general implications for other VR training tools, and suggestions for future research are discussed.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Berthiaume, Maxine and Kinateder, Max and Emond, Bruno and Cooper, Natalia and Obeegadoo, Ishika and Lapointe, Jean-François},
	month = jan,
	year = {2024},
	keywords = {Artificial Intelligence, Dangerous goods, Digital Education and Educational Technology, First responders, Training, Usability, Virtual reality, Virtual reality evaluation},
	file = {Full Text PDF:files/1281/Berthiaume et al. - 2024 - Evaluation of a virtual reality training tool for firefighters responding to transportation incident.pdf:application/pdf},
}

@article{oriti_harmonize_2023,
	title = {Harmonize: a shared environment for extended immersive entertainment},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Harmonize},
	url = {https://doi.org/10.1007/s10055-021-00585-4},
	doi = {10.1007/s10055-021-00585-4},
	abstract = {Virtual reality (VR) and augmented reality (AR) applications are very diffuse nowadays. Moreover, recent technology innovations led to the diffusion of commercial head-mounted displays for immersive VR: users can enjoy entertainment activities that fill their visual fields, experiencing the sensation of physical presence in these virtual immersive environments. Even if AR and VR are mostly used separately, they can be effectively combined to provide a multi-user shared environment (SE), where two or more users perform some specific tasks in a cooperative or competitive way, providing a wider set of interactions and use cases compared to immersive VR alone. However, due to the differences between the two technologies, it is difficult to develop SEs offering a similar experience for both AR and VR users. This paper presents Harmonize, a novel framework to deploy applications based on SEs with a comparable experience for both AR and VR users. Moreover, the framework is hardware-independent, and it has been designed to be as much extendable to novel hardware as possible. An immersive game has been designed to test and to evaluate the validity of the proposed framework. The assessment of the system through the System Usability Scale questionnaire and the Game Experience Questionnaire shows a positive evaluation.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Oriti, Damiano and Manuri, Federico and Pace, Francesco De and Sanna, Andrea},
	month = dec,
	year = {2023},
	keywords = {Artificial Intelligence, Augmented reality, Collaborative environments, Immersive entertainment, Immersive environments, Shared environments, Virtual reality},
	pages = {3259--3272},
	file = {Full Text PDF:files/1282/Oriti et al. - 2023 - Harmonize a shared environment for extended immersive entertainment.pdf:application/pdf},
}

@article{haghshenas_predictive_2023,
	title = {Predictive digital twin for offshore wind farms},
	volume = {6},
	issn = {2520-8942},
	url = {https://doi.org/10.1186/s42162-023-00257-4},
	doi = {10.1186/s42162-023-00257-4},
	abstract = {As wind turbines continue to grow in size, they are increasingly being deployed offshore. This causes operation and maintenance of wind turbines becoming more challenging. Digitalization is a key enabling technology to manage wind farms in hostile environments and potentially increasing safety and reducing operational and maintenance costs. Digital infrastructure based on Industry 4.0 concept, such as digital twin, enables data collection, visualization, and analysis of wind power analytic at either individual turbine or wind farm level. In this paper, the concept of predictive digital twin for wind farm applications is introduced and demonstrated. To this end, a digital twin platform based on Unity3D for visualization and OPC Unified Architecture (OPC-UA) for data communication is developed. The platform is completed with the Prophet prediction algorithm to detect potential failure of wind turbine components in the near future and presented in augmented reality to enhance user experience. The presentation is intuitive and easy to use. The limitations of the platform include a lack of support for specific features like electronic signature, enhanced failover, and historical data sources. Simulation results based on the Hywind Tampen floating wind farm configuration show our proposed platform has promising potentials for offshore wind farm applications.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Energy Inform},
	author = {Haghshenas, Amirashkan and Hasan, Agus and Osen, Ottar and Mikalsen, Egil Tennfjord},
	month = jan,
	year = {2023},
	keywords = {Artificial Intelligence, Digital twin, Predictive maintenance, Wind energy},
	pages = {1},
	file = {Full Text PDF:files/1283/Haghshenas et al. - 2023 - Predictive digital twin for offshore wind farms.pdf:application/pdf},
}

@article{harris_exploring_2021,
	title = {Exploring sensorimotor performance and user experience within a virtual reality golf putting simulator},
	volume = {25},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-020-00480-4},
	doi = {10.1007/s10055-020-00480-4},
	abstract = {In light of recent advances in technology, there has been growing interest in virtual reality (VR) simulations for training purposes in a range of high-performance environments, from sport to nuclear decommissioning. For a VR simulation to elicit effective transfer of training to the real-world, it must provide a sufficient level of validity, that is, it must be representative of the real-world skill. In order to develop the most effective simulations, assessments of validity should be carried out prior to implementing simulations in training. The aim of this work was to test elements of the physical fidelity, psychological fidelity and construct validity of a VR golf putting simulation. Self-report measures of task load and presence in the simulation were taken following real and simulated golf putting to assess psychological and physical fidelity. The performance of novice and expert golfers in the simulation was also compared as an initial test of construct validity. Participants reported a high degree of presence in the simulation, and there was little difference between real and virtual putting in terms of task demands. Experts performed significantly better in the simulation than novices (p = .001, d = 1.23), and there was a significant relationship between performance on the real and virtual tasks (r = .46, p = .004). The results indicated that the simulation exhibited an acceptable degree of construct validity and psychological fidelity. However, some differences between the real and virtual tasks emerged, suggesting further validation work is required.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Harris, David J. and Buckingham, Gavin and Wilson, Mark R. and Brookes, Jack and Mushtaq, Faisal and Mon-Williams, Mark and Vine, Samuel J.},
	month = sep,
	year = {2021},
	keywords = {Artificial Intelligence, Construct validity, Simulation, Sport, Training, VR},
	pages = {647--654},
	file = {Full Text PDF:files/1284/Harris et al. - 2021 - Exploring sensorimotor performance and user experience within a virtual reality golf putting simulat.pdf:application/pdf},
}

@article{barrow_virtual_2024,
	title = {Virtual reality for biochemistry education: the cellular factory},
	volume = {29},
	issn = {1573-7608},
	shorttitle = {Virtual reality for biochemistry education},
	url = {https://doi.org/10.1007/s10639-023-11826-1},
	doi = {10.1007/s10639-023-11826-1},
	abstract = {Virtual Reality (VR) involves the coupling of visual communication hardware and software. The technology is capable of offering transformative educational practice and is increasingly being adopted within the biochemistry domain to better understand complex biochemical processes. This article documents a pilot study for the efficacy of VR in biochemistry education at undergraduate university level, focusing on the citric acid cycle: a central process for extracting energy in most cellular life forms. 10 participants were equipped with a VR headset and electrodermal activity (EDA) sensors, then immersed within a digital environment where they were able to learn the 8 main steps of the citric acid cycle within a virtual lab by completing 8 levels of activity. Post and pre surveys were taken, along with EDA readings throughout the students’ interaction with VR. Research findings support the hypothesis that VR increase students’ understanding, particularly if students feel engaged, stimulated and intend to use the technology. Moreover, EDA analysis indicated that the majority of participants demonstrate enhanced engagement in the education-based VR-experience as measured by elevated levels of skin conductance, a marker for autonomic arousal and a measure of engagement in an activity.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Barrow, John and Hurst, William and Edman, Joakim and Ariesen, Natasja and Krampe, Caspar},
	month = feb,
	year = {2024},
	keywords = {Artificial Intelligence, Biochemistry, Digital Education and Educational Technology, Immersive Learning, Virtual Reality, Electrodermal Activity, Pedagogy},
	pages = {1647--1672},
	file = {Full Text PDF:files/1285/Barrow et al. - 2024 - Virtual reality for biochemistry education the cellular factory.pdf:application/pdf},
}

@article{wong_development_2024,
	title = {Development of {Immersive} {Virtual} {Reality} {Hospital} {Fire} {Management} and {Evacuation} {Training} {Program} for {Nursing} {Students} in {Hong} {Kong}},
	volume = {5},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-024-02818-0},
	doi = {10.1007/s42979-024-02818-0},
	abstract = {While nursing students never experience hospital fire, they should know how to evacuate the patients under their care. Using Virtual Reality (VR) can offer students a unique leaning experience of this uncommon situation. The purpose of this study was to development a VR simulation program of a hospital fire evacuation training for the nursing students and evaluated the satisfaction of the participants. It was a pilot study conducted in a local private university offering a pre-registration, higher diploma of mental health nursing program. A VR simulation program regarding fire evacuation in a psychiatric hospital was newly developed. The students enrolled in the final year of this program were recruited by convenience sampling. A questionnaire was employed to evaluate participants' satisfaction after they completing the program. Descriptive statistics was employed to analyze the participants' characteristics and quantitative results of the questionnaires. The study was approved by the institution review board of the study university. The overall satisfaction of this workshop was 4.6/5 (n = 65). The highest score was the knowledge (4.7/5), presentation and facilitation skill (4.7/5) of facilitator. Half of the students rated excellent to the design, content and facilities of this workshop. In the narrative comment, some students were satisfied with authentic 3-dimension environment at the VR cave. Besides, the students treasured the chance to act as ward in-charge in evacuation. The first immersive VR simulation program for hospital fire management was developed and generally well-received by the nursing students.},
	language = {en},
	number = {5},
	urldate = {2024-09-05},
	journal = {SN COMPUT. SCI.},
	author = {Wong, Wendy Wing Chi and So, Gary Long Hei},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Crisis management, Mental health nursing, Nursing competences, Nursing education, Student satisfaction, Virtual reality},
	pages = {455},
	file = {Full Text PDF:files/1286/Wong e So - 2024 - Development of Immersive Virtual Reality Hospital Fire Management and Evacuation Training Program fo.pdf:application/pdf},
}

@article{wenk_effect_2023,
	title = {Effect of immersive visualization technologies on cognitive load, motivation, usability, and embodiment},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00565-8},
	doi = {10.1007/s10055-021-00565-8},
	abstract = {Virtual reality (VR) is a promising tool to promote motor (re)learning in healthy users and brain-injured patients. However, in current VR-based motor training, movements of the users performed in a three-dimensional space are usually visualized on computer screens, televisions, or projection systems, which lack depth cues (2D screen), and thus, display information using only monocular depth cues. The reduced depth cues and the visuospatial transformation from the movements performed in a three-dimensional space to their two-dimensional indirect visualization on the 2D screen may add cognitive load, reducing VR usability, especially in users suffering from cognitive impairments. These 2D screens might further reduce the learning outcomes if they limit users’ motivation and embodiment, factors previously associated with better motor performance. The goal of this study was to evaluate the potential benefits of more immersive technologies using head-mounted displays (HMDs). As a first step towards potential clinical implementation, we ran an experiment with 20 healthy participants who simultaneously performed a 3D motor reaching and a cognitive counting task using: (1) (immersive) VR (IVR) HMD, (2) augmented reality (AR) HMD, and (3) computer screen (2D screen). In a previous analysis, we reported improved movement quality when movements were visualized with IVR than with a 2D screen. Here, we present results from the analysis of questionnaires to evaluate whether the visualization technology impacted users’ cognitive load, motivation, technology usability, and embodiment. Reports on cognitive load did not differ across visualization technologies. However, IVR was more motivating and usable than AR and the 2D screen. Both IVR and AR rea ched higher embodiment level than the 2D screen. Our results support our previous finding that IVR HMDs seem to be more suitable than the common 2D screens employed in VR-based therapy when training 3D movements. For AR, it is still unknown whether the absence of benefit over the 2D screen is due to the visualization technology per se or to technical limitations specific to the device.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Wenk, N. and Penalver-Andres, J. and Buetler, K. A. and Nef, T. and Müri, R. M. and Marchal-Crespo, L.},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, Augmented Reality, Cognitive Load, Embodiment, Immersive Virtual Reality, Motivation, Usability},
	pages = {307--331},
	file = {Full Text PDF:files/1287/Wenk et al. - 2023 - Effect of immersive visualization technologies on cognitive load, motivation, usability, and embodim.pdf:application/pdf},
}

@article{henshall_virtual_2018,
	title = {Virtual reality’s effect on parameter optimisation for crowd-sourced procedural animation},
	volume = {34},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-018-1501-2},
	doi = {10.1007/s00371-018-1501-2},
	abstract = {Procedural animation systems are capable of synthesising life-like organic motion automatically. However, due to extensive parameterisation, tuning these systems can be very difficult. Not only are there potentially hundreds of interlinked parameters, the resultant animation can be very subjective and the process is difficult to automate effectively. In this paper, we describe a crowd-sourced approach to procedural animation parameter optimisation using genetic algorithms. We test our approach by asking users to interactively rate a population of virtual dolphins to a prescribed behavioural criterion. Our results show that within a few generations a group of users can successfully tune the system towards a desired behaviour. Our secondary motivation is to investigate whether there are differences in animation and behavioural preference between observations made using a standard desktop monitor and those made in virtual reality (VR). We describe a study where users tuned two sets of dolphin animation systems in parallel, one using a normal monitor and another using an Oculus Rift. Our results indicate that being immersed in VR leads to some key differences in preferred behaviour.},
	language = {en},
	number = {9},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Henshall, Gareth I. and Teahan, William J. and Cenydd, Llyr Ap},
	month = sep,
	year = {2018},
	keywords = {Artificial Intelligence, Genetic algorithm, Parameter optimisation, Procedural animation, Virtual reality},
	pages = {1255--1268},
	file = {Full Text PDF:files/1288/Henshall et al. - 2018 - Virtual reality’s effect on parameter optimisation for crowd-sourced procedural animation.pdf:application/pdf},
}

@article{krosl_vr-based_2018,
	title = {A {VR}-based user study on the effects of vision impairments on recognition distances of escape-route signs in buildings},
	volume = {34},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-018-1517-7},
	doi = {10.1007/s00371-018-1517-7},
	abstract = {In workplaces or publicly accessible buildings, escape routes are signposted according to official norms or international standards that specify distances, angles and areas of interest for the positioning of escape-route signs. In homes for the elderly, in which the residents commonly have degraded mobility and suffer from vision impairments caused by age or eye diseases, the specifications of current norms and standards may be insufficient. Quantifying the effect of symptoms of vision impairments like reduced visual acuity on recognition distances is challenging, as it is cumbersome to find a large number of user study participants who suffer from exactly the same form of vision impairments. Hence, we propose a new methodology for such user studies: By conducting a user study in virtual reality (VR), we are able to use participants with normal or corrected sight and simulate vision impairments graphically. The use of standardized medical eyesight tests in VR allows us to calibrate the visual acuity of all our participants to the same level, taking their respective visual acuity into account. Since we primarily focus on homes for the elderly, we accounted for their often limited mobility by implementing a wheelchair simulation for our VR application.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Krösl, Katharina and Bauer, Dominik and Schwärzler, Michael and Fuchs, Henry and Suter, Georg and Wimmer, Michael},
	month = jun,
	year = {2018},
	keywords = {Artificial Intelligence, User study, Virtual reality, Vision impairment simulation, Wheelchair simulation},
	pages = {911--923},
	file = {Full Text PDF:files/1289/Krösl et al. - 2018 - A VR-based user study on the effects of vision impairments on recognition distances of escape-route.pdf:application/pdf},
}

@article{hurst_virtual_2022,
	title = {Virtual conference design: features and obstacles},
	volume = {81},
	issn = {1573-7721},
	shorttitle = {Virtual conference design},
	url = {https://doi.org/10.1007/s11042-022-12402-4},
	doi = {10.1007/s11042-022-12402-4},
	abstract = {The Covid-19 pandemic has forced a change in the way people work, and the location that they work from. The impact has caused significant disruption to education, the work environment and how social interactions take place. Online user habits have also changed due to lockdown restrictions and virtual conferencing software has become a vital cog in team communication. In result, a spate in software solutions have emerged in order to support the challenges of remote learning and working. The conferencing software landscape is now a core communication solution for company-wide interaction, team discussions, screen sharing and face-to-face contact. Yet the number of existing platforms is diverse. In this article, a systematic literature review investigation on virtual conferencing is presented. As output from the analysis, 67 key features and 74 obstacles users experience when interacting with virtual conferencing technologies are identified from 60 related open-source journal articles from 5 digital library repositories.},
	language = {en},
	number = {12},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Hurst, William and Withington, Adam and Kolivand, Hoshang},
	month = may,
	year = {2022},
	keywords = {Artificial Intelligence, Covid-19, User experience, User journey mapping, Virtual conferencing},
	pages = {16901--16919},
	file = {Full Text PDF:files/1290/Hurst et al. - 2022 - Virtual conference design features and obstacles.pdf:application/pdf},
}

@article{koyama_precomputed_2019,
	title = {Precomputed optimal one-hop motion transition for responsive character animation},
	volume = {35},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-019-01693-8},
	doi = {10.1007/s00371-019-01693-8},
	abstract = {Characters in interactive 3D applications are often animated by creating transitions from one motion clip to another in response to user input. It is not trivial, however, to achieve quick, natural-looking transitions between two arbitrary motion clips, especially when the two motions are dissimilar. To tackle this problem, we present a simple framework called optimal one-hop motion transition, which creates quick, natural-looking transitions on the fly without requiring careful manual specifications. The key ideas are (1) to insert a short intermediate motion clip, called a hop, between the source and destination motion clips, and (2) to select such a hop motion clip and its temporal alignment in an optimal way by solving a search problem. In the search problem, our framework tries to balance the naturalness of the resulting transitions and the responsiveness to user input. This search can be precomputed and the results can be stored in a lookup table, making the runtime cost to play an optimal transition negligible. We demonstrate that our framework is easily integrated into a widely used game engine, and that it greatly improves the quality of transitions in practical scenarios.},
	language = {en},
	number = {6},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Koyama, Yuki and Goto, Masataka},
	month = jun,
	year = {2019},
	keywords = {Artificial Intelligence, Character animation, Motion graphs, Motion transition},
	pages = {1131--1142},
	file = {Full Text PDF:files/1291/Koyama e Goto - 2019 - Precomputed optimal one-hop motion transition for responsive character animation.pdf:application/pdf},
}

@article{lee_application_2023,
	title = {Application of virtual reality for peritoneal dialysis exchange learning in patients with end-stage renal disease and cognitive impairment},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-022-00728-1},
	doi = {10.1007/s10055-022-00728-1},
	abstract = {Cognitive impairment is not uncommon in patients with end-stage renal disease and can make it more difficult for these patients to carry out peritoneal dialysis (PD) on their own. Their attempts to do so may result in adverse consequences such as peritonitis. PD exchange is a complex procedure demanding knowledge and skill which requires close supervision and guidance by a renal nurse specialist. In this study, a non-immersive virtual reality (VR) training program using a Leap motion hand tracking device was developed to facilitate patients’ understanding and learning of the PD exchange procedure before attempting real task practice. This study was a two-center single-blinded randomized controlled trial on 23 incident PD patients. Patients in the experimental group received 8 sessions of VR training, while patients in the control were provided with printed educational materials. The results showed that there were significant differences between the two groups in performance of the overall PD exchange sequence, especially on the crucial steps. VR had a patient satisfaction rate of 89\%, and all patients preferred to have the VR aid incorporated in PD training. Our findings conclude VR can be a useful aid in the training and reinforcement of PD exchange procedures, with distinct merits of being free from restrictions of time, space, and manpower.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Lee, Connie M. S. and Fong, Kenneth N. K. and Mok, Maggie M. Y. and Lam, M. K. and Kung, Y. and Chan, Paven P. W. and Ma, Maggie K. M. and Lui, S. L. and Kwan, Lorraine P. Y. and Chu, W. L. and Hui, P. C. and Yau, Christina S. F. and Kwan, Ivan W. L. and Chan, Kelsey Y. M. and Chan, T. M.},
	month = sep,
	year = {2023},
	keywords = {Artificial Intelligence, Cognitive impairment, End-stage renal disease, Peritoneal dialysis, Virtual reality},
	pages = {1571--1583},
	file = {Full Text PDF:files/1292/Lee et al. - 2023 - Application of virtual reality for peritoneal dialysis exchange learning in patients with end-stage.pdf:application/pdf},
}

@article{calandra_immersive_2023,
	title = {Immersive virtual reality and passive haptic interfaces to improve procedural learning in a formal training course for first responders},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-022-00704-9},
	doi = {10.1007/s10055-022-00704-9},
	abstract = {One key aspect for the safety and success of first responders’ operations is the compliance, during the intervention, with all the safety procedures and prescribed behaviors. Although real-world simulation exercises are considered as the best way to verify if operators are ready to handle emergency situations, they are not always a viable approach. Firefighting courses, for example, do not usually include this kind of activities, due to the numerous hazards related to deploying controlled fires for the simulation. However, traditional training approaches based on class lessons and multimedia learning material may not be particularly effective for teaching practical skills and procedural behaviors. In this work, the use of a Virtual Reality Training Simulation (VRTS) combined with passive haptic interfaces and a real-time fire simulation logic is investigated as a complement to a traditional video-based training approach used in the context of forest firefighting. The teaching of safety concepts and correct use of individual firefighting tools was selected as a use case, and a user study involving 45 trainees was carried out in the context of an existing training course. One third of the trainees attended the traditional video-based lessons of the course, whereas the remaining ones also took part to a practice training session, half of them with the devised VRTS, the others in the real world. Experimental results showed that the additional use of the devised VRTS improved the trainees’ procedural learning, as well as their motivation and perceived quality of the overall learning experience.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Calandra, Davide and De Lorenzis, Federico and Cannavò, Alberto and Lamberti, Fabrizio},
	month = jun,
	year = {2023},
	keywords = {Artificial Intelligence, Fire simulation, First responders, Forest firefighting, Passive haptics, Video-based training, Virtual reality},
	pages = {985--1012},
	file = {Full Text PDF:files/1293/Calandra et al. - 2023 - Immersive virtual reality and passive haptic interfaces to improve procedural learning in a formal t.pdf:application/pdf},
}

@article{gomez-sirvent_assessment_2024,
	title = {Assessment of music performance anxiety in a virtual auditorium through the study of ambient lighting and audience distance},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-00947-8},
	doi = {10.1007/s10055-024-00947-8},
	abstract = {Performance anxiety is a common problem affecting musicians’ concentration and well-being. Musicians frequently encounter greater challenges and emotional discomfort when performing in front of an audience. Recent research suggests an important relationship between the characteristics of the built environment and people’s well-being. In this study, we explore modifying the built environment to create spaces where musicians are less aware of the presence of the audience and can express themselves more comfortably. An experiment was conducted with 61 conservatory musicians playing their instrument in a virtual auditorium in front of an audience of hundreds of virtual humans. They performed at different distances from the audience and under different levels of ambient lighting, while their eye movements were recorded. These data, together with questionnaires, were used to analyse the way the environment is perceived. The results showed that reducing the light intensity above the audience made the view of the auditorium more calming, and the same effect was observed when the distance between the audience and the musician was increased. Eye-tracking data showed a significant reduction in saccadic eye movements as the distance from the audience increased. This work provides a novel approach to architecture influence on musicians’ experience during solo performances. The findings are useful to designers and researchers.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Gómez-Sirvent, José L. and Fernández-Sotos, Alicia and Fernández-Caballero, Antonio and Fernández-Sotos, Desirée},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Eye tracking, Music, Neuroarchitecture, Performance anxiety, Virtual reality},
	pages = {71},
	file = {Full Text PDF:files/1294/Gómez-Sirvent et al. - 2024 - Assessment of music performance anxiety in a virtual auditorium through the study of ambient lightin.pdf:application/pdf},
}

@article{othman_usability_2022,
	title = {Usability evaluation of a virtual reality smartphone app for a living museum},
	volume = {21},
	issn = {1615-5297},
	url = {https://doi.org/10.1007/s10209-021-00820-4},
	doi = {10.1007/s10209-021-00820-4},
	abstract = {This paper elaborates the empirical evidence of a usability evaluation of a VR and non-VR virtual tour application for a living museum. The System Usability Scale (SUS) was used in between participants experiments (Group 1: non-VR version and Group 2: VR version) with 40 participants. The results show that the mean scores of all components for the VR version are higher compared to the non-VR version, overall SUS score (72.10 vs 68.10), usability score (75.50 vs 71.70), and learnability (58.40 vs 57.00). Further analysis using a two-tailed independent t test showed no difference between the non-VR and VR versions. Additionally, no significant difference was observed between the groups in the context of gender, nationality, and prior experience (other VR tour applications) for overall SUS score, usability score, and learnability score. Α two-tailed independent t test indicated no significant difference in the usability score between participants with VR experience and no VR experience. However, a significant difference was found between participants with VR experience and no VR experience for both SUS score (t(38) = 2.17, p = 0.037) and learnability score (t(38) = 2.40, p = 0.021). The independent t test results indicated a significant difference between participant with and without previous visits to SCV for the usability score (t(38) = −2.31, p = 0.027), while there was no significant differences observed in other components. It can be concluded that both versions passed based on the SUS score. However, the sub-scale usability and learnability scores indicated some usability issue.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Univ Access Inf Soc},
	author = {Othman, Mohd Kamal and Nogoibaeva, Altynai and Leong, Lai San and Barawi, Mohamad Hardyman},
	month = nov,
	year = {2022},
	keywords = {Artificial Intelligence, Google Cardboard, Mobile guide, Smartphone app, Usability, User experience, Virtual reality, Virtual tour},
	pages = {995--1012},
	file = {Full Text PDF:files/1295/Othman et al. - 2022 - Usability evaluation of a virtual reality smartphone app for a living museum.pdf:application/pdf},
}

@article{dobre_immersive_2022,
	title = {Immersive machine learning for social attitude detection in virtual reality narrative games},
	volume = {26},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-022-00644-4},
	doi = {10.1007/s10055-022-00644-4},
	abstract = {People can understand how human interaction unfolds and can pinpoint social attitudes such as showing interest or social engagement with a conversational partner. However, summarising this with a set of rules is difficult, as our judgement is sometimes subtle and subconscious. Hence, it is challenging to program Non-Player Characters (NPCs) to react towards social signals appropriately, which is important for immersive narrative games in Virtual Reality (VR). We collaborated with two game studios to develop an immersive machine learning (ML) pipeline for detecting social engagement. We collected data from participants-NPC interaction in VR, which was then annotated in the same immersive environment. Game design is a creative process and it is vital to respect designer’s creative vision and judgement. We therefore view annotation as a key part of the creative process. We trained a reinforcement learning algorithm (PPO) with imitation learning rewards using raw data (e.g. head position) and socially meaningful derived data (e.g. proxemics); we compared different ML configurations including pre-training and a temporal memory (LSTM). The pre-training and LSTM configuration using derived data performed the best (84\% F1-score, 83\% accuracy). The models using raw data did not generalise. Overall, this work introduces an immersive ML pipeline for detecting social engagement and demonstrates how creatives could use ML and VR to expand their ability to design more engaging experiences. Given the pipeline’s results for social engagement detection, we generalise it for detecting human-defined social attitudes.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Dobre, Georgiana Cristina and Gillies, Marco and Pan, Xueni},
	month = dec,
	year = {2022},
	keywords = {Artificial intelligence, Artificial Intelligence, Expressive body language, Gaming, Human–computer interaction, Virtual agents, Virtual reality},
	pages = {1519--1538},
	file = {Full Text PDF:files/1296/Dobre et al. - 2022 - Immersive machine learning for social attitude detection in virtual reality narrative games.pdf:application/pdf},
}

@article{reski_open_2020,
	title = {Open data exploration in virtual reality: a comparative study of input technology},
	volume = {24},
	issn = {1434-9957},
	shorttitle = {Open data exploration in virtual reality},
	url = {https://doi.org/10.1007/s10055-019-00378-w},
	doi = {10.1007/s10055-019-00378-w},
	abstract = {In this article, we compare three different input technologies (gamepad, vision-based motion controls, room-scale) for an interactive virtual reality (VR) environment. The overall system is able to visualize (open) data from multiple online sources in a unified interface, enabling the user to browse and explore displayed information in an immersive VR setting. We conducted a user interaction study (\$\$n=24\$\$; \$\$n=8\$\$ per input technology, between-group design) to investigate experienced workload and perceived flow of interaction. Log files and observations allowed further insights and comparison of each condition. We have identified trends that indicate user preference of a visual (virtual) representation, but no clear trends regarding the application of physical controllers (over vision-based controls), in a scenario that encouraged exploration with no time limitations.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Reski, Nico and Alissandrakis, Aris},
	month = mar,
	year = {2020},
	keywords = {3D gestural input, Artificial Intelligence, Comparative study, Gamepad, Room-scale virtual reality, Virtual reality, Vision-based motion controls},
	pages = {1--22},
	file = {Full Text PDF:files/1297/Reski e Alissandrakis - 2020 - Open data exploration in virtual reality a comparative study of input technology.pdf:application/pdf},
}

@article{gao_exploring_2023,
	title = {Exploring {Gender} {Differences} in {Computational} {Thinking} {Learning} in a {VR} {Classroom}: {Developing} {Machine} {Learning} {Models} {Using} {Eye}-{Tracking} {Data} and {Explaining} the {Models}},
	volume = {33},
	issn = {1560-4306},
	shorttitle = {Exploring {Gender} {Differences} in {Computational} {Thinking} {Learning} in a {VR} {Classroom}},
	url = {https://doi.org/10.1007/s40593-022-00316-z},
	doi = {10.1007/s40593-022-00316-z},
	abstract = {Understanding existing gender differences in the development of computational thinking skills is increasingly important for gaining valuable insights into bridging the gender gap. However, there are few studies to date that have examined gender differences based on the learning process in a realistic classroom context. In this work, we aim to investigate gender classification using students’ eye movements that reflect temporal human behavior during a computational thinking lesson in an immersive VR classroom. We trained several machine learning classifiers and showed that students’ eye movements provide discriminative information for gender classification. In addition, we employed a Shapley additive explanation (SHAP) approach for feature selection and further model interpretation. The classification model trained with the selected (best) eye movement feature set using SHAP achieved improved performance with an average accuracy of over \$\$70{\textbackslash}\%\$\$. The SHAP values further explained the classification model by identifying important features and their impacts on the model output, namely gender. Our findings provide insights into the use of eye movements for in-depth investigations of gender differences in learning activities in VR classroom setups that are ecologically valid and may provide clues for providing personalized learning support and tutoring in such educational systems or optimizing system design.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Int J Artif Intell Educ},
	author = {Gao, Hong and Hasenbein, Lisa and Bozkir, Efe and Göllner, Richard and Kasneci, Enkelejda},
	month = dec,
	year = {2023},
	keywords = {Artificial Intelligence, Computational thinking, Digital Education and Educational Technology, Explainable AI, Eye movements, Gender classification, Machine learning, Virtual reality},
	pages = {929--954},
	file = {Full Text PDF:files/1298/Gao et al. - 2023 - Exploring Gender Differences in Computational Thinking Learning in a VR Classroom Developing Machin.pdf:application/pdf},
}

@article{wang_behere_2023,
	title = {{BeHere}: a {VR}/{SAR} remote collaboration system based on virtual replicas sharing gesture and avatar in a procedural task},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {{BeHere}},
	url = {https://doi.org/10.1007/s10055-023-00748-5},
	doi = {10.1007/s10055-023-00748-5},
	abstract = {In this paper, we focus on the significance of remote collaboration using virtual replicas, avatar, and gesture on a procedural task in industry; thus, we present a Virtual Reality (VR)/Spatial Augmented Reality (SAR) remote collaboration system, BeHere, based on 3D virtual replicas and sharing gestures and avatar. BeHere enables a remote expert in VR to guide a local worker in real-time to complete a procedural task in the real-world. For the remote VR site, we construct a 3D virtual environment using virtual replicas, and the user can manipulate them by using gestures in an intuitive interaction and see their partners’ 3D virtual avatar. For the local site, we use SAR to enable the local worker to see instructions projected onto the real-world based on the shared virtual replicas and gestures. We conducted a formal user study to evaluate the prototype system in terms of performance, social presence, workload, and ranking and user preference. We found that the combination of visual cues of gestures, avatar, and virtual replicas plays a positive role in improving user experience, especially for remote VR users. More significantly, our study provides useful information and important design implications for further research on the use of gesture-, gaze- and avatar-based cues as well as virtual replicas in VR/AR remote collaboration on a procedural task in industry.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Wang, Peng and Wang, Yue and Billinghurst, Mark and Yang, Huizhen and Xu, Peng and Li, Yanhong},
	month = jun,
	year = {2023},
	keywords = {Artificial Intelligence, Hand gesture, Human–computer interaction, Remote collaboration, Spatial augmented reality, Virtual reality, Virtual replicas or avatar},
	pages = {1409--1430},
	file = {Full Text PDF:files/1299/Wang et al. - 2023 - BeHere a VRSAR remote collaboration system based on virtual replicas sharing gesture and avatar in.pdf:application/pdf},
}

@article{radhakrishnan_investigating_2023,
	title = {Investigating the effectiveness of immersive {VR} skill training and its link to physiological arousal},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-022-00699-3},
	doi = {10.1007/s10055-022-00699-3},
	abstract = {This paper details the motivations, design, and analysis of a study using a fine motor skill training task in both VR and physical conditions. The objective of this between-subjects study was to (a) investigate the effectiveness of immersive virtual reality for training participants in the ‘buzz-wire’ fine motor skill task compared to physical training and (b) investigate the link between participants’ arousal with their improvements in task performance. Physiological arousal levels in the form of  electro-dermal activity (EDA) and  ECG (Electrocardiogram) data were collected from 87 participants, randomly distributed across the two conditions. Results indicated that VR training is as good as, or even slightly better than, training in physical training in improving task performance. Moreover, the participants in the VR condition reported an increase in self-efficacy and immersion, while marginally significant differences were observed in the presence and the temporal demand (retrieved from NASA-TLX measurements). Participants in the VR condition showed on average less arousal than those in the physical condition. Though correlation analyses between performance metrics and arousal levels did not depict any statistically significant results, a closer examination of EDA values revealed that participants with lower arousal levels during training, across conditions, demonstrated better improvements in performance than those with higher arousal. These findings demonstrate the effectiveness of VR in training and the potential of using arousal and training performance data for designing adaptive VR training systems. This paper also discusses implications for researchers who consider using biosensors and VR for motor skill experiments.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Radhakrishnan, Unnikrishnan and Chinello, Francesco and Koumaditis, Konstantinos},
	month = jun,
	year = {2023},
	keywords = {Artificial Intelligence, Electro-dermal activity, Heart rate variability, Immersive virtual reality, Physiological arousal, Skill training},
	pages = {1091--1115},
	file = {Full Text PDF:files/1300/Radhakrishnan et al. - 2023 - Investigating the effectiveness of immersive VR skill training and its link to physiological arousal.pdf:application/pdf},
}

@article{cannavo_immersive_2024,
	title = {Immersive movies: the effect of point of view on narrative engagement},
	volume = {39},
	issn = {1435-5655},
	shorttitle = {Immersive movies},
	url = {https://doi.org/10.1007/s00146-022-01622-9},
	doi = {10.1007/s00146-022-01622-9},
	abstract = {Cinematic virtual reality (CVR) offers filmmakers a wide range of possibilities to explore new techniques regarding movie scripting, shooting and editing. Despite the many experiments performed so far both with both live action and computer-generated movies, just a few studies focused on analyzing how the various techniques actually affect the viewers’ experience. Like in traditional cinema, a key step for CVR screenwriters and directors is to choose from which perspective the viewers will see the scene, the so-called point of view (POV). The aim of this paper is to understand to what extent watching an immersive movie from a specific POV could impact the narrative engagement (NE), i.e., the viewers’ sensation of being immersed in the movie environment and being connected with its characters and story. Two POVs that are typically used in CVR, i.e., first-person perspective (1-PP) and external perspective (EP), are investigated through a user study in which both objective and subjective metrics were collected. The user study was carried out by leveraging two live action 360° short films with distinct scripts. The results suggest that the 1-PP experience could be more pleasant than the EP one in terms of overall NE and narrative presence, or even for all the NE dimensions if the potential of that POV is specifically exploited.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {AI \& Soc},
	author = {Cannavò, Alberto and Castiello, Antonio and Pratticò, F. Gabriele and Mazali, Tatiana and Lamberti, Fabrizio},
	month = aug,
	year = {2024},
	keywords = {360°, Artificial Intelligence, Cinematic VR, External perspective, First-person perspective, Immersive videos, Omnidirectional, User study},
	pages = {1811--1825},
	file = {Full Text PDF:files/1321/Cannavò et al. - 2024 - Immersive movies the effect of point of view on narrative engagement.pdf:application/pdf},
}

@article{dzardanova_virtual_2022,
	title = {Virtual reality as a communication medium: a comparative study of forced compliance in virtual reality versus physical world},
	volume = {26},
	issn = {1434-9957},
	shorttitle = {Virtual reality as a communication medium},
	url = {https://doi.org/10.1007/s10055-021-00564-9},
	doi = {10.1007/s10055-021-00564-9},
	abstract = {There are reasons to consider virtual reality (VR) as a newly arrived communication medium that ought to be differentiated from all other forms of mediated communication, since it is the first and only medium with the potential to enable incorporation of the full spectrum of both verbal and non-verbal cues. The present paper is part of a broader scheme in investigating potential differentiations in interpersonal communication between the physical world  and VR. Our experimental design builds upon the existing knowledge base of forced compliance experiments; the set-up involved a comparative study of two groups (N = 46) performing tasks under the authoritative influence of a researcher who applied persuasion techniques. Results indicate that VR-mediated communication is as intricate as face to face, since subjects were equally or more compliant, with the nature of information exchanged (e.g. fact-based, morality-based, etc.) being a contributing factor, whilst exemplifying under-development and future applications of VR collaborative environments.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Dzardanova, Elena and Kasapakis, Vlasios and Gavalas, Damianos and Sylaiou, Stella},
	month = jun,
	year = {2022},
	keywords = {Artificial Intelligence, Cognitive dissonance, Computer-mediated communication, Face-to-face communication, Forced compliance, Virtual reality, VR-mediated communication},
	pages = {737--757},
	file = {Full Text PDF:files/1322/Dzardanova et al. - 2022 - Virtual reality as a communication medium a comparative study of forced compliance in virtual reali.pdf:application/pdf},
}

@article{kwon_verification_2019,
	title = {Verification of the possibility and effectiveness of experiential learning using {HMD}-based immersive {VR} technologies},
	volume = {23},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-018-0364-1},
	doi = {10.1007/s10055-018-0364-1},
	abstract = {This paper examines the possibility of experiential learning in a virtual space using head-mounted-display-based immersive virtual reality (VR) technologies. Experiential learning refers to learning through direct experiences in the context of learning. Realistically, experiential learning is impossible in most cases, but VR technologies allowing direct interaction with virtual environments and objects are being developed and commercialized. These technologies are predicted to enhance vividness, interactivity, presence, flow, and experientiality, and increase the expectations of the possibility of experiential learning using VR. Thus, in this study, an experiment was conducted to verify such possibility. The analysis of the experiment results showed that the tactile interactivity and presence improved with the use of enhanced interaction technologies in VR, and in terms of experientiality, the experiment participants became highly aware of the “exploratory stage,” referring to the level of experience of being exposed to an interesting site and directly touching an object in the currently enhanced VR in providing direct tactile and locomotive interactivity. Furthermore, the fact that the learning effect is also partially enhanced was discovered. Accordingly, it was determined that experiential learning using VR is possible based on the experiment results, which showed that the enhanced vividness and interactivity of VR technologies allow the users to closely recognize virtual experiences as direct experiences, and that the learning effect is enhanced. It was also determined that experiential learning in a virtual environment that is identical to an experience in reality would be made possible in the near future based on continued technological development.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Kwon, Chongsan},
	month = mar,
	year = {2019},
	keywords = {Artificial Intelligence, Authentic virtual reality, Experiential learning, Locomotive interactivity, Presence, Tactile interactivity, Virtual reality},
	pages = {101--118},
	file = {Full Text PDF:files/1323/Kwon - 2019 - Verification of the possibility and effectiveness of experiential learning using HMD-based immersive.pdf:application/pdf},
}

@article{dubey_information_2021,
	title = {Information {Theoretic} {Model} to {Simulate} {Agent}-{Signage} {Interaction} for {Wayfinding}},
	volume = {13},
	issn = {1866-9964},
	url = {https://doi.org/10.1007/s12559-019-09689-1},
	doi = {10.1007/s12559-019-09689-1},
	abstract = {Signage systems are critical for communicating spatial information during wayfinding among a plethora of noise in the environment. A proper signage system can improve wayfinding performance and user experience by reducing the perceived complexity of the environment. However, previous models of sign-based wayfinding do not incorporate realistic noise or quantify the reduction in perceived complexity from the use of signage. Drawing upon concepts from information theory, we propose and validate a new agent-signage interaction model that quantifies available wayfinding information from signs for wayfinding. We conducted two online crowd-sourcing experiments to compute the distribution of a sign’s visibility and an agent’s decision-making confidence as a function of observation angle and viewing distance. We then validated this model using a virtual reality (VR) experiment with trajectories from human participants. The crowd-sourcing experiments provided a distribution of decision-making entropy (conditioned on visibility) that can be applied to any sign/environment. From the VR experiment, a training dataset of 30 trajectories was used to refine our model, and the remaining test dataset of 10 trajectories was compared with agent behavior using dynamic time warping (DTW) distance. The results revealed a reduction of 38.76\% in DTW distance between the average trajectories before and after refinement. Our refined agent-signage interaction model provides realistic predictions of human wayfinding behavior using signs. These findings represent a first step towards modeling human wayfinding behavior in complex real environments in a manner that can incorporate several additional random variables (e.g., environment layout).},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Cogn Comput},
	author = {Dubey, Rohit K. and Thrash, Tyler and Kapadia, Mubbasir and Hoelscher, Christoph and Schinazi, Victor R.},
	month = jan,
	year = {2021},
	keywords = {Agent-based model, Artificial intelligence, Artificial Intelligence, Cognitive agent, Entropy, Information theory, Signage system},
	pages = {189--206},
	file = {Full Text PDF:files/1324/Dubey et al. - 2021 - Information Theoretic Model to Simulate Agent-Signage Interaction for Wayfinding.pdf:application/pdf},
}

@article{ruiz_why_2023,
	title = {Why don’t we trace? {A} study on the barriers to software traceability in practice},
	volume = {28},
	issn = {1432-010X},
	shorttitle = {Why don’t we trace?},
	url = {https://doi.org/10.1007/s00766-023-00408-9},
	doi = {10.1007/s00766-023-00408-9},
	abstract = {Researchers have proposed numerous tools, methods, and techniques for establishing and maintaining software traceability. Despite its acknowledged importance, researchers argue that traceability is still “a sought-after, yet often elusive quality in software-intensive systems”. We have little evidence regarding how creating, managing, and using traceability links vary depending on factors such as organizational contexts, software development practices, and project types. We conduct an empirical study where software development practitioners express their perception regarding the value of software traceability. Via an online survey, 55 participants provided information related to their current traceability practices and needs. Furthermore, we interviewed 14 practitioners to gain a more in-depth understanding. Our study investigates the effect of two independent variables: the software development paradigm and the type of developed software system. Among the several identified findings, our analysis reveals that, although the traceability costs are an inhibitor for adopting more mature traceability practices, the respondents believe that the expected benefits still outweigh envisioned costs. Traceability is mainly performed manually: not only are automated trace retrieval tools scarce, but their offered automation is not expected to replace human involvement.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Requirements Eng},
	author = {Ruiz, Marcela and Hu, Jin Yang and Dalpiaz, Fabiano},
	month = dec,
	year = {2023},
	keywords = {Agile development, Artificial Intelligence, Exploratory study, Safety-critical systems, Software traceability},
	pages = {619--637},
	file = {Full Text PDF:files/1325/Ruiz et al. - 2023 - Why don’t we trace A study on the barriers to software traceability in practice.pdf:application/pdf},
}

@article{li_adapip_2024,
	title = {{AdaPIP}: {Adaptive} picture-in-picture guidance for 360° film watching},
	volume = {10},
	issn = {2096-0662},
	shorttitle = {{AdaPIP}},
	url = {https://doi.org/10.1007/s41095-023-0347-3},
	doi = {10.1007/s41095-023-0347-3},
	abstract = {360° videos enable viewers to watch freely from different directions but inevitably prevent them from perceiving all the helpful information. To mitigate this problem, picture-in-picture (PIP) guidance was proposed using preview windows to show regions of interest (ROIs) outside the current view range. We identify several drawbacks of this representation and propose a new method for 360° film watching called AdaPIP. AdaPIP enhances traditional PIP by adaptively arranging preview windows with changeable view ranges and sizes. In addition, AdaPIP incorporates the advantage of arrow-based guidance by presenting circular windows with arrows attached to them to help users locate the corresponding ROIs more efficiently. We also adapted AdaPIP and Outside-In to HMD-based immersive virtual reality environments to demonstrate the usability of PIP-guided approaches beyond 2D screens. Comprehensive user experiments on 2D screens, as well as in VR environments, indicate that AdaPIP is superior to alternative methods in terms of visual experiences while maintaining a comparable degree of immersion.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Comp. Visual Media},
	author = {Li, Yi-Xiao and Luo, Guan and Xu, Yi-Ke and He, Yu and Zhang, Fang-Lue and Zhang, Song-Hai},
	month = jun,
	year = {2024},
	keywords = {360° videos, Artificial Intelligence, picture-in-picture (PIP), virtual reality (VR), visual guidance},
	pages = {487--503},
	file = {Full Text PDF:files/1326/Li et al. - 2024 - AdaPIP Adaptive picture-in-picture guidance for 360° film watching.pdf:application/pdf},
}

@article{tatnall_editorial_2020,
	title = {Editorial for {EAIT} issue 4, 2020},
	volume = {25},
	issn = {1573-7608},
	url = {https://doi.org/10.1007/s10639-020-10251-y},
	doi = {10.1007/s10639-020-10251-y},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Tatnall, Arthur},
	month = jul,
	year = {2020},
	pages = {2321--2335},
	file = {Full Text PDF:files/1327/Tatnall - 2020 - Editorial for EAIT issue 4, 2020.pdf:application/pdf},
}

@article{cliffe_materialising_2021,
	title = {Materialising contexts: virtual soundscapes for real-world exploration},
	volume = {25},
	issn = {1617-4917},
	shorttitle = {Materialising contexts},
	url = {https://doi.org/10.1007/s00779-020-01405-3},
	doi = {10.1007/s00779-020-01405-3},
	abstract = {This article presents the results of a study based on a group of participants’ interactions with an experimental sound installation at the National Science and Media Museum in Bradford, UK. The installation used audio augmented reality to attach virtual sound sources to a vintage radio receiver from the museum’s collection, with a view to understanding the potentials of this technology for promoting exploration and engagement within museums and galleries. We employ a practice-based design ethnography, including a thematic analysis of our participants’ interactions with spatialised interactive audio, and present an identified sequence of interactional phases. We discuss how audio augmented artefacts can communicate and engage visitors beyond their traditional confines of line-of-sight, and how visitors can be drawn to engage further, beyond the realm of their original encounter. Finally, we provide evidence of how contextualised and embodied interactions, along with authentic audio reproduction, evoked personal memories associated with our museum artefact, and how this can promote interest in the acquisition of declarative knowledge. Additionally, through the adoption of a functional and theoretical aura-based model, we present ways in which this could be achieved, and, overall, we demonstrate a material object’s potential role as an interface for engaging users with, and contextualising, immaterial digital audio archival content.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Pers Ubiquit Comput},
	author = {Cliffe, Laurence and Mansell, James and Greenhalgh, Chris and Hazzard, Adrian},
	month = aug,
	year = {2021},
	keywords = {Artificial Intelligence, Audio augmented reality, Cultural, Experience, Soundscape},
	pages = {623--636},
	file = {Full Text PDF:files/1328/Cliffe et al. - 2021 - Materialising contexts virtual soundscapes for real-world exploration.pdf:application/pdf},
}

@article{konik_learning_2006,
	title = {Learning goal hierarchies from structured observations and expert annotations},
	volume = {64},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-006-7734-8},
	doi = {10.1007/s10994-006-7734-8},
	abstract = {We describe a relational learning by observation framework that automatically creates cognitive agent programs that model expert task performance in complex dynamic domains. Our framework uses observed behavior and goal annotations of an expert as the primary input, interprets them in the context of background knowledge, and returns an agent program that behaves similar to the expert. We map the problem of creating an agent program on to multiple learning problems that can be represented in a “supervised concept learning’’ setting. The acquired procedural knowledge is partitioned into a hierarchy of goals and represented with first order rules. Using an inductive logic programming (ILP) learning component allows our framework to naturally combine structured behavior observations, parametric and hierarchical goal annotations, and complex background knowledge. To deal with the large domains we consider, we have developed an efficient mechanism for storing and retrieving structured behavior data. We have tested our approach using artificially created examples and behavior observation traces generated by AI agents. We evaluate the learned rules by comparing them to hand-coded rules.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Mach Learn},
	author = {Könik, Tolga and Laird, John E.},
	month = sep,
	year = {2006},
	keywords = {Artificial Intelligence, Behavioral cloning, Cognitive agent architectures, Inductive logic programming (ILP), Relational learning, Relational learning by observation},
	pages = {263--287},
	file = {Full Text PDF:files/1329/Könik e Laird - 2006 - Learning goal hierarchies from structured observations and expert annotations.pdf:application/pdf},
}

@article{terkaj_framework_2024,
	title = {A framework for virtual learning in industrial engineering education: development of a reconfigurable virtual learning factory application},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {A framework for virtual learning in industrial engineering education},
	url = {https://doi.org/10.1007/s10055-024-01042-8},
	doi = {10.1007/s10055-024-01042-8},
	abstract = {Advances in digital factory technologies are offering great potential to innovate higher education, by enabling innovative learning approaches based on virtual laboratories that increase the involvement of students while delivering realistic experiences. This article introduces a framework for the development of virtual learning applications by addressing multidisciplinary requirements. The implementation of the framework can be eased by the use of the proposed virtual learning factory application (VLFA), an open-source solution that takes advantage of virtual reality to support innovative higher-education learning activities in industrial engineering. A complete design and development workflow is described, starting from the identification of the requirements, to the design of software modules and underlying technologies, up to the final implementation. The framework and the VLFA have been tested to implement a serious game related to the design and analysis of manufacturing systems, also collecting the feedback of students and teachers.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Terkaj, Walter and Urgo, Marcello and Kovács, Péter and Tóth, Erik and Mondellini, Marta},
	month = aug,
	year = {2024},
	keywords = {Artificial Intelligence, Digital factory, Higher-education, Virtual laboratory, Virtual reality},
	pages = {148},
	file = {Full Text PDF:files/1330/Terkaj et al. - 2024 - A framework for virtual learning in industrial engineering education development of a reconfigurabl.pdf:application/pdf},
}

@article{zhang_usability_2024,
	title = {Usability of visualizing position and orientation deviations for manual precise manipulation of objects in augmented reality},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-01030-y},
	doi = {10.1007/s10055-024-01030-y},
	abstract = {Manual precise manipulation of objects is an essential skill in everyday life, and Augmented Reality (AR) is increasingly being used to support such operations. In this study, we investigate whether detailed visualizations of position and orientation deviations are helpful for AR-assisted manual precise manipulation of objects. We developed three AR instructions with different visualizations of deviations: the logical deviation baseline instruction, the precise numerical deviations-based instruction, and the intuitive color-mapped deviations-based instruction. All three instructions visualized the required directions for manipulation and the logical values of whether the object met the accuracy requirements. Additionally, the latter two instructions provided detailed visualizations of deviations through numerical text and color-mapping respectively. A user study was conducted with 18 participants to compare the three AR instructions. The results showed that there were no significant differences found in speed, accuracy, perceived ease-of-use, and perceived workload between the three AR instructions. We found that the visualizations of the required directions for manipulation and the logical values of whether the object met the accuracy requirements were sufficient to guide manual precise manipulation. The detailed visualizations of the real-time deviations could not improve the speed and accuracy of manual precise manipulation, and although they could improve the perceived ease-of-use and user experience, the effects were not significant. Based on the results, several recommendations were provided for designing AR instructions to support precise manual manipulation.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Zhang, Xiaotian and He, Weiping and Billinghurst, Mark and Qin, Yunfei and Yang, Lingxiao and Liu, Daisong and Wang, Zenglei},
	month = jul,
	year = {2024},
	keywords = {Artificial Intelligence, Augmented reality, Manual manipulation, Precise manipulation, Visualization},
	pages = {134},
	file = {Full Text PDF:files/1331/Zhang et al. - 2024 - Usability of visualizing position and orientation deviations for manual precise manipulation of obje.pdf:application/pdf},
}

@article{pietra_promoting_2021,
	title = {Promoting eco-driving behavior through multisensory stimulation: a preliminary study on the use of visual and haptic feedback in a virtual reality driving simulator},
	volume = {25},
	issn = {1434-9957},
	shorttitle = {Promoting eco-driving behavior through multisensory stimulation},
	url = {https://doi.org/10.1007/s10055-021-00499-1},
	doi = {10.1007/s10055-021-00499-1},
	abstract = {This paper describes the design and preliminary test of a virtual reality driving simulator capable of conveying haptic and visual messages to promote eco-sustainable driving behavior. The driving simulator was implemented through the Unity game engine; a large street environment, including high-speed and urban sections, was created to examine different driving behaviors. The hardware setup included a gaming driving seat, equipped with a steering wheel and pedals; the virtual scenarios were displayed through an Oculus Rift headset to guarantee an immersive experience. Haptic stimulation (i.e., vibrations) was delivered to the driver through the accelerator pedal, while visual stimuli (i.e., icons and colors) were shown on a virtual head-up display. The sensory feedbacks were presented both alone and in combination, providing information about excessive acceleration and speed. Four different virtual scenarios, each one including a distracting element (i.e., navigator, rain, call, and traffic), were also created. Ten participants tested the simulator. Fuel consumption was evaluated by calculating a mean power index (MPI) in reference to the sensory feedback presentation; physiological reactions and responses to a usability survey were also collected. The results revealed that the haptic and visuo-haptic feedback were responsible for an MPI reduction, respectively, for 14\% and 11\% compared with a condition of no feedback presentation; while visual feedback alone resulted in an MPI increase of 11\%. The efficacy of haptic feedback was also accompanied by a more relaxing physiological state of the users, compared with the visual stimulation. The system’s usability was adequate, although haptic stimuli were rated slightly more intrusive than the visual ones. Overall, these preliminary results highlight how promising the use of the haptic channel can be in communicating and guiding the driver toward a more eco-sustainable behavior.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Pietra, Andrea and Vazquez Rull, Marina and Etzi, Roberta and Gallace, Alberto and Scurati, Giulia Wally and Ferrise, Francesco and Bordegoni, Monica},
	month = dec,
	year = {2021},
	keywords = {Artificial Intelligence, Eco-driving, Haptics, Multisensory, Virtual reality},
	pages = {945--959},
	file = {Full Text PDF:files/1332/Pietra et al. - 2021 - Promoting eco-driving behavior through multisensory stimulation a preliminary study on the use of v.pdf:application/pdf},
}

@article{valtchev_domain_2021,
	title = {Domain randomization for neural network classification},
	volume = {8},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-021-00455-5},
	doi = {10.1186/s40537-021-00455-5},
	abstract = {Large data requirements are often the main hurdle in training neural networks. Convolutional neural network (CNN) classifiers in particular require tens of thousands of pre-labeled images per category to approach human-level accuracy, while often failing to generalized to out-of-domain test sets. The acquisition and labelling of such datasets is often an expensive, time consuming and tedious task in practice. Synthetic data provides a cheap and efficient solution to assemble such large datasets. Using domain randomization (DR), we show that a sufficiently well generated synthetic image dataset can be used to train a neural network classifier that rivals state-of-the-art models trained on real datasets, achieving accuracy levels as high as 88\% on a baseline cats vs dogs classification task. We show that the most important domain randomization parameter is a large variety of subjects, while secondary parameters such as lighting and textures are found to be less significant to the model accuracy. Our results also provide evidence to suggest that models trained on domain randomized images transfer to new domains better than those trained on real photos. Model performance appears to remain stable as the number of categories increases.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {J Big Data},
	author = {Valtchev, Svetozar Zarko and Wu, Jianhong},
	month = jul,
	year = {2021},
	keywords = {Artificial Intelligence, Domain randomization, Neural network classifiers, Synthetic image generation},
	pages = {94},
	file = {Full Text PDF:files/1333/Valtchev e Wu - 2021 - Domain randomization for neural network classification.pdf:application/pdf},
}

@article{harris_attention_2024,
	title = {Attention computing for enhanced visuomotor skill performance: {Testing} the effectiveness of gaze-adaptive cues in virtual reality golf putting},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {Attention computing for enhanced visuomotor skill performance},
	url = {https://doi.org/10.1007/s11042-023-17973-4},
	doi = {10.1007/s11042-023-17973-4},
	abstract = {This work explored how immersive technologies like virtual reality can be exploited for improved motor learning. While virtual reality is becoming a practical replacement for training that is otherwise expensive, dangerous, or inconvenient to deliver, virtual simulations can also enhance the learning process. Based on the concept of ‘attention computing’, we developed and tested a novel ‘gaze-adaptive’ training method within a virtual putting environment augmented with eye and motion tracking. To our knowledge, this work is the first application of attention computing and adaptive virtual reality to sports skill training. Novice golfers were randomly assigned to either standard putting practice in virtual reality (control) or gaze-adaptive training conditions. For gaze-adaptive training, the golf ball was sensitive to the participant’s gaze and illuminated when fixated upon, to prompt longer and more stable pre-shot fixations. We recorded the effect of these training conditions on task performance, gaze control, and putting kinematics. Gaze-adaptive training was successful in generating more expert-like gaze control and putting kinematics, although this did not transfer to improved performance outcomes within the abbreviated training paradigm. These findings suggest that gaze-adaptive environments can enhance visuomotor learning and may be a promising method for augmenting virtual training environments.},
	language = {en},
	number = {21},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Harris, David and Donaldson, Ross and Bray, Max and Arthur, Tom and Wilson, Mark and Vine, Sam},
	month = jun,
	year = {2024},
	keywords = {Adaptive VR, Artificial Intelligence, Attention, Attention computing, Eye movement modelling, Eye movement training},
	pages = {60861--60879},
	file = {Full Text PDF:files/1334/Harris et al. - 2024 - Attention computing for enhanced visuomotor skill performance Testing the effectiveness of gaze-ada.pdf:application/pdf},
}

@article{dallakoti_mixed_2024,
	title = {Mixed reality in surgical telepresence: a novel extended mean value cloning with automatic trimap generation and accurate alpha matting for visualization},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {Mixed reality in surgical telepresence},
	url = {https://doi.org/10.1007/s11042-023-17331-4},
	doi = {10.1007/s11042-023-17331-4},
	abstract = {The aim of this research is to propose an extended mean value cloning algorithm with automatic trimap generation and accurate alpha matting. This implementation improves the visualization accuracy of the merged video by reducing the discolored and smudging artefacts of the remote surgeon’s boundary. It also makes the merge robust for the illumination changes by taking less processing time in real time surgery. The proposed system uses automatic trimap generation from the source video for accurate foreground extraction. Extended mean value cloning with gradient mixing is then applied for the cloning with optimized alpha matting for accurate and realistic video composition. The proposed system improved the visualization accuracy by providing almost 99.7\% visibility of the pixels compared to the state-of-the-art solution, which provides 99.1\% visibility of pixels. The overlay error was reduced from 0.93 mm to 0.63 mm. The processing time was also reduced. The proposed solution processed 8 frames per second, which is less time than the state-of-the-art solution, which processed 5 frames per second. The extended mean value cloning smooths the differences that presented in the target and source frames for seamless and realistic blending of pixels. The automatic trimap generation reduced the risk of false foreground selection and the generated optimal trimaps improved the alpha matte quality, which is optimized to reduce the smudging artefacts completely and to produce accurate visualization of the final merged image.},
	language = {en},
	number = {17},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Dallakoti, Roshan and Alsadoon, Abeer and Prasad, P. W. C. and Al Aloussi, Sarmad and Rashid, Tarik A. and Alsadoon, Omar Hisham and Alrubaie, Ahmad and Haddad, Sami},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Automatic trimap generation, Contour flow, Gradient mixing, Image matting, Mean value cloning, Video composition},
	pages = {49845--49874},
	file = {Full Text PDF:files/1335/Dallakoti et al. - 2024 - Mixed reality in surgical telepresence a novel extended mean value cloning with automatic trimap ge.pdf:application/pdf},
}

@article{ma_summary_2022,
	title = {Summary of {Energy} {Informatics}.{Academy} {Conference} 2022},
	volume = {5},
	issn = {2520-8942},
	url = {https://doi.org/10.1186/s42162-022-00220-9},
	doi = {10.1186/s42162-022-00220-9},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Energy Inform},
	author = {Ma, Zheng and Jørgensen, Birte Holst and Chen, Guangchao and Madsen, Henrik and Duan, Hongbo and da Silva, Luiz Carlos Pereira and Jørgensen, Bo Nørregaard},
	month = dec,
	year = {2022},
	pages = {37},
	file = {Full Text PDF:files/1336/Ma et al. - 2022 - Summary of Energy Informatics.Academy Conference 2022.pdf:application/pdf},
}

@article{datcu_using_2015,
	title = {Using augmented reality for supporting information exchange in teams from the security domain},
	volume = {4},
	issn = {2190-8532},
	url = {https://doi.org/10.1186/s13388-015-0025-9},
	doi = {10.1186/s13388-015-0025-9},
	abstract = {The performance of operational teams in the security domain relies on quick and adequate exchange of context-related information. Currently, this information exchange is mainly based on oral communication. This paperreports on different scenarios from the security domain in which augmented reality (AR) techniques are used to support such information exchange. The scenarios have been elicited using an end-user centred design approach. To support these scenarios, an AR environment has been developed and the usability of the AR support has been evaluated with experts from different operational units in the security domain. The first evaluation shows that the scenarios are well defined and the AR environment can successfully support information exchange in teams operating in the security domain.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Secur Inform},
	author = {Datcu, Dragos and Lukosch, Stephan and Lukosch, Heide and Cidota, Marina},
	month = dec,
	year = {2015},
	keywords = {Artificial Intelligence, Augmented reality, Information exchange, Teamwork, Usability},
	pages = {10},
	file = {Full Text PDF:files/1337/Datcu et al. - 2015 - Using augmented reality for supporting information exchange in teams from the security domain.pdf:application/pdf},
}

@article{park_agent-based_2012,
	title = {An agent-based model and computational framework for counter-terrorism and public safety based on swarm intelligencea},
	volume = {1},
	issn = {2190-8532},
	url = {https://doi.org/10.1186/2190-8532-1-23},
	doi = {10.1186/2190-8532-1-23},
	abstract = {Public safety has been a great concern in recent years as terrorism occurs everywhere. When a public event is held in an urban environment like Olympic games or soccer games, it is important to keep the public safe and at the same time, to have a specific plan to control and rescue the public in the case of a terrorist attack. In order to better position public safety in communities against potential threats, it is of utmost importance to identify existing gaps, define priorities and focus on developing approaches to address those.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Secur Inform},
	author = {Park, Andrew J. and Tsang, Herbert H. and Sun, Mengting and Glässer, Uwe},
	month = dec,
	year = {2012},
	keywords = {Artificial Intelligence, Crowd Behavior, Game Engine, Swarm Intelligence, Terrorist Attack, Visualization Module},
	pages = {23},
	file = {Full Text PDF:files/1338/Park et al. - 2012 - An agent-based model and computational framework for counter-terrorism and public safety based on sw.pdf:application/pdf},
}

@article{de_paiva_guimaraes_olfactory_2022,
	title = {An olfactory display for virtual reality glasses},
	volume = {28},
	issn = {1432-1882},
	url = {https://doi.org/10.1007/s00530-022-00908-8},
	doi = {10.1007/s00530-022-00908-8},
	abstract = {Olfaction has not been explored in virtual reality environments to the same extent as the visual and auditory senses. Much less research has been done with olfactory devices, and very few of them can be easily integrated into virtual reality applications. The inclusion of odor into virtual reality simulations using a chemical device involves challenges such as possible diffusion into undesired areas, slow dissipation, the definition of various parameters (e.g., concentration, frequency, and duration), and an appropriate software solution for controlling the diffusion of the odor. This paper aims to present a non-intrusive, mobile, low cost and wearable olfactory display, and a software service that allows the developer to easily create applications that include olfactory stimuli integrated with virtual reality headset glasses. We also present a case study conducted with 32 people to evaluate their satisfaction when using the olfactory display. Our findings indicate that our solution works as expected, producing odor properly and being easy to integrate to applications.},
	language = {en},
	number = {5},
	urldate = {2024-09-05},
	journal = {Multimedia Systems},
	author = {de Paiva Guimarães, Marcelo and Martins, James Miranda and Dias, Diego Roberto Colombo and Guimarães, Rita de Fátima Rodrigues and Gnecco, Bruno Barberi},
	month = oct,
	year = {2022},
	keywords = {Odor, Olfactory display, Virtual reality},
	pages = {1573--1583},
	file = {Full Text PDF:files/1339/de Paiva Guimarães et al. - 2022 - An olfactory display for virtual reality glasses.pdf:application/pdf},
}

@article{goli_architectural_2022,
	title = {Architectural design game: {A} serious game approach to promote teaching and learning using multimodal interfaces},
	volume = {27},
	issn = {1573-7608},
	shorttitle = {Architectural design game},
	url = {https://doi.org/10.1007/s10639-022-11062-z},
	doi = {10.1007/s10639-022-11062-z},
	abstract = {The present article introduces and develops an educational tool as an interactive digital game for architectural design, allowing the architectural students to challenge their knowledge and experiences. The framework of this educational tool supports a serious open-ended game, which helps students get involved with the game through self-assessment and a multi-modal natural user interface, including gesture recognition and speech recognition in a familiar CAD environment without any right or wrong solutions. The students can immediately compare their game results with the architecture of iconic buildings and get familiar with the complexity of the design process through five different games in the initial version of this tool without the fear of being judged. According to the results of the questionnaire, this tool can simulate the design process, enhance its quality, and thus, assist the learners with developing their required skills with a wide variety of motivations and opportunities for engagement while helping them connect their experiences and activities to their learning and development in a meaningful way to fill the gap between their knowledge acquisition and knowledge application.},
	language = {en},
	number = {8},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Goli, Amir and Teymournia, Fatemeh and Naemabadi, Maedeh and Garmaroodi, Ali Andaji},
	month = sep,
	year = {2022},
	keywords = {Artificial Intelligence, Digital Education and Educational Technology, Early years education, Human-computer interaction, Interaction design, Interactive learning, Interdisciplinarity, Open-ended game},
	pages = {11467--11498},
	file = {Full Text PDF:files/1340/Goli et al. - 2022 - Architectural design game A serious game approach to promote teaching and learning using multimodal.pdf:application/pdf},
}

@article{hall_learning_2015,
	title = {Learning to {Overcome} {Cultural} {Conflict} through {Engaging} with {Intelligent} {Agents} in {Synthetic} {Cultures}},
	volume = {25},
	issn = {1560-4306},
	url = {https://doi.org/10.1007/s40593-014-0031-y},
	doi = {10.1007/s40593-014-0031-y},
	abstract = {Providing opportunities for children to engage with intercultural learning has frequently focused on exposure to the ritual, celebrations and festivals of cultures, with the view that such experiences will result in greater acceptance of cultural differences. Intercultural conflict is often avoided, bringing as it does particular pedagogical, ethical and political dilemmas of which cultures we place in conflict in the multicultural classroom. In this paper we discuss an alternative approach, providing children with an interactive learning experience with synthetic cultures and characters. The agent architecture developed to enable intelligent agents to exhibit culturally appropriate affect and behaviours is outlined. MIXER, an experiential learning application developed for 9–11 year old children on intercultural conflict is described, highlighting the learning goals and approaches. A school-based evaluation of MIXER with 144 UK children is presented. Children demonstrated high levels of comprehension, engagement and enjoyment of MIXER, with MIXER contributing to near and far transfer, supporting children’s cognitive, emotional and behavioural learning and stimulating discussion and debate about how to resolve conflict.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Int J Artif Intell Educ},
	author = {Hall, Lynne and Tazzyman, Sarah and Hume, Colette and Endrass, Birgit and Lim, Mei-Yii and Hofstede, GertJan and Paiva, Ana and Andre, Elisabeth and Kappas, Arvid and Aylett, Ruth},
	month = jun,
	year = {2015},
	keywords = {Artificial Intelligence, Digital Education and Educational Technology, Intelligent agents, Interactive learning, Intercultural conflict, Synthetic cultures},
	pages = {291--317},
	file = {Full Text PDF:files/1361/Hall et al. - 2015 - Learning to Overcome Cultural Conflict through Engaging with Intelligent Agents in Synthetic Culture.pdf:application/pdf},
}

@article{krishnan_implementation_2023,
	title = {Implementation of a hepatitis-themed virtual escape room in pharmacy education: {A} pilot study},
	volume = {28},
	issn = {1573-7608},
	shorttitle = {Implementation of a hepatitis-themed virtual escape room in pharmacy education},
	url = {https://doi.org/10.1007/s10639-023-11745-1},
	doi = {10.1007/s10639-023-11745-1},
	abstract = {As we enter a world of blended learning in higher education, an increased need for adaptation of teaching strategies to enhance engagement has been recognised to amplify learning outcomes online. Gamification has been identified as a creative tool to engage the current cohort of learners who are also characteristically tech-savvy. To this end, escape room games have gained considerable traction in medical and pharmacy education to promote learning, critical thinking and teamwork. In this pilot study we describe the implementation of a 60-minute, web-based hepatitis-themed escape room game within a Year 3 Pharmacotherapy unit at Monash University. A total of 418 students participated in this activity. Students’ knowledge gain on the topic was assessed through a pre- and post-intervention assessment, whereby a statistically significant improvement was seen in the knowledge score following implementation of the gaming activity (58.66\% pre-intervention vs. 72.05\% post-intervention, p {\textless} 0.05). The innovative learning activity was also well perceived by the students. Virtual escape room game is a viable pedagogical approach to teach and reinforce clinical concepts among pharmacy students. With the evolving landscape of education and learner demographics, investment in technology- enhanced game-based learning is a promising trajectory to support students’ growth in a learner-centered environment. A comparison between virtual escape room game and traditional teaching will further inform effectiveness of the gamification on long term knowledge retention.},
	language = {en},
	number = {11},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Krishnan, Sunanthiny and Blebil, Ali Qais and Dujaili, Juman Abdulelah and Chuang, Sara and Lim, Angelina},
	month = nov,
	year = {2023},
	keywords = {Artificial Intelligence, Digital Education and Educational Technology, Educational game, Educational technology, Gamification, Pharmacy education, Virtual escape room},
	pages = {14347--14359},
	file = {Full Text PDF:files/1362/Krishnan et al. - 2023 - Implementation of a hepatitis-themed virtual escape room in pharmacy education A pilot study.pdf:application/pdf},
}

@article{dionisio_leveraging_2021,
	title = {Leveraging {Transmedia} storytelling to engage tourists in the understanding of the destination’s local heritage},
	volume = {80},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-021-10949-2},
	doi = {10.1007/s11042-021-10949-2},
	abstract = {Transmedia Stories are becoming an increasingly important technique for the tourism industry. They are successful tools to engage, inspire and gather audiences online and offline. In this article, we describe the design, implementation and evaluation of Fragments of Laura, a bespoke Transmedia Storytelling (TS) experience designed to involve visitors in developing knowledge and awareness about the cultural and natural heritage of Madeira Island. Fragments of Laura (FoL) is composed of two interconnected components: a Location-Aware Multimedia Story, and a Hypermedia Platform populated with locally collected testimonies and interviews. Results from the extensive evaluation of Fragments of Laura, highlights the potential of interactive multimedia, TS in particular, in engaging tourists with the destination values and community. Our contribution is twofold, on one hand we extend on the state of the art of multimedia interactive storytelling, with the description of the Fragments of Laura TS artifact. On the other hand, results from FoL evaluation highlight how the artifact impacts on the tourism experience its implications for the design of future tourism driven TS experiences.},
	language = {en},
	number = {26},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Dionisio, Mara and Nisi, Valentina},
	month = nov,
	year = {2021},
	keywords = {Artificial Intelligence, Hypermedia, Location-aware multimedia stories, Multimedia, Narrative persuasion, Sustainable tourism, Transmedia storytelling},
	pages = {34813--34841},
	file = {Full Text PDF:files/1363/Dionisio e Nisi - 2021 - Leveraging Transmedia storytelling to engage tourists in the understanding of the destination’s loca.pdf:application/pdf},
}

@article{solmaz_behavioral_2024,
	title = {Behavioral intention, perception and user assessment in an immersive virtual reality environment with {CFD} simulations},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-00985-2},
	doi = {10.1007/s10055-024-00985-2},
	abstract = {This study explores technology acceptance, perception and user assessment of an immersive virtual reality environment with computational fluid dynamics simulations in engineering education. 57 participants from three different institutions tested the virtual reality application. Partial least squares structural equation modeling and interferential statistics were performed to predict and assess interrelations among constructs. Results show that the learning value, content value, intrinsic motivation and personal innovativeness are underlying factors behind students’ intention to use virtual reality. Pair-wise analysis indicates that users’ perceptions matter and positively affect their attitudes. In addition, the virtual reality application helps students perform significantly better in the post-knowledge test. Findings also highlight that prior experience and interest can affect students’ attitudes and behavioral intentions to accept the virtual reality application in education. Our study can guide lecturers and developers to achieve on-target immersive virtual reality learning environments in higher education.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Solmaz, Serkan and Gerling, Kathrin and Kester, Liesbeth and Van Gerven, Tom},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Assessment, Computational fluid dynamics, Immersive learning, Technology acceptance, Virtual reality},
	pages = {88},
	file = {Full Text PDF:files/1364/Solmaz et al. - 2024 - Behavioral intention, perception and user assessment in an immersive virtual reality environment wit.pdf:application/pdf},
}

@article{ponton_fitted_2023,
	title = {Fitted avatars: automatic skeleton adjustment for self-avatars in virtual reality},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Fitted avatars},
	url = {https://doi.org/10.1007/s10055-023-00821-z},
	doi = {10.1007/s10055-023-00821-z},
	abstract = {In the era of the metaverse, self-avatars are gaining popularity, as they can enhance presence and provide embodiment when a user is immersed in Virtual Reality. They are also very important in collaborative Virtual Reality to improve communication through gestures. Whether we are using a complex motion capture solution or a few trackers with inverse kinematics (IK), it is essential to have a good match in size between the avatar and the user, as otherwise mismatches in self-avatar posture could be noticeable for the user. To achieve such a correct match in dimensions, a manual process is often required, with the need for a second person to take measurements of body limbs and introduce them into the system. This process can be time-consuming, and prone to errors. In this paper, we propose an automatic measuring method that simply requires the user to do a small set of exercises while wearing a Head-Mounted Display (HMD), two hand controllers, and three trackers. Our work provides an affordable and quick method to automatically extract user measurements and adjust the virtual humanoid skeleton to the exact dimensions. Our results show that our method can reduce the misalignment produced by the IK system when compared to other solutions that simply apply a uniform scaling to an avatar based on the height of the HMD, and make assumptions about the locations of joints with respect to the trackers.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Ponton, Jose Luis and Ceballos, Víctor and Acosta, Lesly and Ríos, Alejandro and Monclús, Eva and Pelechano, Nuria},
	month = sep,
	year = {2023},
	keywords = {Artificial Intelligence, Avatar, Embodiment, Full-body tracking, Inverse kinematics, User representation, Virtual reality},
	pages = {2541--2560},
	file = {Full Text PDF:files/1365/Ponton et al. - 2023 - Fitted avatars automatic skeleton adjustment for self-avatars in virtual reality.pdf:application/pdf},
}

@article{nordby_system_2024,
	title = {System {Thinking} in {Gamification}},
	volume = {5},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-023-02579-2},
	doi = {10.1007/s42979-023-02579-2},
	abstract = {People spend a lot of time and energy playing videogames (Kapp in The gamification of learning and instruction: game-based methods and strategies for training and education. Pfeiffer an imprint of Wiley, San Francisco, 2012), and as a result, gamification has grown from a buzzword into a discipline. Since 2012, the authors have experimented with system thinking as a methodology for developing gamification and will present examples in this article. The primary objectives are to study how system thinking can be used to understand, design, develop and document gamifications, and how psychology and pedagogics can be integrated in the process to enhance the learning. This is an observational case study that gives examples of how students (i) use system thinking to understand and clarify the gamification case using system analysis and (ii) use system dynamics to simulate cases and predict user responses. Students begin system analysis once the gamification idea is developed and their goals and the case parameters are established, and it includes making casual loop diagrams, flow charts, and reference behavior patterns. Students then find and experiment with numerical data for the case and use system dynamics to simulate the gamification and predict the user results. The pedagogy is problem based and grounded in traditional problem-based learning and situated learning. This article shows how system thinking allows students and professionals to develop a deeper and more tangible understanding of the research materials and presumptions they have when engaging in any given gamification scenario. System thinking also provides tools to test research material and hypotheses in a more structured, manageable, and palpable way. Although we have discovered several ways system thinking can benefit gamification design, the research has also revealed new areas where system thinking could be explored further.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {SN COMPUT. SCI.},
	author = {Nordby, Anders and Vibeto, Håvard and Mobbs, Sophie and Sverdrup, Harald U.},
	month = feb,
	year = {2024},
	keywords = {Artificial Intelligence, Game development, Games and learning, Gamification, Pedagogy, Serious games, System thinking},
	pages = {299},
	file = {Full Text PDF:files/1366/Nordby et al. - 2024 - System Thinking in Gamification.pdf:application/pdf},
}

@article{zalake_internet-based_2021,
	title = {Internet-based tailored virtual human health intervention to promote colorectal cancer screening: design guidelines from two user studies},
	volume = {15},
	issn = {1783-8738},
	shorttitle = {Internet-based tailored virtual human health intervention to promote colorectal cancer screening},
	url = {https://doi.org/10.1007/s12193-020-00357-5},
	doi = {10.1007/s12193-020-00357-5},
	abstract = {To influence user behaviors, Internet-based virtual humans (VH) have been used to deliver health interventions. When developing Internet-based VH health interventions, the developers have to make several design decisions on VH’s appearance, role, language, or medium. The design decisions can affect the outcomes of the Internet-based VH health intervention. To help make design decisions, the current paper presents design guidelines drawn from two studies. The two studies used Internet-based VH health intervention to promote colorectal cancer (CRC) screening. The two studies examined the influence of visual design and the influence of the information medium on user intentions to pursue more health information. In the first study, the qualitative analysis of the focus group (n = 73 users in 13 focus groups) transcripts shows that the VH’s visual realism, the VH’s healthcare role, and the presence of a local healthcare provider’s logo influenced the user perceptions of the intervention. The findings from the focus groups were used to iterate the intervention and derive design guidelines. In the second study (n = 1400), the analysis of online surveys from users after the VH-based intervention showed that to positively influence the user intentions to pursue the health topic further, the results recommend the use of an animated VH to deliver health information compared to other mediums of information delivery, such as text. The analysis also shows that very few user comments were related to the VH’s appearance after visual design iterations in the second study. The design guidelines from the two studies can be used by developers when using VH-based interventions to positively influence users’ intention to change behaviors.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {J Multimodal User Interfaces},
	author = {Zalake, Mohan and Tavassoli, Fatemeh and Duke, Kyle and George, Thomas and Modave, Francois and Neil, Jordan and Krieger, Janice and Lok, Benjamin},
	month = jun,
	year = {2021},
	keywords = {Artificial Intelligence, Colorectal Cancer, Health, Internet-based, Intervention, LIWC, Virtual humans},
	pages = {147--162},
	file = {Full Text PDF:files/1367/Zalake et al. - 2021 - Internet-based tailored virtual human health intervention to promote colorectal cancer screening de.pdf:application/pdf},
}

@article{marin-lora_improved_2022,
	title = {Improved perception of ceramic molds through augmented reality},
	volume = {81},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-022-13168-5},
	doi = {10.1007/s11042-022-13168-5},
	abstract = {Augmented Reality techniques allow the user to visualize part of the real world through a display device by incorporating graphical information into the existing physical information. In this sense, it is important to know how the physical presence of the user in the augmented reality experience can affect the perception and evaluation of the product. To this end, this work presents a theoretical framework that explains how users perceive and evaluate the benefits and quality of augmentation with augmented reality through their physical presence, compared to visualizing the same experience through a video. The application was developed for the exhibition and sale of ceramic molds. Users viewed graphical information about the mold, placed between them and the screen while seeing themselves in the television as if it was a mirror. The experiments showed that the integration of the product into the environment and the spatial presence of the users had a positive effect on the perceived value in terms of usefulness and enjoyment, improved comfort in the purchase decision, and reinforced the overall opinion of the product.},
	language = {en},
	number = {30},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Marín-Lora, Carlos and Sotoca, Jose M. and Chover, Miguel},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Augmented reality, Ceramic molds, Decision comfort, Product demonstration, Spatial presence},
	pages = {43373--43390},
	file = {Full Text PDF:files/1368/Marín-Lora et al. - 2022 - Improved perception of ceramic molds through augmented reality.pdf:application/pdf},
}

@article{bienroth_spatially_2022,
	title = {Spatially resolved transcriptomics in immersive environments},
	volume = {5},
	issn = {2524-4442},
	url = {https://doi.org/10.1186/s42492-021-00098-6},
	doi = {10.1186/s42492-021-00098-6},
	abstract = {Spatially resolved transcriptomics is an emerging class of high-throughput technologies that enable biologists to systematically investigate the expression of genes along with spatial information. Upon data acquisition, one major hurdle is the subsequent interpretation and visualization of the datasets acquired. To address this challenge, VR-Cardiomicsis presented, which is a novel data visualization system with interactive functionalities designed to help biologists interpret spatially resolved transcriptomic datasets. By implementing the system in two separate immersive environments, fish tank virtual reality (FTVR) and head-mounted display virtual reality (HMD-VR), biologists can interact with the data in novel ways not previously possible, such as visually exploring the gene expression patterns of an organ, and comparing genes based on their 3D expression profiles. Further, a biologist-driven use-case is presented, in which immersive environments facilitate biologists to explore and compare the heart expression profiles of different genes.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Vis. Comput. Ind. Biomed. Art},
	author = {Bienroth, Denis and Nim, Hieu T. and Garkov, Dimitar and Klein, Karsten and Jaeger-Honz, Sabrina and Ramialison, Mirana and Schreiber, Falk},
	month = jan,
	year = {2022},
	keywords = {Artificial Intelligence, Fish tank virtual reality, Head-mounted display, Immersive analytics, Immersive environment, Spatial transcriptomics, Spatially-resolved transcriptomics, Virtual reality},
	pages = {2},
	file = {Full Text PDF:files/1369/Bienroth et al. - 2022 - Spatially resolved transcriptomics in immersive environments.pdf:application/pdf},
}

@article{xie_exploring_2022,
	title = {Exploring structural relations among computer self-efficacy, perceived immersion, and intention to use virtual reality training systems},
	volume = {26},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-022-00656-0},
	doi = {10.1007/s10055-022-00656-0},
	abstract = {The use of virtual reality (VR) training systems for education has grown in popularity in recent years. Scholars have reported that self-efficacy and interactivity are important predictors of learning outcomes in virtual learning environments, but little empirical research has been conducted to explain how computer self-efficacy (as a subcategory of self-efficacy) and perceived immersion (as a correlate of interactivity) are connected to the intention to use VR training systems. The present study aims to determine which factors significantly influence behavioral intention when students are exposed to VR training systems via an updated technology acceptance frame by incorporating the constructs of computer self-efficacy and perceived immersion simultaneously. We developed a VR training system regarding circuit connection and a reliable and validated instrument including 9 subscales. The sample data were collected from 124 junior middle school students and 210 senior high school students in two schools located in western China. The samples were further processed into a structural equation model with path analysis and cohort analysis. The results showed that the intention to use VR training systems was indirectly influenced by computer self-efficacy but directly influenced by perceived immersion (β = 0.451). However, perceived immersion seemed to be influenced mostly by learner interaction (β = 0.332). Among external variables, learner interaction (β = 0.149) had the largest total effect on use intention, followed by facilitating conditions (β = 0.138), computer self-efficacy (β = 0.104), experimental fidelity (β = 0.083), and subjective norms (β = 0.077). The moderating roles of gender differences, grade level, and previous experience in structural relations were also identified. The findings of the present study highlight the ways in which factors and associations are considered in the practical development of VR training systems.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Xie, Tao and Zheng, Ling and Liu, Geping and Liu, Leping},
	month = dec,
	year = {2022},
	keywords = {Artificial Intelligence, Computer self-efficacy, Intention to use, Perceived immersion, TAM, VR training systems},
	pages = {1725--1744},
	file = {Full Text PDF:files/1370/Xie et al. - 2022 - Exploring structural relations among computer self-efficacy, perceived immersion, and intention to u.pdf:application/pdf},
}

@article{weerasinghe_exploring_2023,
	title = {Exploring the future building: representational effects on projecting oneself into the future office space},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Exploring the future building},
	url = {https://doi.org/10.1007/s10055-022-00673-z},
	doi = {10.1007/s10055-022-00673-z},
	abstract = {While virtual reality (VR) has been explored in the field of architecture, its implications on people who experience their future office space in such a way has not been extensively studied. In this explorative study, we are interested in how VR and other representation methods support users in projecting themselves into their future office space and how this might influence their willingness to relocate. In order to compare VR with other representations, we used (i) standard paper based floor plans and renders of the future building (as used by architects to present their creations to stakeholders), (ii) a highly-detailed virtual environment of the same building experienced on a computer monitor (desktop condition), and (iii) the same environment experienced on a head mounted display (VR condition). Participants were randomly assigned to conditions and were instructed to freely explore their representation method for up to 15 min without any restrictions or tasks given. The results show, that compared to other representation methods, VR significantly differed for the sense of presence, user experience and engagement, and that these measures are correlated for this condition only. In virtual environments, users were observed looking at the views through the windows, spent time on terraces between trees, explored the surroundings, and even “took a walk” to work. Nevertheless, the results show that representation method influences the exploration of the future building as users in VR spent significantly more time exploring the environment, and provided more positive comments about the building compared to users in either desktop or paper conditions. We show that VR representation used in our explorative study increased users’ capability to imagine future scenarios involving their future office spaces, better supported them in projecting themselves into these spaces, and positively affected their attitude towards relocating.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Weerasinghe, Maheshya and Čopič Pucihar, Klen and Ducasse, Julie and Quigley, Aaron and Toniolo, Alice and Miguel, Angela and Caluya, Nicko and Kljun, Matjaž},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, Immersive VR environments, Job relocation, Sense of presence, User engagement, User experience},
	pages = {51--70},
	file = {Full Text PDF:files/1371/Weerasinghe et al. - 2023 - Exploring the future building representational effects on projecting oneself into the future office.pdf:application/pdf},
}

@article{wu_understanding_2019,
	title = {Understanding freehand gestures: a study of freehand gestural interaction for immersive {VR} shopping applications},
	volume = {9},
	issn = {2192-1962},
	shorttitle = {Understanding freehand gestures},
	url = {https://doi.org/10.1186/s13673-019-0204-7},
	doi = {10.1186/s13673-019-0204-7},
	abstract = {Unlike retail stores, in which the user is forced to be physically present and active during restricted opening hours, online shops may be more convenient, functional and efficient. However, traditional online shops often have a narrow bandwidth for product visualizations and interactive techniques and lack a compelling shopping context. In this paper, we report a study on eliciting user-defined gestures for shopping tasks in an immersive VR (virtual reality) environment. We made a methodological contribution by providing a varied practice for producing more usable freehand gestures than traditional elicitation studies. Using our method, we developed a gesture taxonomy and generated a user-defined gesture set. To validate the usability of the derived gesture set, we conducted a comparative study and answered questions related to the performance, error count, user preference and effort required from end-users to use freehand gestures compared with traditional immersive VR interaction techniques, such as the virtual handle controller and ray-casting techniques. Experimental results show that the freehand-gesture-based interaction technique was rated to be the best in terms of task load, user experience, and presence without the loss of performance (i.e., speed and error count). Based on our findings, we also developed several design guidelines for gestural interaction.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Hum. Cent. Comput. Inf. Sci.},
	author = {Wu, Huiyue and Luo, Weizhou and Pan, Neng and Nan, Shenghuan and Deng, Yanyi and Fu, Shengqian and Yang, Liuqingqing},
	month = dec,
	year = {2019},
	keywords = {Artificial Intelligence, Elicitation study, Immersive virtual reality, Online shopping, User experience, User-defined gestures},
	pages = {43},
	file = {Full Text PDF:files/1372/Wu et al. - 2019 - Understanding freehand gestures a study of freehand gestural interaction for immersive VR shopping.pdf:application/pdf},
}

@article{hudson_what_2023,
	title = {What can science fiction tell us about the future of artificial intelligence policy?},
	volume = {38},
	issn = {1435-5655},
	url = {https://doi.org/10.1007/s00146-021-01273-2},
	doi = {10.1007/s00146-021-01273-2},
	abstract = {This paper addresses the gap between familiar popular narratives describing Artificial Intelligence (AI), such as the trope of the killer robot, and the realistic near-future implications of machine intelligence and automation for technology policy and society. The authors conducted a series of interviews with technologists, science fiction writers, and other experts, as well as a workshop, to identify a set of key themes relevant to the near future of AI. In parallel, they led the analysis of almost 100 recent works of science fiction stories with AI themes to develop a preliminary taxonomy of AI in science fiction. These activities informed the commissioning of six original works of science fiction and non-fiction response essays on the themes of “intelligence” and “justice” that were published as part of the Slate Future Tense Fiction series in 2019 and 2020. Our findings indicate that artificial intelligence remains deeply ambiguous both in the policy and cultural contexts: we struggle to define the boundaries and the agency of machine intelligence, and consequently find it difficult to govern or interact with such systems. However, our findings also suggest more productive avenues of inquiry and framing that could foster both better policy and better narratives around AI.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {AI \& Soc},
	author = {Hudson, Andrew Dana and Finn, Ed and Wylie, Ruth},
	month = feb,
	year = {2023},
	keywords = {Artificial intelligence, Artificial Intelligence, Futures, Medical Ethics, Policy futures, Science fiction, Technology policy},
	pages = {197--211},
	file = {Full Text PDF:files/1373/Hudson et al. - 2023 - What can science fiction tell us about the future of artificial intelligence policy.pdf:application/pdf},
}

@article{harris_assessing_2021,
	title = {Assessing the learning and transfer of gaze behaviours in immersive virtual reality},
	volume = {25},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00501-w},
	doi = {10.1007/s10055-021-00501-w},
	abstract = {Virtual reality (VR) has clear potential for improving simulation training in many industries. Yet, methods for testing the fidelity, validity and training efficacy of VR environments are, in general, lagging behind their adoption. There is limited understanding of how readily skills learned in VR will transfer, and what features of training design will facilitate effective transfer. Two potentially important elements are the psychological fidelity of the environment, and the stimulus correspondence with the transfer context. In this study, we examined the effectiveness of VR for training police room searching procedures, and assessed the corresponding development of perceptual-cognitive skill through eye-tracking indices of search efficiency. Participants (n = 54) were assigned to a VR rule-learning and search training task (FTG), a search only training task (SG) or a no-practice control group (CG). Both FTG and SG developed more efficient search behaviours during the training task, as indexed by increases in saccade size and reductions in search rate. The FTG performed marginally better than the CG on a novel VR transfer test, but no better than the SG. More efficient gaze behaviours learned during training were not, however, evident during the transfer test. These findings demonstrate how VR can be used to develop perceptual-cognitive skills, but also highlight the challenges of achieving transfer of training.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Harris, David J. and Hardcastle, Kyle J. and Wilson, Mark R. and Vine, Samuel J.},
	month = dec,
	year = {2021},
	keywords = {Artificial Intelligence, Fidelity, Police, Policing, Training, Validity, VR},
	pages = {961--973},
	file = {Full Text PDF:files/1374/Harris et al. - 2021 - Assessing the learning and transfer of gaze behaviours in immersive virtual reality.pdf:application/pdf},
}

@article{kara_assessing_2024,
	title = {Assessing the adoption of the {Yavuz} {Battleship} application in the mixed reality environment using the technology acceptance model},
	volume = {30},
	issn = {1432-1882},
	url = {https://doi.org/10.1007/s00530-024-01277-0},
	doi = {10.1007/s00530-024-01277-0},
	abstract = {This study concentrates on developing a mixed reality (MR) app for the historic ship Yavuz Battleship, also known as “SMS Goeben,” and assessing its acceptance using the technology acceptance model (TAM). Mixed reality blends real and virtual environments to create novel spatial experiences, bridging the gap between the virtual and real worlds. This technology enables designers to craft immersive, interactive environments that enhance users' perception of physical reality. The study employs a multi-stage methodology, including historical research, 3D model creation, digital information integration, Unity program transfer, MR environment development with the Mixed Reality Toolkit (MRTK), and implementation as a Microsoft Hololens 2 app. TAM is employed to gauge MR acceptance. Evaluation results affirm the Yavuz MR app's TAM alignment, providing structural support for similar architectural science inquiries. The research underscores the need to explore MR's architectural applications and TAM's role in measuring MR acceptance.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Multimedia Systems},
	author = {Kara, Murat and Çakıcı Alp, Neşe},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Hololens 2, Mixed reality, Structural equation modeling, Technology acceptance model, Yavuz Battleship (SMS Goeben)},
	pages = {76},
	file = {Full Text PDF:files/1375/Kara e Çakıcı Alp - 2024 - Assessing the adoption of the Yavuz Battleship application in the mixed reality environment using th.pdf:application/pdf},
}

@article{reeves_back_2024,
	title = {Back to the {Control} {Room}: {Managing} {Artistic} {Work}},
	volume = {33},
	issn = {1573-7551},
	shorttitle = {Back to the {Control} {Room}},
	url = {https://doi.org/10.1007/s10606-022-09436-5},
	doi = {10.1007/s10606-022-09436-5},
	abstract = {Control rooms have long been a key domain of investigation in HCI and CSCW as sites for understanding distributed work and fragmented settings, as well as the role and design of digital technologies in that work. Although research has tended to focus mainly on ‘command and control’ configurations, such as rail transport, ambulance dispatch, air traffic and CCTV rooms, centres of coordination shaped by artistic and performative concerns have much to contribute. Our study examines how a professional team of artists and volunteers stage manage and direct the performance of a mixed reality game from a central control room, with remote runners performing live video streaming from the streets nearby to online players. We focus on the work undertaken by team members to bring this about, exploring three key elements that enable it. First, we detail how team members oriented to the work as an artistic performance produced for an audience, how they produced compelling, varied content for online players, and how the quality of the work was ongoingly assessed. Second, we unpack the organisational hierarchy in the control room’s division of labour, and how this was designed to manage the challenges of restricted informational visibility there. Third, we explore the interactional accomplishment of the performance by looking at the role of radio announcements from the event’s director to orchestrate how the performance developed over time. Announcements were used to resolve trouble and provide instructions for avoiding future performative problems; but more centrally, to give artistic direction to runners in order to shape the performance itself. To close we discuss how this study of a performance impacts CSCW’s understandings of control room work, how the problem of ‘diffuse’ tasks like artistic work is co-ordinated, and how orientations towards quality as an artistic concern is manifest in / as control room practices. We also reflect on hierarchical and horizontal control room arrangements, and the role of video as both collaborative resource and product.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Comput Supported Coop Work},
	author = {Reeves, Stuart and Greiffenhagen, Christian and Perry, Mark},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Control rooms, Ethnography, Ethnomethodology, Organisational work, Performance, Video analysis},
	pages = {59--102},
	file = {Full Text PDF:files/1376/Reeves et al. - 2024 - Back to the Control Room Managing Artistic Work.pdf:application/pdf},
}

@article{bodini_envisioning_2024,
	title = {Envisioning the future of virtual production in filmmaking: {A} remote co-design study},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {Envisioning the future of virtual production in filmmaking},
	url = {https://doi.org/10.1007/s11042-023-16308-7},
	doi = {10.1007/s11042-023-16308-7},
	abstract = {Virtual Production is a new process in the audiovisual industry, taking advantage of emerging technologies and attracting a significant degree of interest in academic and industrial research. This article documents a research process focusing on co-design of innovative solutions for Virtual Production relying on immersive technologies. Two remote collaborative workshops were organized involving audiovisual professionals covering different roles in different phases of the making process of audiovisual production. A range of innovative design concepts was generated as part of the research following group-based iterative discussion and evaluation. The study has contributed a set of innovative design solutions in relation to applications of immersive technologies in the audiovisual industry. The authors argue that the methods adopted have the potential to serve as a blueprint for design and implementation of future remote collaborative co-design processes in relation to audiovisual studies and, more generally, across disciplinary boundaries.},
	language = {en},
	number = {7},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Bodini, Aimone and Manohar, Arthi and Colecchia, Federico and Harrison, David and Garaj, Vanja},
	month = feb,
	year = {2024},
	keywords = {Artificial Intelligence, Audiovisual production, Augmented reality, Co-design, Remote workshop, Virtual reality},
	pages = {19015--19039},
	file = {Full Text PDF:files/1377/Bodini et al. - 2024 - Envisioning the future of virtual production in filmmaking A remote co-design study.pdf:application/pdf},
}

@article{kolivand_integration_2021,
	title = {An integration of enhanced social force and crowd control models for high-density crowd simulation},
	volume = {33},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-020-05385-6},
	doi = {10.1007/s00521-020-05385-6},
	abstract = {Social force model is one of the well-known approaches that can successfully simulate pedestrians’ movements realistically. However, it is not suitable to simulate high-density crowd movement realistically due to the model having only three basic crowd characteristics which are goal, attraction, and repulsion. Therefore, it does not satisfy the high-density crowd condition which is complex yet unique, due to its capacity, density, and various demographic backgrounds of the agents. Thus, this research proposes a model that improves the social force model by introducing four new characteristics which are gender, walking speed, intention outlook, and grouping to make simulations more realistic. Besides, the high-density crowd introduces irregular behaviours in the crowd flow, which is stopping motion within the crowd. To handle these scenarios, another model has been proposed that controls each agent with two different states: walking and stopping. Furthermore, the stopping behaviour was categorized into a slow stop and sudden stop. Both of these proposed models were integrated to form a high-density crowd simulation framework. The framework has been validated by using the comparison method and fundamental diagram method. Based on the simulation of 45,000 agents, it shows that the proposed framework has a more accurate average walking speed (0.36 m/s) compared to the conventional social force model (0.61 m/s). Both of these results are compared to the real-world data which is 0.3267 m/s. The findings of this research will contribute to the simulation activities of pedestrians in a highly dense population.},
	language = {en},
	number = {11},
	urldate = {2024-09-05},
	journal = {Neural Comput \& Applic},
	author = {Kolivand, Hoshang and Rahim, Mohd Shafry and Sunar, Mohd Shahrizal and Fata, Ahmad Zakwan Azizul and Wren, Chris},
	month = jun,
	year = {2021},
	keywords = {Artificial Intelligence, Crowd control models, Crowd simulation, High-density crowd simulation, Social force model},
	pages = {6095--6117},
	file = {Full Text PDF:files/1378/Kolivand et al. - 2021 - An integration of enhanced social force and crowd control models for high-density crowd simulation.pdf:application/pdf},
}

@article{dominguez-dager_holograms_2024,
	title = {Holograms for seamless integration of remote students in the classroom},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-023-00924-7},
	doi = {10.1007/s10055-023-00924-7},
	abstract = {The new global scenario imposed by the SARS-CoV-2 virus has given rise to an atypical and problematic situation in multiple spheres. In very little time, the abrupt change from face-to-face to remote has not only required a rapid widespread use of digital technology, but also a change in methodology and communicative interactions. In the field of education, teachers have had to interact in new environments, with the combined use of face-to-face and non-face-to-face teaching being a major challenge. This paper presents the design and implementation of a cyber presence system for educational environments using Microsoft’s HoloLens 2 Mixed Reality (MR) headset. A software tool is developed that improves teaching scenarios through communication in mixed environments. The tool enables teachers to integrate the students in the classroom in a common space with remote students connected by videoconference. Our system is not limited to education, however, as it can also be deployed in any setting that requires remote communication, such as companies and governmental institutions.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Dominguez-Dager, Bessie and Gomez-Donoso, Francisco and Roig-Vila, Rosabel and Escalona, Felix and Cazorla, Miguel},
	month = jan,
	year = {2024},
	keywords = {Artificial Intelligence, Education, Holograms, HoloLens 2, Mixed reality, Remote communication},
	pages = {24},
	file = {Full Text PDF:files/1379/Dominguez-Dager et al. - 2024 - Holograms for seamless integration of remote students in the classroom.pdf:application/pdf},
}

@article{ghergulescu_totcompute_2016,
	title = {{ToTCompute}: {A} {Novel} {EEG}-{Based} {TimeOnTask} {Threshold} {Computation} {Mechanism} for {Engagement} {Modelling} and {Monitoring}},
	volume = {26},
	issn = {1560-4306},
	shorttitle = {{ToTCompute}},
	url = {https://doi.org/10.1007/s40593-016-0111-2},
	doi = {10.1007/s40593-016-0111-2},
	abstract = {Engagement influences participation, progression and retention in game-based e-learning (GBeL). Therefore, GBeL systems should engage the players in order to support them to maximize their learning outcomes, and provide the players with adequate feedback to maintain their motivation. Innovative engagement monitoring solutions based on players’ behaviour are needed to enable engagement monitoring in a non-disturbing way, without interrupting the game-play and game flow. Furthermore, generic metrics and automatic mechanisms for their engagement monitoring and modelling are needed. One important metric that was used for engagement modelling is TimeOnTask, which represents the duration of time required by the player to complete a task. This paper proposes ToTCompute (TimeOnTask Threshold Computation), a novel mechanism that automatically computes - in a task-dependent manner - TimeOnTask threshold values after which student engagement decreases with a given percentage from his initial level of engagement (e.g., after 2 min student engagement will fall with 10 \% from his initial level). In this way the mechanism enables engagement modelling at a higher granularity and further enables engagement-based adaptation in GBeL systems. ToTCompute makes use of game-playing information and EEG signals collected through an initial testing session. The results of an experimental case study have shown that ToTCompute can be used to automatically compute threshold values for the TimeOnTask generic engagement metric, which explains up to 76.2 \% of the variance in engagement change. Furthermore, the results confirmed the usefulness of the mechanism as the TimeOnTask threshold value is highly task-dependent, and setting its value manually for multiple game tasks would be a laborious process.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Int J Artif Intell Educ},
	author = {Ghergulescu, Ioana and Muntean, Cristina Hava},
	month = sep,
	year = {2016},
	keywords = {Artificial Intelligence, Computational algorithms, Digital Education and Educational Technology, EEG, Engagement, Game-based e-learning, Modelling, Monitoring, Motivation, Player behaviour, Time series analysis, Time-on-task},
	pages = {821--854},
	file = {Full Text PDF:files/1380/Ghergulescu e Muntean - 2016 - ToTCompute A Novel EEG-Based TimeOnTask Threshold Computation Mechanism for Engagement Modelling an.pdf:application/pdf},
}

@article{laera_evaluating_2023,
	title = {Evaluating an augmented reality interface for sailing navigation: a comparative study with a immersive virtual reality simulator},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Evaluating an augmented reality interface for sailing navigation},
	url = {https://doi.org/10.1007/s10055-022-00706-7},
	doi = {10.1007/s10055-022-00706-7},
	abstract = {Sailing navigation is an activity that requires acquiring and processing information from the surrounding environment. The advancement of technology has enabled sailboats to have an increasing number of onboard sensors that make sailing more user-friendly. However, data provided by these sensors are still visualized on 2D digital displays that imitate traditional analog interfaces. Although these displays are strategically placed on the sailboat, the user needs to divert attention from the primary navigation task to look at them, thus spending a significant amount of cognitive resources. AR-based technologies have the potential to overcome these limitations by displaying information registered in the real environment, but there are no studies in the literature for validating the effectiveness of this technology in the field of sailing. Thus, we designed a head-mounted display AR-based interface to assist users in monitoring wind data to avoid user diversion from the primary task of sailing. We conducted a user study involving 45 participants in an Immersive Virtual Reality simulated environment. We collected objective and subjective measures to compare the AR-based interface with a traditional data visualization system. The AR-based interface outperformed the traditional data visualization system regarding reaction time, cognitive load, system usability, and user experience.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Laera, Francesco and Manghisi, Vito Modesto and Evangelista, Alessandro and Uva, Antonio Emmanuele and Foglia, Mario Massimo and Fiorentino, Michele},
	month = jun,
	year = {2023},
	keywords = {Artificial Intelligence, Augmented reality, Cognitive load, Human–computer interaction, Nautical, Sailing, User study},
	pages = {929--940},
	file = {Full Text PDF:files/1401/Laera et al. - 2023 - Evaluating an augmented reality interface for sailing navigation a comparative study with a immersi.pdf:application/pdf},
}

@article{rahman_game-based_2024,
	title = {Game-based learning in metaverse: {Virtual} chemistry classroom for chemical bonding for remote education},
	issn = {1573-7608},
	shorttitle = {Game-based learning in metaverse},
	url = {https://doi.org/10.1007/s10639-024-12575-5},
	doi = {10.1007/s10639-024-12575-5},
	abstract = {Virtual classrooms based on the metaverse or virtual reality are useful and effective for imparting basic chemistry concepts. Interactive and immersive environments can effectively teach fundamental chemistry concepts, such as chemical bonding and formulas, thereby making these otherwise abstract and intangible ideas more accessible and understandable. With the outbreak of Covid-19, e-learning platforms have also been developed for chemistry education. However, these platforms are unable to make learning chemistry interactive and enjoyable. Therefore, there is a need to motivate students to learn basic chemistry concepts in an immersive and interactive environment. In this paper, we propose an immersive virtual reality-based Virtual Chemistry Classroom for Chemical Bonding (VC3B) to facilitate the learning of chemical bonding and formulas through a game-based learning approach. It includes two different games for learning chemical bonding and formulas. In the first game, molecule construction, students reconstruct the structure of molecules by rearranging the atoms in order to learn about chemical bonding. In the second game, chemical formula, students compose the chemical formula of a given compound to help them memorize chemical formulas. The study, conducted on 90 middle school students, employed a randomized controlled study design, dividing participants into three groups. Each group learned about chemical bonding and formulas through three different mediums. After conducting the experiment, the students were given a questionnaire to evaluate the usability of VC3B. The results of the study were positive, with participants finding the VC3B to be more interactive than traditional book and online lecture methods. Participants were also motivated to learn and enhance their knowledge of chemistry.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Rahman, Hameedur and Wahid, Samiya Abdul and Ahmad, Faizan and Ali, Numan},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Digital Education and Educational Technology, Immersive learning environment, Virtual bonding, Virtual chemistry classroom, Virtual chemistry education, Virtual reality},
	file = {Full Text PDF:files/1402/Rahman et al. - 2024 - Game-based learning in metaverse Virtual chemistry classroom for chemical bonding for remote educat.pdf:application/pdf},
}

@article{bertino_intelligent_2021,
	title = {Intelligent {IoT} systems for civil infrastructure health monitoring: a research roadmap},
	volume = {1},
	issn = {2730-7239},
	shorttitle = {Intelligent {IoT} systems for civil infrastructure health monitoring},
	url = {https://doi.org/10.1007/s43926-021-00009-4},
	doi = {10.1007/s43926-021-00009-4},
	abstract = {This paper addresses the problem of efficient and effective data collection and analytics for applications such as civil infrastructure monitoring and emergency management. Such problem requires the development of techniques by which data acquisition devices, such as IoT devices, can: (a) perform local analysis of collected data; and (b) based on the results of such analysis, autonomously decide further data acquisition. The ability to perform local analysis is critical in order to reduce the transmission costs and latency as the results of an analysis are usually smaller in size than the original data. As an example, in case of strict real-time requirements, the analysis results can be transmitted in real-time, whereas the actual collected data can be uploaded later on. The ability to autonomously decide about further data acquisition enhances scalability and reduces the need of real-time human involvement in data acquisition processes, especially in contexts with critical real-time requirements. The paper focuses on deep neural networks and discusses techniques for supporting transfer learning and pruning, so to reduce the times for training the networks and the size of the networks for deployment at IoT devices. We also discuss approaches based on machine learning reinforcement techniques enhancing the autonomy of IoT devices.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Discov Internet Things},
	author = {Bertino, E. and Jahanshahi, M. R. and Singla, A. and Wu, R.-T.},
	month = feb,
	year = {2021},
	keywords = {Analytics at the Edge, Artificial Intelligence, Autonomous IoT devices, Deep Neural Networks},
	pages = {3},
	file = {Full Text PDF:files/1403/Bertino et al. - 2021 - Intelligent IoT systems for civil infrastructure health monitoring a research roadmap.pdf:application/pdf},
}

@article{gil-lopez_recognizing_2023,
	title = {Recognizing shopper demographics from behavioral responses in a virtual reality store},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-023-00767-2},
	doi = {10.1007/s10055-023-00767-2},
	abstract = {The use of virtual reality (VR) technology in the context of retail is a significant trend in current consumer research, as it offers market researchers a unique opportunity to measure purchase behavior more realistically. Yet, effective methods for assessing the virtual shopping experience based on consumer’s demographic characteristics are still lacking. In this study, we examine the validity of behavioral biometrics for recognizing the gender and age of customers in an immersive VR environment. We used behavior measures collected from eye-tracking, body posture (head and hand), and spatial navigation sources. Participants (n = 57) performed three tasks involving two different purchase situations. Specifically, one task focused on free browsing through the virtual store, and two other tasks focused on product search. A set of behavioral features categorized as kinematic, temporal, and spatial domains was processed based on two strategies. First, the relevance of such features in recognizing age and gender with and without including the spatial segmentation of the virtual space was statistically analyzed. Second, a set of implicit behavioral features was processed and demographic characteristics were recognized using a statistical supervised machine learning classifier algorithm via a support vector machine. The results confirmed that both approaches were significantly insightful for determining the gender and age of buyers. Also, the accuracy achieved when applying the machine learning classifier ({\textgreater} 70\%) indicated that the combination of all metrics and tasks was the best classification strategy. The contributions of this work include characterizing consumers in v-commerce spaces according to the shopper’s profile.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Gil-López, Cristina and Guixeres, Jaime and Moghaddasi, Masoud and Khatri, Jaikishan and Marín-Morales, Javier and Alcañiz, Mariano},
	month = sep,
	year = {2023},
	keywords = {Artificial Intelligence, Consumer demographics, Eye-tracking (ET), Machine learning, Navigation, Shopping experience, Virtual reality, Virtual store},
	pages = {1937--1966},
	file = {Full Text PDF:files/1404/Gil-López et al. - 2023 - Recognizing shopper demographics from behavioral responses in a virtual reality store.pdf:application/pdf},
}

@article{bourke_space_2013,
	title = {A {Space} {Filling} {Algorithm} for {Generating} {Procedural} {Geometry} and {Texture}},
	volume = {3},
	issn = {2010-2283},
	url = {https://doi.org/10.7603/s40601-013-0004-2},
	doi = {10.7603/s40601-013-0004-2},
	abstract = {Here we present an algorithm for procedurally generating a range of digital assets including 2 dimensional textures and 2.5 dimensional texture roughness. The approach involves placing shapes randomly, without overlap and with a monotonically decreasing area, within a region on a plane (the 2 dimensional texture). If the process is continued to infinity then the result is space filling thus providing a variable and potentially infinite degree of visual detail. It will be proposed and illustrated that the process is independent of the actual shape being used and as such can find application to a range of texture effects. As a means of generating texture and form procedurally the result has the other desirable property of being fractal, that is, self similar across scales which is characteristic of many packings that occur in nature.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {GSTF J Comput},
	author = {Bourke, Paul},
	month = jul,
	year = {2013},
	keywords = {packing, procedural, space filling, texture, tiling},
	pages = {4},
	file = {Full Text PDF:files/1405/Bourke - 2013 - A Space Filling Algorithm for Generating Procedural Geometry and Texture.pdf:application/pdf},
}

@article{larreina-morales_audio_2024,
	title = {Audio description in video games? {Persons} with visual disabilities weigh in},
	volume = {23},
	issn = {1615-5297},
	shorttitle = {Audio description in video games?},
	url = {https://doi.org/10.1007/s10209-023-01036-4},
	doi = {10.1007/s10209-023-01036-4},
	abstract = {In recent years, important advances have taken place to improve game accessibility for all types of players. However, audio description (AD), the access service that translates images into words, is yet to be widely implemented in mainstream games. This paper presents part of the results of the Researching Audio Description Project: Translation, Delivery and New Scenarios (RAD). One of the main objectives of the project is to investigate the potential inclusion of AD in video games in order to improve their accessibility and to contribute to a more enjoyable experience for persons with visual disabilities. First, the evolution of game accessibility is discussed, including the latest developments in the field, both from the industry and research perspectives. Secondly, the RAD Project is presented. Thirdly, the data collected from a survey addressed to blind and low vision persons in Spain is described, for which 106 valid answers were received. Survey topics include the game accessibility barriers encountered by participants, their desired solutions, and their interest in the potential application of AD. Finally, results are discussed regarding similar studies, limitations, and future research. Survey participants are interested in including AD in video games, particularly in non-interactive sections such as cutscenes. Other pressing issues for the game industry regarding accessibility are improving screen reader compatibility, enhancing sounds, and exploring the technical feasibility of game AD in real-time action.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Univ Access Inf Soc},
	author = {Larreina-Morales, María Eugenia and Mangiron, Carme},
	month = jun,
	year = {2024},
	keywords = {Artificial Intelligence, Audio description, Game accessibility, Survey, Visual disability},
	pages = {577--588},
	file = {Full Text PDF:files/1406/Larreina-Morales e Mangiron - 2024 - Audio description in video games Persons with visual disabilities weigh in.pdf:application/pdf},
}

@article{kinnula_worksome_2018,
	title = {‘{Worksome} but {Rewarding}’ –{Stakeholder} {Perceptions} on {Value} in {Collaborative} {Design} {Work}},
	volume = {27},
	issn = {1573-7551},
	url = {https://doi.org/10.1007/s10606-018-9328-y},
	doi = {10.1007/s10606-018-9328-y},
	abstract = {In this paper, we examine collaborative design projects in school contexts with many different stakeholders. We look at the value created for and by different stakeholders, focusing on value as a benefit, which is experienced – perceived and determined – by the beneficiaries themselves in the value co-creation process. As our focus is in “value-in-use”, i.e., value which emerges through activities taking place in a specific space, time, and context, we define value through subjective experience of people involved. We apply in our study the concept of value co-creation, where value is understood emerging from collaborative activity between actors participating in the activity. We see that the value co-creation lens provides a useful means for the CSCW community to scrutinize and make sense of collaborative design projects. We categorized the perceived value for each stakeholder and discuss how these categories can help in gaining a deeper understanding of the value gained in collaborative design work as well as how value co-creation lens in more general can be used as a tool in collaborative design projects.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Comput Supported Coop Work},
	author = {Kinnula, Marianne and Iivari, Netta and Isomursu, Minna and Laari-Salmela, Sari},
	month = dec,
	year = {2018},
	keywords = {Artificial Intelligence, children, collaborative design work, principals, school, service-dominant logic, stakeholders, teachers, value co-creation},
	pages = {463--494},
	file = {Full Text PDF:files/1407/Kinnula et al. - 2018 - ‘Worksome but Rewarding’ –Stakeholder Perceptions on Value in Collaborative Design Work.pdf:application/pdf},
}

@article{hadjipanayi_arousing_2022,
	title = {Arousing a wide range of emotions within educational virtual reality simulation about major depressive disorder affects knowledge retention},
	volume = {26},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00568-5},
	doi = {10.1007/s10055-021-00568-5},
	abstract = {The experience of using an educational application, concerning a major depressive disorder simulation, could be anything but pleasant, so the challenges of creating such an application are ample. In this research, the effects of the emotional experience of the players, deriving from the positive ending of the virtual reality (VR) simulation’s embedded narrative or the lack of it, are evaluated. Alongside the investigation of a possible link between the emotional impact of the simulation and information retention, the overall effect of the application in relation to VR presence and body ownership is appraised. Thirty participants over 18 years old tested the application, using an Oculus Rift head-mounted display with a joystick, and their data were recorded by a pre- and a post-questionnaire. The 30 participants have been separated into groups of 15, where the positive ending was accessible to only one of the two groups. The group which experienced the positive ending reported a significant correlation of emotional impact and knowledge retention.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Hadjipanayi, Christos and Michael-Grigoriou, Despina},
	month = mar,
	year = {2022},
	keywords = {Affective states, Artificial Intelligence, Game narrative, Knowledge retention, Major depressive disorder, Virtual reality},
	pages = {343--359},
	file = {Full Text PDF:files/1408/Hadjipanayi e Michael-Grigoriou - 2022 - Arousing a wide range of emotions within educational virtual reality simulation about major depressi.pdf:application/pdf},
}

@article{rebollo_multimedia_2022,
	title = {Multimedia augmented reality game for learning math},
	volume = {81},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-021-10821-3},
	doi = {10.1007/s11042-021-10821-3},
	abstract = {The traditional method for learning the multiplication tables is a repetitive and boring task. Teachers try to find new methods to motivate children in this tedious duty, and one of the lines to consider is to integrate en- tertainment into educational processes. This work presents a new multimedia interaction approach in order to allow children to practice these math opera- tions and have fun. The learning process has been gamified by means of two mini-games designed for mobile platforms, based on meromictic or repetitive learning. The genre of these mini-games have been selected according to chil- dren preferences: one turn-based fighting and other throwing-objects game. A series of proposed multiplications have to be solved during the play to per- form the player actions. Moreover, in order to support learning engagement, both have been visualized through Augmented Reality, combining real and virtual reality. This paper discusses the good results of mixing entertainment with some learning tasks, due to the engagement of the children to the mobile based games. A pilot study has been performed in order to evaluate the learn- ing effectiveness and usability of the proposal. Results support that playing the video games makes this tedious multiplication practice more enjoyable and attractive for children so they improve their math skills.},
	language = {en},
	number = {11},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Rebollo, Cristina and Remolar, Inmaculada and Rossano, Veronica and Lanzilotti, Rosa},
	month = may,
	year = {2022},
	keywords = {Artificial Intelligence, Augmented reality, Education, Engagement, Serious games},
	pages = {14851--14868},
	file = {Full Text PDF:files/1409/Rebollo et al. - 2022 - Multimedia augmented reality game for learning math.pdf:application/pdf},
}

@article{masterton_case_2024,
	title = {A case study of a virtual reality-based drink driving educational tool},
	volume = {83},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-023-17658-y},
	doi = {10.1007/s11042-023-17658-y},
	abstract = {Road traffic accidents are of major concern and could be reduced by better education. This paper describes the development of a virtual reality (VR) app that mimics the effects of alcohol on a driver. It was developed using Unity (version 2019.3.14), smartphone and Google Cardboard™. The user experiences a car following a predetermined route that is lined with trees and objects (traffic lights, road signs and other cars) which they need to spot and react to. By using graphical filters and time delays the driver has a feeling of being under the influence of alcohol. Twenty volunteers (18–60 years old; mean age ± sd 25.5 ± 11.6) participated in its evaluation. Data were collected on concentration times, reaction speed and observation of objects in both the alcohol simulated (impaired) and non-simulated (unimpaired) runs. Data were analysed using paired t-test. The result showed that people spent longer concentrating on objects in the impaired vs unimpaired run (10.72 ± 5.07 vs 5.30 ± 4.22 s n:20; p {\textless} 0.0001). The average reaction speed to objects in the unimpaired run was lower than in the impaired run (1.44 ± 0.66 vs 2.66 ± 0.28 s n:20; p {\textless} 0.001). Seventeen out of twenty subjects spotted all the required objects in the unimpaired whereas only seven out of twenty spotted all the objects in the impaired run (p {\textless} 0.001). The authors have shown that an inexpensive VR app can be used to demonstrate to users the effect that alcohol can have on concentration, reaction speeds and observational skills.},
	language = {en},
	number = {18},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Masterton, Callum and Wilson, Andrew Sean},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Drink driving, Driver education, Road accident, Road safety, Virtual reality},
	pages = {55351--55363},
	file = {Full Text PDF:files/1410/Masterton e Wilson - 2024 - A case study of a virtual reality-based drink driving educational tool.pdf:application/pdf},
}

@article{khodabandeh_exploring_2023,
	title = {Exploring the viability of augmented reality game- enhanced education in {WhatsApp} flipped and blended classes versus the face-to-face classes},
	volume = {28},
	issn = {1573-7608},
	url = {https://doi.org/10.1007/s10639-022-11190-6},
	doi = {10.1007/s10639-022-11190-6},
	abstract = {The application of augmented reality games (ARG) as an emerging innovative technology has become a significant component of instructional learning contexts in recent years. ARG-based education as a form of student-centered learning situates students in a learning environment that integrates virtual elements with physical environments through three-dimensional pictures and videos on mobile devices for educational purposes. To connect the use of digital tools into the language classrooms and allow learners to view the real world, this study examined the viability of ARG-enhanced education on English foreign language (EFL) learners’ learning of giving and asking for directions in flipped and blended contexts. The study involved 60 EFL elementary students of homogenous English proficiency, organized into two comparative and one control group, with 20 participants in each group. For the pre-test, an 18-item multiple-choice test with one written and oral question was administered to assess the learners’ knowledge of asking for and giving directions. After taking the pre-test, the two comparative groups received 16 sessions of ARG-enhanced education (one with a blended and the other with a flipped classroom approach), while the control group received placebo instructions. The flipped group received the instructional materials preceding the online group, while the blended group received instruction in both online and face-to-face classes. The control group received instruction in a face-to-face context. After 8-weeks of treatment sessions, all participants of the study took the post-test. According to the results, both flipped and blended groups receiving ARG-enhanced education performed better than the control group in learning how to give and ask for directions. The results of this study may pave the way for EFL teachers and students to use ARG-based technology in online and traditional classes.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Khodabandeh, Farzaneh},
	month = jan,
	year = {2023},
	keywords = {Artificial Intelligence, Augmented Reality Games, Blended class, Digital Education and Educational Technology, Flipped class, Giving and asking for directions, Learning English, Teaching English, WhatsApp},
	pages = {617--646},
	file = {Full Text PDF:files/1411/Khodabandeh - 2023 - Exploring the viability of augmented reality game- enhanced education in WhatsApp flipped and blende.pdf:application/pdf},
}

@article{seuffert_omniglasses_2024,
	title = {{OmniGlasses}: an optical aid for stereo vision {CNNs} to enable omnidirectional image processing},
	volume = {35},
	issn = {1432-1769},
	shorttitle = {{OmniGlasses}},
	url = {https://doi.org/10.1007/s00138-024-01534-2},
	doi = {10.1007/s00138-024-01534-2},
	abstract = {Stereo vision is a key technology for 3D scene reconstruction from image pairs. Most approaches process perspective images from commodity cameras. These images, however, have a very limited field of view and only picture a small portion of the scene. In contrast, omnidirectional images, also known as fisheye images, exhibit a much larger field of view and allow a full 3D scene reconstruction with a small amount of cameras if placed carefully. However, omnidirectional images are strongly distorted which make the 3D reconstruction much more sophisticated. Nowadays, a lot of research is conducted on CNNs for omnidirectional stereo vision. Nevertheless, a significant gap between estimation accuracy and throughput can be observed in the literature. This work aims to bridge this gap by introducing a novel set of transformations, namely OmniGlasses. These are incorporated into the architecture of a fast network, i.e., AnyNet, originally designed for scene reconstruction on perspective images. Our network, Omni-AnyNet, produces accurate omnidirectional distance maps with a mean absolute error of around 13 cm at 48.4 fps and is therefore real-time capable.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Machine Vision and Applications},
	author = {Seuffert, Julian B. and Perez Grassi, Ana C. and Ahmed, Hamza and Seidel, Roman and Hirtz, Gangolf},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Epipolar geometry, Fisheye, Look up table, Omnidirectional, Stereo vision, View synthesis},
	pages = {58},
	file = {Full Text PDF:files/1412/Seuffert et al. - 2024 - OmniGlasses an optical aid for stereo vision CNNs to enable omnidirectional image processing.pdf:application/pdf},
}

@article{mayor_long_2024,
	title = {Long short-term memory prediction of user’s locomotion in virtual reality},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-00962-9},
	doi = {10.1007/s10055-024-00962-9},
	abstract = {Nowadays, there is still a challenge in virtual reality to obtain an accurate displacement prediction of the user. This could be a future key element to apply in the so-called redirected walking methods. Meanwhile, deep learning provides us with new tools to reach greater achievements in this type of prediction. Specifically, long short-term memory recurrent neural networks obtained promising results recently. This gives us clues to continue researching in this line to predict virtual reality user’s displacement. This manuscript focuses on the collection of positional data and a subsequent new way to train a deep learning model to obtain more accurate predictions. The data were collected with 44 participants and it has been analyzed with different existing prediction algorithms. The best results were obtained with a new idea, the use of rotation quaternions and the three dimensions to train the previously existing models. The authors strongly believe that there is still much room for improvement in this research area by means of the usage of new deep learning models.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Mayor, Jesus and Calleja, Pablo and Fuentes-Hurtado, Felix},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Deep learning, Locomotion, User prediction, Virtual reality},
	pages = {65},
	file = {Full Text PDF:files/1413/Mayor et al. - 2024 - Long short-term memory prediction of user’s locomotion in virtual reality.pdf:application/pdf},
}

@article{damnuy_empowering_2015,
	title = {Empowering {Users} to {Game} {Dovelopment} {Platform} for {Visually} {Impaired} {Student}},
	volume = {4},
	issn = {2010-2283},
	url = {https://doi.org/10.7603/s40601-014-0006-8},
	doi = {10.7603/s40601-014-0006-8},
	abstract = {Computer-exercise game is well served for visually impaired students to engage their physical and mental health exercises. However, these type of games are increasingly unchallenged for long-term usage due to apathy patterns and restricted contents. In this paper, we describe how such drawbacks can be avoided by offering the game development platform. The platform provides a service-based architecture, which empowers teachers to build or to customize numerous game designs for entertainment and learning purpose without jeopardy. Based on our evaluation study, we found evidence that teachers of visually impaired students are satisfied with the game development approach in terms of system reliability and software usability. In addition, the teachers also have very good perception in the aspects of trust, responsiveness, applicability, usefulness and enthusiasm toward the game development approach.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {GSTF J Comput},
	author = {Damnuy, Narin and Witosurapot, Suntorn and Matayong, Sureena},
	month = sep,
	year = {2015},
	keywords = {Active game, Empowering users, Game development platform, Service-based architecture, Visually impaired students},
	pages = {6},
	file = {Full Text PDF:files/1414/Damnuy et al. - 2015 - Empowering Users to Game Dovelopment Platform for Visually Impaired Student.pdf:application/pdf},
}

@article{mozaffari_impacts_2023,
	title = {Impacts of augmented reality on foreign language teaching: a case study of {Persian} language},
	volume = {82},
	issn = {1573-7721},
	shorttitle = {Impacts of augmented reality on foreign language teaching},
	url = {https://doi.org/10.1007/s11042-022-13370-5},
	doi = {10.1007/s11042-022-13370-5},
	abstract = {The use of information technology in the field of foreign language teaching as an auxiliary tool is very important. In a foreign language classroom, place is just an abstract concept; where the language is separated from the community, culture and places in which it is used. Augmented reality is a technology in which virtual components are simultaneously combined with the real environment. Our aim in this study is to investigate the effects of location-based augmented reality in teaching Persian as a foreign language. In this study, after consulting with professors in the field of Persian language teaching and reviewing similar researches, we came to the conclusion that nothing has been done to teach Persian language using augmented reality. Therefore, a Persian game based on augmented reality was designed and implemented and then evaluated. For evaluation, two methods have been used; the user and the heuristic evaluation. Experts in the field of Persian language teaching, human-computer interaction and a number of language learners participated in the evaluation. Their feedback shows that the use of augmented reality increases satisfaction, enthusiasm and interaction with the environment and people, and also makes the process of learning and memorizing concepts more efficient.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Mozaffari, Sonia and Hamidi, Hamid Reza},
	month = jan,
	year = {2023},
	keywords = {Artificial Intelligence, Augmented reality, Foreign Language, Language Teaching, Persian},
	pages = {4735--4748},
	file = {Full Text PDF:files/1415/Mozaffari e Hamidi - 2023 - Impacts of augmented reality on foreign language teaching a case study of Persian language.pdf:application/pdf},
}

@article{alrashidi_examining_2024,
	title = {Examining the feasibility of immersive virtual reality to measure upper limb motor performance in typically developing children and adolescents},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-00996-z},
	doi = {10.1007/s10055-024-00996-z},
	abstract = {Over the last five years, virtual reality (VR) has become more popular in pediatric physiotherapy. In this study, we assessed the feasibility and acceptability of measuring upper-limb movements in typically-developing children and adolescents using an immersive virtual reality (iVR) headset. Thirty-six typically-developing children (age: 12 ± 2.1 y) were recruited and required to draw circles in a custom-built virtual environment using a Meta-Quest-2 headset. Outcomes were the System Usability Scale (SUS), Developmental Coordination Disorder-Questionnaire (DCD-Q), and three metrics of circle drawing performance (movement time, mean velocity and circle roundness). The mean score for the SUS was 74 ± 11, indicating good levels of acceptability and usability when the participants used the headset. No strong relationships were observed between the circle drawing metrics and DCD-Q scores (rho = {\textless} 0.3, p = {\textgreater} 0.05), but circle roundness ratios were positively and significantly correlated with SUS scores (rho = 0.5, p = 0.003). No adverse effects associated with iVR use were reported for any participants. This study showed that iVR is a viable method to measure upper-limb motor performance in children and adolescents, highlighting the potential value of this tool in pediatric physiotherapy practice.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Alrashidi, Mohammed and Evans, Jack O. and Tomlinson, Richard J. and Williams, Craig A. and Buckingham, Gavin},
	month = apr,
	year = {2024},
	keywords = {Artificial Intelligence, Circle drawing task, Head-mounted displays, Pediatric rehabilitation, Technology-based assessment, Virtual environment},
	pages = {99},
	file = {Full Text PDF:files/1416/Alrashidi et al. - 2024 - Examining the feasibility of immersive virtual reality to measure upper limb motor performance in ty.pdf:application/pdf},
}

@article{martins_physioland_2024,
	title = {Physioland: a motivational complement of physical therapy for patients with neurological diseases},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {Physioland},
	url = {https://doi.org/10.1007/s11042-023-16051-z},
	doi = {10.1007/s11042-023-16051-z},
	abstract = {The number of patients with mobility constraints is increasing as a result of neurological diseases. From the substantiation of the lost functions recoveries, it was possible to determine that the nervous system is able to reorganize itself expressing its property called neuroplasticity. Physical therapy is the well-known way to encourage and promote this ability. However, repetitive traditional physical therapy exercises may become boring and patients eventually abandon their physiotherapeutic programs. The development of new environments that motivate patients to continue with their treatments may be a suitable alternative or complementary tool. Serious games seems to be the ideal tool to provide them. Thus, the purpose of this paper is to present Physioland, a serious game already developed which can be a motivational complement for the physical therapy of patients with neurological diseases. Physioland is a non-invasive system that uses Image Processing Techniques and Artificial Intelligence to monitor patients and adapts some exercises of traditional physical therapy to electronic game situations. To determine whether Physioland would be motivating and challenging enough to increase a patient's desire to perform the exercises and continue/complete the rehabilitation process the game was tested in a clinical environment using two samples: one with twelve health professionals in the area of physiotherapy and the other with eleven patients with neurological diseases. The research team carried out a questionnaire-based survey. This questionnaire is an adaptation of another one already validated in the literature—the Technology Acceptance Model (TAM). For the analysis of the data obtained with the Likert scale, percentages were calculated. The answers to the open questions were subject to a content analysis. The results showed that the developed game, Physioland, proved to be highly motivating for patients at the physiotherapy clinic where it was tested. If the results are similar in other clinics, Physioland, can be used as a good and effective complement to traditional physical therapy for patients with neurological diseases.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Martins, Tiago and Carvalho, Vítor and Soares, Filomena and Leão, Celina},
	month = jan,
	year = {2024},
	keywords = {Artificial Intelligence, Mobility problems, Neurological diseases, Neuronal plasticity, Patient motivation, Physical therapy, Physioland, Serious games},
	pages = {12035--12057},
	file = {Full Text PDF:files/1417/Martins et al. - 2024 - Physioland a motivational complement of physical therapy for patients with neurological diseases.pdf:application/pdf},
}

@article{evans_using_2023,
	title = {Using immersive virtual reality to remotely examine performance differences between dominant and non-dominant hands},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-023-00794-z},
	doi = {10.1007/s10055-023-00794-z},
	abstract = {Circle drawing may be a useful task to study upper-limb function in patient populations. However, previous studies rely on expensive and bulky robotics to measure performance. For clinics or hospitals with limited budgets and space, this may be unfeasible. Virtual reality (VR) provides a portable and low-cost tool with integrated motion capture. It offers potentially a more feasible medium by which to assess upper-limb motor function. Prior to use with patient populations, it is important to validate and test the capabilities of VR with healthy users. This study examined whether a VR-based circle drawing task, completed remotely using participant’s own devices, could capture differences between movement kinematics of the dominant and non-dominant hands in healthy individuals. Participants (n = 47) traced the outline of a circle presented on their VR head-mounted displays with each hand, while the positions of the hand-held controllers were continuously recorded. Although there were no differences observed in the size or roundness of circles drawn with each hand, consistent with prior literature our results did show that the circles drawn with the dominant hand were completed faster than those with the non-dominant hand. This provides preliminary evidence that a VR-based circle drawing task may be a feasible method for detecting subtle differences in function in clinical populations.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Evans, Jack Owen and Tsaneva-Atanasova, Krasimira and Buckingham, Gavin},
	month = sep,
	year = {2023},
	keywords = {Artificial Intelligence, Assessment, Dominant hand, Meta Quest, Non-dominant hand, Stroke, Upper limb},
	pages = {2211--2226},
	file = {Full Text PDF:files/1418/Evans et al. - 2023 - Using immersive virtual reality to remotely examine performance differences between dominant and non.pdf:application/pdf},
}

@article{alqithami_serious-gamification_2021,
	title = {A serious-gamification blueprint towards a normalized attention},
	volume = {8},
	issn = {2198-4026},
	url = {https://doi.org/10.1186/s40708-021-00127-3},
	doi = {10.1186/s40708-021-00127-3},
	abstract = {Attention is an important commodity in the human skills set. It can be trained to overcome deficits in the short term which might be based on multiple cognitive complications to entail inability to keep focus and mined wondering. On the long term, however, it might be a symptom of chronic diseases that acquire attention to include the spectra of many mental health disorders, e.g., attention deficit hyperactivity disorder (ADHD). This paper, therefore, introduces a generic reference model that guides in the design of proper treatment method for patients in short of attention to engage in a game-based environment in order to enhance the behavior of their current state of attention which may hopefully lead to a better focus. When considering the volatility of traditional cognitive behavioral therapies (CBTs), the model reflects and analyzes evolving serious games design directed for the treatment of ADHD. It serves as an instrument that spawn over a specific treatment design since it introduces essential components that depicts essential units of traditional CBT when they are modularly combined. The components will be introduced and the processes of the reference model will be elaborated as a roadmap for the formation and the operation of augmented reality treatment games.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Brain Inf.},
	author = {Alqithami, Saad},
	month = apr,
	year = {2021},
	keywords = {ADHD, Agent-based modeling, Artificial Intelligence, Attention, Cognitive behavioral therapy, Game design},
	pages = {6},
	file = {Full Text PDF:files/1419/Alqithami - 2021 - A serious-gamification blueprint towards a normalized attention.pdf:application/pdf},
}

@article{wen_scenario_2020,
	title = {A scenario generation pipeline for autonomous vehicle simulators},
	volume = {10},
	issn = {2192-1962},
	url = {https://doi.org/10.1186/s13673-020-00231-z},
	doi = {10.1186/s13673-020-00231-z},
	abstract = {To develop a realistic simulator for autonomous vehicle testing, the simulation of various scenarios that may occur near vehicles in the real world is necessary. In this paper, we propose a new scenario generation pipeline focused on generating scenarios in a specific area near an autonomous vehicle. In this method, a scenario map is generated to define the scenario simulation area. A convolutional neural network (CNN)-based scenario agent selector is introduced to evaluate whether the selected agents can generate a realistic scenario, and a collision event detector handles the collision message to trigger an accident event. The proposed event-centric action dispatcher in the pipeline enables agents near events to perform related actions when the events occur near the autonomous vehicle. The proposed scenario generation pipeline can generate scenarios containing pedestrians, animals, and vehicles, and, advantageously, no user intervention is required during the simulation. In addition, a virtual environment for autonomous driving is also implemented to test the proposed scenario generation pipeline. The results show that the CNN-based scenario agent selector chose agents that provided realistic scenarios with 92.67\% accuracy, and the event-centric action dispatcher generated a visually realistic scenario by letting the agents surrounding the event generate related actions.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Hum. Cent. Comput. Inf. Sci.},
	author = {Wen, Mingyun and Park, Jisun and Cho, Kyungeun},
	month = jun,
	year = {2020},
	keywords = {Artificial intelligence, Artificial Intelligence, Autonomous driving, Convolutional neural network, Scenario generation},
	pages = {24},
	file = {Full Text PDF:files/1420/Wen et al. - 2020 - A scenario generation pipeline for autonomous vehicle simulators.pdf:application/pdf},
}

@article{matsangidou_participatory_2023,
	title = {Participatory design and evaluation of virtual reality physical rehabilitation for people living with dementia},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-022-00639-1},
	doi = {10.1007/s10055-022-00639-1},
	abstract = {Emerging research confirms the need for technologically enhanced solutions to support non-pharmacological interventions which can improve the quality of life, the mental and physical health of demented people. Several types of research examined if virtual reality can be an effective solution. This paper aims to present the cyclic process of prototyping, testing, analysing, and refining the VR system in real-world clinical settings. Seven people with moderate to severe dementia were recruited. The experiment required the patients to attend three virtual reality iterations of rapid prototyping with user testing. All three iterations involved training activities with upper body movements similar to their usual physical training. A mixed-methods design measured affect and emotional behaviour using the Observed Emotion Rating Scale and the Visual Analog Scale. Content analysis was conducted following observations and interviews. During each iteration of rapid prototyping with user testing, quantitative measurements of performance, independence and time were recorded. Eye tracking and movement information were captured by the system. Finally, a simplified version of the presence and usability scales evaluated the system. The results of this study provide further evidence that virtual reality can play a significant role in the improvement of people’s with dementia physical training and emotional health when is appropriately designed. The results present the vital factors which should be incorporated in a virtual reality system which are: 1) a simple interactions modality; 2) visible visual targets and continuous feedback; 3) personalized virtual environments; 4) personalized range of movements.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Matsangidou, Maria and Frangoudes, Fotos and Schiza, Eirini and Neokleous, Kleanthis C. and Papayianni, Ersi and Xenari, Katerian and Avraamides, Marios and Pattichis, Constantinos S.},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, Dementia, Emotional health, Eye tracking, Physical rehabilitation, Virtual reality},
	pages = {421--438},
	file = {Full Text PDF:files/1441/Matsangidou et al. - 2023 - Participatory design and evaluation of virtual reality physical rehabilitation for people living wit.pdf:application/pdf},
}

@article{ardito_special_2019,
	title = {Special {Issue} on {Advances} in {Human}-{Computer} {Interaction}},
	volume = {78},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-019-7690-5},
	doi = {10.1007/s11042-019-7690-5},
	language = {en},
	number = {10},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Ardito, Carmelo and De Marsico, Maria and Gadia, Davide and Maggiorini, Dario and Mariani, Ilaria and Ripamonti, Laura and Santoro, Carmen},
	month = may,
	year = {2019},
	pages = {13353--13359},
	file = {Full Text PDF:files/1442/Ardito et al. - 2019 - Special Issue on Advances in Human-Computer Interaction.pdf:application/pdf},
}

@article{garcia-carrillo_testing_2024,
	title = {Testing driver warning systems for off-road industrial vehicles using a cyber-physical simulator},
	issn = {1783-8738},
	url = {https://doi.org/10.1007/s12193-024-00435-y},
	doi = {10.1007/s12193-024-00435-y},
	abstract = {ADAS (Advanced Driver Assistance Systems) are becoming increasingly popular in on-road vehicles due to their safety, productivity, and cost savings. In the same way, off-road vehicles can benefit from ADAS systems to improve the security of drivers and workers in industrial settings. In this work, we study, design, and develop a novel security system to be integrated into industrial vehicles. This system is built to provide one-way Human Computer Interaction, from the computer to the human, so providing, through the interaction with the ADAS system, feedback to drivers about their surroundings, such as nearby workers, and thus helping to avoid collisions and prevent incidents. The study evaluates the quality of different feedback mechanisms, with the goal of designing the ADAS that produces the best User eXperience (UX). These feedback mechanisms are generated by LEDs in different display formats and colors, as well as with haptic feedback. We created a hybrid testbed using a realistic ADAS and a forklift simulator, integrating the system into a physical structure that resembles an industrial vehicle (a forklift) and used a computer-based simulation of a warehouse to gather the information from users. We performed a study with 36 people for the evaluation of the different feedback mechanisms tested, evaluating the results both from an objective point of view based on the results of the simulation, and a subjective point of view through a questionnaire and the stress of the users in each test.},
	language = {en},
	urldate = {2024-09-05},
	journal = {J Multimodal User Interfaces},
	author = {Garcia-Carrillo, Dan and Garcia, Roberto and Pañeda, Xabiel G. and Mourao, Filipa and Melendi, David and Corcoba, Victor and Paiva, Sara},
	month = jun,
	year = {2024},
	keywords = {ADAS, Artificial Intelligence, HCI, Human-ADAS interaction, Industrial vehicles, Security},
	file = {Full Text PDF:files/1443/Garcia-Carrillo et al. - 2024 - Testing driver warning systems for off-road industrial vehicles using a cyber-physical simulator.pdf:application/pdf},
}

@article{trevena_vr_2024,
	title = {{VR} interventions aimed to induce empathy: a scoping review},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {{VR} interventions aimed to induce empathy},
	url = {https://doi.org/10.1007/s10055-024-00946-9},
	doi = {10.1007/s10055-024-00946-9},
	abstract = {To assess the methods and outcomes of virtual reality (VR), interventions aimed at inducing empathy and to evaluate if VR could be used in this manner for disability support worker (DSW) training, as well as highlight areas for future research. The authors conducted a scoping review of studies that used VR interventions to induce empathy in participants. We searched three databases for articles published between 1960 and 2021 using “virtual reality” and “empathy” as key terms. The search yielded 707 articles, and 44 were reviewed. VR interventions largely resulted in enhanced empathy skills for participants. Most studies agreed that VR’s ability to facilitate perspective-taking was key to inducing empathy for participants. Samples were often limited to the context of healthcare, medicine, and education. This literature provides preliminary evidence for the technology’s efficacy for inducing empathy. Identified research gaps relate to limited studies done, study quality and design, best practice intervention characteristics, populations and outcomes of interest, including lack of transfer and data across real-world settings.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Trevena, Lee and Paay, Jeni and McDonald, Rachael},
	month = mar,
	year = {2024},
	keywords = {Artificial Intelligence, Disability support worker, Empathy, Scoping review, Technology, Training, Virtual reality},
	pages = {80},
	file = {Full Text PDF:files/1444/Trevena et al. - 2024 - VR interventions aimed to induce empathy a scoping review.pdf:application/pdf},
}

@article{marques_is_2023,
	title = {Is social presence (alone) a general predictor for good remote collaboration? comparing video and augmented reality guidance in maintenance procedures},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Is social presence (alone) a general predictor for good remote collaboration?},
	url = {https://doi.org/10.1007/s10055-023-00770-7},
	doi = {10.1007/s10055-023-00770-7},
	abstract = {A common practice in scenarios of remote collaboration is to provide a representation from distributed team members, aiming to positively influence the level of social presence and in turn the work effort. Nevertheless, these stimuli can lead to fractured learning experiences, since collaborators need to split attention among the task, the shared information, and the counterpart representation. This paper explored how the last affects social presence, and other dimensions of collaboration, as well as task resolution in scenarios of remote guidance. A user study was conducted, comparing two distinct conditions: traditional video chat (team members representation always visible) and Augmented Reality (AR) annotations (collaborators representation never available). These were selected due to ongoing research with partners from the industry sector, following the insights of a participatory design process. A real-life use-case was considered, i.e., synchronous maintenance task with 4 completion stages that required a remote expert using a computer to guide 37 on-site participants wielding a handheld device. The results of the study are described and discussed based on data analysis, showing that the majority of participants preferred the AR-based condition, despite the absence of the expert representation.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Marques, Bernardo and Ferreira, Carlos and Silva, Samuel and Dias, Paulo and Santos, Beatriz Sousa},
	month = sep,
	year = {2023},
	keywords = {Artificial Intelligence, Augmented reality annotations, Collaborative process, Remote collaboration, Social presence, Task resolution, User study, Video stream},
	pages = {1783--1796},
	file = {Full Text PDF:files/1445/Marques et al. - 2023 - Is social presence (alone) a general predictor for good remote collaboration comparing video and au.pdf:application/pdf},
}

@article{hogan_assessment_2023,
	title = {Assessment of prospective memory after stroke utilizing virtual reality},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00576-5},
	doi = {10.1007/s10055-021-00576-5},
	abstract = {Prospective Memory (PM) is the ability to remember to do something in the future. It is often impaired after stroke and can impact on an individual’s level of independence and daily functioning. PM tasks have been criticized for their lack of ecological validity wherein test results may not be related to actual performance in daily life. With ecological validity in mind, the Virtual Reality Prospective Memory Shopping Task (VRPMST) was designed to assess two types of PM, time- and event-based. This study aimed to examine the ecological and convergent validity of the VRPMST in comparison to an experimental (Lexical Decision PM Task) and clinical measure of PM (Cambridge PM Test). Twelve individuals with stroke and 12 controls were administered three PM measures, three neuropsychological measures, and two user-friendliness questionnaires, one for the experimental PM measure and one for the VRPMST. Individuals with stroke showed impairments in PM compared to controls on all three PM measures, particularly time-based PM. Individuals with stroke were found to monitor time significantly less than controls on both the experimental PM measure and the VRPMST. The VRPMST was found to be sensitive in measuring PM, have better ecological validity when compared to the experimental PM measure, and good convergent validity. The findings of this study have helped to clarify that PM impairment does exist after stroke, possibly due to a problem in strategic monitoring. In addition, we have demonstrated how VR technology can be used to design a measure of cognitive function commonly impaired after stroke.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Hogan, Christy and Cornwell, Petrea and Fleming, Jennifer and Man, David W. K. and Shum, David H. K.},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, Memory, Neuropsychological test, Psychometrics, Stroke, Virtual reality},
	pages = {333--346},
	file = {Full Text PDF:files/1446/Hogan et al. - 2023 - Assessment of prospective memory after stroke utilizing virtual reality.pdf:application/pdf},
}

@article{horst_back_2024,
	title = {Back to reality: transition techniques from short {HMD}-based virtual experiences to the physical world},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {Back to reality},
	url = {https://doi.org/10.1007/s11042-021-11317-w},
	doi = {10.1007/s11042-021-11317-w},
	abstract = {Every Virtual Reality (VR) experience has to end at some point. While there already exist concepts to design transitions for users to enter a virtual world, their return from the physical world should be considered, as well, as it is a part of the overall VR experience. We call the latter outro-transitions. In contrast to offboarding of VR experiences, that takes place after taking off VR hardware (e.g., HMDs), outro-transitions are still part of the immersive experience. Such transitions occur more frequently when VR is experienced periodically and for only short times. One example where transition techniques are necessary is in an auditorium where the audience has individual VR headsets available, for example, in a presentation using PowerPoint slides together with brief VR experiences sprinkled between the slides. The audience must put on and take off HMDs frequently every time they switch from common presentation media to VR and back. In a such a one-to-many VR scenario, it is challenging for presenters to explore the process of multiple people coming back from the virtual to the physical world at once. Direct communication may be constrained while VR users are wearing an HMD. Presenters need a tool to indicate them to stop the VR session and switch back to the slide presentation. Virtual visual cues can help presenters or other external entities (e.g., automated/scripted events) to request VR users to end a VR session. Such transitions become part of the overall experience of the audience and thus must be considered. This paper explores visual cues as outro-transitions from a virtual world back to the physical world and their utility to enable presenters to request VR users to end a VR session. We propose and investigate eight transition techniques. We focus on their usage in short consecutive VR experiences and include both established and novel techniques. The transition techniques are evaluated within a user study to draw conclusions on the effects of outro-transitions on the overall experience and presence of participants. We also take into account how long an outro-transition may take and how comfortable our participants perceived the proposed techniques. The study points out that they preferred non-interactive outro-transitions over interactive ones, except for a transition that allowed VR users to communicate with presenters. Furthermore, we explore the presenter-VR user relation within a presentation scenario that uses short VR experiences. The study indicates involving presenters that can stop a VR session was not only negligible but preferred by our participants.},
	language = {en},
	number = {15},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Horst, Robin and Naraghi-Taghi-Off, Ramtin and Rau, Linda and Dörner, Ralf},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Asymmetric virtual environments, Games Engineering, Offboarding, Presentation slides, Short VR experiences, Transitions, Virtual Reality},
	pages = {46683--46706},
	file = {Full Text PDF:files/1447/Horst et al. - 2024 - Back to reality transition techniques from short HMD-based virtual experiences to the physical worl.pdf:application/pdf},
}

@article{halabi_augmented_2021,
	title = {Augmented reality flavor: cross-modal mapping across gustation, olfaction, and vision},
	volume = {80},
	issn = {1573-7721},
	shorttitle = {Augmented reality flavor},
	url = {https://doi.org/10.1007/s11042-021-11321-0},
	doi = {10.1007/s11042-021-11321-0},
	abstract = {Gustatory display research is still in its infancy despite being one of the essential everyday senses that human practice while eating and drinking. Indeed, the most important and frequent tasks that our brain deals with every day are foraging and feeding. The recent studies by psychologists and cognitive neuroscientist revealed how complex multisensory rely on the integration of cues from all the human senses in any flavor experiences. The perception of flavor is multisensory and involves combinations of gustatory and olfactory stimuli. The cross-modal mapping between these modalities needs to be more explored in the virtual environment and simulation, especially in liquid food. In this paper, we present a customized wearable Augmented Reality (AR) system and olfaction display to study the effect of vision and olfaction on the gustatory sense. A user experiment and extensive analysis conducted to study the influence of each stimulus on the overall flavor, including other factors like age, previous experience in Virtual Reality (VR)/AR, and beverage consumption. The result showed that smell contributes strongly to the flavor with less contribution to the vision. However, the combination of these stimuli can deliver richer experience and a higher belief rate. Beverage consumption had a significant effect on the flavor belief rate. Experience is correlated with stimulus and age is correlated with belief rate, and both indirectly affected the belief rate.},
	language = {en},
	number = {30},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Halabi, Osama and Saleh, Mohammad},
	month = dec,
	year = {2021},
	keywords = {Artificial Intelligence, Augmented reality, Cross-modal mapping, Flavor, Gustation, Olfactory display},
	pages = {36423--36441},
	file = {Full Text PDF:files/1448/Halabi e Saleh - 2021 - Augmented reality flavor cross-modal mapping across gustation, olfaction, and vision.pdf:application/pdf},
}

@article{pinto_deep_2021,
	title = {Deep learning and multivariate time series for cheat detection in video games},
	volume = {110},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-021-06055-x},
	doi = {10.1007/s10994-021-06055-x},
	abstract = {Online video games drive a multi-billion dollar industry dedicated to maintaining a competitive and enjoyable experience for players. Traditional cheat detection systems struggle when facing new exploits or sophisticated fraudsters. More advanced solutions based on machine learning are more adaptive but rely heavily on in-game data, which means that each game has to develop its own cheat detection system. In this work, we propose a novel approach to cheat detection that doesn’t require in-game data. Firstly, we treat the multimodal interactions between the player and the platform as multivariate time series. We then use convolutional neural networks to classify these time series as corresponding to legitimate or fraudulent gameplay. Our models achieve an average accuracy of respectively 99.2\% and 98.9\% in triggerbot and aimbot (two widespread cheats), in an experiment to validate the system’s ability to detect cheating in players never seen before. Because this approach is based solely on player behavior, it can be applied to any game or input method, and even various tasks related to modeling human activity.},
	language = {en},
	number = {11},
	urldate = {2024-09-05},
	journal = {Mach Learn},
	author = {Pinto, José Pedro and Pimenta, André and Novais, Paulo},
	month = dec,
	year = {2021},
	keywords = {Artificial Intelligence, Deep learning, Human–computer interaction, Multivariate time series, Video games},
	pages = {3037--3057},
	file = {Full Text PDF:files/1449/Pinto et al. - 2021 - Deep learning and multivariate time series for cheat detection in video games.pdf:application/pdf},
}

@article{williams_evaluating_2013,
	title = {Evaluating {Player} {Strategies} in the {Design} of a {Hot} {Hand} {Game}},
	volume = {3},
	issn = {2010-2283},
	url = {https://doi.org/10.7603/s40601-013-0006-0},
	doi = {10.7603/s40601-013-0006-0},
	abstract = {The user’s strategy and their approach to decisionmaking are two important concerns when designing user-centric software. While decision-making and strategy are key factors in a wide range of business systems from stock market trading to medical diagnosis, in this paper we focus on the role these factors play in a serious computer game. Players may adopt individual strategies when playing a computer game. Furthermore, different approaches to playing the game may impact on the effectiveness of the core mechanics designed into the game play. In this paper we investigate player strategy in relation to two serious games designed for studying the ‘hot hand’. The ‘hot hand’ is an interesting psychological phenomenon originally studied in sports such as basketball. The study of ‘hot hand’ promises to shed further light on cognitive decision-making tasks applicable to domains beyond sport. The ‘hot hand’ suggests that players sometimes display above average performance, get on a hot streak, or develop ‘hot hands’. Although this is a widely held belief, analysis of data in a number of sports has produced mixed findings. While this lack of evidence may indicate belief in the hot hand is a cognitive fallacy, alternate views have suggested that the player’s strategy, confidence, and risk-taking may account for the difficulty of measuring the hot hand. Unfortunately, it is difficult to objectively measure and quantify the amount of risk taking in a sporting contest. Therefore to investigate this phenomenon more closely we developed novel, tailor-made computer games that allow rigorous empirical study of ‘hot hands’. The design of such games has some specific design requirements. The gameplay needs to allow players to perform a sequence of repeated challenges, where they either fail or succeed with about equal likelihood. Importantly the design also needs to allow players to choose a strategy entailing more or less risk in response to their current performance. In this paper we compare two hot hand game designs by collecting empirical data that captures player performance in terms of success and level of difficulty (as gauged by response time). We then use a variety of analytical and visualization techniques to study player strategies in these games. This allows us to detect a key design flaw the first game and validate the design of the second game for use in further studies of the hot hand phenomenon.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {GSTF J Comput},
	author = {Williams, Paul and Nesbitt, Keith and Eidels, Ami and Washburn, Mark and Cornforth, David},
	month = jul,
	year = {2013},
	keywords = {Artificial Intelligence, Evaluation, Games, Psychology, User-centered design},
	pages = {6},
	file = {Full Text PDF:files/1450/Williams et al. - 2013 - Evaluating Player Strategies in the Design of a Hot Hand Game.pdf:application/pdf},
}

@article{de_back_benefits_2020,
	title = {Benefits of immersive collaborative learning in {CAVE}-based virtual reality},
	volume = {17},
	issn = {2365-9440},
	url = {https://doi.org/10.1186/s41239-020-00228-9},
	doi = {10.1186/s41239-020-00228-9},
	abstract = {How to make the learning of complex subjects engaging, motivating, and effective? The use of immersive virtual reality offers exciting, yet largely unexplored solutions to this problem. Taking neuroanatomy as an example of a visually and spatially complex subject, the present study investigated whether academic learning using a state-of-the-art Cave Automatic Virtual Environment (CAVE) yielded higher learning gains compared to conventional textbooks. The present study leveraged a combination of CAVE benefits including collaborative learning, rich spatial information, embodied interaction and gamification. Results indicated significantly higher learning gains after collaborative learning in the CAVE with large effect sizes compared to a textbook condition. Furthermore, low spatial ability learners benefitted most from the strong spatial cues provided by immersive virtual reality, effectively raising their performance to that of high spatial ability learners. The present study serves as a concrete example of the effective design and implementation of virtual reality in CAVE settings, demonstrating learning gains and thus opening opportunities to more pervasive use of immersive technologies for education. In addition, the study illustrates how immersive learning may provide novel scaffolds to increase performance in those who need it most.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Int J Educ Technol High Educ},
	author = {de Back, Tycho T. and Tinga, Angelica M. and Nguyen, Phong and Louwerse, Max M.},
	month = nov,
	year = {2020},
	keywords = {Artificial Intelligence, Collaborative learning, Digital Education and Educational Technology, Improving classroom teaching, Interactive learning environments, Media in education, Virtual reality},
	pages = {51},
	file = {Full Text PDF:files/1451/de Back et al. - 2020 - Benefits of immersive collaborative learning in CAVE-based virtual reality.pdf:application/pdf},
}

@article{michalski_improving_2023,
	title = {Improving real-world skills in people with intellectual disabilities: an immersive virtual reality intervention},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Improving real-world skills in people with intellectual disabilities},
	url = {https://doi.org/10.1007/s10055-023-00759-2},
	doi = {10.1007/s10055-023-00759-2},
	abstract = {Virtual reality (VR) is a promising tool for training life skills in people with intellectual disabilities. However, there is a lack of evidence surrounding the implementation, suitability, and effectiveness of VR training in this population. The present study investigated the effectiveness of VR training for people with intellectual disabilities by assessing (1) their ability to complete basic tasks in VR, (2) real-world transfer and skill generalisation, and (3) the individual characteristics of participants able to benefit from VR training. Thirty-two participants with an intellectual disability of varying severity completed a waste management training intervention in VR that involved sorting 18 items into three bins. Real-world performance was measured at pre-test, post-test, and delayed time points. The number of VR training sessions varied as training ceased when participants met the learning target (≈ 90\% correct). A survival analysis assessed training success probability as a function of the number of training sessions with participants split by their level of adaptive functioning (as measured on the Adaptive Behaviour Assessment System Third Edition). The learning target was met by 19 participants (59.4\%) within ten sessions (Mdn = 8.5, IQR 4–10). Real-world performance significantly improved from pre- to post-test and pre- to delayed test. There was no significant difference from post- to delayed test. Further, there was a significant positive relationship between adaptive functioning and change in the real-world assessment from the pre-test to the post- and delayed tests. VR facilitated the learning of most participants, which led to demonstrations of real-world transfer and skill generalisation. The present study identified a relationship between adaptive functioning and success in VR training. The survival curve may assist in planning future studies and training programs.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Michalski, Stefan Carlo and Gallomarino, Nicholas Charles and Szpak, Ancret and May, Kieran William and Lee, Gun and Ellison, Caroline and Loetscher, Tobias},
	month = dec,
	year = {2023},
	keywords = {Adaptive functioning, Artificial Intelligence, Cybersickness, Intellectual disability, Learning, Skill generalisation, Training, Transfer, Virtual reality},
	pages = {3521--3532},
	file = {Full Text PDF:files/1452/Michalski et al. - 2023 - Improving real-world skills in people with intellectual disabilities an immersive virtual reality i.pdf:application/pdf},
}

@article{weller_redirected_2022,
	title = {Redirected walking in virtual reality with auditory step feedback},
	volume = {38},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-022-02565-4},
	doi = {10.1007/s00371-022-02565-4},
	abstract = {We present a novel approach of redirected walking (RDW) based on step feedback sounds to redirect users in virtual reality. The main idea is to achieve path manipulation by changing step noises to deviate the users, who still believe that they are walking a straight line. Our approach can be combined with traditional visual approaches for RDW based on eye-blinking. Moreover, we have conducted a user study in a large area (\$\$10{\textbackslash}times 20\$\$m) using a within-subject design. We achieved a translational redirection of 1.7m in average with pure audio feedback. Moreover, our results show that visual methods can amplify the deviation of our new auditory approach by 80cm in average at the distance of 20 m.},
	language = {en},
	number = {9},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Weller, Rene and Brennecke, Benjamin and Zachmann, Gabriel},
	month = sep,
	year = {2022},
	keywords = {Artificial Intelligence, Auditory Step, Redirected Walking, Virtual Reality},
	pages = {3475--3486},
	file = {Full Text PDF:files/1453/Weller et al. - 2022 - Redirected walking in virtual reality with auditory step feedback.pdf:application/pdf},
}

@article{baldoni_definition_2024,
	title = {Definition of guidelines for virtual reality application design based on visual attention},
	volume = {83},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-023-17488-y},
	doi = {10.1007/s11042-023-17488-y},
	abstract = {In virtual reality applications, head-mounted displays allow users to explore virtual surroundings, thus creating a high sense of immersion. However, due to the novelty of the technology and the possibility of freely enjoying a \$\$360{\textasciicircum}{\textbackslash}circ \$\$virtual world, users can get distracted and divert their attention from the content of the application. In this work, we define a set of guidelines for the design of virtual reality applications for enhancing the users’ attention. To the best of our knowledge, this is one of the first attempts to provide general guidelines for virtual application design based on visual attention. More specifically, we analyze the different categories of factors that contribute to the user’s responsiveness and define a set of experiments for measuring the user’s promptness with respect to visual stimuli with different features and in the presence of audio/visual distractions. Experimental tests have been carried out with 36 volunteers. The users’ reaction time has been recorded and the performed analysis allowed the definition of a set of guidelines based on individual, operational, and technological factors for the design of virtual reality applications optimized in terms of user attention. In particular, statistical tests demonstrated that the presence of distractions leads to significantly different reaction times with respect to the case of no distractions, and that users belonging to different age intervals have significantly different behaviors. Moreover, the optimal placement of objects has been identified and the impact of cybersickness has been analyzed.},
	language = {en},
	number = {16},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Baldoni, Sara and Hadj Sassi, Mohamed Saifeddine and Carli, Marco and Battisti, Federica},
	month = may,
	year = {2024},
	keywords = {Artificial Intelligence, Attention-driven application design, Reaction time, Virtual reality},
	pages = {49615--49640},
	file = {Full Text PDF:files/1454/Baldoni et al. - 2024 - Definition of guidelines for virtual reality application design based on visual attention.pdf:application/pdf},
}

@article{marin-lora_creating_2024,
	title = {Creating a treadmill running video game with smartwatch interaction},
	volume = {83},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-023-17752-1},
	doi = {10.1007/s11042-023-17752-1},
	abstract = {In recent years, indoor or at-home sports have experienced significant growth. However, monotony is a common challenge in these static physical activities. Exergames, a genre of video games that combines physical activity and entertainment, have emerged as an attractive solution. Nevertheless, running on a treadmill and engaging in other activities simultaneously presents additional challenges. The balance and concentration required during running while interacting with a video game demand a special focus on the design of the Exergame. This paper presents a mobile Exergame designed specifically for treadmill running, utilizing interaction with a smartwatch. The game offers natural environments where, through smartwatch technology, it interprets the player's movements, transforming them into running speed and interactive actions by detecting gestures within the game. The main objective is to provide users with a satisfying gaming experience tailored to the characteristics of treadmill running. Particular emphasis has been placed on prioritizing the playful component of this Exergame, recognizing its relevance in the context of treadmill running. To evaluate the achievement of objectives and the proposed hypothesis, a comparative study was conducted between the proposed Exergame and a treadmill running simulator. Participants experienced both experiences and subsequently completed the Game Experience Questionnaire (GEQ), specifically the In-game GEQ version. The results obtained indicate that participants had a better gaming experience in the Exergame than in the simulator. These findings highlight the importance of prioritizing the playful component in Exergames and provide guidelines for future improvements and developments in the field.},
	language = {en},
	number = {19},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Marín-Lora, Carlos and Chover, Miguel and Martín, Micaela Yanet and García-Rytman, Linda},
	month = jun,
	year = {2024},
	keywords = {Artificial Intelligence, Cadence, Exergame, Gestures, Interaction, Mobile, Running, Smartwatch, Treadmill},
	pages = {57709--57729},
	file = {Full Text PDF:files/1455/Marín-Lora et al. - 2024 - Creating a treadmill running video game with smartwatch interaction.pdf:application/pdf},
}

@article{tadeja_exploring_2021,
	title = {Exploring gestural input for engineering surveys of real-life structures in virtual reality using photogrammetric {3D} models},
	volume = {80},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-021-10520-z},
	doi = {10.1007/s11042-021-10520-z},
	abstract = {Photogrammetry is a promising set of methods for generating photorealistic 3D models of physical objects and structures. Such methods may rely solely on camera-captured photographs or include additional sensor data. Digital twins are digital replicas of physical objects and structures. Photogrammetry is an opportune approach for generating 3D models for the purpose of preparing digital twins. At a sufficiently high level of quality, digital twins provide effective archival representations of physical objects and structures and become effective substitutes for engineering inspections and surveying. While photogrammetric techniques are well-established, insights about effective methods for interacting with such models in virtual reality remain underexplored. We report the results of a qualitative engineering case study in which we asked six domain experts to carry out engineering measurement tasks in an immersive environment using bimanual gestural input coupled with gaze-tracking. The qualitative case study revealed that gaze-supported bimanual interaction of photogrammetric 3D models is a promising modality for domain experts. It allows the experts to efficiently manipulate and measure elements of the 3D model. To better allow designers to support this modality, we report design implications distilled from the feedback from the domain experts.},
	language = {en},
	number = {20},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Tadeja, Sławomir Konrad and Lu, Yupu and Rydlewicz, Maciej and Rydlewicz, Wojciech and Bubas, Tomasz and Kristensson, Per Ola},
	month = aug,
	year = {2021},
	keywords = {Artificial Intelligence, Digital twinning, Immersive analytics, Industrial visual analytics, Photogrammetry, Virtual reality, Virtual reality content},
	pages = {31039--31058},
	file = {Full Text PDF:files/1456/Tadeja et al. - 2021 - Exploring gestural input for engineering surveys of real-life structures in virtual reality using ph.pdf:application/pdf},
}

@article{zheng_consensus_2023,
	title = {Consensus decision-making in artificial swarms via entropy-based local negotiation and preference updating},
	volume = {17},
	issn = {1935-3820},
	url = {https://doi.org/10.1007/s11721-023-00226-3},
	doi = {10.1007/s11721-023-00226-3},
	abstract = {This paper presents an entropy-based consensus algorithm for a swarm of artificial agents with limited sensing, communication, and processing capabilities. Each agent is modeled as a probabilistic finite state machine with a preference for a finite number of options defined as a probability distribution. The most preferred option, called exhibited decision, determines the agent’s state. The state transition is governed by internally updating this preference based on the states of neighboring agents and their entropy-based levels of certainty. Swarm agents continuously update their preferences by exchanging the exhibited decisions and the certainty values among the locally connected neighbors, leading to consensus towards an agreed-upon decision. The presented method is evaluated for its scalability over the swarm size and the number of options and its reliability under different conditions. Adopting classical best-of-N target selection scenarios, the algorithm is compared with three existing methods, the majority rule, frequency-based method, and k-unanimity method. The evaluation results show that the entropy-based method is reliable and efficient in these consensus problems.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Swarm Intell},
	author = {Zheng, Chuanqi and Lee, Kiju},
	month = dec,
	year = {2023},
	keywords = {Artificial Intelligence, Consensus algorithm, Distributed decision-making, Swarm robotics},
	pages = {283--303},
	file = {Full Text PDF:files/1457/Zheng e Lee - 2023 - Consensus decision-making in artificial swarms via entropy-based local negotiation and preference up.pdf:application/pdf},
}

@article{kallai_postural_2023,
	title = {Postural instability-induced compensative movements in virtual reality},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-022-00716-5},
	doi = {10.1007/s10055-022-00716-5},
	abstract = {During virtual reality usage, two egocentric mental representations are constructed simultaneously. The first representation is rooted in the physical reality in which VR is set up, and the second originates from the mental construction of a computer-generated virtual environment. In both cases, participants configure their posture based on multimodal stimuli while responding to environmental cues. In most cases, the postural cues provided by the digital and real environment may be conflicting. In this study, 50 right-handed volunteers were enrolled. In a pre-test session, attentional focus-related personality bias (perspective-taking) was assessed, and afterward, postural movements and presence experiences were measured while the participants performed a spatial orientation task in VR. Participants were placed in an upright position with their right hands positioned in front of a physically real point on the laboratory wall. Afterward, participants were exposed to a VR environment in which they performed a room-tilting task. Participants with higher hand-related presence scores showed decreased compensatory hand drift in the VR environment. The rate of contralateral hand drift showed a reversed association with the intensity of the perspective-taking trait. VR-induced postural instability can be attenuated by the compensative hand drift that alleviates the conflicts between the two rival inner VR and outer VR environments that compete for attention and provide different reference cues.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Kállai, János and Zsidó, András Norbert and Tamás, István and Topa, Kristóf and Kállai, Kata M. and Páll, Tamás},
	month = jun,
	year = {2023},
	keywords = {Artificial Intelligence, Attention allocation, Compensative hand drift, Hemispace, Immersion, Posture, Reference taking, Virtual reality},
	pages = {1251--1263},
	file = {Full Text PDF:files/1458/Kállai et al. - 2023 - Postural instability-induced compensative movements in virtual reality.pdf:application/pdf},
}

@article{stocco_model_2023,
	title = {Model vs system level testing of autonomous driving systems: a replication and extension study},
	volume = {28},
	issn = {1573-7616},
	shorttitle = {Model vs system level testing of autonomous driving systems},
	url = {https://doi.org/10.1007/s10664-023-10306-x},
	doi = {10.1007/s10664-023-10306-x},
	abstract = {Offline model-level testing of autonomous driving software is much cheaper, faster, and diversified than in-field, online system-level testing. Hence, researchers have compared empirically model-level vs system-level testing using driving simulators. They reported the general usefulness of simulators at reproducing the same conditions experienced in-field, but also some inadequacy of model-level testing at exposing failures that are observable only in online mode. In this work, we replicate the reference study on model vs system-level testing of autonomous vehicles while acknowledging several assumptions that we had reconsidered. These assumptions are related to several threats to validity affecting the original study that motivated additional analysis and the development of techniques to mitigate them. Moreover, we also extend the replicated study by evaluating the original findings when considering a physical, radio-controlled autonomous vehicle. Our results show that simulator-based testing of autonomous driving systems yields predictions that are close to the ones of real-world datasets when using neural-based translation to mitigate the reality gap induced by the simulation platform. On the other hand, model-level testing failures are in line with those experienced at the system level, both in simulated and physical environments, when considering the pre-failure site, similar-looking images, and accurate labels.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Empir Software Eng},
	author = {Stocco, Andrea and Pulfer, Brian and Tonella, Paolo},
	month = may,
	year = {2023},
	keywords = {Artificial Intelligence, Autonomous driving, Deep neural networks, DNN testing, Model testing, System testing},
	pages = {73},
	file = {Full Text PDF:files/1459/Stocco et al. - 2023 - Model vs system level testing of autonomous driving systems a replication and extension study.pdf:application/pdf},
}

@article{cadet_memory_2022,
	title = {Memory for a virtual reality experience in children and adults according to image quality, emotion, and sense of presence},
	volume = {26},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00537-y},
	doi = {10.1007/s10055-021-00537-y},
	abstract = {Numerous studies have explored the effects of virtual reality (VR) on adults’ cognition. Little is known, however, of these effects in children. The aim of this study was to explore, in both children and adults, the respective roles of the specific factors of VR, such as immersion, sense of presence and emotion, on memory performance. To do so, we used a head-mounted display to present a VR experience in which we manipulated immersion by varying 3D asset quality (High and Low) and emotion by presenting negative, neutral and positive stimuli. 48 adults (Mage = 20.65) and 40 children (Mage = 11.63) were both divided into two experimental groups (High vs. Low 3D model quality). Valence, arousal, and sense of presence were self-assessed by means of questionnaires, while memory of the presented stimuli was assessed using a free recall task. We also performed physiological measurements to provide objective support for our data. Results showed that memory performance was better for emotional than for neutral stimuli regardless of age group, even though children seemed to avoid looking at negative stimuli compared to neutral ones. Memory was predicted by arousal and presence in adults and only by arousal in children. Memory was not impaired by using poor image quality when highly arousing content was displayed. This study revealed that, contrary to adults, the use of poor image quality did not protect children from strong emotional experiences in VR. The roles of familiarity and arousal are discussed to help explain these results.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Cadet, Lénaïc B. and Reynaud, Emanuelle and Chainay, Hanna},
	month = mar,
	year = {2022},
	keywords = {Adults, Artificial Intelligence, Children, Emotion, Head-mounted display, Memory, Physiology, Presence, Virtual reality},
	pages = {55--75},
	file = {Full Text PDF:files/1460/Cadet et al. - 2022 - Memory for a virtual reality experience in children and adults according to image quality, emotion,.pdf:application/pdf},
}

@article{tatnall_editorial_2020-1,
	title = {Editorial for {EAIT} issue 1, 2020},
	volume = {25},
	issn = {1573-7608},
	url = {https://doi.org/10.1007/s10639-019-10083-5},
	doi = {10.1007/s10639-019-10083-5},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Educ Inf Technol},
	author = {Tatnall, Arthur},
	month = jan,
	year = {2020},
	pages = {1--9},
	file = {Full Text PDF:files/1481/Tatnall - 2020 - Editorial for EAIT issue 1, 2020.pdf:application/pdf},
}

@article{de_marsico_sustainable_2021,
	title = {Sustainable, empowering and emotional interactive multimedia},
	volume = {80},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-021-11714-1},
	doi = {10.1007/s11042-021-11714-1},
	language = {en},
	number = {26},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {De Marsico, Maria and Spagnolli, Anna and Pittarello, Fabio and Gamberini, Luciano},
	month = nov,
	year = {2021},
	pages = {34787--34789},
	file = {Full Text PDF:files/1482/De Marsico et al. - 2021 - Sustainable, empowering and emotional interactive multimedia.pdf:application/pdf},
}

@article{badger_developing_2023,
	title = {Developing a virtual reality environment for educational and therapeutic application to investigate psychological reactivity to bullying},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-023-00829-5},
	doi = {10.1007/s10055-023-00829-5},
	abstract = {Understanding how bullying victimisation influences cognitive and emotional processes may help to direct early intervention to prevent the development of psychopathology. In a convenience sample of 67 female adolescents, we assessed the potential of a newly developed classroom-set bullying experience in virtual reality (VR) to evoke psychological reactions. Two VR experiences were co-developed with young people, one neutral and one hostile (bullying). Participants were matched and assigned to a condition based on measures of anxiety, depression, paranoia, and previous bullying, before experiencing either the neutral or hostile scenario. Before and after the VR session, participants completed measures of negative affect and levels of distress. All participants remained immersed for the whole duration, which supports the acceptability of using these VR experiences with more vulnerable participants. Those experiencing the hostile version reported greater negative affect post-immersion compared to those experiencing the neutral version (p = .018; d = 0.61). Although non-significant, a similar outcome was found regarding distress (p = .071; d = 0.37). Whilst we did not find a significant relationship between pre-existing internalisation on negative affect and distress, our sample was limited by containing adolescents with relatively low levels of previous bullying experience. Yet we still found evidence that the VR scenario evoked bullying-related psychological reactions. Further testing with a more representative groups of adolescents, especially those with more experience of bullying, would be advised. The VR scenario could potentially be used in educational and therapeutic settings to enhance empathy towards victimised children or enhance resilience following victimisation.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Badger, Julia R. and Rovira, Aitor and Freeman, Daniel and Bowes, Lucy},
	month = sep,
	year = {2023},
	keywords = {Adolescents, Anxiety, Artificial Intelligence, Bullying, Depression, Psychological reactivity, Virtual reality},
	pages = {2623--2632},
	file = {Full Text PDF:files/1483/Badger et al. - 2023 - Developing a virtual reality environment for educational and therapeutic application to investigate.pdf:application/pdf},
}

@article{thalmann_preface_2022,
	title = {Preface the visual computer (vol 38, issue 12)},
	volume = {38},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-022-02729-2},
	doi = {10.1007/s00371-022-02729-2},
	language = {en},
	number = {12},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Thalmann, Nadia Magnenat},
	month = dec,
	year = {2022},
	pages = {3993--3994},
	file = {Full Text PDF:files/1484/Thalmann - 2022 - Preface the visual computer (vol 38, issue 12).pdf:application/pdf},
}

@article{carpio_evaluating_2023,
	title = {Evaluating the viewer experience of interactive virtual reality movies},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-023-00864-2},
	doi = {10.1007/s10055-023-00864-2},
	abstract = {Significant advances in virtual reality (VR) technology have called into question the traditional methods of cinema storytelling and dissemination. New VR devices, such as the Meta (Oculus) Quest, have expanded the possibilities for viewing movies. The purpose of this study is to compare the emotional and cognitive impacts of VR and traditional 2D movies. In this study, sixty volunteers were divided into two groups and presented a movie (Gala) in 2D or VR format. We employed a multimodal method to assess the cognitive and emotional effects of the film both during and after watching. Our technique combined self-reports, interviews, questionnaires, and objective heart rate and EEG brain activity data. After quantitative and qualitative evaluation, it was discovered, that regardless of media, there was a substantial influence of the movie on the emotional state of the participant’s mood. Moreover, compared to the traditional 2D-movie, the VR movie led to more consistent and robust positive effect on all aspects of self-rated affect. The difference in self-reported mood was corroborated by reduced EEG amplitudes in the beta frequency band, indicating higher levels of positive affectivity, which was only observed for the VR movie. Lastly, the VR movie also leads to overall higher self-rated immersion and engagement than the 2D version. Our results highlight the potential of VR movies to engage and emotionally affect audiences beyond traditional cinema. Moreover, our study highlights the value of using a multidisciplinary method for analysing audience impacts.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Carpio, Rudy and Baumann, Oliver and Birt, James},
	month = dec,
	year = {2023},
	keywords = {Artificial Intelligence, Emotion, Film analysis, Filmmaking, Objective measures, Self-report, Virtual reality},
	pages = {3181--3190},
	file = {Full Text PDF:files/1485/Carpio et al. - 2023 - Evaluating the viewer experience of interactive virtual reality movies.pdf:application/pdf},
}

@article{fajnerova_virtual_2023,
	title = {Virtual reality environment for exposure therapy in obsessive–compulsive disorder: a validation study},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Virtual reality environment for exposure therapy in obsessive–compulsive disorder},
	url = {https://doi.org/10.1007/s10055-023-00837-5},
	doi = {10.1007/s10055-023-00837-5},
	abstract = {Obsessive–compulsive disorder (OCD) is characterised by recurrent, repetitive, and unwanted thoughts or impulses triggering significant anxiety. Exposure and response prevention is currently the first-line therapy for OCD. The goal of this validation study was to confirm the potential of the VR house environment that incorporates OCD-specific items that cluster around major symptom dimensions: ‘contamination’, ‘symmetry’, ‘checking’ and ‘hoarding’ to induce anxiety and compulsive behaviour in patients with OCD.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Fajnerová, Iveta and Francová, Anna and Taranzová, Kateřina and Darmová, Barbora and Kosová, Eliška and Stopková, Pavla},
	month = sep,
	year = {2023},
	keywords = {Anxiety, Artificial Intelligence, Medical applications, Obsessive–compulsive disorder, Presence, Virtual reality exposure therapy},
	pages = {2691--2701},
	file = {Full Text PDF:files/1486/Fajnerová et al. - 2023 - Virtual reality environment for exposure therapy in obsessive–compulsive disorder a validation stud.pdf:application/pdf},
}

@article{barneche-naya_assisted_2024,
	title = {Assisted navigation for digital architectural walkthroughs in {Natural} {User} {Interface}-based installations},
	volume = {23},
	issn = {1615-5297},
	url = {https://doi.org/10.1007/s10209-023-01034-6},
	doi = {10.1007/s10209-023-01034-6},
	abstract = {The architectural experience of visiting a virtual building and exploring its contents requires tools to provide visitors with accessible and pleasurable ways to conduct their stroll and facilitate the contemplation and enjoyment of the contents displayed. Natural user interfaces (NUI) are effective and engaging tools for interacting with digital content. In this paper, the authors combine their previous work in the research lines of natural user interfaces and assisted navigation, putting them together on a UX test scenario and studying their combined effect. The testbed consisted of an installation controlled using a depth camera for NUI interaction and applying an attractor-based approach of assisted navigation. The UX experiment consisted of five stages, with three test sets of increasing complexity. The system monitored and recorded the users’ movements on each test during the experiment to extract quantitative data. After each task, users filled out specific questionnaires that provided qualitative information. The study also evaluates the influence of users’ previous expertise on 3D video games in their performance taking the test. The results indicate the benefits of combining both technologies and how they enhance the virtual visit experience. Furthermore, the combined use of natural interaction and assisted navigation facilitates universal access to installations of this kind, frequently found in museums and exhibits, independently of previous user expertise in interactive 3D environments, such as video games.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Univ Access Inf Soc},
	author = {Barneche-Naya, Viviana and Hernández-Ibáñez, Luis A.},
	month = aug,
	year = {2024},
	keywords = {Artificial Intelligence, Assisted navigation, Digital architectural walkthrough, Natural user interfaces, User experience, Virtual museum},
	pages = {1075--1089},
	file = {Full Text PDF:files/1487/Barneche-Naya e Hernández-Ibáñez - 2024 - Assisted navigation for digital architectural walkthroughs in Natural User Interface-based installat.pdf:application/pdf},
}

@article{yip_investigating_2023,
	title = {Investigating the influence of agent modality and expression on agent-mediated fairness behaviours},
	volume = {17},
	issn = {1783-8738},
	url = {https://doi.org/10.1007/s12193-023-00403-y},
	doi = {10.1007/s12193-023-00403-y},
	abstract = {With technological developments, individuals are increasingly able to delegate tasks to autonomous agents that act on their behalf. This may cause individuals to behave more fairly, as involving an agent representative encourages individuals to strategise ahead and therefore adhere to social norms of fairness. Research suggests that an audio smiling agent may further promote fairness as it provides a signal of honesty and trust. What is still unclear is whether presenting a multimodal smiling agent (by using visual and auditory cues) rather than a unimodal smiling agent as normally available commercially (using only an auditory cue e.g., Siri) could amplify the impact of smiles. In the present study, participants (N = 86) played an ultimatum game either directly with another player (control), through a smiling multimodal and unimodal agent or through a neutral multimodal and unimodal agent. Participants’ task was to offer a number of tickets to the other player from a fixed amount. Results showed that when playing the ultimatum game through a smiling multimodal agent, participants offered more tickets to the other player compared to the control condition and the other agent conditions. Hence, exploiting multisensory perception to enhance an agent’s expression may be key for increasing individuals' pro-social behaviour when interacting through such an agent.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {J Multimodal User Interfaces},
	author = {Yip, Hiu Lam and Petrini, Karin},
	month = jun,
	year = {2023},
	keywords = {Agent-mediated decision making, Artificial Intelligence, Autonomous agents, Fairness, Multisensory expression, Smiling, Ultimatum game},
	pages = {65--77},
	file = {Full Text PDF:files/1488/Yip e Petrini - 2023 - Investigating the influence of agent modality and expression on agent-mediated fairness behaviours.pdf:application/pdf},
}

@article{de_marsico_guest_2017,
	title = {Guest {Editorial}: {Multimedia} for {Advanced} {Human}-{Computer} {Interaction}},
	volume = {76},
	issn = {1573-7721},
	shorttitle = {Guest {Editorial}},
	url = {https://doi.org/10.1007/s11042-016-4024-8},
	doi = {10.1007/s11042-016-4024-8},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {De Marsico, Maria and Fogli, Daniela},
	month = feb,
	year = {2017},
	pages = {4849--4854},
	file = {Full Text PDF:files/1489/De Marsico e Fogli - 2017 - Guest Editorial Multimedia for Advanced Human-Computer Interaction.pdf:application/pdf},
}

@article{whitby_designing_2017,
	title = {Designing and {Creating} a {Game} {Installation}},
	volume = {6},
	issn = {2052-773X},
	url = {https://doi.org/10.1007/s40869-017-0039-2},
	doi = {10.1007/s40869-017-0039-2},
	abstract = {A game installation is a game that incorporates the surrounding environment, something that has mostly been explored on a city-wide scale. This paper concerns the creation of a game installation set within a room, and explores locative media, mixed reality games and the fourth wall. A framework for game installations has been designed and tested with a group of participants in a case study. The results in this paper show that the final artefact could be considered a success; recommendations for deeper development of the framework are also provided.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Comput Game J},
	author = {Whitby, Matthew},
	month = sep,
	year = {2017},
	keywords = {Art, Artificial Intelligence, Case study and framework, Game, Installation},
	pages = {85--109},
	file = {Full Text PDF:files/1490/Whitby - 2017 - Designing and Creating a Game Installation.pdf:application/pdf},
}

@article{asadipour_visuohaptic_2017,
	title = {Visuohaptic augmented feedback for enhancing motor skills acquisition},
	volume = {33},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-016-1275-3},
	doi = {10.1007/s00371-016-1275-3},
	abstract = {Serious games are accepted as an effective approach to deliver augmented feedback in motor (re-)learning processes. The multi-modal nature of the conventional computer games (e.g. audiovisual representation) plus the ability to interact via haptic-enabled inputs provides a more immersive experience. Thus, particular disciplines such as medical education in which frequent hands on rehearsals play a key role in learning core motor skills (e.g. physical palpations) may benefit from this technique. Challenges such as the impracticality of verbalising palpation experience by tutors and ethical considerations may prevent the medical students from correctly learning core palpation skills. This work presents a new data glove, built from off-the-shelf components which captures pressure sensitivity designed to provide feedback for palpation tasks. In this work the data glove is used to control a serious game adapted from the infinite runner genre to improve motor skill acquisition. A comparative evaluation on usability and effectiveness of the method using multimodal visualisations, as part of a larger study to enhance pressure sensitivity, is presented. Thirty participants divided into a game-playing group (\$\$n=15\$\$) and a control group (\$\$n=15\$\$) were invited to perform a simple palpation task. The game-playing group significantly outperformed the control group in which abstract visualisation of force was provided to the users in a blind-folded transfer test. The game-based training approach was positively described by the game-playing group as enjoyable and engaging.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Vis Comput},
	author = {Asadipour, Ali and Debattista, Kurt and Chalmers, Alan},
	month = apr,
	year = {2017},
	keywords = {Artificial Intelligence, Augmented feedback, Computer based training, Health care education, Medical palpation, Multi-sensory feedback (visuohaptic), Serious games (computing)},
	pages = {401--411},
	file = {Full Text PDF:files/1491/Asadipour et al. - 2017 - Visuohaptic augmented feedback for enhancing motor skills acquisition.pdf:application/pdf},
}

@article{narciso_assessing_2024,
	title = {Assessing the perceptual equivalence of a firefighting training exercise across virtual and real environments},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-023-00917-6},
	doi = {10.1007/s10055-023-00917-6},
	abstract = {The advantages of Virtual Reality (VR) over traditional training, together with the development of VR technology, have contributed to an increase in the body of literature on training professionals with VR. However, there is a gap in the literature concerning the comparison of training in a Virtual Environment (VE) with the same training in a Real Environment (RE), which would contribute to a better understanding of the capabilities of VR in training. This paper presents a study with firefighters (N = 12) where the effect of a firefighter training exercise in a VE was evaluated and compared to that of the same exercise in a RE. The effect of environments was evaluated using psychophysiological measures by evaluating the perception of stress and fatigue, transfer of knowledge, sense of presence, cybersickness, and the actual stress measured through participants’ Heart Rate Variability (HRV). The results showed a similar perception of stress and fatigue between the two environments; a positive, although not significant, effect of the VE on the transfer of knowledge; the display of moderately high presence values in the VE; the ability of the VE not to cause symptoms of cybersickness; and finally, obtaining signs of stress in participants’ HRV in the RE and, to a lesser extent, signs of stress in the VE. Although the effect of the VE was shown to be non-comparable to that of the RE, the authors consider the results encouraging and discuss some key factors that should be addressed in the future to improve the results of the training VE.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Narciso, David and Melo, Miguel and Rodrigues, Susana and Dias, Duarte and Cunha, João and Vasconcelos-Raposo, José and Bessa, Maximino},
	month = jan,
	year = {2024},
	keywords = {Artificial Intelligence, Biofeedback, Computer graphics, Professional training, Virtual reality},
	pages = {14},
	file = {Full Text PDF:files/1492/Narciso et al. - 2024 - Assessing the perceptual equivalence of a firefighting training exercise across virtual and real env.pdf:application/pdf},
}

@article{dohan_real-walk_2024,
	title = {Real-walk modelling: deep learning model for user mobility in virtual reality},
	volume = {30},
	issn = {1432-1882},
	shorttitle = {Real-walk modelling},
	url = {https://doi.org/10.1007/s00530-023-01200-z},
	doi = {10.1007/s00530-023-01200-z},
	abstract = {This paper presents a study on modelling user free walk mobility in virtual reality (VR) art exhibition. The main objective is to investigate and model users’ mobility sequences during interactions with artwork in VR. We employ a range of machine learning (ML) techniques to define scenes of interest in VR, capturing user mobility patterns. Our approach utilises a long short-term memory (LSTM) model to effectively model and predict users’ future movements in VR environments, particularly in scenarios where clear walking paths and directions are not provided to participants. The DL model demonstrates high accuracy in predicting user movements, enabling a better understanding of audience interactions with the artwork. It opens avenues for developing new VR applications, such as community-based navigation, virtual art guides, and enhanced virtual audience engagement. The results highlight the potential for improved user engagement and effective navigation within virtual environments.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Multimedia Systems},
	author = {Dohan, Murtada and Mu, Mu and Ajit, Suraj and Hill, Gary},
	month = jan,
	year = {2024},
	keywords = {Artificial Intelligence, Dataset, Deep learning, Movement, Navigation, Spatial knowledge, Virtual reality},
	pages = {44},
	file = {Full Text PDF:files/1493/Dohan et al. - 2024 - Real-walk modelling deep learning model for user mobility in virtual reality.pdf:application/pdf},
}

@article{gulhan_aesthetic_2023,
	title = {Aesthetic judgments of {3D} arts in virtual reality and online settings},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-022-00671-1},
	doi = {10.1007/s10055-022-00671-1},
	abstract = {Empirical aesthetics is beginning to branch off from conventional laboratory-based studies, leading to in-situ, immersive, often more accessible experiments. Here, we explored different types of aesthetic judgments of three-dimensional artworks in two contexts: virtual reality (VR), aiming for an immersive experience, and online settings aiming for an accessible setup for a remote audience. Following the pilot experiment conducted to select a set of 3D artworks, in the first experiment, participants freely engaged with virtual artworks via an eye-tracking-enabled VR headset and provided evaluations based on subjective measures of aesthetic experience such as ratings on liking, novelty, complexity, perceived viewing duration; and the objective viewing duration was also recorded. Results showed positive, linear, and mostly moderate correlations between liking and the other perceived judgment attributes. Supplementary eye-tracking data showed a range of viewing strategies and variation in viewing durations between participants and artworks. Results of the second experiment, adapted as a short online follow-up, showed converging evidence on correlations between the different aspects contributing to aesthetic judgments and suggested similarity of judgment strategies across contexts. In both settings, participants provided further insights via exit questionnaires. We speculate that both VR and online settings offer ecologically valid experimental contexts, create immersive visual arts experience, and enhance accessibility to cultural heritage.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Gulhan, Doga and Durant, Szonya and Zanker, Johannes M.},
	month = jun,
	year = {2023},
	keywords = {Aesthetic judgment, Art appreciation, Artificial Intelligence, Empirical aesthetics, Eye-tracking, Online experiment, Virtual reality (VR)},
	pages = {573--589},
	file = {Full Text PDF:files/1494/Gulhan et al. - 2023 - Aesthetic judgments of 3D arts in virtual reality and online settings.pdf:application/pdf},
}

@article{alameda-pineda_special_2020,
	title = {Special {Issue} on {Generating} {Realistic} {Visual} {Data} of {Human} {Behavior}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-020-01319-w},
	doi = {10.1007/s11263-020-01319-w},
	language = {en},
	number = {5},
	urldate = {2024-09-05},
	journal = {Int J Comput Vis},
	author = {Alameda-Pineda, Xavier and Ricci, Elisa and Salah, Albert Ali and Sebe, Nicu and Yan, Shuicheng},
	month = may,
	year = {2020},
	pages = {1376--1377},
	file = {Full Text PDF:files/1495/Alameda-Pineda et al. - 2020 - Special Issue on Generating Realistic Visual Data of Human Behavior.pdf:application/pdf},
}

@article{garcia_co-design_2023,
	title = {Co-design of avatars to embody auditory hallucinations of patients with schizophrenia},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00558-7},
	doi = {10.1007/s10055-021-00558-7},
	abstract = {Auditory hallucinations are common and distressing symptoms of the schizophrenia disease. It is commonly treated with pharmacological approaches but, unfortunately, such an approach is not effective in all patients. In the cases in which the use of antipsychotic drugs is not possible or not recommended, psychotherapeutic interventions are used to help patients gain power and control against hearing voices. Recently, virtual reality technologies have been incorporated to this type of therapies. A virtual representation of their voice (avatar) is created in a controlled computer-based environment, and the patient is encouraged to confront it. Unfortunately, the software tools used in these therapies are not described in depth and, even more important, to the best of our knowledge, their usability, utility and intention to use by therapists, and patients have not been evaluated enough. The involvement of end users in the software development is beneficial in obtaining useful and usable tools. Hence, the two contributions of this paper are (1) the description of an avatar creation system and the main technical details of the configuration of auditory hallucination avatars, and (2) its evaluation from both the therapists’ and the patients’ viewpoints. The evaluation does not only focus on usability, but also assesses the acceptance of the technology as an important indicator of the future use of a new technological tool. Moreover, the most important results, the lessons learned and the main limitations of our study are discussed.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {García, Arturo S. and Fernández-Sotos, Patricia and Vicente-Querol, Miguel A. and Sánchez-Reolid, Roberto and Rodriguez-Jimenez, Roberto and Fernández-Caballero, Antonio},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, Avatar-based therapy, Schizophrenia, Technology acceptance, Usability, User evaluation},
	pages = {217--232},
	file = {Full Text PDF:files/1496/García et al. - 2023 - Co-design of avatars to embody auditory hallucinations of patients with schizophrenia.pdf:application/pdf},
}

@article{riensche_combining_2012,
	title = {Combining modeling and gaming for predictive analytics},
	volume = {1},
	issn = {2190-8532},
	url = {https://doi.org/10.1186/2190-8532-1-11},
	doi = {10.1186/2190-8532-1-11},
	abstract = {Our most significant security challenges involve people. While human behavior has long been studied, computational modeling of human behavior is early in its development. An inherent challenge in modeling of human behavior is efficient and accurate transfer of knowledge from humans to models, and subsequent retrieval. The simulated real-world environments of games present one avenue for knowledge generation and transfer. In this paper we describe our approach of combining modeling and gaming disciplines to develop predictive capabilities, using formal models to inform game development, and using games to provide data for modeling. We also describe the development of a prototype “Illicit Trafficking Game” that we used as a tool to exercise, evaluate and refine our approach. The resulting predictive capability combines human expertise and actions with computational modeling capabilities, resulting in a predictive capability that may approach the richness and diversity of human behaviors we wish to predict.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Secur Inform},
	author = {Riensche, Roderick M. and Whitney, Paul D.},
	month = aug,
	year = {2012},
	keywords = {Artificial Intelligence, Gaming, Modeling, Predictive analytics},
	pages = {11},
	file = {Full Text PDF:files/1497/Riensche e Whitney - 2012 - Combining modeling and gaming for predictive analytics.pdf:application/pdf},
}

@article{postema_value_2024,
	title = {The value of collision feedback in robotic surgical skills training},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-023-00891-z},
	doi = {10.1007/s10055-023-00891-z},
	abstract = {Collision feedback about instrument and environment interaction is often lacking in robotic surgery training devices. The PoLaRS virtual reality simulator is a newly developed desk trainer that overcomes drawbacks of existing robot trainers for advanced laparoscopy. This study aimed to assess the effect of haptic and visual feedback during training on the performance of a robotic surgical task. Robotic surgery-naïve participants were randomized and equally divided into two training groups: Haptic and Visual Feedback (HVF) and No Haptic and Visual Feedback. Participants performed two basic virtual reality training tasks on the PoLaRS system as a pre- and post-test. The measurement parameters Time, Tip-to-tip distance, Path length Left/Right and Collisions Left/Right were used to analyze the learning curves and statistically compare the pre- and post-tests performances. In total, 198 trials performed by 22 participants were included. The visual and haptic feedback did not negatively influence the time to complete the tasks. Although no improvement in skill was observed between pre- and post-tests, the mean rank of the number of collisions of the right grasper (dominant hand) was significantly lower in the HVF feedback group during the second post-test (Mean Rank = 8.73 versus Mean Rank = 14.27, U = 30.00, p = 0.045). Haptic and visual feedback during the training on the PoLaRS system resulted in fewer instrument collisions. These results warrant the introduction of haptic feedback in subjects with no experience in robotic surgery. The PoLaRS system can be utilized to remotely optimize instrument handling before commencing robotic surgery in the operating room.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Postema, Roelf and Hardon, Hidde and Rahimi, A. Masie and Horeman, Roel and Nickel, Felix and Dankelman, Jenny and Bloemendaal, Alexander L. A. and van der Elst, Maarten and van der Peet, Donald L. and Daams, Freek and Hardon, Sem F. and Horeman, Tim},
	month = feb,
	year = {2024},
	keywords = {Artificial Intelligence, Haptic feedback, Patient safety, Robotic surgery, Simulation training, Skills acquisition, Visual feedback},
	pages = {46},
	file = {Full Text PDF:files/1498/Postema et al. - 2024 - The value of collision feedback in robotic surgical skills training.pdf:application/pdf},
}

@article{juan_immersive_2023,
	title = {Immersive virtual reality for upper limb rehabilitation: comparing hand and controller interaction},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Immersive virtual reality for upper limb rehabilitation},
	url = {https://doi.org/10.1007/s10055-022-00722-7},
	doi = {10.1007/s10055-022-00722-7},
	abstract = {Virtual reality shows great potential as an alternative to traditional therapies for motor rehabilitation given its ability to immerse the user in engaging scenarios that abstract them from medical facilities and tedious rehabilitation exercises. This paper presents a virtual reality application that includes three serious games and that was developed for motor rehabilitation. It uses a standalone headset and the user's hands without the need for any controller for interaction. Interacting with an immersive virtual reality environment using only natural hand gestures involves an interaction that is similar to that of real life, which would be especially desirable for patients with motor problems. A study involving 28 participants (4 with motor problems) was carried out to compare two types of interaction (hands vs. controllers). All of the participants completed the exercises. No significant differences were found in the number of attempts necessary to complete the games using the two types of interaction. The group that used controllers required less time to complete the exercise. The performance outcomes were independent of the gender and age of the participants. The subjective assessment of the participants with motor problems was not significantly different from the rest of the participants. With regard to the interaction type, the participants mostly preferred the interaction using their hands (78.5\%). All four participants with motor problems preferred the hand interaction. These results suggest that the interaction with the user’s hands together with standalone headsets could improve motivation, be well accepted by motor rehabilitation patients, and help to complete exercise therapy at home.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Juan, M.-Carmen and Elexpuru, Julen and Dias, Paulo and Santos, Beatriz Sousa and Amorim, Paula},
	month = jun,
	year = {2023},
	keywords = {Artificial Intelligence, Hand gestures, Hand tracking, Motor rehabilitation, Serious games, Standalone headsets, Virtual reality},
	pages = {1157--1171},
	file = {Full Text PDF:files/1499/Juan et al. - 2023 - Immersive virtual reality for upper limb rehabilitation comparing hand and controller interaction.pdf:application/pdf},
}

@article{abdulrahman_exploring_2022,
	title = {Exploring the influence of a user-specific explainable virtual advisor on health behaviour change intentions},
	volume = {36},
	issn = {1573-7454},
	url = {https://doi.org/10.1007/s10458-022-09553-x},
	doi = {10.1007/s10458-022-09553-x},
	abstract = {Virtual advisors (VAs) are being utilised almost in every service nowadays from entertainment to healthcare. To increase the user’s trust in these VAs and encourage the users to follow their advice, they should have the capability of explaining their decisions, particularly, when the decision is vital such as health advice. However, the role of an explainable VA in health behaviour change is understudied. There is evidence that people tend to change their intentions towards health behaviour when the persuasion message is linked to their mental state. Thus, this study explores this link by introducing an explainable VA that provides explanation according to the user’s mental state (beliefs and goals) rather than the agent’s mental state as commonly utilised in explainable agents. It further explores the influence of different explanation patterns that refer to beliefs, goals, or beliefs\&goals on the user’s behaviour change. An explainable VA was designed to advise undergraduate students how to manage their study-related stress by motivating them to change certain behaviours. With 91 participants, the VA was evaluated and the results revealed that user-specific explanation could significantly encourage behaviour change intentions and build good user-agent relationship. Small differences were found between the three types of explanation patterns.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Auton Agent Multi-Agent Syst},
	author = {Abdulrahman, Amal and Richards, Deborah and Bilgin, Ayse Aysin},
	month = apr,
	year = {2022},
	keywords = {Artificial Intelligence, Behaviour change intention, Explainable agents, Personal virtual advisor, Reason explanation, Trust, Working alliance},
	pages = {25},
	file = {Full Text PDF:files/1500/Abdulrahman et al. - 2022 - Exploring the influence of a user-specific explainable virtual advisor on health behaviour change in.pdf:application/pdf},
}

@article{kim_dynamical_2015,
	title = {Dynamical model for gamification of learning ({DMGL})},
	volume = {74},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-013-1612-8},
	doi = {10.1007/s11042-013-1612-8},
	abstract = {The purpose of this paper is to hypothesize ‘Dynamical model of educational effectiveness for the gamification of learning, and to widely announce a pure and right function of game through our model. For the theoretical contribution of gamification, we propose a dynamical model of game based learning that aims to maximize educational effectiveness that correlates with the four main primary factors (curiosity, challenge, fantasy and control). The main idea of this model is based on the correlations of four factors which originating from learning games which are built on the foundations of separate theories: 1) Game Design Features 2) Key Characteristics of a Learning Game 3) a theory of educational environment design known as the ARCS (attention, relevance, confidence, and satisfaction) and 4) the theoretical background of gamification labeled the MDA(mechanics, dynamics and aesthetics) framework. We created a sigmoidal equation for the educational effectiveness of Gamification by analyzing and correlating these factors. Through this dynamical model we will show that the effectiveness of the gamification of learning is educationally superior to traditional ways of learning in specific setting, after an elapsed adaptive time period with reasonable relationship of the four primary factors.},
	language = {en},
	number = {19},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Kim, Jung Tae and Lee, Won-Hyung},
	month = oct,
	year = {2015},
	keywords = {Artificial Intelligence, Game, Game based learning, Gamification, GBL, Learning game},
	pages = {8483--8493},
	file = {Full Text PDF:files/1511/Kim e Lee - 2015 - Dynamical model for gamification of learning (DMGL).pdf:application/pdf},
}

@article{horsak_overground_2023,
	title = {Overground walking while using a virtual reality head mounted display increases variability in trunk kinematics and reduces dynamic balance in young adults},
	volume = {27},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-023-00851-7},
	doi = {10.1007/s10055-023-00851-7},
	abstract = {This study analyzed the effects of walking freely in virtual reality (VR) compared to walking in the real-world on dynamic balance and postural control. For this purpose, nine male and twelve female healthy participants underwent standard 3D gait analysis while walking randomly in a real laboratory and in a room-scale overground VR environment resembling the real laboratory. The VR was delivered to participants by a head-mounted-display which was operated wirelessly and calibrated to the real-world. Dynamic balance and postural control were assessed with (1) the margin of stability (MOS) in the anteroposterior (AP-MOS) and mediolateral (ML-MOS) directions at initial-contact, (2) the relationship between the mediolateral center of mass (COM) position and acceleration at mid-stance with subsequent step width, (3) and trunk kinematics during the entire gait cycle. We observed increased mediolateral (ML) trunk linear velocity variability, an increased coupling of the COM position and acceleration with subsequent step width, and a decrease in AP-MOS while walking in VR but no change in ML-MOS when walking in VR. Our findings suggest that walking in VR may result in a less reliable optical flow, indicated by increased mediolateral trunk kinematic variability, which seems to be compensated by the participants by slightly reweighing sensorimotor input and thereby consciously tightening the coupling between the COM and foot placement to avoid a loss of balance. Our results are particularly valuable for future developers who want to use VR to support gait analysis and rehabilitation.},
	language = {en},
	number = {4},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Horsak, Brian and Simonlehner, Mark and Dumphart, Bernhard and Siragy, Tarique},
	month = dec,
	year = {2023},
	keywords = {Artificial Intelligence, Dynamic stability, Gait analysis, Immersive virtual reality, Motion capturing, Postural control},
	pages = {3021--3032},
	file = {Full Text PDF:files/1512/Horsak et al. - 2023 - Overground walking while using a virtual reality head mounted display increases variability in trunk.pdf:application/pdf},
}

@article{obremski_non-native_2021,
	title = {Non-native speaker perception of {Intelligent} {Virtual} {Agents} in two languages: the impact of amount and type of grammatical mistakes},
	volume = {15},
	issn = {1783-8738},
	shorttitle = {Non-native speaker perception of {Intelligent} {Virtual} {Agents} in two languages},
	url = {https://doi.org/10.1007/s12193-021-00369-9},
	doi = {10.1007/s12193-021-00369-9},
	abstract = {Having a mixed-cultural membership becomes increasingly common in our modern society. It is thus beneficial in several ways to create Intelligent Virtual Agents (IVAs) that reflect a mixed-cultural background as well, e.g., for educational settings. For research with such IVAs, it is essential that they are classified as non-native by members of a target culture. In this paper, we focus on variations of IVAs’ speech to create the impression of non-native speakers that are identified as such by speakers of two different mother tongues. In particular, we investigate grammatical mistakes and identify thresholds beyond which the agents is clearly categorised as a non-native speaker. Therefore, we conducted two experiments: one for native speakers of German, and one for native speakers of English. Results of the German study indicate that beyond 10\% of word order mistakes and 25\% of infinitive mistakes German-speaking IVAs are perceived as non-native speakers. Results of the English study indicate that beyond 50\% of omission mistakes and 50\% of infinitive mistakes English-speaking IVAs are perceived as non-native speakers. We believe these thresholds constitute helpful guidelines for computational approaches of non-native speaker generation, simplifying research with IVAs in mixed-cultural settings.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {J Multimodal User Interfaces},
	author = {Obremski, David and Lugrin, Jean-Luc and Schaper, Philipp and Lugrin, Birgit},
	month = jun,
	year = {2021},
	keywords = {Artificial Intelligence, Intelligent Virtual Agents, Mixed-cultural settings, Verbal behaviour},
	pages = {229--238},
	file = {Full Text PDF:files/1513/Obremski et al. - 2021 - Non-native speaker perception of Intelligent Virtual Agents in two languages the impact of amount a.pdf:application/pdf},
}

@article{pini_augmented_2023,
	title = {Augmented grocery shopping: fostering healthier food purchases through {AR}},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Augmented grocery shopping},
	url = {https://doi.org/10.1007/s10055-023-00792-1},
	doi = {10.1007/s10055-023-00792-1},
	abstract = {Food choices are intimately related to individual health. Therefore, the food we buy should be carefully chosen. However, grocery shopping is typically done in noisy environments, and food products usually present cluttered labels with dense texts that make it hard to properly evaluate relevant nutritional data. Augmented reality (AR) allows a shopper to visualize digitally generated contents onto real objects and to interact with them. In this experiment, we investigated the effects of delivering nutritional information using AR technology on food choices. To this end, we ran a between-participants laboratory experiment in which participants were asked to choose among the products available. The experimental group received the food-related information via AR, while the control group had ordinary access to food packaging. We found that AR technology facilitated the choice of healthier food items. Additionally, participants in the experimental group reported that they based their decisions on nutritional information rather than on the appearance of the package. The present work highlights how AR can be exploited to bring to the foreground information that would otherwise be hard to spot, thereby increasing the consumer’s awareness of the overall characteristics of the product.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Pini, Valentina and Orso, Valeria and Pluchino, Patrik and Gamberini, Luciano},
	month = sep,
	year = {2023},
	keywords = {Artificial Intelligence, Augmented reality, Food choices, Food facts, Human–computer interaction, Nutritional label},
	pages = {2117--2128},
	file = {Full Text PDF:files/1514/Pini et al. - 2023 - Augmented grocery shopping fostering healthier food purchases through AR.pdf:application/pdf},
}

@article{bahreini_fuzzy_2019,
	title = {A fuzzy logic approach to reliable real-time recognition of facial emotions},
	volume = {78},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-019-7250-z},
	doi = {10.1007/s11042-019-7250-z},
	abstract = {This paper represents our newly developed software for emotion recognition from facial expressions. Besides allowing emotion recognition from image files and recorded video files, it uses webcam data to provide real-time, continuous, and unobtrusive facial emotional expressions. It uses FURIA algorithm for unordered fuzzy rule induction to offer timely and appropriate feedback based on learners’ facial expressions. The main objective of this study was first to validate the use of webcam data for a real-time and accurate analysis of facial expressions in e-learning environments. Second, transform these facial expressions to detected emotional states using the FURIA algorithm. We measured the performance of the software with ten participants, provided them with the same computer-based tasks, requested them a hundred times to mimic specific facial expressions, and recorded all sessions on video. We used the recorded video files to feed our newly developed software. We then used two experts’ opinions to annotate and rate participants’ recorded behaviours and to validate the software’s results. The software provides accurate and reliable results with the overall accuracy of 83.2\%, which is comparable to the recognition by humans. This study will help to increase the quality of e-learning.},
	language = {en},
	number = {14},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Bahreini, Kiavash and van der Vegt, Wim and Westera, Wim},
	month = jul,
	year = {2019},
	keywords = {Affective computing, Artificial Intelligence, Emotion recognition, Fuzzy logic, Software development, Statistical data analysis, Webcam, E-learning},
	pages = {18943--18966},
	file = {Full Text PDF:files/1515/Bahreini et al. - 2019 - A fuzzy logic approach to reliable real-time recognition of facial emotions.pdf:application/pdf},
}

@article{garcia-penalvo_special_2019,
	title = {Special issue on exploring new {Natural} {User} {Experiences}},
	volume = {18},
	issn = {1615-5297},
	url = {https://doi.org/10.1007/s10209-017-0578-0},
	doi = {10.1007/s10209-017-0578-0},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Univ Access Inf Soc},
	author = {García-Peñalvo, Francisco J. and Moreno, Lourdes},
	month = mar,
	year = {2019},
	pages = {1--2},
	file = {Full Text PDF:files/1516/García-Peñalvo e Moreno - 2019 - Special issue on exploring new Natural User Experiences.pdf:application/pdf},
}

@article{kober_move_2022,
	title = {Move your virtual body: differences and similarities in brain activation patterns during hand movements in real world and virtual reality},
	volume = {26},
	issn = {1434-9957},
	shorttitle = {Move your virtual body},
	url = {https://doi.org/10.1007/s10055-021-00588-1},
	doi = {10.1007/s10055-021-00588-1},
	abstract = {Virtual reality (VR) is a promising tool for neurological rehabilitation, especially for motor rehabilitation. In the present study, we investigate whether brain activation patterns that are evoked by active movements are comparable when these movements are carried out in reality and in VR. Therefore, 40 healthy adults (20 men, mean age 25.31 years) performed hand movements and viewed these movements in a first-person view in reality, a VR scene showing realistic virtual hands, and a VR scene showing abstract virtual hands, in a randomized order. The VR conditions were presented via an immersive 3D head-mounted display system. EEG activity was assessed over the hand motor areas during and after movement execution. All three conditions led to typical EEG activation patterns over the motor cortex. Hence, brain activation patterns were largely comparable between conditions. However, the VR conditions, especially the abstract VR condition, led to a weaker hemispheric lateralization effect compared to the real-world condition. This indicates that hand models in VR should be realistic to be able to evoke activation patterns in the motor cortex comparable to real-world scenarios.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Kober, Silvia Erika and Settgast, Volker and Brunnhofer, Marlies and Augsdörfer, Ursula and Wood, Guilherme},
	month = jun,
	year = {2022},
	keywords = {Artificial Intelligence, Brain activity, Brain lateralization, Motor task, Presence, Virtual reality},
	pages = {501--511},
	file = {Full Text PDF:files/1517/Kober et al. - 2022 - Move your virtual body differences and similarities in brain activation patterns during hand moveme.pdf:application/pdf},
}

@article{jung_design_2017,
	title = {Design and implementation of a same-user identification system in invoked reality space},
	volume = {76},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-016-4117-4},
	doi = {10.1007/s11042-016-4117-4},
	abstract = {The objective of this study is to solve the problem of user data not being precisely received from sensors because of sensing region limitations in invoked reality (IR) space, distortion of colors or patterns by lighting, and blocking or overlapping of a user by other users. The sensing scope range is thus expanded using multiple sensors in the IR space. Moreover, user feature data are accurately identified by user sensing. Specifically, multiple sensors are employed when not all of user data are sensed because they overlap with data of other users. In the proposed approach, all clients share the user feature data from multiple sensors. Accordingly, each client recognizes that the user is the same individual on the basis of the shared data. Furthermore, the identification accuracy is improved by identifying the user features based on colors and patterns that are less affected by lighting. Therefore, accurate identification of the user feature data is enabled, even under lighting changes. The proposed system was implemented based on system performance analysis standards. The practicality and system performance in identifying the same person using the proposed method were verified through an experiment.},
	language = {en},
	number = {9},
	urldate = {2024-09-05},
	journal = {Multimed Tools Appl},
	author = {Jung, Yunji and Xi, Yulong and Cho, Seoungjae and Song, Wei and Fong, Simon and Cho, Kyungeun},
	month = may,
	year = {2017},
	keywords = {Artificial Intelligence, Feature extraction, Invoked reality, User identification},
	pages = {11429--11447},
	file = {Full Text PDF:files/1518/Jung et al. - 2017 - Design and implementation of a same-user identification system in invoked reality space.pdf:application/pdf},
}

@article{kogler_effects_2022,
	title = {Effects of electrical brain stimulation on brain indices and presence experience in immersive, interactive virtual reality},
	volume = {26},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00612-4},
	doi = {10.1007/s10055-021-00612-4},
	abstract = {The subjective presence experience in virtual reality (VR) is associated with distinct brain activation patterns. Particularly, the dorsolateral prefrontal cortex (DLPFC) seems to play a central role. We investigated the effects of electric brain stimulation (transcranial direct current, tDCS) on the presence experience as well as on brain activity and connectivity. Thirty-eight participants received either anodal (N = 18) or cathodal (N = 20) stimulation of the DLPFC before interacting in an immersive VR as well as sham stimulation. During VR interaction, EEG and heart rate were recorded. After VR interaction, participants rated their subjective presence experience using standardized questionnaires. Cathodal stimulation led to stronger brain connectivity than sham stimulation. Increased brain connectivity was associated with numerically lower levels of subjective presence. Anodal stimulation did not lead to changes in brain connectivity, and no differences in subjective presence ratings were found between the anodal and sham stimulation. These results indicate that cathodal tDCS over the DLPFC leads to a more synchronized brain state, which might hamper the activity in networks, which are generally associated with the evolvement of the subjective presence experience. Our results underline the importance of the DLPFC for the presence experience in VR.},
	language = {en},
	number = {3},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Kogler, Wolfgang and Wood, Guilherme and Kober, Silvia Erika},
	month = sep,
	year = {2022},
	keywords = {Artificial Intelligence, Coherence, EEG, Presence, Transcranial direct current stimulation, Virtual reality},
	pages = {1019--1029},
	file = {Full Text PDF:files/1519/Kogler et al. - 2022 - Effects of electrical brain stimulation on brain indices and presence experience in immersive, inter.pdf:application/pdf},
}

@article{guzsvinecz_investigation_2022,
	title = {Investigation of spatial ability test completion times in virtual reality using a desktop display and the {Gear} {VR}},
	volume = {26},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-021-00509-2},
	doi = {10.1007/s10055-021-00509-2},
	abstract = {The interaction time of students who did spatial ability tests in a virtual reality environment is analyzed. The spatial ability test completion times of 240 and 61 students were measured. A desktop display as well as the Gear VR were used by the former group and by the latter one, respectively. Logistic regression analysis was used to investigate the relationship between the probability of correct answers and completion times, while linear regression was used to evaluate effects and interactions of following factors on test completion times: the users’ gender and primary hand, test type and device used. The findings were that while the completion times are not significantly affected by the users’ primary hand, other factors have significant effects on them: they are decreased by the male gender in itself, while they are increased by solving Mental Rotation Tests or by using the Gear VR. The largest significant increment in interaction time in virtual reality during spatial ability tests is when Mental Rotation Tests are accomplished by males with the Gear VR, while the largest significant decrease in interaction time is when Mental Cutting Tests are completed with a desktop display.},
	language = {en},
	number = {2},
	urldate = {2024-09-05},
	journal = {Virtual Reality},
	author = {Guzsvinecz, Tibor and Orbán-Mihálykó, Éva and Sik-Lányi, Cecília and Perge, Erika},
	month = jun,
	year = {2022},
	keywords = {Artificial Intelligence, Cognitive skills, Desktop display, Gear VR, Human-computer interaction, Interaction time, Mental rotation, Spatial ability, Virtual reality},
	pages = {601--614},
	file = {Full Text PDF:files/1520/Guzsvinecz et al. - 2022 - Investigation of spatial ability test completion times in virtual reality using a desktop display an.pdf:application/pdf},
}

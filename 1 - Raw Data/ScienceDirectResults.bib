@article{BRAUN201759,
title = {Curved - free-form interaction using capacitive proximity sensors},
journal = {Procedia Computer Science},
volume = {109},
pages = {59-66},
year = {2017},
note = {8th International Conference on Ambient Systems, Networks and Technologies, ANT-2017 and the 7th International Conference on Sustainable Energy Information Technology, SEIT 2017, 16-19 May 2017, Madeira, Portugal},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.295},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917309523},
author = {Andreas Braun and Sebastian Zander-Walz and Martin Majewski and Arjan Kuijper},
keywords = {Curved surfaces, capacitive sensing, gestural interaction, virtual reality},
abstract = {Abstract:
Large interactive surfaces have found increased popularity in recent years. However, with increased surface size ergonomics become more important, as interacting for extended periods may cause fatigue. Curved is a large-surface interaction device, designed to follow the natural movement of a stretched arm when performing gestures. It tracks one or two hands above the surface, using an array of capacitive proximity sensors and supports both touch and mid-air gestures. It requires specific object tracking methods and the synchronized measurement from 32 sensors. We have created an example application for users wearing a virtual reality headset while seated that may benefit from haptic feedback and ergonomically shaped surfaces. A prototype with adaptive curvature has been created that allows us to evaluate gesture recognition performance and different surface inclinations.}
}
@article{PIECZYNSKI2024107864,
title = {A fast, lightweight deep learning vision pipeline for autonomous UAV landing support with added robustness},
journal = {Engineering Applications of Artificial Intelligence},
volume = {131},
pages = {107864},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.107864},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624000228},
author = {Dominik Pieczyński and Bartosz Ptak and Marek Kraft and Mateusz Piechocki and Przemysław Aszkowski},
keywords = {Unmanned aerial vehicle, Landing support, Image processing, Deep learning, On-board processing},
abstract = {Despite massive development in aerial robotics, precise and autonomous landing in various conditions is still challenging. This process is affected by many factors, such as terrain shape, weather conditions, and the presence of obstacles. This paper describes a deep learning-accelerated image processing pipeline for accurate detection and relative pose estimation of the UAV with respect to the landing pad. Moreover, the system provides increased safety and robustness by implementing human presence detection and error estimation for both landing target detection and pose computation. Human presence and landing pad location are performed by estimating the presence probability via segmentation. This is followed by the landing pad keypoints’ location regression algorithm, which, in addition to coordinates, provides the uncertainty of presence for each defined landing pad landmark. To perform the aforementioned tasks, a set of lightweight neural network models was selected and evaluated. The resulting measurements of the system’s performance and accuracy are presented for each component individually and for the whole processing pipeline. The measurements are performed using onboard embedded UAV hardware and confirm that the method can provide accurate, low-latency feedback information for safe landing support.}
}
@article{FORTUNA2024104057,
title = {A comparative study of Augmented Reality rendering techniques for industrial assembly inspection},
journal = {Computers in Industry},
volume = {155},
pages = {104057},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104057},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523002075},
author = {Santina Fortuna and Loris Barbieri and Emanuele Marino and Fabio Bruno},
keywords = {Augmented Reality, Industrial assembly inspection, Quality inspection, AR rendering techniques, Industry 4.0},
abstract = {In the manufacturing industry, Augmented Reality (AR) has shown significant potential in enhancing operators’ capabilities while performing inspection and assembly activities. However, the augmented visualization of virtual models on physical components can present challenges and potential misunderstandings, as the visualization mode greatly influences the perception of components and the amount of information received. This study investigates the impact of rendering techniques on user performance during industrial assembly inspection tasks. In particular, a set of rendering techniques, suitable for industrial AR applications, have been selected and implemented through a dedicated AR tool. The selected AR rendering techniques have been compared, in terms of qualitative and quantitative metrics, in order to test their efficiency and effectiveness for industrial assembly inspection activities. 17 domain experts and 33 representative users have been involved in the experimental study which outcomes reveal that the rendering techniques play a significant role in assisting operators for identifying design discrepancies in industrial products. Furthermore, a correlation has been observed between the two AR rendering techniques best suited to support industrial inspection activities and the types of assembly errors detected.}
}
@article{SIGITOV2015257,
title = {Adopting a Game Engine for Large, High-Resolution Displays},
journal = {Procedia Computer Science},
volume = {75},
pages = {257-266},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.246},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037072},
author = {Anton Sigitov and David Scherfgen and André Hinkenjann and Oliver Staadt},
keywords = {large-high-resolution displays, rapid prototyping tool, Unity, tools for education},
abstract = {The steadily decreasing prices of display technologies and computer graphics hardware contribute to the increasing popularity of multiple-display environments, like large, high-resolution displays. It is therefore necessary that educational organizations give the new generation of computer scientists an opportunity to become familiar with this kind of technology. However, there is a lack of tools that allow for getting started easily. Existing frameworks and libraries that provide support for multi-display rendering are often complex in understanding, configuration and extension. This is critical especially in educational context where the time that students have for their projects is limited and quite short. These tools are also rather known and used in research communities only, thus providing less benefit for future non-scientists. In this work we present an extension for the Unity game engine. The extension allows – with a small overhead – for implementation of applications that are apt to run on both single-display and multi-display systems. It takes care of the most common issues in the context of distributed and multi-display rendering like frame, camera and animation synchronization, thus reducing and simplifying the first steps into the topic. In conjunction with Unity, which significantly simplifies the creation of different kinds of virtual environments, the extension affords students to build mock-up virtual reality applications for large, high-resolution displays, and to implement and evaluate new interaction techniques and metaphors and visualization concepts. Unity itself, in our experience, is very popular among computer graphics students and therefore familiar to most of them. It is also often employed in projects of both research institutions and commercial organizations; so learning it will provide students with qualification in high demand.}
}
@article{CARLIN2024100639,
title = {An interactive framework to support decision-making for Digital Twin design},
journal = {Journal of Industrial Information Integration},
volume = {41},
pages = {100639},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100639},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000839},
author = {H M Carlin and P A Goodall and R I M Young and A A West},
keywords = {Digital twin, Decision-support, Design framework, Ontology},
abstract = {Producing a Digital Twin (DT) involves many inter-linking decisions. Existing research tends to describe the parts of a DT and how they work, but not the decision-making that goes into building a DT nor the consideration of alternative design options. There is therefore a need for decision support to guide developers to create DTs efficiently while meeting functional requirements such as accuracy and interoperability. This paper presents an ontology-based decision support framework to achieve this need. Firstly an analysis of the decisions required to create a predictive maintenance DT for an automotive manufacturer is performed. The analysis found that each decision point produces an output by consideration of various influencing factors, such as time constraints, computation limits and the required fidelity of the model. The network of decisions is complex, with the outcomes of earlier decisions influencing later ones. An IDEF0 diagram was found to be a useful way to represent decisions, their dependencies and their cross-linking. This knowledge was used to populate an ontology of DT components for a predictive maintenance DT. The ability of an ontology to describe concepts explicitly using standardised vocabulary ensures the integrity of the decision-making guidance. A demonstration of the functionality of the ontology-based decision support framework was made before an evaluation of the concept. The research is a fundamental component in producing decision support for DT creators so that manufacturers can realise the benefits of a connected, responsive and flexible facility.}
}
@article{SELIN2019283,
title = {Emergency exit planning and simulation environment using gamification, artificial intelligence and data analytics},
journal = {Procedia Computer Science},
volume = {156},
pages = {283-291},
year = {2019},
note = {8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311238},
author = {Jukka Selin and Mika Letonsaari and Markku Rossi},
keywords = {BIM, Gamification, Data Analytics, Simulation, AI},
abstract = {In our research, we explore methods to utilize gamified building data models and data analytics. In this paper, we present a study, where an emergency exit planning and simulation platform is implemented with a commercial center gamified data model. In the study, we compare various emergency exit location options and search the critical areas for customer evacuation. Customized user profiles are used to estimate the movement capabilities of elderly and handicapped people. The feasibility of data analytics methods for analyzing the simulation results is examined. The results show that simulations based on gamification are well-suited tools emergency exit evaluations.}
}
@article{LI2022546,
title = {Research on Key Technologies of Garbage Classification Virtual Simulation Game Development Based on unity3d Technology},
journal = {Procedia Computer Science},
volume = {208},
pages = {546-552},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.075},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922015162},
author = {Yuxuan Li and Yingying Zhu},
keywords = {Unity 3D, shadow creator SDK, refuse classification, virtual reality, game development},
abstract = {In recent years, the problems caused by garbage have become more and more serious, and many countries have begun to advocate the concept of environmental protection. China has also begun to implement the policy of waste classification, but the effect is not ideal. This is because the publicity of waste classification knowledge has not reached good expectations, and the traditional propaganda means such as static graphics and text can not give people a deep impression. Therefore, in order to better achieve the knowledge publicity of garbage classification, under the conditions of the continuous maturity of Internet technology and 5g technology, relying on virtual reality technology, combined with novel game mode and immersive game experience, through games, players' interest in learning garbage classification knowledge is increased, so as to improve the knowledge publicity efficiency of garbage classification. This paper discusses and studies a series of key technologies used in virtual simulation games, which use unity 3D as the development editor of the game engine, 3D Max as the model creation tool, shadow creator SDK as the VR technical support, and action one headset to display the game content.}
}
@article{ADENIYI20242996,
title = {Development of Two Dimension (2D) Game Engine with Finite State Machine (FSM) Based Artificial Intelligence (AI) Subsystem},
journal = {Procedia Computer Science},
volume = {235},
pages = {2996-3006},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.283},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924009621},
author = {Abidemi Emmanuel Adeniyi and Biswajit Brahma and Marion Olubunmi Adebiyi and Joseph Bamidele Awotunde and Rasheed Gbenga Jimoh and Enoch Olasinde and Anjan Bandyopadhyay},
keywords = {Artificial intelligence, 2D game engine, Finite state machine, software},
abstract = {With Al becoming more and more relevant in today’s world, this project aims to develop a 2D game engine with an Al subsystem for state-driven agents, which is rarely implemented by a lot of 2D engines out there. In this study, a 2D game engine was designed with an FSM (Finite State Machine)--based AL subsystem using state-driven game agents. The engine was implemented using the Javascript programming language and the WebGL 2.0 graphics library/API. It is targeted at web-based games/simulations. Components and subsystems include physics, audio, math, rendering, and AI (based on finite-state machines). The FSM-based AI subsystem is a solution aimed at reducing the ambiguity and performance hits associated with creating 2D game AI in the naive approach. The AI subsystem creates an interface for 2D games to be created with a common paradigm, and simulated with a great level of realism. The state machine used in this study is used to represent a variety of behaviours, such as wandering, attacking, and fleeing. The following conclusions were drawn as regards the impact of the AI approach used on rendering performance; the naive approach to implementing game AI is also compared with the FSM approach in terms of rendering (frames per second/ FPS, or frame rate). With the naive approach (vector math) being used to implement AI, there was a drop in the rendering frame rate to 50 FPS. The FSM approach didn’t affect the frame rate, which is usually at 60FPS. With a well-developed FSM (Finite State Machine), game agents can transition between states easily as a response to user input or stimulus from the game environment. The proposed method was tested by creating a prototype game with the 2D game engine. The prototype game was a straightforward side-scrolling platformer featuring a cast of non-player characters (NPCs). This approach to implementing 2D game AI goes a long way toward improving performance and mitigating ambiguity in the code.}
}
@article{HOSSEINI20242310,
title = {Immersive Interaction in Digital Factory: Metaverse in Manufacturing},
journal = {Procedia Computer Science},
volume = {232},
pages = {2310-2320},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.050},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002278},
author = {Shimasadat Hosseini and Ali Abbasi and Luis G. Magalhaes and Jaime C. Fonseca and Nuno M.C. {da Costa} and António H.J. Moreira and João Borges},
keywords = {Industry 4.0, Digital Twin, Shared Reality, Locomotion, Virtual Factory, Metaverse, Immersive Interaction, Human Motion Tracking System},
abstract = {Digital twins and virtual reality are pivotal technologies in the context of Industry 4.0, facilitating the design, simulation, optimization, and remote interaction with production systems. These technologies also present new prospects for developing immersive and hyper-realistic digital factories within the metaverse. This paper aims to enhance collaboration and communication in a 3D virtual world, focusing on shared reality and the metaverse's integration in digital factories. Our main contribution enables users to have full-body immersive interaction with virtual 3D assets and realistic locomotion in a 3D environment, fostering flexibility in collaboration and environment monitoring/control. To achieve this, we propose a systematic methodology for designing and implementing a digital twin-based factory with an IoT infrastructure. By replicating sensors and actuators in digital twins, real-time asset management synchronized with the physical factory is realized, empowering full-body interaction. To enable full-body interaction with virtual equipment, we employ a human motion tracking system. A proof-of-concept case study, developed using Unity3D game engine, Solace PubSub+ event streaming APIs, and Awinda Xsens wearable inertial sensors as the motion tracking system, validates our proposed methodology. The results of the case study demonstrate the successful integration of digital twins, virtual reality, and the metaverse, enabling real-time monitoring and control, full-body interaction, and immersive user experiences in the virtual environment.}
}
@article{NAGALINGAM2015423,
title = {User Experience of Educational Games: A Review of the Elements},
journal = {Procedia Computer Science},
volume = {72},
pages = {423-433},
year = {2015},
note = {The Third Information Systems International Conference 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.123},
url = {https://www.sciencedirect.com/science/article/pii/S187705091503584X},
author = {Vanisri Nagalingam and Roslina Ibrahim},
keywords = {User Experience, Human Computer Interaction, Educational games},
abstract = {Over the recent years, the study on User Experience (UX) have been an area of discussion among Human Computer Interaction (HCI) researchers. User Experience is a branch of HCI which focus on interaction between products and users thus in the era of growing digital games, UX plays an important part in identifying the appropriate or suitable variable in order to evaluate the UX design. This study explore about the UX elements for the purpose of evaluation and design of educational games (EG). EG have captivated most students with the idea of mixing fun with learning. A good framework of UX for educational game will help EG designer to evaluate the UX of their games in order to ensure that they have produced the effective game. Therefore, it is essential to identify the suitable elements in order to model the right UX framework for educational games.}
}
@article{ZHANG20223516,
title = {Using Simulation-software-generated Animations to Investigate Attitudes Towards Autonomous Vehicles Accidents},
journal = {Procedia Computer Science},
volume = {207},
pages = {3516-3525},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.410},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013023},
author = {Qiyuan Zhang and Christopher Wallbridge and Phillip L Morgan and Dylan M Jones},
keywords = {Autonomous driving, autonomous systems, autonomous vehicles accidents, road accidents, drivinng simulation, blame attribution, liability, trust in automation, human-robot interaction, artificial intelligence},
abstract = {Road accidents involving autonomous vehicles are inevitable and have the potential to damage the public's confidence in the technology and ultimately result in its disuse. It's important to understand how people react to such incidents and the influencing factors of blame attribution and trust restoration. Research in this field has started to grow but faces a huge methodological challenge, which is to develop high-fidelity experimental stimuli as realistic representations of accident scenarios in order to elicit valid reactions from human participants. The present paper reviews and evaluates several existing methods used in the research field before proposing an alternative method of generating animated accident sequence using driving simulation software. It is argued that this method strikes a good balance of fidelity, versatility and cost-effectiveness. We also present some preliminary evidence for the effectiveness of variable manipulation using such a methodology.}
}
@article{BHAUMIK2023488,
title = {An Intelligent Virtual Environment for Designers with Reduced Motor Abilities},
journal = {Procedia Computer Science},
volume = {218},
pages = {488-503},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000315},
author = {Rahul Bhaumik and Tarun Kumar and Unais Sait},
keywords = {Virtual Reality, design tool, intelligent interface, gaze-based input, artificial intelligence},
abstract = {Conventional CAD modelling software demands substantial utilisation of input modalities like the keyboard and mouse for creating 3-dimensional (3D) models. The dexterity measures involved in controlling input modalities could pose challenges to users with motor disabilities—including the inability to move their limbs, particularly their upper and lower arms, and fingers, due to traumatic damage or congenital problems. In order to meet these challenges, this paper proposes a virtual reality (VR)-based medium to help users with motor disabilities build simple 3D models for architectural design. The concept of operating buttons using head-gaze in the VR environment has been utilised to perform scaling—a 3D object manipulation method—to create simplified building models. Moreover, navigation in the VR space using tilting of the head has been employed with the user seated on a revolving chair, thus eliminating the need for any limbic movement. Unity game engine was used to develop two variations of the VR model with a different button layout for creating simple cuboidal volumes mimicking buildings in the virtual environment. Both variations have been tested with 32 individuals against a specific performance indicator (i.e., task completion time) and self-reported metrics, such as the perception of effort applied and degree of visual clutter, followed by retrospective participant feedback sessions. One of the VR application's variants (i.e., variant 1) produced promising results regarding overall usability and effort demand. This paper also proposes a methodological framework for an AI-based, intelligent, and adaptive VR application interface that caters to the user's abilities and pain points in real-time. In the future, this framework could be instrumental in creating a comprehensive gaze-based VR tool for 3D modelling having multiple functions to help users with motor disabilities.}
}
@article{SUNAN2023591,
title = {Feasible Technology for Augmented Reality in Fashion Retail by Implementing a Virtual Fitting Room},
journal = {Procedia Computer Science},
volume = {227},
pages = {591-598},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.562},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017295},
author = {Ronald Sumichael Sunan and Samuel Christopher and Novandy Salim and  Anderies and Andry Chowanda},
keywords = {Augmented Reality, Virtual Fitting Room, Kinect, Fashion Retail, Mobile Application},
abstract = {Augmented Reality has become a bridge from the virtual to the real world. This technology has been an implementable choice for fashion companies to provide more information and customization for their user's shopping experience. From the COVID-19 pandemic alongside technological globalization, fashion industries have to find a new way for customers to purchase products. All things considered, AR helped fashion industries keep customers attracted and satisfied. This paper is conducted in a Systematic Literature Review manner to review and compare the AR technologies used by fashion industries. Furthermore, the result would be a proposition of which Augmented Reality technology or platform option is feasible and implementable for the fashion industry. After comparing the technologies used in past papers and their technical and economic feasibility, we propose a 3-term plan. The short-term plan with zero cost and mobile platform-focused application, the medium-term plan with an upgraded version of the garment object from 2D images to 3D objects, and the long-term plan changing the platform used from mobile to a new device. We also conclude that artificial intelligence can also support and improve the augmented reality experience for customers in the fashion retail industry by implementing an AI-powered virtual assistant.}
}
@article{CHHEANG2024103879,
title = {Advanced liver surgery training in collaborative VR environments},
journal = {Computers & Graphics},
volume = {119},
pages = {103879},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000050},
author = {Vuthea Chheang and Danny Schott and Patrick Saalfeld and Lukas Vradelis and Tobias Huber and Florentine Huettl and Hauke Lang and Bernhard Preim and Christian Hansen},
keywords = {Virtual reality, Collaborative VR, Medical training, Liver surgery planning, Laparoscopic surgery training},
abstract = {Virtual surgical training systems are crucial for enabling mental preparation, supporting decision-making, and improving surgical skills. Many virtual surgical training environments focus only on training for a specific medical skill and take place in a single virtual room. However, surgical education and training include the planning of procedures as well as interventions in the operating room context. Moreover, collaboration among surgeons and other medical professionals is only applicable to a limited extent. This work presents a collaborative VR environment similar to a virtual teaching hospital to support surgical training and interprofessional collaboration in a co-located or remote environment. The environment supports photo-realistic avatars and scenarios ranging from planning to training procedures in the virtual operating room. It includes a lobby, a virtual surgical planning room with four surgical planning stations, laparoscopic liver surgery training with the integration of laparoscopic surgical instruments, and medical training scenarios for interprofessional team training in a virtual operating room. Each component was evaluated by domain experts as well as in a series of user studies, providing insights on usability, usefulness, and potential research directions. The proposed environment may serve as a foundation for future medical training simulators.}
}
@article{NUTONEN2023446,
title = {Industrial Robot Training in the Simulation Using the Machine Learning Agent},
journal = {Procedia Computer Science},
volume = {217},
pages = {446-455},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.240},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023183},
author = {Karle Nutonen and Vladimir Kuts and Tauno Otto},
keywords = {simulation, inverse kinematics, machine learning, robotics, Industry 4.0, virtual environment},
abstract = {Continuous change in manufacturing requires robotization, requiring a skilled workforce with robotic skills. It is also important for a manufacturing company to be able to transform its production process quickly. But now it is a long and complex process. The paper presents the simulation of the movement of an industrial robot in a digital environment, to which implemented the inverse kinematics functionality and machine learning model have been applied. The use of machine learning reduces the time required to develop the process and the investment in finding the path of the robot. The results obtained in the application of Bio-ik inverse kinematics and machine learning have been observed and analyzed as a simulation in the created research.}
}
@article{ALONSO2023306,
title = {Real-time rendering and physics of complex dynamic terrains modeled as CSG trees of DEMs carved with spheres},
journal = {Computers & Graphics},
volume = {114},
pages = {306-315},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323001152},
author = {Jesús Alonso and Robert Joan-Arinyo and Antoni Chica},
keywords = {Terrain modeling, CSG, Terrain erosion},
abstract = {We present a novel proposal for modeling complex dynamic terrains that offers real-time rendering, dynamic updates and physical interaction of entities simultaneously. We can capture any feature from landscapes including tunnels, overhangs and caves, and we can conduct a total destruction of the terrain. Our approach is based on a Constructive Solid Geometry tree, where a set of spheres are subtracted from a base Digital Elevation Model. Erosions on terrain are easily and efficiently carried out with a spherical sculpting tool with pixel-perfect accuracy. Real-time rendering performance is achieved by applying a one-direction CPU–GPU communication strategy and using the standard depth and stencil buffer functionalities provided by any graphics processor.}
}
@article{LATINI2024104286,
title = {Investigating the impact of greenery elements in office environments on cognitive performance, visual attention and distraction: An eye-tracking pilot-study in virtual reality},
journal = {Applied Ergonomics},
volume = {118},
pages = {104286},
year = {2024},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2024.104286},
url = {https://www.sciencedirect.com/science/article/pii/S0003687024000632},
author = {Arianna Latini and Ludovica Marcelli and Elisa {Di Giuseppe} and Marco D'Orazio},
keywords = {Eye-tracking, Virtual Reality, Greenery indoors, Cognitive tasks, Office environment},
abstract = {The human-nature connection is one of the main aspects determining supportive and comfortable office environments. In this context, the application of eye-tracking-equipped Virtual Reality (VR) devices to support an evaluation on the effect of greenery elements indoors on individuals’ efficiency and engagement is limited. A new approach to investigate visual attention, distraction, cognitive load and performance in this field is carried out via a pilot-study comparing three virtual office layouts (Indoor Green, Outdoor Green and Non-Biophilic). 63 participants completed cognitive tasks and surveys while measuring gaze behaviour. Sense of presence, immersivity and cybersickness results supported the ecological validity of VR. Visual attention was positively influenced by the proximity of users to the greenery element, while visual distraction from tasks was negatively influenced by the dimension of the greenery. In the presence of greenery elements, lower cognitive loads and more efficient information searching, resulting in improved performance, were also highlighted.}
}
@article{VANWEZEL202389,
title = {Virtual Ray Tracer 2.0},
journal = {Computers & Graphics},
volume = {111},
pages = {89-102},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323000067},
author = {Chris S. {van Wezel} and Willard A. {Verschoore de la Houssaije} and Steffen Frey and Jiří Kosinka},
keywords = {Ray tracing, Computer graphics education, Visualization},
abstract = {Building on our original Virtual Ray Tracer tool, we present Virtual Ray Tracer 2.0, an interactive and gamified application that allows students/users to view and explore the ray tracing process in real-time. The application shows a scene containing a camera casting rays which interact with objects in the scene. Users are able to modify and explore ray properties such as their animation speed, the number of rays and their visual style, as well as the material properties of the objects in the scene. The goal of the application is to help the users – students of Computer Graphics and the general public – to better understand the ray tracing process and its characteristics. This includes not only the basics of ray tracing, but also more advanced concepts such as soft shadows. To invite users to learn and explore, various explanations and scenes are provided by the application at different levels of complexity, each with a step-by-step tutorial. Several user studies showed the effectiveness of the tool in supporting the understanding and teaching of ray tracing. The educational tool is built with the cross-platform engine Unity, and we make it fully available to be extended and/or adjusted to fit the requirements of courses at other institutions, educational tutorials, or of enthusiasts from the general public.}
}
@article{CROISSANT2023100591,
title = {Theories, methodologies, and effects of affect-adaptive games: A systematic review},
journal = {Entertainment Computing},
volume = {47},
pages = {100591},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100591},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000460},
author = {Maximilian Croissant and Guy Schofield and Cade McCall},
keywords = {Adaptation, Emotion, Video games, Affective computing},
abstract = {Affect-adaptive games gained in popularity over the last years in human computer interactions studies, promising potential benefits for player experience, performance, and even health. It is however not yet clear how affective games are being evaluated, what the precise effects are, and how they are based on emotion theoretical concepts that are still not universally agreed upon. This systematic review investigated these questions by analysing relevant high-quality evaluation studies of the effect of affect-adaptive video games on various outcomes in regards to their effects, theoretical assumptions, and methodologies. Out of 3,930 papers, 26 studies were included based on preregistered inclusion and exclusion criteria. A high variance regarding theoretical assumptions and methodological approaches was observed, as well as an overall poor methodological rigour, leading to the conclusion that more work is needed in constructing better methodological standards for game evaluation studies and theoretical considerations when developing and testing affect-adaptive video games.}
}
@article{RALLIS2022513,
title = {A mobile game for enhancing Tourism and Cultural Heritage},
journal = {Procedia Computer Science},
volume = {204},
pages = {513-518},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008006},
author = {Ioannis Rallis and George Kopsiaftis and Ilias Kalisperakis and Christos Stentoumis and Dimitris Koutsomitsos and Vivian Riga},
keywords = {Tourism, Photogrammetry, Mobile Game, Cultural Heritage},
abstract = {This paper briefly describes the overall concept of ”TRAVEL TYCOON GREECE” (TTGR), a novel business simulation game which aims to simulate realistically a complete tourism experience. The latest image processing and computer graphics technologies were utilized to create accurate 3D backgrounds with different levels of detail, which were incorporated in the game engine and serve as a realistic and accurate terrain allowing the user to navigate in selected historical and touristic areas of Greece. A series of real-world scenarios representing multiple components of the tourism section were designed primarily for entertainment and marketing purposes. In order to motivate users to participate in the game or remain active, an incorporated ticketing platform allows users to win offers such as touristic products and services.}
}
@article{MORENOLUMBRERAS2023107064,
title = {CodeCity: A comparison of on-screen and virtual reality},
journal = {Information and Software Technology},
volume = {153},
pages = {107064},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107064},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001732},
author = {David Moreno-Lumbreras and Roberto Minelli and Andrea Villaverde and Jesus M. Gonzalez-Barahona and Michele Lanza},
keywords = {CodeCity, City metaphor, Software visualization, Software evolution, Reverse engineering, Virtual reality, Web, 3D},
abstract = {Context:
Over the past decades, researchers proposed numerous approaches to visualize source code. A popular one is CodeCity, an interactive 3D software visualization representing software system as cities: buildings represent classes (or files) and districts represent packages (or folders). Building dimensions represent values of software metrics, such as number of methods or lines of code. There are many implementations of CodeCity, the vast majority of them running on-screen. Recently, some implementations using virtual reality (VR) have appeared, but the usefulness of CodeCity in VR is still to be proven.
Aim:
Our comparative study aims to answer the question “Is VR well suited for CodeCity, compared to the traditional on-screen implementation?”
Methods:
We performed two experiments with our web-based implementation of CodeCity, which can be used on-screen or in immersive VR. First, we conducted a controlled experiment involving 24 participants from academia and industry. Taking advantage of the obtained feedback, we improved our approach and conducted a second controlled experiment with 26 new participants.
Results:
Our results show that people using the VR version performed the assigned tasks in much less time, while maintaining a comparable level of correctness.
Conclusion:
VR is at least equally well-suited as on-screen for visualizing CodeCity, and likely better.}
}
@article{SUNO20232283,
title = {Virtual Hydrogen, a virtual reality education tool in physics and chemistry},
journal = {Procedia Computer Science},
volume = {225},
pages = {2283-2291},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.219},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013777},
author = {Hiroya Suno and Nobuaki Ohno},
keywords = {Virtual reality, education tool, physics, chemistry},
abstract = {In efforts to explore possibilities of virtual reality (VR) as education tools, we focus on one of the most important subjects taught in undergraduate physics and chemistry classes, namely, the quantum atomic orbitals or wave functions of the hydrogen atom. Our goal is to enable students to experience attractive and enjoyable scenarios which make it more amenable to master the concepts of quantum mechanics or quantum chemistry. Along this line, we have built a virtual environment of the hydrogen atom, named “Virtual Hydrogen”, enabling to explore the atomic orbitals in 3D space. The most fundamental 21 atomic orbitals of hydrogen have been successfully visualized and will be experienced through modern VR head-mounted displays.}
}
@article{MATTHEWS2020330,
title = {Interaction design for paediatric emergency VR training},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {4},
pages = {330-344},
year = {2020},
note = {VR and experiment simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300590},
author = {Tj Matthews and Feng Tian and Tom Dolby},
keywords = {Virtual reality, Medical training, Human-Centred design, Interaction design},
abstract = {Background
Virtual reality (VR) in healthcare training has increased adoption and support, but efforts are still required to mitigate usability concerns.
Methods
This study conducted a usability study of an in-use emergency medicine VR training application, available on commercially available VR hardware and with a standard interaction design. Nine users without prior VR experience but with relevant medical expertise completed two simulation scenarios for a total of 18 recorded sessions. They completed NASA Task Load Index and System Usability Scale questionnaires after each session, and their performance was recorded for the tracking of user errors.
Results and Conclusion
s Our results showed a medium (and potentially optimal) Workload and an above average System Usability Score. There was significant improvement in several factors between users' first and second sessions, notably increased Performance evaluation. User errors with the strongest correlation to usability were not directly tied to interaction design, however, but to a limited 'possibility space'. Suggestions for closing this 'gulf of execution' were presented, including 'voice control' and 'hand-tracking', which are only feasible for this commercial product now with the availability of the Oculus Quest headset. Moreover, wider implications for VR medical training were outlined, and potential next steps towards a standardized design identified.}
}
@article{VIRTANEN2020375,
title = {Interactive dense point clouds in a game engine},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {163},
pages = {375-389},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620300691},
author = {Juho-Pekka Virtanen and Sylvie Daniel and Tuomas Turppa and Lingli Zhu and Arttu Julin and Hannu Hyyppä and Juha Hyyppä},
keywords = {Point cloud, Game engine, VR},
abstract = {With the development of 3D measurement systems, dense colored point clouds are increasingly available. However, up to now, their use in interactive applications has been restricted by the lack of support for point clouds in game engines. In addition, many of the existing applications for point clouds lack the capacity for fluent user interaction and application development. In this paper, we present the development and architecture of a game engine extension facilitating the interactive visualization of dense point clouds. The extension allows the development of game engine applications where users edit and interact with point clouds. To demonstrate the capabilities of the developed extension, a virtual reality head-mounted display is used and the rendering performance is evaluated. The result shows that the developed tools are sufficient for supporting real-time 3D visualization and interaction. Several promising use cases can be envisioned, including both the use of point clouds as 3D assets in interactive applications and leveraging the game engine point clouds in geomatics.}
}
@article{GOLUBEV2018443,
title = {A framework for a multi-agent traffic simulation using combined behavioural models},
journal = {Procedia Computer Science},
volume = {136},
pages = {443-452},
year = {2018},
note = {7th International Young Scientists Conference on Computational Science, YSC2018, 02-06 July2018, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.267},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918315722},
author = {Kirill Golubev and Aleksandr Zagarskikh and Andrey Karsakov},
keywords = {Traffic modelling, car-following models, agent-based modelling, scientific visualization},
abstract = {The task of simulation of urban processes often concerns city traffic and road dynamics. Many assumptions in an urban science may be made by analysing traffic flows and vehicle behaviour on city streets. Since road traffic is a dynamic and possibly spatially large system that depends on many external conditions, approaches like visual analysis and computational steering may be used. There are many traffic modelling frameworks presented in the field, but there are also several drawbacks most of them have: they are limited to a specific model or case; have abstracted logic which disregards some factors affecting vehicle movement; they ignore possible physical interactions which are not described in a model or use too many simplifications in vehicle interaction simulations. In this paper, a novel agent-based traffic modelling framework is presented, combining different classes of traffic models into a single vehicle agent and allowing the user to set a specific model for each supported class. The framework was made using a game engine, Unreal Engine 4, which allows users to model realistic interaction between vehicles and make realistic visualisations for purposes of visual analysis and computational steering.}
}
@article{LONGO2023100437,
title = {From “prepare for the unknown” to “train for what's coming”: A digital twin-driven and cognitive training approach for the workforce of the future in smart factories},
journal = {Journal of Industrial Information Integration},
volume = {32},
pages = {100437},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100437},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X23000109},
author = {Francesco Longo and Antonio Padovano and Fabio {De Felice} and Antonella Petrillo and Mohaiad Elbasheer},
keywords = {Smart factory, Prescriptive analytics, Knowledge management, Virtual reality, Workforce training, Fuzzy cognitive maps},
abstract = {Despite on-the-job assistance technology is getting popular in the Smart Operator 4.0 literature, non-routine work also requires highly-skilled and pre-trained workers to prevent serious errors and keep high efficiency and safety levels. The question that motivates this study is: ‘is it possible to train industrial workers for what exactly is coming instead of preparing them for a large set of scenarios - even very unlikely ones?’. This work proposes: (i) a structured On-the-Job Training strategy for non-routine tasks, called ‘training-on-the-go’ or ‘prescriptive training’, according to which a Prescriptive Analytics module schedules the training sessions not long before the actual performance, but only when and if needed; (ii) a proof-of-concept of a game-based training system where virtual scenes and context of an industrial site are faithfully recreated thanks to digital twin data and models; (iii) the use of evolutionary-based fuzzy cognitive maps (E-FCM) for the extraction of the workers’ implicit procedural knowledge and for the comparison of mental models of experienced vs. inexperienced workers to assess potential misconceptions or flaws in their decision-making process. This work contributes to the evolution of worker training paradigms and systems from the perspective of human-centric cyber-physical production systems and aims for current gaps in workforce training, i.e. poor timeliness and effectiveness, limited context and industrial information integration, and scarce focus on the experts’ implicit knowledge. An application study with a non-routine task on an offshore oil platform demonstrates how the proposed system facilitates knowledge transfer, offers situational awareness and sustains the workforce competence development process.}
}
@article{RODEN200776,
title = {Toward mobile entertainment: A paradigm for narrative-based audio only games},
journal = {Science of Computer Programming},
volume = {67},
number = {1},
pages = {76-90},
year = {2007},
note = {Special Issue on Aspects of Game Programming},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2006.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S016764230700055X},
author = {Timothy E. Roden and Ian Parberry and David Ducrest},
keywords = {Computer games, Entertainment computing, 3D audio, Interactive audio},
abstract = {The widespread use of sophisticated mobile computing devices has set the stage for a renaissance in audio only entertainment. Traditional visual games are already used widely in cellular phones and similar devices. A significant limitation is the small display size. In contrast, audio only games on suitable mobile hardware need not degrade due to the smaller form factor. This makes audio only games an attractive alternative to visual games. We describe a framework for authoring interactive narrative-based audio only games set in 3D virtual environments. Despite the novelty in audio only gaming, our approach builds on a foundation of several years of research into audio only applications for sight impaired users, augmented reality systems and human–computer interaction studies. In comparison to attempts to provide a realistic user interface, we argue a simple interface enhances both immersion and entertainment value, serendipitously making audio only games practical for mobile computing. Novel features of our system include real-time gameplay and multi-player support. We also describe our software architecture, the current implementation of which uses low-cost existing PC-based hardware and software. In addition, we describe our first game, Dragon’s Roar.}
}
@article{BERGER20152913,
title = {CFD Post-processing in Unity3D},
journal = {Procedia Computer Science},
volume = {51},
pages = {2913-2922},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.476},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915012843},
author = {Matthias Berger and Verina Cristie},
keywords = {CFD post-processing, Unity3D, urban climate, urban designs, visualization},
abstract = {In architecture and urban design, urban climate on is a strong design criterion for outdoor thermal comfort and building's energy performance. Evaluating the effect of buildings on the local climate and vice versa is done by computational fluid dynamics (CFD) methods. The results from CFD are typically visualized through post-processing software closely related to pre-processing and simulation software. The built-in functions are made for engineers and thus, it lacks user-friendliness for real-time exploration of results for architects. To bridge the gap between architect and engineer we propose visualizations based on game engine technology. This paper demonstrates the implementation of CFD to Unity3D conversion and weather data visualization.}
}
@article{ROBYNS20242366,
title = {A Digital Twin of an Off Highway Vehicle based on a Low Cost Camera},
journal = {Procedia Computer Science},
volume = {232},
pages = {2366-2375},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.055},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002321},
author = {Steven Robyns and Wouter Heerwegh and Sam Weckx},
keywords = {Digital Twin, Edge computing, Decision making, Construction 4.0},
abstract = {The operation of heavy equipment in the construction industry requires high-skilled workers: they must safely operate the equipment in an efficient manner to avoid project delays or damages, both to equipment and material as to their colleagues and themselves. Operator productivity and safety are therefore critical in the construction industry. The advancements in technologies such as digitization and automation are more and more applied in the industry, leading to more data and knowledge on the construction equipment and environment. This can be leveraged by a digital twin to provide value for the operator. In this work, we present a digital twin architecture applied to an off highway vehicle, which provides real-time feedback to the operator while operating the vehicle. Monitoring the interactions between an off highway vehicle and workers will result in a safer and more productive environment. This work focuses on detection of the environment and monitoring the state of the vehicle. Images of a low cost camera are used to extract information of the vehicle implement as well as the environment. The motion and state of the machine can be realistically visualized in real-time in Unreal, using a 3D CAD (Computer-Aided Design) geometry model, enriched with additional relevant information and color schemes. A modular pub-sub architecture enables the communication between the sensors and camera, the data processing and the visualization of the digital twin.}
}
@article{REZANAJI2022101077,
title = {Accelerating sailfish optimization applied to unconstrained optimization problems on graphical processing unit},
journal = {Engineering Science and Technology, an International Journal},
volume = {32},
pages = {101077},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621002093},
author = {Hamid {Reza Naji} and Soodeh Shadravan and Hossien {Mousa Jafarabadi} and Hossien Momeni},
keywords = {Sailfish Optimizer (SFO), Accelerated Sailfish Optimizer (ASFO) unconstrained optimization problems, Parallel processing, Shared memory, Graphic processing units, CUDA},
abstract = {The Sailfish Optimizer (SFO) is a metaheuristic algorithm inspired by a group of hunting sailfish that alternates their attacks on group of prey. The SFO algorithm takes advantage of using a simple method for providing the dynamic balance between exploration and exploitation phases, creating the swarm diversity, avoiding local optima, and guaranteeing high convergence speed. However, taking a lot of time to solve optimization problems has become a challenge for metaheuristic algorithms. Due to independence of the metaheuristics components, parallel processing is a good option to reduce the computational time and to find high quality solutions that are close to the optimum with an acceptable cost. Nowadays, combination of parallel processing and metaheuristic algorithms can provide high performance solutions to quickly solve combinatorial optimization problems. In this paper, we elaborate a novel GPU based and accelerated method of sailfish optimizer (ASFO), which improves the execution time and speedup while maintaining the results of optimization in high quality. In depth of study, we present the implementation details and performance observations of ASFO algorithm. Also, a comparative study of accelerated and sequential SFO is performed on a set of standard benchmark optimization functions and it compared with other parallel algorithms to show the speed of proposed algorithm for solving unconstrained optimization problems. The results indicate the ability of proposed approach in continuous, non-separable, non-convex and scalable optimization problems.}
}
@article{WANG2024100150,
title = {Gamifying cultural heritage: Exploring the potential of immersive virtual exhibitions},
journal = {Telematics and Informatics Reports},
volume = {15},
pages = {100150},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100150},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000367},
author = {Hanbing Wang and Ze Gao and Xiaolin Zhang and Junyan Du and Yidan Xu and Ziqi Wang},
keywords = {Cultural Heritage, Gamification, Human–computer interaction, Immersive virtual exhibition, Review},
abstract = {This paper reviews the potential of gamified cultural heritage in immersive virtual exhibitions. A systematic literature review following PRISMA guidelines identified 78 relevant papers from ACM and IEEE databases. Gamification and immersive technologies can provide interactive experiences to engage visitors and enhance their understanding of exhibits’ historical and cultural significance. Theoretical frameworks, including gamification theory, heritage interpretation theory, participatory heritage, immersive experience theory, and pedagogy, guide designing compelling experiences. Case studies like “Rome Reborn”, “Sutton House Stories”, and “Assassin’s Creed: Origins” demonstrate the efficacy of gamification in disseminating heritage. Key strategies include integrating augmented/virtual reality, multimodal data and 3D reconstruction, interactive narratives and gameplay, personalized experiences, advanced interfaces, balancing education and entertainment, and ensuring cultural sensitivity. Future work can explore AI-adaptive experiences, AR/VR integration, remote collaboration, educational game elements, and digital creativity models. Gamification and immersion provide innovative preservation and inheritance of cultural heritage. This review promotes digitalization and identifies literature gaps, supporting reflection on engagement’s past, present, and future. It aims to enable a broader appreciation of cultural heritage through technology.}
}
@article{MASOOD20229393,
title = {A novel method for adaptive terrain rendering using memory-efficient tessellation codes for virtual globes},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part B},
pages = {9393-9408},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822003469},
author = {Zafar Masood and Zheng Jiangbin and Idrees Ahmad and Muhammad Irfan and Nafees Ahmad},
keywords = {Real-time graphics, Terrain modelling, Level of detail, Hardware tessellation, Virtual globes},
abstract = {Virtual globes render large-scale real-world earth’s surface in real-time for geographic information visualization. Modern GPUs have introduced hardware tessellation features for high-performance rendering. Patch-based terrain rendering using hardware tessellation introduces cracks and swimming artifacts during navigation. Hardware tessellation-based terrain rendering methods limit the tessellation factors to power-of-two and render patches with discrete-level-of-detail (DLOD). We present a novel method for adaptive terrain rendering using tessellation codes. In the proposed method, we present novel memory-efficient bit-field-based tessellation codes to encode patch Level-of-detail (LOD). The encoding scheme enabled patch rendering with Continuous-level-of-detail (CLOD) on tessellation hardware. The view-dependent simplification algorithm simplifies and encodes the patch’s Level-of-detail (LOD) in tessellation codes. The proposed method, based on tessellation codes, tessellates patches as uniformly spaced vertices grid. A novel relocation algorithm, based on tessellation codes, relocates uniformly spaced vertices to construct an encoded simplified vertices grid to avoid boundary cracks and swimming artifacts. The tessellation codes consumed 28 kilobytes of memory for a patch size of 65. The proposed method rendered 2.5 million triangles with a frame rate of 570 frames-per second for an ultra-HD display. The proposed method achieved a high simplification and rendering rate with low computational and memory usage as compared to state-of-the-art methods.}
}
@article{STRANNEGARD2024101235,
title = {Survival games for humans and machines},
journal = {Cognitive Systems Research},
volume = {86},
pages = {101235},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101235},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000299},
author = {Claes Strannegård and Niklas Engsner and Simon Ulfsbäcker and Sebastian Andreasson and John Endler and Ann Nordgren},
keywords = {Human versus machine performance, Pure survival games, Deep reinforcement learning, Random worlds, Artificial general intelligence},
abstract = {Survival games can be described as video games where the player searches for food and treasures, while avoiding obstacles and hostile attacks. Ms.Pac-Man and Minecraft are two well-known examples. Currently there are AI models that outperform human players at Ms.Pac-Man, while AI models playing Minecraft above the human level has been a long-standing challenge. This paper concerns what we call pure survival games, which take place in previously unseen worlds containing only food, water, and obstacles. The challenge of the player is to navigate and survive in those worlds by continuously finding resources and avoiding obstacles. Arguably, animals need to master physical analogues of pure survival games in order to survive and reproduce. Here we begin to explore human and machine performance on pure survival games. We define two games called the Grid game and the Terrain game and two corresponding AI agents based on deep reinforcement learning: the Grid agent and the Terrain agent. We explore to what extent these agents can match human performance and how their performance is affected by variations in their perception, memory, and reward models. We find that (1) the Terrain agent performs above human level, while the Grid agent performs below human level; (2) the smell, touch, and interoception models contribute significantly to the performance of the Grid agent; (3) the memory model contributes significantly to the performance of the Grid agent; and (4) the performance of the Grid agent is relatively stable under three quite different reward signals, including one that rewards survival and nothing else.}
}
@article{SCHIEBER2024103907,
title = {Indoor Synthetic Data Generation: A Systematic Review},
journal = {Computer Vision and Image Understanding},
volume = {240},
pages = {103907},
year = {2024},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2023.103907},
url = {https://www.sciencedirect.com/science/article/pii/S1077314223002874},
author = {Hannah Schieber and Kubilay Can Demir and Constantin Kleinbeck and Seung Hee Yang and Daniel Roth},
keywords = {Synthetic data generation, Indoor synthetic data, Domain randomization},
abstract = {Objective:
Deep learning-based object recognition, 6D pose estimation, and semantic scene understanding require a large amount of training data to achieve generalization. Time-consuming annotation processes, privacy, and security aspects lead to a scarcity of real-world datasets. To overcome this lack of data, synthetic data generation has been proposed, including multiple facets in the area of domain randomization to extend the data distribution. The objective of this review is to identify methods applied for synthetic data generation aiming to improve 6D pose estimation, object recognition, and semantic scene understanding in indoor scenarios. We further review methods used to extend the data distribution and discuss best practices to bridge the gap between synthetic and real-world data.
Methods:
We adhered to the guidelines of the systematic PRISMA technique. Three databases, IEEE Xplore, Springer Link, and ACM, and an additional manual search were conducted. In total, we identified 241 studies and included 34 in our systematic review.
Conclusion:
In summary, synthetic data generation has been performed using crop-out methods, graphic APIs, 3D modeling or authoring tools, or game engine-based methods. To extend the data distribution, varying scene parameters, i.e., lighting conditions or textures and the use of distracting objects in the scene are promising.}
}
@article{SINGH2020103275,
title = {Combining gaze and AI planning for online human intention recognition},
journal = {Artificial Intelligence},
volume = {284},
pages = {103275},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2020.103275},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218307628},
author = {Ronal Singh and Tim Miller and Joshua Newn and Eduardo Velloso and Frank Vetere and Liz Sonenberg},
keywords = {Intention recognition, Gaze, Planning},
abstract = {Intention recognition is the process of using behavioural cues, such as deliberative actions, eye gaze, and gestures, to infer an agent's goals or future behaviour. In artificial intelligence, one approach for intention recognition is to use a model of possible behaviour to rate intentions as more likely if they are a better ‘fit’ to actions observed so far. In this paper, we draw from literature linking gaze and visual attention, and we propose a novel model of online human intention recognition that combines gaze and model-based AI planning to build probability distributions over a set of possible intentions. In human-behavioural experiments (n=40) involving a multi-player board game, we demonstrate that adding gaze-based priors to model-based intention recognition improved the accuracy of intention recognition by 22% (p<0.05), determined those intentions ≈90 seconds earlier (p<0.05), and at no additional computational cost. We also demonstrate that, when evaluated in the presence of semi-rational or deceptive gaze behaviours, the proposed model is significantly more accurate (9% improvement) (p<0.05) compared to a model-based or gaze only approaches. Our results indicate that the proposed model could be used to design novel human-agent interactions in cases when we are unsure whether a person is honest, deceitful, or semi-rational.}
}
@article{LIU2020132,
title = {Two-phase real-time rendering method for realistic water refraction},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {2},
pages = {132-141},
year = {2020},
note = {Special issue on Visual interaction and its application},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2019.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300164},
author = {Hongli Liu and Honglei Han and Guangzheng Fei},
keywords = {Real-time rendering, Refraction, Liquid rendering},
abstract = {Background
Realistic rendering has been an important goal of several interactive applications, which requires an efficient virtual simulation of many special effects that are common in the real world. However, refraction is often ignored in these applications. Rendering the refraction effect is extremely complicated and time-consuming.
Methods
In this study, a simple, efficient, and fast rendering technique of water refraction effects is proposed. This technique comprises a broad and narrow phase. In the broad phase, the water surface is considered flat. The vertices of underwater meshes are transformed based on Snell's Law. In the narrow phase, the effects of waves on the water surface are examined. Every pixel on the water surface mesh is collected by a screen-space method with an extra rendering pass. The broad phase redirects most pixels that need to be recalculated in the narrow phase to the pixels in the rendering buffer.
Results
We analyzed the performances of three different conventional methods and ours in rendering refraction effects for the same scenes. The proposed method obtains higher frame rate and physical accuracy comparing with other methods. It is used in several game scene, and realistic water refraction effects can be generated efficiently.
Conclusions
The two-phase water refraction method produces a tradeoff between efficiency and quality. It is easy to implementin modern game engines, and thus improve the quality of rendering scenes in video games or other real-time applications.}
}
@article{RONNOW202136,
title = {Fast analytical motion blur with transparency},
journal = {Computers & Graphics},
volume = {95},
pages = {36-46},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321000066},
author = {Mads J.L. Rønnow and Ulf Assarsson and Marco Fratarcangeli},
keywords = {Real-time rendering, Motion blur, Parallel computing},
abstract = {We introduce a practical parallel technique to achieve real-time motion blur for textured and semi-transparent triangles with high accuracy using modern commodity GPUs. In our approach, moving triangles are represented as prisms. Each prism is bounded by the initial and final position of the triangle during one animation frame and three bilinear patches on the sides. Each prism covers a number of pixels for a certain amount of time according to its trajectory on the screen. We efficiently find, store and sort the list of prisms covering each pixel including the amount of time the pixel is covered by each prism. This information, together with the color, texture, normal, and transparency of the pixel, is used to resolve its final color. We demonstrate the performance, scalability, and generality of our approach in a number of test scenarios, showing that it achieves a visual quality practically indistinguishable from the ground truth in a matter of just a few milliseconds, including rendering of textured and transparent objects. A supplementary video has been made available online.11Supplementary video available here}
}
@article{UDJAJA2018292,
title = {EKSPANPIXEL BLADSY STRANICA: Performance Efficiency Improvement of Making Front-End Website Using Computer Aided Software Engineering Tool},
journal = {Procedia Computer Science},
volume = {135},
pages = {292-301},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314662},
author = {Yogi Udjaja},
keywords = {Ekspanpixel Bladsy Stranica, Performance, Efficiency, Dynamic Canvas, Front-End, Website, Software Engineering, CASE Tool, Website Application, Time},
abstract = {The purpose of this research is to create a front-end website engine to improve the efficiency of front-end website creation called Expanpixel Bladsy Stranica (EBS). The method of making front-end website engine adopts computer aided software engineering (CASE) tool model, then to make it easier to access anywhere, it is made online (website-based), and evaluated by way of manual creation of front-end website and using EBS. After that the data obtained were analyzed using statistical formula. Results of increasing efficiency of front-end website creation performance that occurred on average by 83.60% of the overall developer.}
}
@article{FULGERI201971,
title = {Can adversarial networks hallucinate occluded people with a plausible aspect?},
journal = {Computer Vision and Image Understanding},
volume = {182},
pages = {71-80},
year = {2019},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2019.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S1077314219300438},
author = {Federico Fulgeri and Matteo Fabbri and Stefano Alletto and Simone Calderara and Rita Cucchiara},
keywords = {GAN, Attribute recognition, Occlusions},
abstract = {When you see a person in a crowd, occluded by other persons, you miss visual information that can be used to recognize, re-identify or simply classify him or her. You can imagine its appearance given your experience, nothing more. Similarly, AI solutions can try to hallucinate missing information with specific deep learning architectures, suitably trained with people with and without occlusions. The goal of this work is to generate a complete image of a person, given an occluded version in input, that should be a) without occlusion b) similar at pixel level to a completely visible people shape c) capable to conserve similar visual attributes (e.g. male/female) of the original one. For the purpose, we propose a new approach by integrating the state-of-the-art of neural network architectures, namely U-nets and GANs, as well as discriminative attribute classification nets, with an architecture specifically designed to de-occlude people shapes. The network is trained to optimize a Loss function which could take into account the aforementioned objectives. As well we propose two datasets for testing our solution: the first one, occluded RAP, created automatically by occluding real shapes of the RAP dataset created by Li et al. (2016) (which collects also attributes of the people aspect); the second is a large synthetic dataset, AiC, generated in computer graphics with data extracted from the GTA video game, that contains 3D data of occluded objects by construction. Results are impressive and outperform any other previous proposal. This result could be an initial step to many further researches to recognize people and their behavior in an open crowded world.}
}
@article{VIJAYALAKSHMI2023100011,
title = {Hybrid defense mechanism against malicious packet dropping attack for MANET using game theory},
journal = {Cyber Security and Applications},
volume = {1},
pages = {100011},
year = {2023},
issn = {2772-9184},
doi = {https://doi.org/10.1016/j.csa.2022.100011},
url = {https://www.sciencedirect.com/science/article/pii/S277291842200011X},
author = {S Vijayalakshmi and S Bose and G Logeswari and T Anitha},
keywords = {Ad hoc networks, Intrusion detection, Game theory, Game engine, Reactive game, Proactive game, Packet dropping attack, AODV},
abstract = {Ad hoc networks are a new perspective of wireless communication for versatile hosts. Security is a colossal worry for ad hoc networks, especially for those security-touchy applications. The huge highlights of ad hoc networks cause both difficulties and openings in accomplishing security objectives. One such aim is to consider the assaults from within the system by compromised nodes correspondingly as to consider harmful assaults propelled from outside the system. Designing an Intrusion Detection System (IDS) that suits the security needs and characteristics of ad hoc networks for viable and proficient performance against intrusions is one potential solution to vanquish vulnerabilities. This paper examines a genuine and hurtful attack called, “Malicious Packet Dropping Attack” in the network layer. To secure against this attack, a novel methodology utilizing game theory is proposed. The proposed system monitors the conduct of the neighbor nodes and conquers the demerits such as false positives present in traditional IDS, thereby providing secure correspondence between nodes that communicate with one another to course the traffic from source to destination. With the existence of malicious nodes, the proposed system has accomplished a 42% increase in the packet delivery ratio.}
}
@article{MIZUTANI2018308,
title = {Whole brain connectomic architecture to develop general artificial intelligence},
journal = {Procedia Computer Science},
volume = {123},
pages = {308-313},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300498},
author = {Haruo Mizutani and Michihiko Ueno and Naoya Arakawa and Hiroshi Yamakawa},
keywords = {connectome, general artificial intelligence, whole brain architecture, empirical neural circuits, efficient engineering},
abstract = {Whole Brain Connectomic Architecture (WBCA) is defined as a software architecture of the artificial intelligence (AI) computing platform which consists of empirical neural circuit information in the entire brain. It is constructed with the aim of developing a general-purpose biologically plausible AI to exert brain-like multiple cognitive functions and behaviors in a computational system. We have developed and implemented several functional machine learning modules, based on open mouse connectomic information, which correspond to specific brain regions. WBCA can accelerate efficient engineering development of the intelligent machines built on the architecture of the biological nervous system.}
}
@article{GONCALVES2023172,
title = {The role of different light settings on the perception of realism in virtual replicas in immersive Virtual Reality},
journal = {Computers & Graphics},
volume = {117},
pages = {172-182},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002583},
author = {Guilherme Gonçalves and Miguel Melo and Pedro Monteiro and Hugo Coelho and Maximino Bessa},
keywords = {Virtual reality, Realism, Lighting, Computer graphics, User study, User perception},
abstract = {Immersive Virtual Reality (IVR) provides a platform where the real world can be replicated to a point where users can act and react in the virtual world as they would in reality. However, rendering visual stimuli is computationally heavy. Thus, optimizations must be done to take advantage of computational systems by studying our perception of reality. This study investigated parameters related to light rendering (Global Illumination, Ambient Occlusion, Screen Space Reflections (SSR) and Direct Shadows) in real-time in a virtual replica of a real place using IVR. Participants experienced both virtual and real rooms with only one flashlight and changed the quality settings of the considered parameters so that their sense of reality would be the closest to the one they felt when they experienced the real room. Participants were given a budget to drive them to prioritize what parameters, and their level of quality, are the most important for their sense of reality. Results indicated that participants considered Global Illumination the most important factor, closely followed by Direct Shadows. Ambient Occlusion and Reflections (Screen Space Reflections) were the less prioritized parameters. We conclude that in a lighting setting where only dynamic lights are used, Global Illumination and Direct Shadows should be prioritized over SSR Reflections and Ambient Occlusion when computational power is limited.}
}
@article{ZHU2020454,
title = {Development of augmented reality serious games with a vibrotactile feedback jacket},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {5},
pages = {454-470},
year = {2020},
note = {VR/AR research and commercial applications in Singapore},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300735},
author = {Lingfei Zhu and Qi Cao and Yiyu Cai},
keywords = {Augmented reality, AR serious games, Vibrotactile feedback jacket, Game scenes},
abstract = {Background
In the past few years, augmented reality (AR) has rapidly advanced and has been applied in different fields. One of the successful AR applications is the immersive and interactive serious games, which can be used for education and learning purposes.
Methods
In this project, a prototype of an AR serious game is developed and demonstrated. Gamers utilize a head-mounted device and a vibrotactile feedback jacket to explore and interact with the AR serious game. Fourteen vibration actuators are embedded in the vibrotactile feedback jacket to generate immersive AR experience. These vibration actuators are triggered in accordance with the designed game scripts. Various vibration patterns and intensity levels are synthesized in different game scenes. This article presents the details of the entire software development of the AR serious game, including game scripts, game scenes with AR effects design, signal processing flow, behavior design, and communication configuration. Graphics computations are processed using the graphics processing unit in the system.
Results/Conclusions
The performance of the AR serious game prototype is evaluated and analyzed. The computation loads and resource utilization of normal game scenes and heavy computation scenes are compared. With 14 vibration actuators placed at different body positions, various vibration patterns and intensity levels can be generated by the vibrotactile feedback jacket, providing different real-world feedback. The prototype of this AR serious game can be valuable in building large-scale AR or virtual reality educational and entertainment games. Possible future improvements of the proposed prototype are also discussed in this article.}
}
@article{BEZGODOV20152729,
title = {The Framework for Rapid Graphics Application Development: The Multi-scale Problem Visualization},
journal = {Procedia Computer Science},
volume = {51},
pages = {2729-2733},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.406},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915012144},
author = {Alexey Bezgodov and Andrey Karsakov and Aleksandr Zagarskikh and Vladislav Karbovskii},
keywords = {Visualization, Multi-scale, GIS, Virtual reality, Software development},
abstract = {Interactive real-time visualization plays a significant role in simulation research domain. Multi-scale problems are in need of high performance visualization with good quality and the same could be said about other problem domains, e.g. big data analysis, physics simulation, etc. The state of the art shows that a universal tool for solving such problem is non-existent. Modern computer graphics requires enormous efforts to implement efficient algorithms on modern GPUs and GAPIs. In the first part of our paper we introduce a framework for rapid graphics application development and its extensions for multi-scale problem visualization. In the second part of the paper we provide a prototype of multi-scale problem's solution in simulation and monitoring of high-precision agent movements starting from behavioral patterns in an airport and up to world-wide flight traffic. Finally we summarize our results and speculate about future investigations.}
}
@article{CALLARI2024104206,
title = {“Braking bad”: The influence of haptic feedback and tram driver experience on emergency braking performance},
journal = {Applied Ergonomics},
volume = {116},
pages = {104206},
year = {2024},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104206},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023002442},
author = {Tiziana C. Callari and Louise Moody and Michael Mortimer and Hans Stefan and Ben Horan and Stewart Birrell},
keywords = {Tram, Haptic feedback, Road safety, Emergency braking, Streetcar},
abstract = {Trams are experiencing a resurgence with worldwide network expansion driven by the need for sustainable and efficient cities. Trams often operate in shared or mixed-traffic environments, which raise safety concerns, particularly in hazardous situations. This paper adopts an international, mixed-methods approach, conducted through two interconnected studies in Melbourne (Australia) and Birmingham (UK). The first study involved qualitative interviews, while the second was an experimental study involving a virtual reality (VR) simulator and haptic master controller (i.e., speed lever). In tram operations, master controllers play a critical role in ensuring a smooth ride, which directly influences passenger safety and comfort. The objective was to understand how a master control system, enhanced with additional haptic feedback, could improve tram driver braking performance and perceptions in safety-critical scenarios. Interview results indicate that the use of the emergency brake is considered the final or ultimate choice by drivers, and their driving experience is a moderating factor in limiting its application. Combined with the experimental results, this paper highlights how implementing haptic feedback within a master controller can reduce the performance disparity between novice and experienced tram drivers.}
}
@article{ANTUNES2022e00237,
title = {Virtual simulations of ancient sites inhabited by autonomous characters: Lessons from the development of Easy-population},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {26},
pages = {e00237},
year = {2022},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2022.e00237},
url = {https://www.sciencedirect.com/science/article/pii/S2212054822000261},
author = {Rui Filipe Antunes and Luís Correia},
keywords = {Artificial life, Intelligent tools for digital reconstruction, Story-telling and other forms of communication, ICT technologies In support of creating new cultural experiences or digital artifacts},
abstract = {In this paper, we report the learnings from the development of Easy-Population, a software tool for generation of autonomous populations in virtual simulations and its use in the creation of a 3D recreation of a historical site. In the process, we discuss the animation of virtual historical simulations inhabited by autonomous characters. We claim the incompleteness of these simulations in representing ancient places to conclude the necessity of further research emphasizing cultural and social representation in this type of simulation. This paper contributes with new insights that help us to identify main areas for future research in this domain of knowledge. First, we start by presenting a quick overview of the field of autonomous populations in virtual simulations of ancient sites. Then, we describe the authoring tool that we have developed to simplify the process of animating such simulations with virtual populations, developed as part of the EU funded project BIHC: Bio Inspired Human Crowds. This tool was used to create a Virtual Reality experience on the simulation of Xelb, the medieval city of Silves in the South of Portugal. Finally, we present and discuss the results of two inquiries produced next to an audience of field experts on the Cultural Heritage sector and the broad public. These provide us with key insights helping us in understanding what future research in this field should take into consideration, in order to expand the use of autonomous populations in the field of cultural heritage as useful instruments in the creation of educational tools.}
}
@article{DAS2022103538,
title = {Intrinsic image decomposition using physics-based cues and CNNs},
journal = {Computer Vision and Image Understanding},
volume = {223},
pages = {103538},
year = {2022},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2022.103538},
url = {https://www.sciencedirect.com/science/article/pii/S1077314222001163},
author = {Partha Das and Sezer Karaoglu and Theo Gevers},
keywords = {Computer vision, Physics based vision, Intrinsics image decomposition, Deep learning},
abstract = {Intrinsic image decomposition is the decomposition of an image into its reflectance and shading components. The intrinsic image decomposition problem is inherently ill-posed, since there can be multiple solutions to compute the intrinsic components forming the same image. In this paper, we explore the use of physics-based priors. We also propose a new architecture that separates the learning components in a stacked manner. We explore various ways of integrating such priors into a deep learning system. Our method is trained and tested on a large synthetic garden dataset to assess its performance. It is evaluated and compared to state-of-the-art methods using two standard intrinsic datasets. Finally, the pre-trained network is tested on real world images to show the generalisation capabilities of the network.}
}
@article{THEES2020106316,
title = {Effects of augmented reality on learning and cognitive load in university physics laboratory courses},
journal = {Computers in Human Behavior},
volume = {108},
pages = {106316},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106316},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220300704},
author = {Michael Thees and Sebastian Kapp and Martin P. Strzys and Fabian Beil and Paul Lukowicz and Jochen Kuhn},
keywords = {Augmented reality, Smartglasses, Cognitive load, Split-attention effect, STEM laboratory courses, Physics laboratory courses},
abstract = {Recent studies emphasize a positive impact of learning with augmented reality (AR) systems in various instructional scenarios. Especially combining real and virtual learning components according to spatial and temporal contiguity principles is claimed to foster learning and to reduce extraneous cognitive processing. We applied these principles to a physics laboratory experiment examining heat conduction where students measure the temperature along heated metal rods via a thermal imaging camera. However, the traditional setup leads to a time delay between measuring and receiving data, and spatially separates relevant visualizations causing resource-consuming search processes. Using see-through smartglasses, traditional displays were transformed into virtual representations which were anchored to corresponding objects of the experimental setup, resulting in an integrated AR view of real-time data. Both traditional and AR-assisted workflows of data collection were investigated in a field study with undergraduate students (N=74) during a graded laboratory course. Performance and cognitive load were assessed as dependent variables. Although the AR condition did not show a learning gain in a conceptual knowledge test, they nonetheless reported a significant lower extraneous cognitive load than the traditional condition. These results contrast with recent findings on AR and integrated formats but reveal a significant impact on cognitive load research.}
}
@article{SZABO20241258,
title = {High performance GPU graphics API abstraction layer in C# for real-time graphics},
journal = {Procedia Computer Science},
volume = {235},
pages = {1258-1267},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924007956},
author = {Dávid Szabó and Dr. Zoltán Illés},
keywords = {C#, OpenGL, Vulkan, graphics, rendering, real-time},
abstract = {Real-Time rendering is the technique which allows us to have graphical applications in our everyday life, whether it is a 3D game or a tool with graphical user interface. Nowadays graphics rendering is handled by the GPU (Graphics Processing Unit) in our device. There are many layers of abstraction above the programming of GPUs through libraries and graphics engines, though the most low-level way of accessing a GPU in user-mode applications is using a Graphics API. Due to the need of high performance and low-level capabilities usually these APIs are used from C or C++, but we realized the need to utilize these APIs in higher level languages as well. In our approach we're using the .NET C# language for developing multi-platform real-time graphical applications instead of the C or C++ languages. Using the modern .NET environment, we're able to use Graphics APIs for rendering onto common .NET UI Frameworks while consuming all our previously implemented C# libraries and .NET technologies in the same application. To maintain compatibility with multiple platforms we're developing a library system allowing the use different Graphics APIs from the same C# source-code. The library system contains a Graphics API abstraction layer with multiple Graphics API implementations of this layer in C# and a C# to shader language compiler for cross-API shader development in C#. In this paper, we're proposing our considerations for implementing a library to be able to use the Vulkan and OpenGL APIs through a single C# codebase. We provide solutions for multi-platform rendering and dealing with the low-level challenges of using the two deeply different APIs, while maintaining performance capable to do real-time rendering.}
}
@article{EHRHARDT201914,
title = {Taking visual motion prediction to new heightfields},
journal = {Computer Vision and Image Understanding},
volume = {181},
pages = {14-25},
year = {2019},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2019.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1077314219300207},
author = {Sebastien Ehrhardt and Aron Monszpart and Niloy J. Mitra and Andrea Vedaldi},
abstract = {While the basic laws of Newtonian mechanics are well understood, explaining a physical scenario still requires manually modeling the problem with suitable equations and estimating the associated parameters. In order to be able to leverage the approximation capabilities of artificial intelligence techniques in such physics related contexts, researchers have handcrafted relevant states, and then used neural networks to learn the state transitions using simulation runs as training data. Unfortunately, such approaches are unsuited for modeling complex real-world scenarios, where manually authoring relevant state spaces tend to be tedious and challenging. In this work, we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data while internally modeling non-homogeneous environment and in the process enable long-term physical extrapolation. We develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates. We evaluate our setup, to extrapolate motion of rolling ball(s) on bowls of varying shape and orientation, and on arbitrary heightfields using only images as input. We report significant improvements over existing image-based methods both in terms of accuracy of predictions and complexity of scenarios; and report competitive performance with approaches that, unlike us, assume access to internal physical states.}
}
@article{MONTES2022103756,
title = {A computational model of Ostrom's Institutional Analysis and Development framework},
journal = {Artificial Intelligence},
volume = {311},
pages = {103756},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103756},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000960},
author = {Nieves Montes and Nardine Osman and Carles Sierra},
keywords = {Institutional Analysis and Development framework, Rules, Normative multiagent systems, Game theory, Logic programming},
abstract = {The Institutional Analysis and Development (IAD) framework developed by Elinor Ostrom and colleagues provides great conceptual clarity on the immensely varied topic of social interactions. In this work, we propose a computational model to examine the impact that any of the variables outlined in the IAD framework has on the resulting social interactions. Of particular interest are the rules adopted by a community of agents, as they are the variables most susceptible to change in the short term. To provide systematic descriptions of social interactions, we define the Action Situation Language (ASL) and provide a game engine capable of automatically generating formal game-theoretical models out of ASL descriptions. Then, by incorporating any agent decision-making models, the connection from a rule configuration description to the outcomes encouraged by it is complete. Overall, our model enables any community of agents to perform what-if analysis, where they can foresee and examine the impact that a set of regulations will have on the social interaction they are engaging in. Hence, they can decide whether their implementation is desirable.}
}
@article{ZAGARSKIKH20152928,
title = {Efficient Visualization of Urban Simulation Data Using Modern GPUs},
journal = {Procedia Computer Science},
volume = {51},
pages = {2928-2932},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.481},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915012892},
author = {Aleksandr Zagarskikh and Andrey Karsakov and Alexey Bezgodov},
keywords = {GIS, Visualization, City science, Virtual globe, GPU},
abstract = {Visualization of simulation results in major urban areas is a difficult task. Multi-scale processes and connectivity of the urban environment may require interactive visualization of dynamic scenes with lots of objects at different scales. To visualize these scenes it is not always possible to use standard GIS systems. Wide distribution of high-performance gaming graphics cards has led to the emergence of specialized frameworks, which are able to cope with such kinds of visualization. This paper presents a framework and special algorithms that take full advantage of the GPU to render the urban simulation data over a virtual globe. The experiments on a scalability of the framework have showed that the framework is successfully deals with the visualization of up to two million moving agents and up to eight million of fixed points of interest on top of the virtual globe without detriment to smoothness of the image.}
}
@article{KATKURI2024100361,
title = {Autonomous UAV Navigation using Deep Learning-Based Computer Vision Frameworks: A Systematic Literature Review},
journal = {Array},
pages = {100361},
year = {2024},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2024.100361},
url = {https://www.sciencedirect.com/science/article/pii/S2590005624000274},
author = {Aditya Vardhan Reddy Katkuri and Hakka Madan and Narendra Khatri and Antar Shaddad Hamed Abdul-Qawy and K. Shridhar Patnaik},
keywords = {Autonomous UAV, Deep Learning, Computer Vision, Systematic Literature Review, UAV Applications, You Only Look Once (YOLO) Framework},
abstract = {The increasing use of unmanned aerial vehicles (UAVs) in both military and civilian applications, such as infrastructure inspection, package delivery, and recreational activities, underscores the importance of enhancing their autonomous functionalities. Artificial intelligence, particularly deep learning-based computer vision (DL-based CV), plays a crucial role in this enhancement. This paper aims to provide a systematic literature review (SLR) of Scopus-indexed research studies published from 2019 to 2023, focusing on DL-based CV approaches for autonomous UAV applications. By analyzing 173 studies, we categorize the research into four domains: sensing and inspection, landing, surveillance and tracking, and search and rescue. Our review reveals a significant increase in research utilizing computer vision for UAV applications, with over 39.5% of studies employing the You Only Look Once (YOLO) framework. We discuss the key findings, including the dominant trends, challenges, and opportunities in the field, and highlight emerging technologies such as in-sensor computing. This review provides valuable insights into the current state and future directions of DL-based CV for autonomous UAVs, emphasizing its growing significance as legislative frameworks evolve to support these technologies.}
}
@article{ULLMANN2025100832,
title = {SyDRA: An approach to understand game engine architecture},
journal = {Entertainment Computing},
volume = {52},
pages = {100832},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100832},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002003},
author = {Gabriel C. Ullmann and Yann-Gaël Guéhéneuc and Fabio Petrillo and Nicolas Anquetil and Cristiano Politowski},
keywords = {Game engines, Coupling, Impact analysis, Controlled experiment},
abstract = {Game engines are tools to facilitate video game development. They provide graphics, sound, and physics simulation features, which would have to be otherwise implemented by developers. Even though essential for modern commercial video game development, game engines are complex and developers often struggle to understand their architecture, leading to maintainability and evolution issues that negatively affect video game productions. In this paper, we present the Subsystem-Dependency Recovery Approach (SyDRA), which helps game engine developers understand game engine architecture and therefore make informed game engine development choices. By applying this approach to 10 open-source game engines, we obtain architectural models that can be used to compare game engine architectures and identify and solve issues of excessive coupling and folder nesting. Through a controlled experiment, we show that the inspection of the architectural models derived from SyDRA enables developers to complete tasks related to architectural understanding and impact analysis in less time and with higher correctness than without these models.}
}
@article{CONESA2023106257,
title = {A multi-agent framework for collaborative geometric modeling in virtual environments},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106257},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106257},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623004414},
author = {J. Conesa and F.J. Mula and M. Contero and J.D. Camba},
keywords = {Agent-based system, Collaborative systems, Virtual reality},
abstract = {The use of collaborative applications in which several individuals interact to solve a problem is an important strategy both in educational settings as well as professional environments. Virtual Reality (VR) technology, in particular, has acquired a predominant role in many industries. However, as the level of immersion and realism of the VR experience increases, so does the demand for computational resources, as rendering processes become increasingly intensive and can negatively affect other communication processes that are critical in collaborative environments. In this paper, we present a software framework based on multi-agent systems that enables the separation of rendering processes from internal data management tasks and communication processes between users, which are prevalent in collaborative virtual reality applications. The results of our validation studies show that, compared to other techniques based on protocol or network enhancements, the proposed architecture can significantly improve collaborative processes in general, and virtual reality-based applications in particular.}
}
@article{FLOTYNSKI2021766,
title = {Knowledge-Based Management of Virtual Training Scenarios},
journal = {Procedia Computer Science},
volume = {192},
pages = {766-775},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.079},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015672},
author = {Jakub Flotyński and Krzysztof Walczak and Paweł Sobociński and Adam Gałązkiewicz},
keywords = {semantic web, knowledge representation, ontologies, training, virtual reality, 3D content},
abstract = {Virtual reality (VR) gains increasing attention as a method of implementing training systems in different domains, in particular, when real training is potentially dangerous for the trainees or the environment, or requires expensive equipment. The essential element of professional training is domain-specific knowledge, which can be represented using the semantic web approach. It enables reasoning as well as complex queries against the representation of training scenarios, which can be valuable for teaching purposes. However, the available methods and tools for creating VR training systems do not use semantic knowledge representation. Currently, the creation, modification, and management of training scenarios require skills in programming and computer graphics. Hence, they are unavailable to domain experts without expertise in IT. In this paper, we propose an ontology-based representation and a method of modeling VR training scenarios. In our approach, trainees’ activities, potential mistakes as well as equipment and its possible errors are represented using domain knowledge understandable to domain experts. We illustrate the approach by modeling VR training scenarios for electrical operators of high-voltage installations.}
}
@article{KARSAKOV2015730,
title = {Improving Visualization Courses in Russian Higher Education in Computational Science and High Performance Computing},
journal = {Procedia Computer Science},
volume = {66},
pages = {730-739},
year = {2015},
note = {4th International Young Scientist Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.11.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915034328},
author = {Andrey Karsakov and Anna Bilyatdinova and Alexey Bezgodov},
keywords = {Visualization course, Computational science, Higher education},
abstract = {In order to keep up with the fast-paced and widespread technologies and applications of visualization, worldwide education community is actively implementing visualization courses in curricula of undergraduate and graduate programs. A study of the state of the art in the teaching visualization in Russian higher education shows the necessity to improve the quality and breadth of knowledge of the visualization courses. In this paper we propose our approach to overcome the national and historical challenges in teaching visualization in Russian STEM higher education on the example of Computational Science and High Performance Computing double degree Master's programs in ITMO University. We offer a smooth transition to the modern relevant syllabus content by presenting two courses’ designs with same width but with various depth in knowledge that should to be studied. At the end of the paper we give some discussions about future works in development visualization courses in Russia.}
}
@article{ROMAN2024460,
title = {The Use of Augmented Reality as a University Teaching Strategy in Health Sciences Programs: A Scoping Review},
journal = {Procedia Computer Science},
volume = {238},
pages = {460-467},
year = {2024},
note = {The 15th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) / The 7th International Conference on Emerging Data and Industry 4.0 (EDI40), April 23-25, 2024, Hasselt University, Belgium},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924012845},
author = {Fabian Roman and Karina Lastre Meza and Diva Mendoza and Sonia Rodriguez Cano},
keywords = {Augmented Reality, Higher Education, Health, Education Technology, Teaching},
abstract = {The purpose of this systematic review of the literature is to identify, analyze and synthesize the findings found in the last 10 years regarding the issue of AR in academic health programs and to know the impact it has on students’ learning. At the methodological level, the databases Scopus, PubMed, Web of Science and Science Direct were consulted, inclusion and exclusion criteria were established for the selection of the most relevant articles, methodological quality and relevance were analyzed. As a result, 16 articles suggest that AR has a positive impact on health disciplines, by fostering greater interactivity, motivation, understanding of concepts, acquisition of skills, retention of knowledge and personalization of learning. The systematic review concludes that further scientific evidence is needed to determine the effectiveness of AR in learning and its impact on the development of competences and learning outcomes in current education.}
}
@article{AUNG2023200295,
title = {Development of a novel robot-assisted vocabulary learning system using pure synthetic data},
journal = {Intelligent Systems with Applications},
volume = {20},
pages = {200295},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200295},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323001205},
author = {Zaw Htet Aung and Chuenchat Songsaksuppachok and Potjanee Kanchanapiboon and Panrasee Ritthipravat},
keywords = {Synthetic data generation, Synthetic-to-real transfer, Object detection, Robot assisted language learning, Human-robot interaction},
abstract = {This study investigates the application of deep learning methods trained on synthetic data for the robust detection of vocabulary flashcards, an essential aspect of robot-assisted language learning (RALL). Despite its importance, flashcard detection has not been extensively researched in RALL systems, and it poses significant challenges due to the extensive data collection and annotation needed, especially given the large number of classes involved. These models need to generalize well to different real-world environments. To address these issues, a novel robotic platform designed for flashcard-based vocabulary learning is proposed, supported by a synthetic data generation pipeline using high dynamic range images (HDRIs) and synthetic human actors. The proposed pipeline offers an efficient data generation method and significantly enhances model generalisability to various environments. The proposed method was evaluated with five object detection models based on several challenging real-world datasets, each containing more than 200 class labels. The object detection models trained based on the proposed synthetic datasets (HDRI and HDRI+Humans) demonstrated outstanding performance, achieving median mean average precision (mAP) scores of 0.797 and 0.778. The proposed method achieves this performance without needing to be trained with real data. A comprehensive analysis comparing the proposed method with other synthetic data generation techniques is presented, and its potential for improving vocabulary flashcard detection in RALL systems is highlighted. The data and models are available at https://github.com/aimlab-mu/aimrobot.}
}
@article{NICHOLAS2023107774,
title = {Sideffect GamePlan: Development of an alcohol and other drug serious game for high school students using a systematic and iterative user-centred game development framework},
journal = {Computers in Human Behavior},
volume = {145},
pages = {107774},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107774},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223001255},
author = {Joanna Nicholas and Brennen Mills and Sara Hansen and Stephen J. Bright and Joseph Scott and Imogen Ridout and Jess Watson and Heather Boyd and Luke Brook and Luke Hopper},
keywords = {Secondary education, Alcohol and other drugs, Health education, Serious game design, Harm reduction, Games},
abstract = {Serious games have shown to be effective in improving motivation to learn, knowledge and retention, thus are being increasingly used for alcohol and other drug (AOD) education. This paper outlines the development of an online AOD serious game for in-class use by Australian secondary school teachers for students in Years 9–10. Adapted from Edwards et al. (2018), the seven-step systematic and iterative user-centred development framework included: (1) Forming an expert multidisciplinary design team, (2) Defining the problem and establishing user preferences, (3) Incorporating the evidence base, (4) Serious game design, (5) Incorporating behavioural and psychological theory, (6) Developing a logic model and investigating causal pathways, and (7) User testing. High school students (n = 8), health and physical education teachers (n = 7), and parents (n = 8) were engaged throughout different stages of the development process to inform development and provide feedback on considerations for promoting engagement, acceptability, and usability of the game amongst both students and teachers. Overall, participants rated game acceptability and usability favourably and would recommend the game for learning about AOD. Constructive feedback and suggestions for improvements from user testing sessions were implemented to form the final version of the game and module. The next step is to test Sideffect GamePlan in a simulated classroom environment before piloting in school settings.}
}
@article{ALENE2024106063,
title = {Virtual reality visualization of geophysical flows: A framework},
journal = {Environmental Modelling & Software},
volume = {177},
pages = {106063},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106063},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224001245},
author = {Gebray H. Alene and Shafaq Irshad and Adina Moraru and Ivan Depina and Oddbjørn Bruland and Andrew Perkis and Vikas Thakur},
keywords = {Geophysical flows, Framework, Natural hazards, Numerical modeling, Virtual reality, Visualization},
abstract = {This paper presents a comprehensive Virtual Reality (VR) based framework for visualizing numerical simulations of geophysical flows in a realistic and immersive manner. The framework allows integrating output data from various mesh-based Eulerian numerical models into a VR environment, enabling users to interact with and explore the terrain and geophysical flows through the VR experience. Three case studies, including a snow avalanche, quick clay landslide, and flash flood in Norway, demonstrate its versatility. The VR environment offers intuitive menus and user interactions, allowing users to read flow depth and velocity values, facilitating a direct link between numerical data and their visual representation. This framework can reshape geophysical flow hazard identification and disaster management by integrating physics-based numerical modeling results into VR Environments, thus enhancing knowledge dissemination among experts, the general public, non-expert stakeholders, and policymakers. The paper also highlights challenges and opportunities identified during the integration, guiding future developments.}
}
@article{NEKTARIOSBOLIERAKIS2023201,
title = {Training on LSA lifeboat operation using Mixed Reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {3},
pages = {201-212},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000128},
author = {Spyridon {Nektarios Bolierakis} and Margarita Kostovasili and Lazaros Karagiannidis and Angelos Amditis},
keywords = {Augmented Reality, Mixed Reality, Maritime training, Cruise industry, Lifeboat operation, Lifeboat maintenance, H2020 research project},
abstract = {Background
This work aims to provide an overview of the Mixed Reality (MR) technology's use in maritime industry for training purposes. Current training procedures cover a broad range of procedural operations for Life-Saving Appliances (LSA) lifeboats; however, several gaps and limitations have been identified related to the practical training that can be addressed through the use of MR. Augmented, Virtual and Mixed Reality applications are already used in various fields in maritime industry, but their full potential have not been yet exploited. SafePASS project aims to exploit MR advantages in the maritime training by introducing a relevant application focusing on use and maintenance of LSA lifeboats.
Methods
An MR Training application is proposed supporting the training of crew members in equipment usage and operation, as well as in maintenance activities and procedures. The application consists of the training tool that trains crew members on handling lifeboats, the training evaluation tool that allows trainers to assess the performance of trainees, and the maintenance tool that supports crew members to perform maintenance activities and procedures on lifeboats. For each tool, an indicative session and scenario workflow are implemented, along with the main supported interactions of the trainee with the equipment.
Results
The application has been tested and validated both in lab environment and using a real LSA lifeboat, resulting to improved experience for the users that provided feedback and recommendations for further development. The application has also been demonstrated onboard a cruise ship, showcasing the supported functionalities to relevant stakeholders that recognized the added value of the application and suggested potential future exploitation areas.
Conclusions
The MR Training application has been evaluated as very promising in providing a user-friendly training environment that can support crew members in LSA lifeboat operation and maintenance, while it is still subject to improvement and further expansion.}
}
@article{BOONBRAHM201512,
title = {Realistic Simulation in Virtual Fitting Room Using Physical Properties of Fabrics},
journal = {Procedia Computer Science},
volume = {75},
pages = {12-16},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036509},
author = {Poonpong Boonbrahm and Charlee Kaewrat and Salin Boonbrahm},
keywords = {Virtual fitting room, 3D simulation, Physical propoerties of fabrics},
abstract = {Virtual Dressing Room (VDR) is the popular topics during the past 10 years and it seemed to have the bright future when Microsoft's Kinect tracking system appeared in the market. The VDR is not only helping the online customer making decision on buying apparels but also helping customers in department store to save time in waiting for the fitting room. In facts, there are few parts of VDR that make virtual fitting more realistic and tracking the movement of the body is only one of them. The other parts are dealing with the clothing and how they interact with the body even when the customer moves around. The virtual fitting rooms available in the market today and on the website used 2D dresses putting in front of the body. These dresses did not fit on the body and did not interact with the body except they were appeared on the front part of the body. In order to make the simulation process of the virtual fitting room more realistic, 3D virtual dress will be used along with the physical interaction of the fabrics and the environment. In this research, we have to define the shape of the 3D dress and the physical properties of the fabrics. Since the dress did not come as one piece of fabrics but composed of many pieces with different textures and physical properties, for example, some parts is thicker than others or made with different materials, so all these issues have to be taken into account. In this research, the interaction occurred between fabric and the environment were calculated and applied. This covered all of the interactions occurred on the fabrics such as stretching, bending, damping, accelerating, colliding and gravity pulling. The simulation results can tell the difference among customers wearing jean, satin, silk or cotton. In this research, Unity 3D game engine was used for simulation process along with Maya for model creation and Microsoft's Kinect2 was used for tracking the dress and the body.}
}
@article{FERREIRA20242521,
title = {Digital twinning for smart restoration of classic cars},
journal = {Procedia Computer Science},
volume = {232},
pages = {2521-2530},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.070},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002473},
author = {Frederico Ferreira and Vasco Amaral and Fernando {Brito e Abreu}},
keywords = {digital twin, classic cars, bodywork restoration, digital transformation, industry 4.0, IoT},
abstract = {Classic cars hold substantial value in the automotive industry, and the restoration process plays a pivotal role in increasing their worth. Ensuring the certification of these restoration processes is vital, as is keeping owners well-informed about the ongoing procedures taking place in their appreciated vehicles. By monitoring and controlling these restoration activities, the management of classic car workshops can effectively optimize their operations, empower owners with pertinent information, and preserve the authentic nature of these vintage automobiles. This study aims to develop a digital twin system for a classic car bodywork restoration shop workshop. The latter integrates multiple sensor technologies, including location tracking, humidity and temperature sensing, accelerometer monitoring, and smart plugs, to facilitate the identification of ongoing activities of classic car bodywork restoration process instances. By leveraging these sensors, the digital twin system may simulate and control workshop operations more effectively. We perform a systematic rapid review of related work and, based on state-of-the-art practices, we identify existing architectures and software applications used for creating digital twin systems. Then, we propose the architecture for our digital twin system, detailing its functionalities. We aim at contributing to advancing digital twin technology in classic car bodywork restoration, enhancing its authenticity, and fostering improved management practices, and overall experience for classic car owners.}
}
@article{WANG2024200115,
title = {A pose generation model for animated characters based on DCNN and PFNN},
journal = {Systems and Soft Computing},
volume = {6},
pages = {200115},
year = {2024},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2024.200115},
url = {https://www.sciencedirect.com/science/article/pii/S2772941924000449},
author = {Boli Wang},
keywords = {DCNN, ResNet, 3D pose estimation, Animation, Pose generation, Phase function neural network},
abstract = {In the current field of animation and gaming, the action collection cost for 3D animated character generation is high, and the accuracy of action recognition is poor. Therefore, to reduce the cost of generating 3D animated characters and improve the similarity between animated characters and real humans, a 3D action recognition and animated character generation model based on ResNet and phase function neural network is proposed. The experiment outcomes denote that the raised model begins to converge at 50 iterations, with a minimum loss value of 0.13. The convergence speed and loss value are better than other models. In human pose classification, the raised algorithm has the highest accuracy of 99.46 % and an average accuracy of 99.13 %. The highest classification precision and average precision are 97.79 % and 97.33 %, respectively. In terms of human pose orientation classification, the average accuracy and precision of the raised algorithm are 98.09 % and 97.41 %, respectively, which are also higher than other models. In addition, the mean per joint position error of the proposed algorithm is the highest at 80.1 mm and the lowest at 79.3 mm, respectively. The average recognition time for each image is only 46.8 ms, which is lower than other algorithms. In addition, the average update times of the algorithm and the Unreal Engine are 39.28 ms and 27.52 ms, respectively, and both run at different frame rates. The above results indicate that the proposed 3D human pose recognition and animated character generation model based on ResNet and phase function neural network can not only improve the accuracy of pose recognition, but also improve recognition speed, effectively reducing the cost of 3D animated character generation. The animation character generation method includes data collection and the application after data collection, which shows the various roles that deep learning technology can play in the field of computer graphics animation, and also provides excellent solutions for other computer graphics problems.}
}
@article{KIRYUKHIN2020192,
title = {Development of a virtual analogue of uranium-graphite subcritical assembly and visualization of the neutron flux distribution in virtual reality},
journal = {Procedia Computer Science},
volume = {169},
pages = {192-197},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.135},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302581},
author = {Pavel Kiryukhin and Alexander Shcherbakov and Vladislav Romanenko and Pavel Pugachev and Dmitriy Khomyakov and Georgy Tikhomirov and Egor Zadeba},
keywords = {Virtual reality, Nuclear reactor physics, Simulation, Education},
abstract = {The article describes the new software product developed at MEPhI. It represents a virtual reality simulation of an experiment on a subcritical uranium-graphite assembly. This practical work plays an important role in the training of young specialists studying the physics of nuclear reactors. However not all students have access to real experimental facilities, this fact makes it necessary to complement real experiment with simulation in virtual reality that allows to accurately reproduce the actions that the student performs during the real practical work. This approach let to increase the efficiency of the educational process and even expand the capabilities of real experimental assembly by visualizing physical processes during its operation.}
}
@article{POGLITSCH2024103942,
title = {XR technologies to enhance the emotional skills of people with autism spectrum disorder: A systematic review},
journal = {Computers & Graphics},
volume = {121},
pages = {103942},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103942},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000773},
author = {Christian Poglitsch and Saeed Safikhani and Erin List and Johanna Pirker},
keywords = {Extended reality (XR), Virtual reality (VR), Augmented reality (AR), Autism spectrum disorder, Emotional skills, Emotion recognition},
abstract = {In this paper, we present a systematic review of the applications of (1) Extended Reality (XR), (2) Augmented Reality (AR), and (3) Virtual Reality (VR) technologies to enhance emotion recognition and emotion expression in people with Autism Spectrum Disorder (ASD). ASD can affect various abilities, and poses challenges to the recognition of emotions in others, which is often referred to as “social blindness”. Treating this condition typically requires intensive one-on-one or small-group therapy sessions, which can be costly and limited in terms of availability. With the growing number of diagnoses of ASD, concerns have risen regarding a potential “lost generation” that may face difficulties in fulfilling its potential. Through this comprehensive review, we aim to provide an overview of innovative approaches that use XR technologies to improve the learning experience of individuals with ASD.}
}
@article{NISHIKAWA20224055,
title = {Analysis of cerebral blood flow state during a mental rotation task to assess spatial perception ability},
journal = {Procedia Computer Science},
volume = {207},
pages = {4055-4064},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.468},
url = {https://www.sciencedirect.com/science/article/pii/S187705092201362X},
author = {Reina Nishikawa and Hirokazu Miura and Hirokazu Takib},
keywords = {Near-infrared spectroscopy, mental rotation, spatial perception ability},
abstract = {Spatial recognition is the ability to quickly and accurately grasp the position, posture, direction, size, shape, spacing, speed, etc. of objects in three-dimensional space, and is said to be established through the cooperation of multiple sensory organs such as vision and hearing, and controlled by the right brain. Currently, paper tests such as MRT and MCT are the most common methods for measuring spatial recognition ability. Although measurement methods and learning systems using AR and VR have also been proposed, objective evaluation indices have yet to be established. Therefore, it is considered necessary to measure and analyze the state of brain activity during spatial recognition in order to objectively evaluate spatial recognition. Group comparisons based on task performance showed a significant increase in oxygenated hemoglobin in the higher group compared to the lower group in both score and clear time. In addition, a comparison between the right and left brains showed that the right brain tended to have a higher percentage of increased oxygenated hemoglobin, which is consistent with the view that spatial recognition ability is significantly right-brained. These results suggest that the mental rotation task is closely related to prefrontal regions and that changes in cerebral blood flow in the right brain are reflected in spatial recognition ability.}
}
@article{FUKUDA2019179,
title = {An indoor thermal environment design system for renovation using augmented reality},
journal = {Journal of Computational Design and Engineering},
volume = {6},
number = {2},
pages = {179-188},
year = {2019},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S228843001830068X},
author = {Tomohiro Fukuda and Kazuki Yokoi and Nobuyoshi Yabuki and Ali Motamedi},
keywords = {Environmental design, Indoor thermal environment, Intuitive visualization, Interactive environment, Computational Fluid Dynamics (CFD), Augmented Reality (AR)},
abstract = {The renovation projects of buildings and living spaces, which aim to improve the thermal environment, are gaining importance because of energy saving effects and occupants' health considerations. However, the indoor thermal design is not usually performed in a very efficient manner by stakeholders, due to the limitations of a sequential waterfall design process model, and due to the difficulty in comprehending the CFD simulation results for stakeholders. On the other hand, indoor greenery has been introduced to buildings as a method for adjusting the thermal condition. Creating a VR environment, which can realistically and intuitively visualize a thermal simulation model is very time consuming and the resulting VR environment created by 3D computer graphics objects is disconnected from the reality and does not allow design stakeholders to experience the feelings of the real world. Therefore, the objective of this research is to develop a new AR-based methodology for intuitively visualizing indoor thermal environment for building renovation projects. In our proposed system, easy-to-comprehend visualization of CFD results augment the real scenes to provide users with information about thermal effects of their renovation design alternatives interactively. Case studies to assess the effect of indoor greenery alternatives on the thermal environment are performed. In conclusion, integrating CFD and AR provides users with a more natural feeling of the future thermal environment. The proposed method was evaluated feasible and effective.}
}
@article{BUROVA2022103663,
title = {Asynchronous industrial collaboration: How virtual reality and virtual tools aid the process of maintenance method development and documentation creation},
journal = {Computers in Industry},
volume = {140},
pages = {103663},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103663},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000604},
author = {Alisa Burova and John Mäkelä and Hanna Heinonen and Paulina Becerril Palma and Jaakko Hakulinen and Viveka Opas and Sanni Siltanen and Roope Raisamo and Markku Turunen},
keywords = {Virtual reality, Industrial collaboration, Industrial maintenance, Technical documentation, Virtual tools, Asynchronous collaboration, Collaborative virtual reality, Maintenance method development},
abstract = {In the light of Industry 4.0, the field of Industrial Maintenance faces a large digital transformation, adopting Extended Reality (XR) technologies to aid industrial operations. For the manufacturing corporations that provide maintenance services, the efficiency of industrial maintenance plays a crucial role in the competitiveness and is tightly related to the technical documentation supporting maintenance. However, the process of documentation creation faces several challenges due to lack of access to the physical equipment and difficulties in remote communication between globally distributed departments. To address these shortcomings, this research investigates the utilization of Virtual Reality (VR) to facilitate asynchronous collaboration of globally dispersed departments involved in the pipeline of maintenance method and documentation creation. The presented proof-of-concept (the COVE-VR platform) has been developed as an academia-industry collaboration and evaluated iteratively with subject matter experts. The proposed VR platform consists of two virtual environments and eight virtual tools, which allow interaction with virtual prototypes (3D CAD models) and means of digital content creation. Our findings show the high relevance of the developed solution for the needs of industrial departments and the ability to support asynchronous collaboration among them. This article delivers qualitative findings on the value of VR technology and presents guidelines on how to develop virtual tools for digital content creation within VR, adaptable to other industrial contexts. We suggest providing embedded guidance and design consistency to ensure smooth interactions with virtual tools and further discuss the importance of proper positioning, the transparency of operations and the information property of generated content.}
}
@article{JOHRI2024100213,
title = {Crafting the techno-functional blocks for Metaverse - A review and research agenda},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {1},
pages = {100213},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000028},
author = {Amar Johri and Anu Sayal and Chaithra N and Janhvi Jha and Navya Aggarwal and Darshan Pawar and Veethika Gupta and Ashulekha Gupta},
keywords = {Artificial intelligence, Augmented reality, Blockchain, Metaverse, Virtual reality},
abstract = {The "Metaverse," a term popularized by Neal Stephenson's novel Snow Crash, has been discussed in the science fiction community for decades, but technological advancements have only recently made it a reality. The Metaverse is an all-encompassing, interconnected virtual environment where users can freely communicate with one another and digital content. This article examines how various technologies, primarily Virtual Reality (VR) and Augmented Reality (AR), have contributed to the development of the Metaverse (AR). These innovations have revolutionized the way we interact with digital media by enabling us to have genuine, realistic experiences. In addition, we examine the Metaverse technologies that make it possible to construct a fully realized, functional virtual world. Among these are recent advances in artificial intelligence (AI), cryptocurrencies, spatial and peripheral computing, and other fields. Our research investigates the advantages and disadvantages of these technologies, as well as how they may influence the future of the Metaverse. Furthermore, the article explores the darker aspects of the Metaverse, particularly the emergence of the "dark verse," which underscores the potential for organized illicit activities within the Internet due to insufficient oversight and governance of the Metaverse.}
}
@article{GONCALVES20231333,
title = {Cognitive Rehabilitation: A Comparison Model of a Digital Environment based on Serious Games and the Traditional Methods},
journal = {Procedia Computer Science},
volume = {219},
pages = {1333-1340},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.418},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923004271},
author = {Helena Isabel Marques Gonçalves and Firmino Oliveira {da Silva}},
keywords = {Serious Games, Health, Neuro-Cognitive Rehabilitation, Traditional Methods},
abstract = {Technological innovation contributes to a personalised and integrated approach in education, journalism, communication, management, marketing, and above all, in healthcare. Facing the increasing number of new patients and the shortage of health specialists, the emergence of innovative technologies has brought the fields of health and technology closer together. This research aims to study digital environments in the context of neurocognitive rehabilitation, for the care of patients with deficits in cognitive functions, supported by Serious Games (SG). Typically, neurological patients suffer from cognitive deficits in executive, visuospatial, attention and/or memory functions. In this context, SG prefigure an appropriated tool that combines healthcare and rehabilitation, which allows the connection of healthcare specialists with a platform, as the connection of patients with life and disease companions with potential for collaboration and recording the evolution of the rehabilitation process. Aiming to identify the characteristics and requirements of SG in exposing neurological patients to a safe technological environment that simulates rehabilitation activities supported by SG (developed and oriented to their specificities), but also to make a comparison with traditional methods (TM). A model of comparison between the two paradigms was elaborated, which allowed the collection of requirements and characteristics for future developments. In conclusion, SG are not strictly better than traditional treatment (TT) methods in this context, but the elaborated comparison tends to point out that SG are sovereign to TM in improving training and producing quality data (in safety) for analysis, which allows the total rehabilitation process to be better conducted.}
}
@article{JARRETT2021100502,
title = {Exploring and interrogating astrophysical data in virtual reality},
journal = {Astronomy and Computing},
volume = {37},
pages = {100502},
year = {2021},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2021.100502},
url = {https://www.sciencedirect.com/science/article/pii/S2213133721000561},
author = {T.H. Jarrett and A. Comrie and L. Marchetti and A. Sivitilli and S. Macfarlane and F. Vitello and U. Becciani and A.R. Taylor and J.M. {van der Hulst} and P. Serra and N. Katz and M.E. Cluver},
keywords = {Virtual reality, Data visualisation, Radio astrophysics, 3D catalogues, Volumetric rendering},
abstract = {Scientists across all disciplines increasingly rely on machine learning algorithms to analyse and sort datasets of ever increasing volume and complexity. Although trends and outliers are easily extracted, careful and close inspection will still be necessary to explore and disentangle detailed behaviour, as well as identify systematics and false positives. We must therefore incorporate new technologies to facilitate scientific analysis and exploration. Astrophysical data is inherently multi-parameter, with the spatial-kinematic dimensions at the core of observations and simulations. The arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as well as the availability of versatile development tools for video games, has enabled scientists to deploy such technology to effectively interrogate and interact with complex data. In this paper we present development and results from custom-built interactive VR tools, called the iDaVIE suite, that are informed and driven by research on galaxy evolution, cosmic large-scale structure, galaxy–galaxy interactions, and gas/kinematics of nearby galaxies in survey and targeted observations. In the new era of Big Data ushered in by major facilities such as the SKA and LSST that render past analysis and refinement methods highly constrained, we believe that a paradigm shift to new software, technology and methods that exploit the power of visual perception, will play an increasingly important role in bridging the gap between statistical metrics and new discovery. We have released a beta version of the iDaVIE software system that is free and open to the community.}
}
@article{HUANG2023104019,
title = {BIM-supported drone path planning for building exterior surface inspection},
journal = {Computers in Industry},
volume = {153},
pages = {104019},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104019},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001690},
author = {Xiongwei Huang and Yongping Liu and Lizhen Huang and Sverre Stikbakke and Erling Onstein},
keywords = {Building information modeling, Drone, Coverage path planning, Virtual simulation, Building inspection},
abstract = {Digitalization in the architectural, engineering, and construction (AEC) industry highlights the interdisciplinary collaboration between complex systems to provide fast and efficient services. This paper incorporates Building Information Modeling (BIM) and drone, and generates feasible paths for exterior building surface inspections. A systematic approach was proposed, focusing on the overall automatic procedure from the BIM model to the actual flight of the drone in a real-world building environment. The proposed method comprises five main steps: BIM to point cloud represented surface model generation, viewpoint determination, path planning, virtual simulation, and actual flight. In particular, the basic function could generate an inspection path for the whole building that guarantees high coverage rates, obstacle avoidance, and collision-free operation. The advanced function provides four types of building-specific decomposition strategies capable of overcoming the complex building structure shape, enabling reasonable inspection ranges, and enhancing computation efficiency. To simulate a drone’s inspection procedure and evaluate its performance in various contexts, an integrated platform based on a game engine and Airsim was developed. In addition, a case study was conducted to demonstrate the effectiveness of the method. The results demonstrate that the proposed BIM-enabled path planning can assist drones in conducting building inspection tasks with considerable applicability and flexibility.}
}
@article{ROCA2024107412,
title = {Co-evolving scenarios and simulated players to locate bugs that arise from the interaction of software models of video games},
journal = {Information and Software Technology},
volume = {169},
pages = {107412},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107412},
url = {https://www.sciencedirect.com/science/article/pii/S095058492400017X},
author = {Isis Roca and Óscar Pastor and Carlos Cetina and Lorena Arcega},
keywords = {Bug localization, Model interaction, Game software engineering, Search-based software engineering, Model-driven engineering},
abstract = {Context:
Game Software Engineering (GSE) is a field that focuses on developing and maintaining the software part of video games. A key component of video game development is the utilization of game engines, with many engines using software models to capture various aspects of the game.
Objective:
A challenge that GSE faces is the localization of bugs, mainly when working with large and intricated software models. Additionally, the interaction between software models (i.e. bosses, enemies, or environmental elements) during gameplay is often a significant source of bugs. In response to this challenge, we propose a co-evolution approach for bug localization in the software models of video games, called CoEBA.
Methods:
The CoEBA approach leverages Search-Based Software Engineering (SBSE) techniques to locate bugs in software models while considering their interactions. We conducted an evaluation in which we applied our approach to a commercial video game, Kromaia. We compared our approach with a state-of-the-art baseline approach that relied on the bug localization approach used by Kromaia’s developers and a random search used as a sanity check.
Results:
Our co-evolution approach outperforms the baseline approach in precision, recall, and F-measure. In addition, to provide evidence of the significance of our results, we conducted a statistical analysis. that shows significant differences in precision and recall values.
Conclusion:
The proposed approach, CoEBA, which considers the interaction between software models, can identify and locate bugs that other bug localization approaches may have overlooked.}
}
@article{SIMONOV2019404,
title = {Applying Behavior characteristics to decision-making process to create believable game AI},
journal = {Procedia Computer Science},
volume = {156},
pages = {404-413},
year = {2019},
note = {8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.222},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931141X},
author = {Andrey Simonov and Aleksandr Zagarskikh and Victor Fedorov},
keywords = {Game artificial intelligence, Utility theory, Decision-making systems, Believable behavior},
abstract = {With the development of artificial intelligence in computer games the problem of creating characters with believable and diverse behavior to inhabit in-game worlds becomes more and more actual. A big number of required characters and high standards of a modern game artificial intelligence makes the problem even more complex. In this paper we propose a utility-based decision-making model which gives the possibility to generate characters with believable behavior. The believability of such characters comes from their decision-making process that takes into account not only assessment of game environment, but also their personal characteristics and social status. Designed model was used to generate AI driven characters for a development of player’s opponents with personality traits for a computer card strategy. The model was used to simulate human flows on main railroad hub of 2014 Winter Olympics in order to reveal areas where pedestrian flow should be controlled.}
}
@article{HERY2024920,
title = {The Development and Prototyping of Game Modeling At A Family Entertainment Center, Utilizing Web-Based Arduino Technology For Calculating CAGR},
journal = {Procedia Computer Science},
volume = {234},
pages = {920-927},
year = {2024},
note = {Seventh Information Systems International Conference (ISICO 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.080},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924004381},
author = { Hery and Jonathan Budiman and Andree E. Widjaja and Calandra A. Haryani and Riswan E. Tarigan},
keywords = {Arduino, Development, Family entertainment center, Prototyping, Web based},
abstract = {The family entertainment center serves as the primary entertainment intermediary, catering especially to young people and adults through various games, such as virtual reality and the latest entertainment revolution of 3D technology. In order to sustain growth, family entertainment centers must regularly invest in new machines. The process of financing these investments must be precise to streamline company costs and ensure visitor satisfaction. The leadership's decision-making factors for investments can be effectively supported through technology. Therefore, the objective of this research is to create a prototype modeling system that simplifies the ongoing installation process and integrates Compound Annual Growth Rate (CAGR) figures through Arduino, displaying them on a website. The end result of this research is a game prototype model for a family center that effectively assists managers in decision-making, particularly in evaluating machine ratings based on visitor transactions and revenue for each machine using the CAGR metric.}
}
@article{TONG201596,
title = {Rapid Deployment and Evaluation of Mobile Serious Games: A Cognitive Assessment Case Study},
journal = {Procedia Computer Science},
volume = {69},
pages = {96-103},
year = {2015},
note = {The 7th International Conference on Advances in Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915031749},
author = {Tiffany Tong and Victor Guana and Andrea Jovanovic and Fiona Tran and Golnaz Mozafari and Mark Chignell and Eleni Stroulia},
keywords = {cognitive assessments, human computer interaction, human factors, serious games, usability},
abstract = {Serious games are proposed as a more efficient and enjoyable way to carry out cognitive assessment. We compare prediction of cognitive ability with a purpose-built serious game and with a similar game built using a game engine. In an experiment conducted with 28 participants, performance on the two games is assessed relative to three cognitive abilities, using two different tablet sizes and two different input methods. The results for the game-engine variant were similar to the purpose-built game, where both games significantly predicted performance on the three cognitive abilities, and were sensitive to the effects of age. Performance on both games was not significantly affected by tablet size or input method. These results support earlier findings that serious games can provide valid cognitive assessment, and they show that game engines can be used to develop serious games for cognitive assessment, cost effectively and without loss of predictive validity.}
}
@article{HOLM2024106159,
title = {Virtual forests for decision support and stakeholder communication},
journal = {Environmental Modelling & Software},
volume = {180},
pages = {106159},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106159},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224002202},
author = {Stefan Holm and Janine Schweier},
keywords = {3D visualization, Digital twin, Forest growth simulator, Forest planning, Game engine, Unity},
abstract = {Challenges in forest management are increasing due to climate change and its associated risks. Considering the needs and demands of various stakeholders leads to more complex decision-making. The increasing amount and quality of available geographic, forest and individual tree data, the combination of this data, and the use of forest growth simulators make it possible to support forest managers in this decision-making process. Our aim was to develop a strong visualization instrument that can be used in both forest planning and stakeholder communication. We present a solution based on a game engine, where data from multiple sources (terrain data, satellite imagery, tree data) is combined into a virtual environment. The user can move freely inside this virtual forest, look at the forest from arbitrary perspectives, and observe its development over the years under different management scenarios. We demonstrate the usefulness of this approach with a study region in Switzerland.}
}
@article{ZHAO2020354,
title = {Virtual simulation experiment of the design and manufacture of a beer bottle-defect detection system},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {4},
pages = {354-367},
year = {2020},
note = {VR and experiment simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300553},
author = {Yuxiang Zhao and Xiaowei An and Nongliang Sun},
keywords = {Virtual simulation experiment, Beer bottle defect detection, Image processing, Training tool},
abstract = {Background
Machine learning-based beer bottle-defect detection is a complex technology that runs automatically; however, it consumes considerable memory, is expensive, and poses a certain danger when training novice operators. Moreover, some topics are difficult to learn from experimental lectures, such as digital image processing and computer vision. However, virtual simulation experiments have been widely used to good effect within education. A virtual simulation of the design and manufacture of a beer bottle-defect detection system will not only help the students to increase their image-processing knowledge, but also improve their ability to solve complex engineering problems and design complex systems.
Methods
The hardware models for the experiment (camera, light source, conveyor belt, power supply, manipulator, and computer) were built using the 3DS MAX modeling and animation software. The Unreal Engine 4 (UE4) game engine was utilized to build a virtual design room, design the interactive operations, and simulate the system operation.
Results
The results showed that the virtual-simulation system received much better experimental feedback, which facilitated the design and manufacture of a beer bottle-defect detection system. The specialized functions of the functional modules in the detection system, including a basic experimental operation menu, power switch, image shooting, image processing, and manipulator grasping, allowed students (or virtual designers) to easily build a detection system by retrieving basic models from the model library, and creating the beer-bottle transportation, image shooting, image processing, defect detection, and defective-product removal. The virtual simulation experiment was completed with image processing as the main body.
Conclusions
By mainly focusing on bottle mouthdefect detection, the detection system dedicates more attention to the user and the task. With more detailed tasks available, the virtual system will eventually yield much better results as a training tool for imageprocessing education. In addition, a novel visual perception-thinking pedagogical framework enables better comprehension than the traditional lecture-tutorial style.}
}
@article{CASASNOVAS2024107919,
title = {Experimental evaluation of interactive Edge/Cloud Virtual Reality gaming over Wi-Fi using unity render streaming},
journal = {Computer Communications},
volume = {226-227},
pages = {107919},
year = {2024},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2024.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0140366424002585},
author = {Miguel Casasnovas and Costas Michaelides and Marc Carrascosa-Zamacois and Boris Bellalta},
keywords = {Virtual Reality, Wi-Fi, Cloud gaming, Edge computing, Unity, WebRTC},
abstract = {Virtual Reality (VR) streaming enables end-users to seamlessly immerse themselves in interactive virtual environments using even low-end devices. However, the quality of the VR experience heavily relies on Wireless Fidelity (Wi-Fi) performance, since it serves as the last hop in the network chain. Our study delves into the intricate interplay between Wi-Fi and VR traffic, drawing upon empirical data and leveraging a Wi-Fi simulator. In this work, we further evaluate Wi-Fi’s suitability for VR streaming in terms of the Quality of Service (QoS) it provides. In particular, we employ Unity Render Streaming to remotely stream real-time VR gaming content over Wi-Fi 6 using Web Real-Time Communication (WebRTC), considering a server physically located at the network’s edge, near the end user. Our findings demonstrate the system’s sustained network performance, showcasing minimal round-trip time (RTT) and jitter at 60 and 90 frames per second (fps). In addition, we uncover the characteristics and patterns of the generated traffic streams, unveiling a distinctive video transmission approach inherent to WebRTC-based services: the systematic packetization of video frames (VFs) and their transmission in discrete batches at regular intervals, regardless of the targeted frame rate. This interval-based transmission strategy maintains consistent video packet delays across video frame rates but leads to increased Wi-Fi airtime consumption. Our results demonstrate that shortening the interval between batches is advantageous, as it enhances Wi-Fi efficiency and reduces delays in delivering complete frames.}
}
@article{SENESI2024831,
title = {User-Centred Product Design with Photorealistic Virtual Prototypes: A Case Study on Process Optimisation for Aesthetic Quality Enhancement},
journal = {Procedia Computer Science},
volume = {232},
pages = {831-838},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924000838},
author = {Paolo Senesi and Marco Mandolini and Barbara Lonzi and Riccardo Rosati},
keywords = {Generative Model, Texture Mapping, 3D Modelling, Virtual Prototyping},
abstract = {In Industry 4.0, companies must focus on human-centred design for a competitive edge. The 4USER project aims to establish a user-centred design method using an interactive Photorealistic Virtual Prototype (PVP) based on Extended Reality (XR) technology, objectifying customer requirements into technical specifications. The PVP overcomes limitations associated with traditional physical prototypes, serving as a quality benchmark for final products. The research focuses on a case study involving the development of sports rifles, emphasising the importance of aesthetic quality. The proposed semi-automatic process in Blender enables the generation of low-poly PVPs, incorporating hyper-realistic textures and high-frequency details. In particular, the overall process is composed of the following steps: i) wooden texture generation via the Wasserstein Generative Adversarial Network (WGAN); ii) model creation based on a low poly “Shrinkwrap Cage”; iii) integration of generated textures into the UV-mapped model. This approach accelerates the product development cycle, reduces costs, and facilitates efficient quality control.}
}
@article{DALEY2024100529,
title = {Wearables to detect independent variables, objective task performance, and metacognitive states},
journal = {Machine Learning with Applications},
volume = {15},
pages = {100529},
year = {2024},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2024.100529},
url = {https://www.sciencedirect.com/science/article/pii/S2666827024000057},
author = {Matthew S. Daley and Jeffrey B. Bolkhovsky and Rachel Markwald and Timothy Dunn},
keywords = {Wearable technologies, Predictive modeling, Metacognition, Supervised learning, Support vector machines},
abstract = {Wearable biometric tracking devices are becoming increasingly common, providing users with physiological metrics such as heart rate variability (HRV) and skin conductance. We hypothesize that these metrics can be used as inputs for machine learning models to detect independent variables, such as target prevalence or hours awake, objective task performance, and metacognitive states. Over the course of 1–25 h awake, 40 participants completed four sessions of a simulated mine hunting task while non-invasive wearables collected physiological and behavioral data. The collected data were used to generate multiple machine learning models to detect the independent variables of the experiment (e.g., time awake and target prevalence), objective task performance, or metacognitive states. The strongest generated model was the time awake detection model (area under the curve = 0.92). All other models performed much closer to chance (area under the curve = 0.57–0.66), suggesting the model architecture used in this paper can detect time awake but falls short in other domains.}
}
@article{MOHAMMADLANGARI2024102724,
title = {Improving the performance of RRT path planning of excavators by embedding heuristic rules},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102724},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102724},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003720},
author = {Seied {Mohammad Langari} and Faridaddin Vahdatikhaki and Amin Hammad},
keywords = {Earthwork, Excavator, Path Planning, Rapidly Exploring Random Tree},
abstract = {Improving the safety and productivity of earthwork operations is of paramount importance, especially in congested sites where collisions are more probable. Real-time Location Systems and Automated Machine Guidance and Control are expected to improve both the safety and productivity of earthwork operations by providing excavator operators with a higher level of support regarding the path planning of excavators based on site conditions. However, in spite of the large number of studies related to automated path planning of excavators using well-established algorithms from robotics, such as Rapidly-exploring Random Trees and Probabilistic Roadmaps, these studies do not fully consider the engineering constraints of the equipment and do not result in smooth and optimal paths that can be applied in practice. This paper aims to develop a more practical algorithm for the path planning of excavators by embedding heuristic rules and engineering constraints specific to excavators. The proposed method is implemented and tested in a game engine environment. The efficiency of the proposed method in generating a collision-free path, which is expected to result in improved productivity, is validated both quantitatively and visually. The comparative results with other recent and modified versions of the RRT algorithm show that the proposed algorithm is able to find a more realistic path in a shorter time.}
}
@article{ESFAHLANI201942,
title = {Mixed reality and remote sensing application of unmanned aerial vehicle in fire and smoke detection},
journal = {Journal of Industrial Information Integration},
volume = {15},
pages = {42-49},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X18300773},
author = {Shabnam Sadeghi Esfahlani},
keywords = {Fire detection, Autonomous flight, Crazyflie 2.0, Monocular camera, Computer vision},
abstract = {This paper proposes the development of a system incorporating inertial measurement unit (IMU), a consumer-grade digital camera and a fire detection algorithm simultaneously with a nano Unmanned Aerial Vehicle (UAV) for inspection purposes. The video streams are collected through the monocular camera and navigation relied on the state-of-the-art indoor/outdoor Simultaneous Localisation and Mapping (SLAM) system. It implements the robotic operating system (ROS) and computer vision algorithm to provide a robust, accurate and unique inter-frame motion estimation. The collected onboard data are communicated to the ground station and used the SLAM system to generate a map of the environment. A robust and efficient re-localization was performed to recover from tracking failure, motion blur, and frame lost in the data received. The fire detection algorithm was deployed based on the color, movement attributes, temporal variation of fire intensity and its accumulation around a point. The cumulative time derivative matrix was utilized to analyze the frame-by-frame changes and to detect areas with high-frequency luminance flicker (random characteristic). Color, surface coarseness, boundary roughness, and skewness features were perceived as the quadrotor flew autonomously within the clutter and congested area. Mixed Reality system was adopted to visualize and test the proposed system in a physical environment, and the virtual simulation was conducted through the Unity game engine. The results showed that the UAV could successfully detect fire and flame, autonomously fly towards and hover around it, communicate with the ground station and simultaneously generate a map of the environment. There was a slight error between the real and virtual UAV calibration due to the ground truth data and the correlation complexity of tracking real and virtual camera coordinate frames.}
}
@article{LI20221516,
title = {Non-photorealistic Visualization of 3D City Models using Visual Variables in Virtual Reality Environments},
journal = {Procedia Computer Science},
volume = {214},
pages = {1516-1521},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.338},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020506},
author = {Bingchan Li and Zhangsong Luo and Bo Mao},
keywords = {visual variable, 3D city model, non-photorealistic visualization, virtual reality},
abstract = {Visual variables are important factors for visualization in 2D maps, and the study of non-photorealistic visualization of 3D city models in virtual environments using visual variables is also necessary to represent the attributes of city such as energy consumption. This study proposed a set of visual variable mapping model based on 3D geographical environment and conducted user cognitive test. The goal of this research is to verify the applicability of visual variables in 3D space and explore the impact of disturbing visual variables on user cognition. The results show that visualization of compound visual variables in 3D space is more effective than single variable mapping, and it makes up for the shortcomings of single mapping. It is effective to study the visualization effect in three-dimensional geographical environment using virtual reality technology. The results from this case study provide guidance for 3D user-centric visualization technology and make contribute to the development of smart city.}
}
@article{SIMONOV2018453,
title = {Multi-agent crowd simulation on large areas with utility-based behavior models: Sochi Olympic Park Station use case},
journal = {Procedia Computer Science},
volume = {136},
pages = {453-462},
year = {2018},
note = {7th International Young Scientists Conference on Computational Science, YSC2018, 02-06 July2018, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.266},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918315710},
author = {Andrey Simonov and Aleksandr Lebin and Bogdan Shcherbak and Aleksandr Zagarskikh and Andrey Karsakov},
keywords = {Artificial intelligence, Crowd Simulation, Multi-agent modeling},
abstract = {Visualization of human flows and crowd behavior is a complex problem of multi-agent modeling. It can be applied to a various set of problems, from emergency case planning to city life visualization. In this paper, we propose a system to build composite behavior structures for models with a big number of agents. It is designed with combining game development technologies of creating artificial intelligence for ambient characters and traditional multi-agent modeling methods. The system was applied to simulate and to visualize human flows on Sochi Olympic Park station during 2014 Winter Olympics and nowadays in order to reveal areas where pedestrian flow should be controlled.}
}
@article{PEUHKURINEN2011645,
title = {Using RDF Data as Basis for 3D Window Management in Mobile Devices},
journal = {Procedia Computer Science},
volume = {5},
pages = {645-652},
year = {2011},
note = {The 2nd International Conference on Ambient Systems, Networks and Technologies (ANT-2011) / The 8th International Conference on Mobile Web Information Systems (MobiWIS 2011)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.07.084},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911004091},
author = {Antti Peuhkurinen and Tommi Mikkonen and Mikko Terho},
keywords = {Window Manager, Game Engine, 3D, RDF},
abstract = {Computer users are commonly familiar with the WIMP (Windows, Icons, Menus and Pointing device) paradigm that has been the dominant design for decades. Despite its common use, the WIMP paradigm has a fundamental problem: it clutters the precious screen space with a plethora of open windows. The cluttering becomes an even bigger problem when using mobile devices that have smaller screens than conventional computers. Furthermore, accuracy issues commonly arise with touch screens. In this paper, we are introducing a 3D window manager for mobile devices which aims at solving the above issues by providing simple yet powerful interaction mechanisms and graphics as well as the use of only RDF data instead of application specific content. For improved user experience the user interface has been designed with playful use in mind, and this gives the system unique look and feel.}
}
@article{CIRULIS2015199,
title = {Virtualization of Digitalized Cultural Heritage and Use Case Scenario Modeling for Sustainability Promotion of National Identity},
journal = {Procedia Computer Science},
volume = {77},
pages = {199-206},
year = {2015},
note = {ICTE in regional Development 2015 Valmiera, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.384},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915038946},
author = {Arnis Cirulis and Lucio Tommaso De Paolis and Mikheil Tutberidze},
keywords = {Visualization, Cultural heritage, Gamification, sustainability},
abstract = {Nowadays, in the digitalization era, it is becoming ever more pertinent to have reasonable use of digital content. This consideration should also apply to cultural heritage, not only conservation. In the context of globalization, smart actions for content use should be carried out to provide sustainability of national identities. In the last decades, new technologies have been developed and used for digital conservation in the form of three-dimensional computer models with varying size, starting from very small museum exhibits and ending with the largest cathedrals and castles. The benefits computer models are undeniable. By prototyping and reverse engineering, significant exhibits are developed providing possibilities not only to see, but also to hold in one's hands and interact thereby allowing a better understanding of historical events and their meaning. Three-dimensional visualization provides virtual tours in different places and in different times. Unfortunately, development of such content is very expensive. Furthermore, technologies for a successful immersion level are in the development phase. This applies to criteria of content quality and availability and functionality of interaction. It is also vital to understand what elements to virtualize and how it lines up with the provision of national identity. Competition and tourism promotion conditions are undoubtedly a significant driving force for technological development, but, from an identity sustainability point of view, it is important that they go hand in hand. The aim of this research is to develop a baseline design for a set of technologies and the use of virtual and augmented reality to find recommendations for the sustainability of the national identity of countries via the prism of cultural heritage. Thereby providing planned and global technological solutions that are not aimed towards individual museums and separate objects of cultural heritage but focused on the overall region in the specialisation of ancient sites.}
}
@article{YIANNAKOULIAS2024102142,
title = {Parameterizing agent-based models using an online game},
journal = {Computers, Environment and Urban Systems},
volume = {112},
pages = {102142},
year = {2024},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2024.102142},
url = {https://www.sciencedirect.com/science/article/pii/S0198971524000711},
author = {Niko Yiannakoulias and Michel Grignon and Tara Marshall},
keywords = {Research gaming, Agent-based models, Online data collection, Route choice},
abstract = {Agent-based models (ABMs) of human systems are often parameterized using real-world data. For some ABMs this is not possible because the reality upon which the models are based does not exist or is not generalizable from one setting to another. In this paper we implement an online decision game to parameterize an agent-based model of pedestrian and cyclist route choice decisions in a neighbourhood. Our conceptual framework is to use an experimental game to log decision-making behaviour, summarize this behaviour into a decision model, and then transfer this model to an ABM. The product of this framework is an ABM with agents informed by human decision making made within the game, rather than the real world. The results of our analysis suggest that the decision model is consistent with some general theory about decision making, but the ABM illustrates some unique and contextually specific patterns of flow. ABMs parameterized with game data may be useful for forecasting the effects of change on urban transportation infrastructure.}
}
@article{IKENO2021101380,
title = {An enhanced 3D model and generative adversarial network for automated generation of horizontal building mask images and cloudless aerial photographs},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101380},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101380},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001336},
author = {Kazunosuke Ikeno and Tomohiro Fukuda and Nobuyoshi Yabuki},
keywords = {Deep learning, Generative adversarial network, Semantic segmentation, Mask image, Training data, Urban planning and design},
abstract = {Information extracted from aerial photographs is widely used in the fields of urban planning and design. An effective method for detecting buildings in aerial photographs is to use deep learning to understand the current state of a target region. However, the building mask images used to train the deep learning model must be manually generated in many cases. To overcome this challenge, a method has been proposed for automatically generating mask images by using textured three-dimensional (3D) virtual models with aerial photographs. Some aerial photographs include clouds, which degrade image quality. These clouds can be removed by using a generative adversarial network (GAN), which leads to improvements in training quality. Therefore, the objective of this research was to propose a method for automatically generating building mask images by using 3D virtual models with textured aerial photographs. In this study, using GAN to remove clouds in aerial photographs improved training quality. A model trained on datasets generated by the proposed method was able to detect buildings in aerial photographs with IoU = 0.651.}
}
@article{ALCE201435,
title = {Feasibility Study of Ubiquitous Interaction Concepts},
journal = {Procedia Computer Science},
volume = {39},
pages = {35-42},
year = {2014},
note = {The 6th international conference on Intelligent Human Computer Interaction, IHCI 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914014252},
author = {Günter Alce and Lars Thern and Klas Hermodsson and Mattias Wallergård},
keywords = {Natural User Interfaces, Virtual and Augmented Reality, Ubiquitous computing},
abstract = {There are all sorts of consumer electronics in a home environment. Using “apps” to interact with each device is neither feasible nor practical in an ubicomp future. Prototyping and evaluating interaction concepts for this future is a challenge. This paper proposes four concepts for device discovery and device interaction implemented in a virtual environment. The interaction concepts were compared in a controlled experiment for evaluation and comparison. Some statistically significant differences and subjective preferences could be observed in the quantitative and qualitative data respectively. Overall, the results indicate that the proposed interaction concepts were found natural and easy to use.}
}
@article{KIDO2021101281,
title = {Assessing future landscapes using enhanced mixed reality with semantic segmentation by deep learning},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101281},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101281},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000367},
author = {Daiki Kido and Tomohiro Fukuda and Nobuyoshi Yabuki},
keywords = {Mixed reality, Dynamic occlusion handling, Landscape index estimation, Landscape design, Video communication, Deep learning},
abstract = {Architecture, engineering, and construction projects need to be promoted in harmony with the natural environment and with the aim of preserving people’s living environment. At the planning and design stage, decision-makers and stakeholders share and assess landscape images during and after construction in order to avoid as much uncertainty as possible when performing environmental impact assessment. Given the lack of a standard visualization method for future landscapes that do not yet exist, mixed reality (MR), which overlays virtual content onto a real scene, has attracted attention in the field of landscape design. One challenge in MR is occlusion, which occurs when virtual objects obscure physical objects that should be rendered in the foreground. In MR-based landscape visualization, the distance between the MR camera and real objects located in front of the virtual objects might vary and might be large, causing difficulty for existing occlusion handling methods. In the process of landscape design, an evidence-based approach has also become important. Landscape index estimation using semantic segmentation by deep learning, which can recognize the surrounding environment, has been actively studied for landscape assessment. In this study, semantic segmentation by deep learning was integrated into an MR system to enable dynamic occlusion handling and landscape index estimation for both existing and designed landscape assessment. This system can be operated on a mobile device with video communication over the internet by connecting to real-time semantic segmentation on a high-performance personal computer. The applicability of the developed system is demonstrated through accuracy verification and case studies.}
}
@article{REINALDO2021773,
title = {Prototyping "Color in Life" EduGame for Dichromatic Color Blind Awareness},
journal = {Procedia Computer Science},
volume = {179},
pages = {773-780},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921000995},
author = {Ivan Reinaldo and Nadia Sarah Pulungan and Herru Darmadi},
keywords = {color blind, educational game, gaming experience, mobile game},
abstract = {The purpose of this research was to escalate players’ knowledge on color blindness by designing an educational video game which design was oriented to dichromatism color blind. The topic selection was based on the lack of players’ deeper knowledge on color blindness. The graphic and gameplay selection on this research was adjusted to the chosen color blind category. Research methods were conducted by analysis, development, and evaluation. Analysis was done by questionnaire. Development was done by game design document, UML, storyboard, and was implemented using Unity Game Engine. Evaluation on 35 players, which are 32 with normal eyes and 3 with color blindness, was done by two approaches, which are t-test and questionnaire. The result of t-test was t(34) = -7.704, p < 0.05 and Enjoyment score on CEGE is 0.763 for normal eyes and 0.651 for colorblind. To conclude, there was an improvement on knowledge from the video game and the design was enjoyable.}
}
@article{TAI2021274,
title = {Augmented reality-based visual-haptic modeling for thoracoscopic surgery training systems},
journal = {Virtual Reality & Intelligent Hardware},
volume = {3},
number = {4},
pages = {274-286},
year = {2021},
note = {Special Issue on Virtual reality and Augmented Reality in Medical Simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579621000528},
author = {Yonghang Tai and Junsheng Shi and Junjun Pan and Aimin Hao and Victor Chang},
keywords = {Augmented reality, VATS, Surgery training, XPBD},
abstract = {Background
Compared with traditional thoracotomy, video-assisted thoracoscopic surgery (VATS) has less minor trauma, faster recovery, higher patient compliance, but higher requirements for surgeons. Virtual surgery training simulation systems are important and have been widely used in Europe and America. Augmented reality (AR) in surgical training simulation systems significantly improve the training effect of virtual surgical training, although AR technology is still in its initial stage. Mixed reality has gained increased attention in technology-driven modern medicine but has yet to be used in everyday practice.
Methods
This study proposed an immersive AR lobectomy within a thoracoscope surgery training system, using visual and haptic modeling to study the potential benefits of this critical technology. The content included immersive AR visual rendering, based on the cluster-based extended position-based dynamics algorithm of soft tissue physical modeling. Furthermore, we designed an AR haptic rendering systems, whose model architecture consisted of multi-touch interaction points, including kinesthetic and pressure-sensitive points. Finally, based on the above theoretical research, we developed an AR interactive VATS surgical training platform.
Results
Twenty-four volunteers were recruited from the First People's Hospital of Yunnan Province to evaluate the VATS training system. Face, content, and construct validation methods were used to assess the tactile sense, visual sense, scene authenticity, and simulator performance.
Conclusions
The results of our construction validation demonstrate that the simulator is useful in improving novice and surgical skills that can be retained after a certain period of time. The video-assisted thoracoscopic system based on AR developed in this study is effective and can be used as a training device to assist in the development of thoracoscopic skills for novices.}
}
@article{UNTERGUGGENBERGER2023155,
title = {Vulkan all the way: Transitioning to a modern low-level graphics API in academia},
journal = {Computers & Graphics},
volume = {111},
pages = {155-165},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323000249},
author = {Johannes Unterguggenberger and Bernhard Kerbl and Michael Wimmer},
keywords = {Real-time rendering, GPU, Graphics API, Teaching, Vulkan, Programming framework},
abstract = {For over two decades, the OpenGL API provided users with the means for implementing versatile, feature-rich, and portable real-time graphics applications. Consequently, it has been widely adopted by practitioners and educators alike and is deeply ingrained in many curricula that teach real-time graphics for higher education. Over the years, the architecture of graphics processing units (GPUs) incrementally diverged from OpenGL’s conceptual design. The more recently introduced Vulkan API provides a more modern, fine-grained approach for interfacing with the GPU, which allows a high level of controllability and, thereby, deep insights into the inner workings of modern GPUs. This property makes the Vulkan API especially well suitable for teaching graphics programming in university education, where fundamental knowledge shall be conveyed. Hence, it stands to reason that educators who have their students’ best interests at heart should provide them with corresponding lecture material. However, Vulkan is notoriously verbose and rather challenging for first-time users, thus transitioning to this new API bears a considerable risk of failing to achieve expected teaching goals. In this paper, we document our experiences after teaching Vulkan in both introductory and advanced graphics courses side-by-side with conventional OpenGL. A collection of surveys enables us to draw conclusions about perceived workload, difficulty, and students’ acceptance of either approach. In doing so, we identify suitable conditions and recommendations for teaching Vulkan to both undergraduate and graduate students.}
}
@article{HU202120,
title = {Asyncflow: A visual programming tool for game artificial intelligence},
journal = {Visual Informatics},
volume = {5},
number = {4},
pages = {20-25},
year = {2021},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X21000498},
author = {Zhipeng Hu and Changjie Fan and Qiwei Zheng and Wei Wu and Bai Liu},
keywords = {Visual programming, Flowchart, Game Artificial Intelligence},
abstract = {Visual programming tools are widely applied in the game industry to assist game designers in developing game artificial intelligence (game AI) and gameplay. However, testing multiple game engines is a time-consuming operation, which degrades development efficiency. To provide an asynchronous platform for game designers, this paper introduces Asyncflow, an open-source visual programming solution. It consists of a flowchart maker for game logic explanation and a runtime framework integrating an asynchronous mechanism based on an event-driven architecture. Asyncflow supports multiple programming languages and can be easily embedded in various game engines to run flowcharts created by game designers.}
}
@article{PONSEN200759,
title = {Knowledge acquisition for adaptive game AI},
journal = {Science of Computer Programming},
volume = {67},
number = {1},
pages = {59-75},
year = {2007},
note = {Special Issue on Aspects of Game Programming},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2007.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167642307000548},
author = {Marc Ponsen and Pieter Spronck and Héctor Muñoz-Avila and David W. Aha},
keywords = {Computer games, Artificial intelligence, Real-time strategy, Reinforcement learning, Dynamic scripting, Evolutionary algorithm, Knowledge acquisition},
abstract = {Game artificial intelligence (AI) controls the decision-making process of computer-controlled opponents in computer games. Adaptive game AI (i.e., game AI that can automatically adapt the behaviour of the computer players to changes in the environment) can increase the entertainment value of computer games. Successful adaptive game AI is invariably based on the game’s domain knowledge. We show that an offline evolutionary algorithm can learn important domain knowledge in the form of game tactics (i.e., a sequence of game actions) for dynamic scripting, an offline algorithm inspired by reinforcement learning approaches that we use to create adaptive game AI. We compare the performance of dynamic scripting under three conditions for defeating non-adaptive opponents in a real-time strategy game. In the first condition, we manually encode its tactics. In the second condition, we manually translate the tactics learned by the evolutionary algorithm, and use them for dynamic scripting. In the third condition, this translation is automated. We found that dynamic scripting performs best under the third condition, and both of the latter conditions outperform manual tactic encoding. We discuss the implications of these results, and the performance of dynamic scripting for adaptive game AI from the perspective of machine learning research and commercial game development.}
}
@article{IMBERT2013364,
title = {Adding Physical Properties to 3D Models in Augmented Reality for Realistic Interactions Experiments},
journal = {Procedia Computer Science},
volume = {25},
pages = {364-369},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012490},
author = {Nicolas Imbert and Frederic Vignat and Charlee Kaewrat and Poonpong Boonbrahm},
keywords = {Augmented Reality, Physical Properties, Realistic Interaction},
abstract = {Augmented Reality is the combination of virtual objects (created by computer i.e. video, texts or 3D computer models) overlay on top of real world image. Applications of Augmented Reality can be ranged from advertising, edutainment, education, engineering, medicine to industrial manufacturing. In basic applications, like in advertisement or games, users only see the actions or interact with part of the screen designed for initiate some actions. In order to make users have realistic experiences, the interaction amongst virtual objects in Augmented Reality must be restricted to the law of Physics. Virtual objects can have their own dimensions, volumes or weights. When interaction between virtual objects occurred, the collision for example, they should not penetrate each other. The objects will react to each other by the law of Physics. With this concept, all kinds of experiments can be tested or practiced without spending a lot of fortunes with the real setup ranging from simple science experiment, medical training or even assembly process of equipment. In this research, Unity 3D game engine is used on Vuforia platform. Unity is a fully integrated development engine for creating games and other interactive 3D content and Vuforia platform make it possible to write a single native application that runs on almost all smartphones and tablets. To test the concept, 8 pieces of virtual 3D puzzle modules were created using 8 markers. Each virtual module was assigned with physical properties such dimensions, shapes and positions. When assemble the puzzle, each piece of the marker must be able to move around so that the virtual modules can fit to each other. By lifting and rotating the markers, the virtual module will snap with the other proper virtual part, forming virtual 3D puzzle. The virtual module will not penetrate each other because they have their own territory due to their dimensions. With this experiment, users will have a realistic feeling on assembling the virtual model. The concept can be implemented for experiments that are dangerous or expensive to setup. Experiments related to interaction between objects such as physics and chemistry experiments, engineering and medical training are the main targets for using this kind of technology.}
}
@article{PLAVSIC2024100,
title = {VR-based digital twin for remote monitoring of mining equipment: Architecture and a case study},
journal = {Virtual Reality & Intelligent Hardware},
volume = {6},
number = {2},
pages = {100-112},
year = {2024},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000852},
author = {Jovana Plavšić and Ilija Mišković},
keywords = {Virtual reality, Digital twin, Condition monitoring, Mining equipment},
abstract = {Background
Traditional methods for monitoring mining equipment rely primarily on visual inspections, which are time-consuming, inefficient, and hazardous. This article introduces a novel approach to monitoring mission-critical systems and services in the mining industry by integrating virtual reality (VR) and digital twin (DT) technologies. VR-based DTs enable remote equipment monitoring, advanced analysis of machine health, enhanced visualization, and improved decision making.
Methods
This article presents an architecture for VR-based DT development, including the developmental stages, activities, and stakeholders involved. A case study on the condition monitoring of a conveyor belt using real-time synthetic vibration sensor data was conducted using the proposed methodology. The study demonstrated the application of the methodology in remote monitoring and identified the need for further development for implementation in active mining operations. The article also discusses interdisciplinarity, choice of tools, computational resources, time and cost, human involvement, user acceptance, frequency of inspection, multiuser environment, potential risks, and applications beyond the mining industry.
Results
The findings of this study provide a foundation for future research in the domain of VR-based DTs for remote equipment monitoring and a novel application area for VR in mining.}
}
@article{ARJUN202249,
title = {Interactive Sensor Dashboard for Smart Manufacturing},
journal = {Procedia Computer Science},
volume = {200},
pages = {49-61},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002137},
author = {Somnath Arjun and LRD Murthy and Pradipta Biswas},
keywords = {Information Visualization, Eye Tracking, Sensor netwrok, Interaction, Virtual Reality},
abstract = {This paper presents a smart sensor dashboard for a digital twin of a smart manufacturing workshop. We described the development of the digital twin followed by three user studies on the visualization and interaction aspects of the smart sensor dashboard. The first two user studies evaluated ocular parameters and users’ response for different 2D and 3D graphs rendered on 2D screen and VR headset. The bar chart found to generate most accurate users’ response in both 2D and 3D case. The third study recreated the Fitts’ Law task in 3D and compared visual and haptic feedback. We found that haptic feedback significantly improved quantitative metrics of interaction than a no-feedback case, whereas multimodal feedback is significantly improved qualitative metrics of the interaction. Results from the study can be utilized to design VR environments with interactive graphs.}
}
@article{ZHAO2023101992,
title = {The time course of spatial knowledge acquisition for different digital navigation aids},
journal = {Computers, Environment and Urban Systems},
volume = {103},
pages = {101992},
year = {2023},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2023.101992},
url = {https://www.sciencedirect.com/science/article/pii/S0198971523000558},
author = {Hantao Zhao and Lisa Frese and Claudio Venzin and Daniel Kaszás and Raphael P. Weibel and Christoph Hölscher and Victor R. Schinazi and Tyler Thrash},
keywords = {Virtual reality, CAVE, Wayfinding, Map, Spatial learning, Eye tracking},
abstract = {Digital maps on personal devices (e.g., phones) are common tools used to aid navigation. Different types of digital maps can influence spatial knowledge acquisition, and this effect might depend on whether the user interacts with an forward-up or north-up map. In spatial cognition theory, these differences can be used to support either sequential or continuous theories of spatial knowledge acquisition. To test these hypotheses, we compared spatial learning of participants (N = 67) after navigation in a virtual city with either a forward-up map, a north-up map, or a guiding arrow (i.e., a control) as a navigation aid. Critically, participants were tested on landmark recognition tasks and judgments of relative direction (JRDs) after each of four navigation blocks. We also examined mental workload during map usage using eye tracking in terms of the distributions of fixations on the maps. The results indicated that, regardless of navigation aid, participants improved on both landmark recognition and JRD tasks over blocks of trials. In addition, we found an interaction between block and navigation aid. This interaction suggests that participants in the forward-up map group initially produced more JRD errors than the north-up map group but that the two groups performed similarly in later blocks as they became more familiar with the environment. These findings are consistent with the eye-tracking data, which suggested a decrease in mental workload as evidenced by an increase in fixation distributions over blocks of trials. Together, these results suggest that participants with any of these navigation aids perform similarly on some tasks (supporting sequential theory), although the time course of learning may differ between map types (supporting continuous theory).}
}
@article{CIEZA2018352,
title = {Educational Mobile Application of Augmented Reality Based on Markers to Improve the Learning of Vowel Usage and Numbers for Children of a Kindergarten in Trujillo},
journal = {Procedia Computer Science},
volume = {130},
pages = {352-358},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918304046},
author = {Edwin Cieza and David Lujan},
keywords = {Mobile app, Learning, Augmented reality, Markers, Development tools},
abstract = {The main objective of this research was to improve the level of understanding of the usage of vowels and numbers for children over 4 years of age in the Juana Alarco de Dammert Nursery School in Trujillo through an educational mobile application that is composed of the unit development platform, monodevelopment, Andriod Studio, Vuforia using the programming language C# that was made based on the development methodology of extreme software programming. The research design is a pre-experimental experiment grade which was composed of 10 children over the age of 4 of the nursery school and was used as a method of data analysis Student T test. In addition, with the implemented application it was possible to increase the level of academic performance of vowel usage by 27.60% and the use of numbers by 22.60%. It was concluded that with the implementation of the educational mobile application of augmented reality, the level of understanding of vowel usage and numbers had improved in the Juana Alarco de Dammart nursery school children.}
}
@article{FERNANDEZCHAVES2021107440,
title = {ViMantic, a distributed robotic architecture for semantic mapping in indoor environments},
journal = {Knowledge-Based Systems},
volume = {232},
pages = {107440},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107440},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007024},
author = {D. Fernandez-Chaves and J.R. Ruiz-Sarmiento and N. Petkov and J. Gonzalez-Jimenez},
keywords = {Semantic maps, Robotic architecture, Mobile robots, Unity 3D, ROS, Object detection, Detectron2, Robot@Home},
abstract = {Semantic maps augment traditional representations of robot workspaces, typically based on their geometry and/or topology, with meta-information about the properties, relations and functionalities of their composing elements. A piece of such information could be: fridges are appliances typically found in kitchens and employed to keep food in good condition. Thereby, semantic maps allow for the execution of high-level robotic tasks in an efficient way, e.g. “Hey robot, Store the leftover salad”. This paper presents ViMantic, a novel semantic mapping architecture for the building and maintenance of such maps, which brings together a number of features as demanded by modern mobile robotic systems, including: (i) a formal model, based on ontologies, which defines the semantics of the problem at hand and establishes mechanisms for its manipulation; (ii) techniques for processing sensory information and automatically populating maps with, for example, objects detected by cutting-edge CNNs; (iii) distributed execution capabilities through a client–server design, making the knowledge in the maps accessible and extendable to other robots/agents; (iv) a user interface that allows for the visualization and interaction with relevant parts of the maps through a virtual environment; (v) public availability, hence being ready to use in robotic platforms. The suitability of ViMantic has been assessed using Robot@Home, a vast repository of data collected by a robot in different houses. The experiments carried out consider different scenarios with one or multiple robots, from where we have extracted satisfactory results regarding automatic population, execution times, and required size in memory of the resultant semantic maps.}
}
@article{FERNANDO2024356,
title = {Game-based Activity Design in Primary School Students’ Learning Style Detection},
journal = {Procedia Computer Science},
volume = {239},
pages = {356-363},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.182},
url = {https://www.sciencedirect.com/science/article/pii/S187705092401425X},
author = {Pumudu A. Fernando and H.K. Salinda Premadasa},
keywords = {learning style, game-based learning, game design, fedler silverman, generation Alpha},
abstract = {Generation Alpha, the present primary school cohort born after 2010, has significant exposure to mobile devices and gaming. Adopting a "One Size Fits All" approach in modern teaching methods may not be effective, as it overlooks individual learning preferences. Personalized learning can be facilitated by identifying a student’s learning style (LS). Adaptive learning based on LS has been found to have positive effects in several studies. However, traditional learning style detection techniques such as questionnaires and self-assessments can be time-consuming and demotivating for primary school students. This study aims to propose a game-based activity framework as an alternative to the Index of Learning Style (ILS) questionnaire linked with Felder Silverman Learning Style Model for LS detection. The proposed game was evaluated with a sample of sixty students, and preliminary results indicate that the game outperforms the original ILS questionnaire in terms of student engagement and motivation to complete LS activities, achieving an overall satisfaction rate of 87.5%. The second phase of the research will focus on evaluating the accuracy of LS prediction using the designed game, which is currently ongoing.}
}
@article{MANH20222698,
title = {G2L: A Global to Local Alignment Method for Unsupervised Domain Adaptive Semantic Segmentation},
journal = {Procedia Computer Science},
volume = {207},
pages = {2698-2707},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.328},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012170},
author = {Nguyen Viet Manh and Kieu Dang Nam and Dinh Viet Sang and Thi-Oanh Nguyen},
keywords = {Unsupervised domain adaptation, semantic segmentation, adversarial training, self training, pseudo-label denoising, style transfer},
abstract = {Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a source dataset with dense pixel-level annotations to an unlabeled target dataset. However, the performance of UDA methods often suffers from the domain shift, which is the discrepancy between the feature distributions of the two domains. There have been several attempts to match these distributions at the image level marginally. However, due to the so-called category-level domain shift, such global alignments do not guarantee a good separability of deep features extracted from different categories in the target domain. As a result, the generated pseudo-labels can be noisy and thus poison the learning process on the target domain. Some recent methods focus on denoising the pseudo-labels online using category-wise information. This paper introduces a novel UDA method called Global-to-Local alignment (G2L) that leverages fine-grained adversarial training and a newly proposed chromatic Fourier transform to address the image-level domain shift from a global perspective. Next, our method deals with the category-level domain shift under a local view. Specifically, we propose a long-tail category rating strategy as well as apply dynamic confidence thresholds and category-wise priority weights when generating and denoising the pseudo-labels to favor rare categories. Finally, self-distillation is used to boost the final segmentation results. Experiments on popular benchmarks GTA5 → Cityscapes and SYNTHIA → Cityscapes show that our method yields superior accuracy performance than other state-of-the-art methods.}
}
@article{KHANAL201449,
title = {Collaborative virtual reality based advanced cardiac life support training simulator using virtual reality principles},
journal = {Journal of Biomedical Informatics},
volume = {51},
pages = {49-59},
year = {2014},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414000902},
author = {Prabal Khanal and Akshay Vankipuram and Aaron Ashby and Mithra Vankipuram and Ashish Gupta and Denise Drumm-Gurnee and Karen Josey and Linda Tinker and Marshall Smith},
keywords = {Computer uses in education – , Multimedia information systems – , Serious games, Computer applications in medicine, Advanced cardiac life support, Medical team training},
abstract = {Background
Advanced Cardiac Life Support (ACLS) is a series of team-based, sequential and time constrained interventions, requiring effective communication and coordination of activities that are performed by the care provider team on a patient undergoing cardiac arrest or respiratory failure. The state-of-the-art ACLS training is conducted in a face-to-face environment under expert supervision and suffers from several drawbacks including conflicting care provider schedules and high cost of training equipment.
Objective
The major objective of the study is to describe, including the design, implementation, and evaluation of a novel approach of delivering ACLS training to care providers using the proposed virtual reality simulator that can overcome the challenges and drawbacks imposed by the traditional face-to-face training method.
Methods
We compare the efficacy and performance outcomes associated with traditional ACLS training with the proposed novel approach of using a virtual reality (VR) based ACLS training simulator. One hundred and forty-eight (148) ACLS certified clinicians, translating into 26 care provider teams, were enrolled for this study. Each team was randomly assigned to one of the three treatment groups: control (traditional ACLS training), persuasive (VR ACLS training with comprehensive feedback components), or minimally persuasive (VR ACLS training with limited feedback components). The teams were tested across two different ACLS procedures that vary in the degree of task complexity: ventricular fibrillation or tachycardia (VFib/VTach) and pulseless electric activity (PEA).
Results
The difference in performance between control and persuasive groups was not statistically significant (P=.37 for PEA and P=.1 for VFib/VTach). However, the difference in performance between control and minimally persuasive groups was significant (P=.05 for PEA and P=.02 for VFib/VTach). The pre-post comparison of performances of the groups showed that control (P=.017 for PEA, P=.01 for VFib/VTach) and persuasive (P=.02 for PEA, P=.048 for VFib/VTach) groups improved their performances significantly, whereas minimally persuasive group did not (P=.45 for PEA, P=.46 for VFib/VTach). Results also suggest that the benefit of persuasiveness is constrained by the potentially interruptive nature of these features.
Conclusions
Our results indicate that the VR-based ACLS training with proper feedback components can provide a learning experience similar to face-to-face training, and therefore could serve as a more easily accessed supplementary training tool to the traditional ACLS training. Our findings also suggest that the degree of persuasive features in VR environments have to be designed considering the interruptive nature of the feedback elements.}
}
@article{PEDRAZAHUESO2015161,
title = {Rehabilitation Using Kinect-based Games and Virtual Reality},
journal = {Procedia Computer Science},
volume = {75},
pages = {161-168},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.233},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036947},
author = {Miguel Pedraza-Hueso and Sergio Martín-Calzón and Francisco Javier Díaz-Pernas and Mario Martínez-Zarzuela},
keywords = {Serious games, Virtual Reality, Kinect},
abstract = {This paper introduces the development of a customized virtual reality system based on a serious game which allows the user to carry out physical and cognitive rehabilitation therapies using a natural user interface based on Microsoft© Kinect. Within these serious games you can find the exergames. It is a type of serious game which aims to stimulate body mobility through an immersive experience that situates the user inside virtual interactive landscapes. This type of game has become popular in recent years thanks to the creation of consoles like Nintendo Wii, Playstation or Xbox, which use gestural interaction game interfaces. Likewise, these technologies have become extremely useful tools in rehabilitation, and they are expected to permit a reduction of costs in socio-sanitary environments. The proposed virtual reality platform consists of different types of exercises by which the user is able to train or rehabilitate several aspects such as strength, aerobic or cognitive capacities. The system has been modelled so that the physical presence of a therapist is not required during the course of the session and there is no need to wear any kind of marker or sensor. Moreover, all parameters of the different exercises can be configured without the physical presence of a therapist. The reports of each session can also be read offline, therefore, the therapist will always know if a user has performed the session in a good way and act accordingly modifying whatever he deems necessary in the patient's therapy. Due to these facts the system developed and presented in this article is a rehabilitation system based on remote assistance. It is important that this type of serious games accomplish all the functionalities a videogame fulfils at the same time that accomplish specific functionalities in its therapeutic environment. Remarkably, it is required an adaptation to the patient's abilities to avoid frustration and provide immediate feedback to the user during the exercises. In our system, the users are monitored and receive an audio-visual feedback during the session, so that they know in real-time if they are correctly doing the exercises of the specific therapy that was designed for them.}
}
@article{MIYATA2018295,
title = {Modeling emotion and inference as a value calculation system},
journal = {Procedia Computer Science},
volume = {123},
pages = {295-301},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.046},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300474},
author = {Masahiro Miyata and Takashi Omori},
keywords = {Emotion, Decision Making, Value Calculation System, Model, Probabilistic Inference},
abstract = {There have been many studies on the modeling of a relation between emotion and decision making. Though the effect of emotion on inference in making a decision is evident, its computational mechanism, especially for intuitive inference, is not yet clear. Therefore, in this paper, we discuss the possibility of the computational modeling of an intuitive inference guided by emotion in which random-seeming neural excitation plays the role of a probability-based parallel search of values. First, we show a possible architecture of the intuitive inference in which the system of multiple values affects the process of action decision. Then, we focus on an effect of the value-control mechanism for intuitive inference in a path-finding task. In a computer simulation, we aimed to simulate a model of value management in which multiple value components of the brainstem are controlled for an action search. Though the brainstem seems simple, the model includes the essence of the resolution of conflicts between multiple values.}
}
@article{CYRINO2022551,
title = {An Intuitive VR-based Environment for Monitoring and Control of Electrical Power Substations},
journal = {Procedia Computer Science},
volume = {201},
pages = {551-558},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004847},
author = {Gabriel F. Cyrino and Camilo L. Barreto and Leandro R. Mattioli and Alexandre Cardoso and Edgard A. Lamounier and Gerson F.M. Lima and Daniel S. Ramos},
keywords = {Substations Operation, Operator Training, Virtual Reality, 3D interfaces},
abstract = {A Virtual Reality System provides an intuitive tridimensional user interface, with distinct interaction when compared to a traditional 2D layout, allowing the user to manipulate data similar to the real world. Such systems give the users physical and cognitive immersion, by granting a mental model compatible with field operation. In this work, we present the development aspects and some results of a Virtual Reality based solution to provide a more natural and intuitive environment for controlling electrical operation centers. The solution intends to minimize the issues caused by the operation of electric power substations due to the lack of spatial and functional information on the traditional operation interfaces. The research is being carried out with the collaboration between the energy electric company of Minas Gerais–Brazil (CEMIG) and the Federal University of Uberlândia. Besides operating generation and transmission actives, the environment presents great potential for the training of operators and other professionals of support and maintenance.}
}
@article{VERHULST2021106951,
title = {Do VR and AR versions of an immersive cultural experience engender different user experiences?},
journal = {Computers in Human Behavior},
volume = {125},
pages = {106951},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106951},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221002740},
author = {Isabelle Verhulst and Andy Woods and Laryssa Whittaker and James Bennett and Polly Dalton},
keywords = {Virtual reality, Augmented reality, User experience, Presence, Enjoyment, Engagement},
abstract = {Although Virtual Reality (VR) and Augmented Reality (AR) user experiences have received large amounts of recent research interest, a direct comparison of different immersive technologies' user experiences has not often been conducted. This study compared user experiences of one VR and two AR versions of an immersive gallery experience ‘Virtual Veronese’, measuring multiple aspects of user experience, including enjoyment, presence, cognitive, emotional and behavioural engagement, using a between-subjects design, at the National Gallery in London, UK. Analysis of the self-reported survey data (N = 368) showed that enjoyment was high on all devices, with the Oculus Quest (VR) receiving higher mean scores than both AR devices, Magic Leap and Mira Prism. In relation to presence, the elements ‘spatial presence’, ‘involvement’, and ‘sense of being there’ received a higher mean score on the Oculus Quest than on both AR devices, and on ‘realism’ the Oculus Quest scored significantly higher than the Magic Leap. Cognitive engagement was similar between the three devices, with only ‘I knew what to do’ being rated higher for Quest than Mira Prism. Emotional engagement was similar between the devices. Behavioural engagement was high on all devices, with only ‘I would like to see more experiences like this’ being higher for Oculus Quest than Mira Prism. Negative effects including nausea were rarely reported. Differences in user experiences were likely partly driven by differences in immersion levels between the devices.}
}
@article{OUALI2022158,
title = {Augmented Reality for Scene Text Recognition, Visualization and Reading to Assist Visually Impaired People},
journal = {Procedia Computer Science},
volume = {207},
pages = {158-167},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009218},
author = {Imene OUALI and Mohamed BEN HALIMA and Ali WALI},
keywords = {Text Visualization, Text detection, Text recognition, Natural Scene, Augmented Reality, VGG19},
abstract = {Reading traffic signs while driving a car for visually impaired people and people with visual problems is a very difficult task for them. This task is encountered every day, sometimes incorrect reading of traffic signs can lead to very serious results. In particular, the Arabic language is very difficult, making recognizing and viewing Arabic text a difficult task. In this context, we are looking for an effective solution to remove errors and results that can sometimes end someone's life. This article aims to correctly read traffic signs with Arabic text using augmented reality technology. Our system is composed of three modules. The first is text detection and recognition. The second is Text visualization. The third is Text to speech methods conversion. With this system, the user can have two different results. The first result is visual with much-improved text and enhancement. The second result is sound, he can hear the text aloud. This system is very applicable and effective for daily life. To assess the effectiveness of our work, we offer a survey to a group of visually impaired people to give their opinion on the use of our application. The results have been good for most people.}
}
@article{LEE2020443,
title = {VEGO: A novel design towards customizable and adjustable head-mounted display for VR},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {5},
pages = {443-453},
year = {2020},
note = {VR/AR research and commercial applications in Singapore},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300759},
author = {Jia Ming Lee and Xinxing Xia and Clemen OW and Felix Chua and Yunqing Guan},
keywords = {Virtual reality, Head mounted display, Vergence accommodation conflict, Inter-pupil distance},
abstract = {Background
Virtual Reality (VR) technologies have advanced fast and have been applied to a wide spectrum of sectors in the past few years. VR can provide an immersive experience to users by generating virtual images and displaying the virtual images to the user with a head-mounted display (HMD) which is a primary component of VR. Normally, an HMD contains a list of hardware components, e.g., housing pack, micro LCD display, microcontroller, optical lens, etc. Settings of VR HMD to accommodate the user's inter-pupil distance (IPD) and the user's eye focus power are important for the user's experience with VR.
Methods
Although various methods have been developed towards IPD and focus adjustments for VR HMD, the increased cost and complexity impede the possibility for users who wish to assemble their own VR HMD for various purposes, e.g., DIY teaching, etc. In our paper, we present a novel design towards building a customizable and adjustable HMD for VR in a cost-effective manner. Modular design methodology is adopted, and the VR HMD can be easily printed with 3D printers. The design also features adjustable IPD and variable distance between the optical lens and the display. It can help to mitigate the vergence and accommodation conflict issue.
Results
A prototype of the customizable and adjustable VR HMD has been successfully built up with off-the-shelf components. A VR software program running on Raspberry Pi board has been developed and can be utilized to show the VR effects. A user study with 20 participants is conducted with positive feedback on our novel design.
Conclusions
Modular design can be successfully applied for building up VR HMD with 3D printing. It helps to promote the wide application of VR at affordable costs while featuring flexibility and adjustability.}
}
@article{BOGES202012,
title = {Virtual reality framework for editing and exploring medial axis representations of nanometric scale neural structures},
journal = {Computers & Graphics},
volume = {91},
pages = {12-24},
year = {2020},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2020.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0097849320300789},
author = {Daniya Boges and Marco Agus and Ronell Sicat and Pierre J. Magistretti and Markus Hadwiger and Corrado Calì},
keywords = {Ultrastructural analysis, Medial axis representation, Immersive environments, Virtual reality in neuroscience},
abstract = {We present a novel virtual reality (VR) based framework for the exploratory analysis of nanoscale 3D reconstructions of cellular structures acquired from rodent brain samples through serial electron microscopy. The system is specifically targeted on medial axis representations (skeletons) of branched and tubular structures of cellular shapes, and it is designed for providing to domain scientists: i) effective and fast semi-automatic interfaces for tracing skeletons directly on surface-based representations of cells and structures, ii) fast tools for proofreading, i.e., correcting and editing of semi-automatically constructed skeleton representations, and iii) natural methods for interactive exploration, i.e., measuring, comparing, and analyzing geometric features related to cellular structures based on medial axis representations. Neuroscientists currently use the system for performing morphology studies on sparse reconstructions of glial cells and neurons extracted from a sample of the somatosensory cortex of a juvenile rat. The framework runs in a standard PC and has been tested on two different display and interaction setups: PC-tethered stereoscopic head-mounted display (HMD) with 3D controllers and tracking sensors, and a large display wall with a standard gamepad controller. We report on a user study that we carried out for analyzing user performance on different tasks using these two setups.}
}
@article{PARAMARTHA2023874,
title = {Multimedia Application based on Virtual Reality to Introduce College Majors in Universities},
journal = {Procedia Computer Science},
volume = {227},
pages = {874-883},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.594},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017611},
author = {Damar Harip Paramartha and M.Afdhal Arief Malik and Selvi Dian Pertiwi and Reinert Yosua Rumagit},
keywords = {Virtual Reality, College Major, Universities},
abstract = {The goal of this project is to create virtual reality-based multimedia programs that can introduce users to fresh university experiences and information. Many prospective students are currently selecting the incorrect major, which could have negative effects on those prospective students. The Game Development Life Cycle, often known as the GDLC approach, is the development process method employed in this application. The GDLC process begins with initiation and progresses through pre-production, testing, beta, and release. Several respondents have tested the application. According to the survey's findings, 74.5% of respondents said that this application may encourage students to select the majors they are passionate about. According to the testimonial results, 77.1% of respondents thought this application was simple to understand.}
}
@article{SERRANOLAGUNA2012203,
title = {Tracing a Little for Big Improvements: Application of Learning Analytics and Videogames for Student Assessment},
journal = {Procedia Computer Science},
volume = {15},
pages = {203-209},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008344},
author = {Ángel Serrano-Laguna and Javier Torrente and Pablo Moreno-Ger and Baltasar Fernández-Manjón},
keywords = {Learning Analytics, Educational games, Data mining, Assessment},
abstract = {Assessment is essential to establish the failure or success of any educational activity. To measure the acquisition of the knowledge covered by the activity and also to determine the effectiveness of the activity itself. The increasing adoption of new technologies is promoting the use of new types of activities in schools, like educational video games that in some cases are developed by the teachers themselves. In this kind of activity, interactivity increases compared to traditional activities (e.g. reading a document), which can be a powerful source of data to feed learning analytics systems that infer knowledge about the effectiveness of the educational process. In this paper, we discuss how a part of the students’ assessment can be achieved semi-automatically by logging the interaction with educational video games. We conclude that even the application of rather simple tracking techniques means an advantage compared to other systems that are fed with less quality data.}
}
@article{SUGIANTO2023623,
title = {3D Modelling Building of District Johar Baru Using ArcGIS Pro and CityEngine},
journal = {Procedia Computer Science},
volume = {227},
pages = {623-631},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.566},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017337},
author = {Edi Sugianto and Johan Fernando Hosea and Bakti Amirul Jabar and Edy Irwansyah and Devi Fitrianah},
keywords = {3D Modelling, Level of Detail, ArcGIS, CityEngine},
abstract = {Various image-based approaches are currently available for 3 Dimensional (3D) modelling. Among them, two main geomatics techniques are photogrammetry and laser scanning. Owing to the limited availability of photographs to the general populace, the employment of satellite and aerial photogrammetry is rather intricate. In light of Jakarta's absence of a comprehensive three-dimensional portrayal, there is an impetus to generate accurate, precise, and detailed tri-dimensional visualizations of a particular sector within Jakarta, specifically Johar Baru. This endeavor shall be undertaken through a technique termed LoD, with ArcGIS and CityEngine serving as the fundamental instruments for this initiative. To do this, we provided a framework that leverages existing technologies and data sources to reduce the time and work needed for prospective 3D indoor routing applications. Our indoor and building 3D models were created using the CityEngine procedural modelling approach from CAD files and building footprints. This work does not address data delivery or administration; it primarily focuses on 3D visualization. Cadastral, visualization, and non-functional requirements are the three main areas of 3D visualization requirements against which CityEngine was assessed. A case study corresponding to a real issue that has already been recognized in Portugal is used to evaluate the problem. The results are encouraging. The CityEngine won't be the best choice for all users because of the steep learning curve. Using CityEngine, we can show and design a 3D reality in Johar Baru.}
}
@article{CLARKE201238,
title = {PR:EPARe: A Game-Based Approach to Relationship Guidance for Adolescents},
journal = {Procedia Computer Science},
volume = {15},
pages = {38-44},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.056},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008186},
author = {Samantha Clarke and Sylvester Arnab and Ian Dunwell and Katherine Brown},
keywords = {Relationship education, Participatory design, Serious games, Blended learning, Game-based intervention},
abstract = {Ensuring adolescents are equipped with the necessary skills to handle coercion and pressure from peers is a central component of effective relationship education. However, for teachers attempting to convey these principles, didactic methods have been shown to meet with limited success, as the highest-risk students may fail to engage with the subject matter in a meaningful fashion. In this paper, the potential a digital game may hold as a component of a blended learning solution to this problem is explored though the development of PR:EPARe (Positive Relationships: Eliminating Coercion and Pressure in Adolescent Relationships). Adopting a participatory design approach, designers considered relevant input from stakeholders, subject experts, teachers and students in the development of PR:EPARe. Participatory involvement has allowed the game to be developed in such a way that draws focus on the role of the end user to extend from the traditional concern of the student's learning needs to consider that of the practitioner's needs as another primary condition of successful game based learning. An examination of the first section of the PR:EPARe game is undertaken through a cluster randomized control trial of 507 students across three UK schools. Using ANOVA to demonstrating significant differences between control and game groups (p<0.05) for responses to a range of questions on preparedness and self-efficacy. An overall significant positive effect of the game over time when compared to the control (p<0.001) is observed. Based on these preliminary findings, the participatory approach to development is shown to lead to a developed game which is well- received by students, offering the potential to provide a valuable resource for teachers attempting to address this difficult subject within a classroom-based context.}
}
@article{DAVID2024103890,
title = {The Salient360! toolbox: Handling gaze data in 3D made easy},
journal = {Computers & Graphics},
volume = {119},
pages = {103890},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103890},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000177},
author = {Erwan David and Jesús Gutiérrez and Melissa Lè-Hoa Võ and Antoine Coutrot and Matthieu {Perreira Da Silva} and Patrick {Le Callet}},
keywords = {Toolbox, Gaze tracking, 360°stimuli, Processing, Comparison, Visualisation},
abstract = {Eye tracking has historically been a very popular tool. The data it records allow us to understand how people behave and what they attend to within our visual world; under this perspective the experiments, applications and use-cases are endless. Therefore, it is not surprising to witness a strong rise in the use of eXtended Reality (XR) devices with embedded eye trackers in research. These devices allow for less obtrusive experimenting conditions, and a significantly higher experimental control compared to traditional desktop testing. The use of eye tracking in XR is increasing and so is the need for a toolbox enabling consensus about eye tracking methods in 3D. We present the Salient360! toolbox: it implements functions to identify saccades and fixations and output gaze features (e.g., saccade directions) to generate saliency maps, fixation maps, and scanpath data. It implements comparisons of gaze data with methods adapted to 3D. We plan continuous improvements of the toolbox as the community develops new tools and methods dedicated to 360°gaze tracking. We hope that this toolbox will spark discussions about the methodology of 3D gaze processing, facilitate running experiments, and improve studying gaze in 3D. https://github.com/David-Ef/salient360Toolbox}
}
@article{SLOB2023107815,
title = {Virtual reality-based digital twins for greenhouses: A focus on human interaction},
journal = {Computers and Electronics in Agriculture},
volume = {208},
pages = {107815},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.107815},
url = {https://www.sciencedirect.com/science/article/pii/S016816992300203X},
author = {Naftali Slob and William Hurst and Rick {van de Zedde} and Bedir Tekinerdogan},
keywords = {Digital twin, Virtual reality, Greenhouse, Simulation sickness, Cyber sickness},
abstract = {The agricultural domain is experiencing an increasing use of digital twin technologies in greenhouse horticulture for the betterment of monitoring production. Research in this domain has started leading towards more advanced ways of visually interacting with these digital twins, and this could be by means of immersive technologies (such as virtual reality) to better allow farmers to feel a sense of presence when exploring these digital copies. Yet there are many remaining challenges for the technology’s integration and more studies are needed into user interaction; specifically regarding simulation sickness to cater for comfort during prolonged and regular use. Thus, this article documents the survey of 30 participants by means of the Simulator Sickness Questionnaire (before and after interaction) when using an immersive digital greenhouse twin environment. Findings indicate that users who experience simulation sickness tend to provide a lower evaluation score of the digital twin and their prior experience of gaming (on varied devices) affects the overall evaluation of the digital environment. Further, the level of playability and realism (referred to as convenience), statistically affects the end users’ level of sickness. For tracking crop growth by means of digital greenhouse twins, where prolonged user interaction may take place in a virtual environment, these are notable considerations for digital twin developers.}
}
@article{YU201837,
title = {Space-based Collision Avoidance Framework for Autonomous Vehicles},
journal = {Procedia Computer Science},
volume = {140},
pages = {37-45},
year = {2018},
note = {Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.290},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831963X},
author = {Jinke Yu and Leonard Petnga},
keywords = {Collision avoidance, Autonomous vehicles, Artificial intelligence, Spatio-temporal algorithms, AirSim},
abstract = {High confidence in the safe operation of autonomous systems remains a critical hurdle on their path to becoming ubiquitous. Recent accidents of Uber and Google driverless cars illustrate the difficulty ahead. Leading collision avoidance framework for autonomous systems fail to properly capture and account for the high variability of geometries, shapes, and sizes of the agents (e.g., 18 wheels truck vs. 4 doors sedan), capabilities that are critical in situations with high risk of accident (e.g., intersection crossing). We introduce a simple and efficient multi-agent collision avoidance framework for Autonomous Vehicles (AV) in various collision configurations (i.e., glancing, away, clipping). Machine learning techniques are proposed to properly train the autonomous systems involved. Vehicle-to-Vehicle (V2V) communication technologies and shape-based spatial-temporal collision avoidance algorithms are leveraged to ensure the accurate prediction of the collision and correct decision on the appropriate steps to avoid its occurrence. A prototype implementation and simulation is currently under development for a clipping collision problem at a lightless intersection crossing using the AirSim platform.}
}
@article{DOWNS2021102113,
title = {Assessing Industrial Robot agility through international competitions},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {70},
pages = {102113},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102113},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520303239},
author = {Anthony Downs and Zeid Kootbally and William Harrison and Pavel Pilliptchak and Brian Antonishek and Murat Aksu and Craig Schlenoff and Satyandra K. Gupta},
keywords = {Performance evaluation, Industrial Robotics, Artificial intelligence, Agility, Competitions},
abstract = {Manufacturing and Industrial Robotics have reached a point where to be more useful to small and medium sized manufacturers, the systems must become more agile and must be able to adapt to changes in the environment. This paper describes the process for creating and the lessons learned over multiple years of the Agile Robotics for Industrial Automation Competition (ARIAC) being run by the National Institute of Standards and Technology.}
}
@article{BRUNNSTROM2020116005,
title = {Latency impact on Quality of Experience in a virtual reality simulator for remote control of machines},
journal = {Signal Processing: Image Communication},
volume = {89},
pages = {116005},
year = {2020},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2020.116005},
url = {https://www.sciencedirect.com/science/article/pii/S0923596520301648},
author = {Kjell Brunnström and Elijs Dima and Tahir Qureshi and Mathias Johanson and Mattias Andersson and Mårten Sjöström},
keywords = {Quality of Experience (QoE), Virtual reality, Latency, Head-Mounted Displays (HMD), Forestry crane},
abstract = {In this article, we have investigated a VR simulator of a forestry crane used for loading logs onto a truck. We have mainly studied the Quality of Experience (QoE) aspects that may be relevant for task completion, and whether there are any discomfort related symptoms experienced during the task execution. QoE experiments were designed to capture the general subjective experience of using the simulator, and to study task performance. The focus was to study the effects of latency on the subjective experience, with regards to delays in the crane control interface. Subjective studies were performed with controlled delays added to the display update and hand controller (joystick) signals. The added delays ranged from 0 to 30 ms for the display update, and from 0 to 800 ms for the hand controller. We found a strong effect on latency in the display update and a significant negative effect for 800 ms added delay on latency in the hand controller (in total approx. 880 ms latency including the system delay). The Simulator Sickness Questionnaire (SSQ) gave significantly higher scores after the experiment compared to before the experiment, but a majority of the participants reported experiencing only minor symptoms. Some test subjects ceased the test before finishing due to their symptoms, particularly due to the added latency in the display update.}
}
@article{HAMM2019349,
title = {Guidetomeasure-OT: A mobile 3D application to improve the accuracy, consistency, and efficiency of clinician-led home-based falls-risk assessments},
journal = {International Journal of Medical Informatics},
volume = {129},
pages = {349-365},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618304337},
author = {Julian Hamm and Arthur Money and Anita Atwal},
keywords = {Technology for health, Health informatics, Falls prevention, Occupational therapy, 3D mobile visualisation, Measurement guidance, Falls risk factors},
abstract = {Background
A key falls prevention intervention delivered within occupational therapy is the home environment falls-risk assessment process. This involves the clinician visiting the patient’s home and using a 2D paper-based measurement guidance booklet to ensure that all measurements are taken and recorded accurately. However, 30% of all assistive devices installed within the home are abandoned by patients, in part as a result of the inaccurate measurements being recorded as part of the home environment falls-risk assessment process. In the absence of more appropriate and effective guidance, high levels of device abandonment are likely to persist.
Aim
This study presents guidetomeasure-OT, a mobile 3D measurement guidance application designed to support occupational therapists in carrying out home environment falls-risk assessments. Furthermore, this study aims to empirically evaluate the performance of guidetomeasure-OT compared with an equivalent paper-based measurement guidance booklet.
Methods
Thirty-five occupational therapists took part in this within-subjects repeated measures study, delivered within a living lab setting. Participants carried out the home environment falls-risk assessment process under two counterbalanced treatment conditions; using 3D guidetomeasure-OT; and using a 2D paper-based guide. Systems Usability Scale questionnaires and semi-structured interviews were completed at the end of both task. A comparative statistical analysis explored performance relating to measurement accuracy, measurement accuracy consistency, task completion time, and overall system usability, learnability, and effectiveness of guidance. Interview transcripts were analysed using inductive and deductive thematic analysis, the latter was informed by the Unified Theory of Acceptance and Use of Technology model.
Results
The guidetomeasure-OT application significantly outperformed the 2D paper-based guidance in terms task efficiency (p <  0.001), learnability (p <  0.001), system usability (p <  0.001), effectiveness of guidance (p =  0.001). Regarding accuracy, in absolute terms, guidetomeasure-OT produced lower mean error differences for 11 out of 12 items and performed significantly better for six out of 12 items (p = < 0.05). In terms of SUS, guidetomeasure-OT scored 83.7 compared with 70.4 achieved by the booklet. Five high-level themes emerged from interviews: Performance Expectancy, Effort Expectancy, Social Influence, Clinical Benefits, and Augmentation of Clinical Practice. Participants reported that guidetomeasure-OT delivered clearer measurement guidance that was more realistic, intuitive, precise and usable than the paper-based equivalent. Audio instructions and animated prompts were seen as being helpful in reducing the learning overhead required to comprehend measurement guidance and maintain awareness of task progression.
Conclusions
This study reveals that guidetomeasure-OT enables occupational therapists to carry out significantly more accurate and efficient home environment falls-risk assessments, whilst also providing a measurement guide tool that is considered more usable compared with the paper-based measurement guide that is currently used by clinicians in practice. These results are significant as they indicate that mobile 3D visualisation technologies can be effectively deployed to improve clinical practice, particularly within the home environment falls-risk assessment context. Furthermore, the empirical findings constitute overcoming the challenges associated with the digitisation of health care and delivery of new innovative and enabling technological solutions that health providers and policy makers so urgently need to ease the ever-increasing burden on existing public resources. Future work will focus on the development and empirical evaluation of a mobile 3D application for patient self-assessment and automated assistive equipment prescription. Furthermore, broader User Experience aspects of the application design and the interaction mechanisms that are made available to the user could be considered so as to minimize the effect of cognitive overloading and optimise user performance.}
}
@article{DENHAAN2020104855,
title = {The Virtual River Game: Gaming using models to collaboratively explore river management complexity},
journal = {Environmental Modelling & Software},
volume = {134},
pages = {104855},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104855},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220309129},
author = {R.J. {den Haan} and M.C. {van der Voort} and F. Baart and K.D. Berends and M.C. {van den Berg} and M.W. Straatsma and A.J.P. Geenen and S.J.M.H. Hulscher},
keywords = {Serious gaming, Social learning, Water management, Stakeholder participation, Participatory decision-making, Tangible interaction},
abstract = {Serious games are increasingly used as tools to facilitate stakeholder participation and stimulate social learning in environmental management. We present the Virtual River Game that aims to support stakeholders in collaboratively exploring the complexity of a changed river management paradigm in the Netherlands. The game uses a novel, hybrid interface design that features a bidirectional coupling of a physical game board to computer models. We ran five game sessions involving both domain experts and non-experts to assess the game's value as a participatory tool. The results show that the game was effective in enabling participants to collaboratively experiment with various river interventions and in stimulating social learning. As a participatory tool, the game appears to be valuable to introduce non-expert stakeholders to Dutch river management. We further discuss how the hybrid interface combines qualities usually found in board and computer games that are beneficial in engaging stakeholders and stimulating learning.}
}
@article{BEKTAS2024103909,
title = {Gaze-enabled activity recognition for augmented reality feedback},
journal = {Computers & Graphics},
volume = {119},
pages = {103909},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103909},
url = {https://www.sciencedirect.com/science/article/pii/S009784932400044X},
author = {Kenan Bektaş and Jannis Strecker and Simon Mayer and Kimberly Garcia},
keywords = {Pervasive eye tracking, Augmented reality, Attention, Human activity recognition, Context-awareness, Ubiquitous computing},
abstract = {Head-mounted Augmented Reality (AR) displays overlay digital information on physical objects. Through eye tracking, they provide insights into user attention, intentions, and activities, and allow novel interaction methods based on this information. However, in physical environments, the implications of using gaze-enabled AR for human activity recognition have not been explored in detail. In an experimental study with the Microsoft HoloLens 2, we collected gaze data from 20 users while they performed three activities: Reading a text, Inspecting a device, and Searching for an object. We trained machine learning models (SVM, Random Forest, Extremely Randomized Trees) with extracted features and achieved up to 89.6% activity-recognition accuracy. Based on the recognized activity, our system—GEAR—then provides users with relevant AR feedback. Due to the sensitivity of the personal (gaze) data GEAR collects, the system further incorporates a novel solution based on the Solid specification for giving users fine-grained control over the sharing of their data. The provided code and anonymized datasets may be used to reproduce and extend our findings, and as teaching material.}
}
@article{LI2022418,
title = {RADepthNet: Reflectance-aware monocular depth estimation},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {5},
pages = {418-431},
year = {2022},
note = {Computer graphics for metaverse},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000808},
author = {Chuxuan Li and Ran Yi and Saba Ghazanfar Ali and Lizhuang Ma and Enhua Wu and Jihong Wang and Lijuan Mao and Bin Sheng},
keywords = {Monocular depth estimation, Deep learning, Intrinsic image decomposition},
abstract = {Background
Monocular depth estimation aims to predict a dense depth map from a single RGB image, and has important applications in 3D reconstruction, automatic driving, and augmented reality. However, existing methods directly feed the original RGB image into the model to extract depth features without avoiding the interference of depth-irrelevant information on depth-estimation accuracy, which leads to inferior performance.
Methods
To remove the influence of depth-irrelevant information and improve the depth-prediction accuracy, we propose RADepthNet, a novel reflectance-guided network that fuses boundary features. Specifically, our method predicts depth maps using the following three steps: (1) Intrinsic Image Decomposition. We propose a reflectance extraction module consisting of an encoder-decoder structure to extract the depth-related reflectance. Through an ablation study, we demonstrate that the module can reduce the influence of illumination on depth estimation. (2) Boundary Detection. A boundary extraction module, consisting of an encoder, refinement block, and upsample block, was proposed to better predict the depth at object boundaries utilizing gradient constraints. (3) Depth Prediction Module. We use an encoder different from (2) to obtain depth features from the reflectance map and fuse boundary features to predict depth. In addition, we proposed FIFADataset, a depth-estimation dataset applied in soccer scenarios.
Results
Extensive experiments on a public dataset and our proposed FIFADataset show that our method achieves state-of-the-art performance.}
}
@article{TASFI2023110401,
title = {Dynamic Successor Features for transfer learning and guided exploration},
journal = {Knowledge-Based Systems},
volume = {267},
pages = {110401},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110401},
url = {https://www.sciencedirect.com/science/article/pii/S095070512300151X},
author = {Norman Tasfi and Eder Santana and Luisa Liboni and Miriam Capretz},
keywords = {Reinforcement learning, Successor features, Transfer learning, Deep learning},
abstract = {The Successor Feature framework for Reinforcement Learning algorithms improves task transfer by decomposing the learned state–action value function. The decomposition involves two components, one that captures future-expected state features and the other that models the task-related reward structure. However, successful transfer between tasks depends heavily on how the reward function changes, possibly leading to failure of the original Successor Feature formulation. This paper proposes the Dynamic Successor Feature framework, DynSF, by extending the mathematical formulation of the original Successor Feature framework to center around a learned state-transition model. Under this formulation, the state-transition model dynamically induces the acting policy. The flexibility of DynSF also extends to the architecture, requiring only a state-transition model and a small vector of parameters. This architecture provides immense flexibility in the choice of the model used to learn the state-transition model. The DynSF framework is evaluated and compared to other baseline algorithms through several experiments in a continuous grid world environment, a robotic Reacher, and pixels in the Doom environment.}
}
@article{TADEJA2024103919,
title = {Immersive presentations of real-world medical equipment through interactive VR environment populated with the high-fidelity 3D model of mobile MRI unit},
journal = {Computers & Graphics},
volume = {120},
pages = {103919},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103919},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000542},
author = {Sławomir Konrad Tadeja and Thomas Bohné and Kacper Godula and Artur Cybulski and Magdalena Maria Woźniak},
keywords = {Virtual reality, VR, Immersive interface, Magnetic resonance imaging, MRI, Virtual presentation},
abstract = {The primary goal behind the system presented in this paper is to investigate the efficacy of using virtual reality (VR) for showcasing sizable medical equipment. Specifically, we focused on a mobile magnetic resonance imaging (MRI) scanner mounted on a truck trailer. The latter is integral to the mobile MRI setup and must be presented as part of the immersive experience. Therefore, we not only have to depict the medical apparatus but also provide the means of understanding its surroundings. This is especially important to radiologists and other medical personnel to ascertain if a given mobile medical facility fulfills their needs and wants. Furthermore, despite such MRI devices being designed for mobility, their long-distance transportation can be time-consuming, troublesome and expensive. Therefore, we can observe the need for showcasing such mobile MRI units without additional cost and burden related to transportation. To achieve this, we designed an immersive environment in which the users can interact with the real-life scale 3D model of a mobile MRI. In addition, we also verified the usability and expressiveness of our system using established heuristical approaches.}
}
@article{GREIPL2021106946,
title = {When the brain comes into play: Neurofunctional correlates of emotions and reward in game-based learning},
journal = {Computers in Human Behavior},
volume = {125},
pages = {106946},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106946},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221002697},
author = {S. Greipl and E. Klein and A. Lindstedt and K. Kiili and K. Moeller and H.-O. Karnath and J. Bahnmueller and J. Bloechle and M. Ninaus},
keywords = {Emotions, Game-based learning, Human-computer interaction, Brain, FMRI, Reward},
abstract = {Accumulating evidence identifies emotions as drivers of effective learning. In parallel, game-based learning was found to emotionally engage learners, allegedly harnessing the fundamental tie between emotions and cognition. Questioning further whether and how game-based learning elicit emotional processes, the current fMRI study examined the neurofunctional correlates of game-based learning by directly comparing a game-based and a non-game-based version of a digital learning task. We evaluated neurofunctional activation patterns within a comprehensive set of brain areas involved in emotional and reward processes (e.g. amygdala or ventral tegmental area) when participants received feedback. With only a few exceptions, decoding of these brain areas’ activation patterns indicated predominantly stronger relative activation in the game-based task version. As such, our results substantiate on a neurofunctional level that game-based learning leads to an invigoration of learning processes through processes of reward and emotional engagement.}
}
@article{HUBER2023107948,
title = {Game elements enhance engagement and mitigate attrition in online learning tasks},
journal = {Computers in Human Behavior},
volume = {149},
pages = {107948},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107948},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002996},
author = {Stefan E. Huber and Rodolpho Cortez and Kristian Kiili and Antero Lindstedt and Manuel Ninaus},
keywords = {Educational games, Game-based learning, Games, Distance education and online learning, Media in education, Human-computer interface, Gamification},
abstract = {A growing body of literature suggests that adding game elements to learning tasks indirectly influences the learning process by increasing engagement with the tasks. The present study aims to advance learning engagement research by examining an often neglected subcomponent of behavioral engagement, attrition. Implementing two equivalent versions of a learning task, differing solely in the presence of game elements, allowed unequivocal attribution of any effect on the presence of game elements. Conducting the study in an online learning environment allowed further a highly unconstrained examination of the effects of game elements on attrition. We found that game elements affected both participant attrition and engagement of participants who completed the learning task. Participants with low self-efficacy were particularly prone to drop out in the non-game condition. Game elements also affected both learning efficacy and efficiency. We further found task attractivity to partially mediate the effect of game elements on learning outcomes. The results suggest that by facilitating engagement via task attractivity game elements can compensate to some extent for the increased cognitive demand that the game elements induce. We finally discuss the importance of considering the interrelations between learner characteristics, game elements, and engagement for interpreting results on learning performance measures.}
}
@article{SCHOFIELD2023101172,
title = {An improved semi-synthetic approach for creating visual-inertial odometry datasets},
journal = {Graphical Models},
volume = {126},
pages = {101172},
year = {2023},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2023.101172},
url = {https://www.sciencedirect.com/science/article/pii/S1524070323000036},
author = {Sam Schofield and Andrew Bainbridge-Smith and Richard Green},
keywords = {Visual inertial odometry, Semi-synthetic dataset, Robot virtual reality},
abstract = {Capturing outdoor visual-inertial datasets is a challenging yet vital aspect of developing robust visual-inertial odometry (VIO) algorithms. A significant hurdle is that high-accuracy-ground-truth systems (e.g., motion capture) are not practical for outdoor use. One solution is to use a “semi-synthetic” approach that combines rendered images with real IMU data. This approach can produce sequences containing challenging imagery and accurate ground truth but with less simulated data than a fully synthetic sequence. Existing methods (used by popular tools/datasets) record IMU measurements from a visual-inertial system while measuring its trajectory using motion capture, then rendering images along that trajectory. This work identifies a major flaw in that approach, specifically that using motion capture alone to estimate the pose of the robot/system results in the generation of inconsistent visual-inertial data that is not suitable for evaluating VIO algorithms. However, we show that it is possible to generate high-quality semi-synthetic data for VIO algorithm evaluation. We do so using an open-source full-batch optimisation tool to incorporate both mocap and IMU measurements when estimating the IMU’s trajectory. We demonstrate that this improved trajectory results in better consistency between the IMU data and rendered images and that the resulting data improves VIO trajectory error by 79% compared to existing methods. Furthermore, we examine the effect of visual-inertial data inconsistency (as a result of trajectory noise) on VIO performance to provide a foundation for future work targeting real-time applications.}
}
@article{MELEIRO201466,
title = {Natural User Interfaces in the Motor Development of Disabled Children},
journal = {Procedia Technology},
volume = {13},
pages = {66-75},
year = {2014},
note = {SLACTIONS 2013: Research conference on virtual worlds – Learning with simulations},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314000206},
author = {Pedro Meleiro and Rui Rodrigues and João Jacob and Tiago Marques},
keywords = {Natural User Interfaces, motor disability, skeleton tracking, motion sensor, assisted rehabilitation},
abstract = {This study describes a framework based upon body tracking devices and aimed at assisting children with motor impairments and aims at understanding what positive contribute it can deliver for their rehabilitation process. A case study is defined featuring two motor disorders that take advantage of the technological specifications, as well as the types of exercise appropriate for this context. The developed framework collects motricity data by asking the user to mimic the movements of a previously recorded exercise, and is thoroughly detailed in this paper. The results obtained evidence the data collected regarding the user performance denotes certain motor patterns of the disorder, making it apt to be applied as an auxiliary tool for impairments diagnosis. A few tracking issues indicate that the technologies selected can be applied in a real context to assist in rehabilitation sessions, but require additional evaluation metrics to support its conclusions.}
}
@article{SETIONO2021781,
title = {Enhancing Player Experience in Game With Affective Computing},
journal = {Procedia Computer Science},
volume = {179},
pages = {781-788},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921000843},
author = {Doyo Setiono and David Saputra and Kaleb Putra and Jurike V. Moniaga and Andry Chowanda},
keywords = {Affective Computing, Facial Expression Recognition, Virtual Pet Simulation, Game Experiences},
abstract = {This research aims to implement Affective Computing as part of game design by capturing, processing, and interpreting the player’s emotions to enhance the player’s experience in the game. We argue by implementing affective computing in the game will statistically enhance the players game experiences. Two almost identical games designed and developed to prove the claim. One game imbued with the affective computing system by capturing players emotions from their facial expressions as the game input, when the other game only implement a touch screen system for the input. Both games then were evaluated in two groups of 50 respondents in each group. A combination of Game Experiences Questionnaire and Immersiveness Game Questionnaire used to evaluate the game player experiences in this research. The result concludes that most of the game experiences score in the game with FER system implemented was statistically increased compared to the one with-out the FER system implemented, except for Q3 (p = 0.06), and Q9 (p = 0.08).}
}
@article{SVATON20141445,
title = {Improving Strategy in Robot Soccer Game by Sequence Extraction},
journal = {Procedia Computer Science},
volume = {35},
pages = {1445-1454},
year = {2014},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.08.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914011697},
author = {Václav Svatoň and Jan Martinovič and Kateřina Slaninová and Tomáš Bureš},
keywords = {robot soccer, strategy, rule, sequence},
abstract = {Robot Soccer is a very attractive platform in terms of research. It contains a number of challenges in the areas of robot control, artificial intelligence and image analysis. This article presents a look at the overall architecture of the game and describes some results of our experiments in analysis and optimization of strategies using sequence extraction. We have extracted sequences of game situations from the log of a game played in our simulator, as they occurred during the game. Afterwards, these sequences were compared by methods LCS, LCSS and T-WLCS, which are usually used for sequence comparison in the sequence alignment area. Using these methods, we are able to visualize the relations between the sequences of game situations and clusters of similar game situations in a graph. In conclusion, a possible description improvement of these game situations is introduced. Therefore, a possible strategy improvement to ensure a smoother and faster performing of actions defined by these situations is described.}
}
@article{AKBAR2019388,
title = {Enhancing Game Experience with Facial Expression Recognition as Dynamic Balancing},
journal = {Procedia Computer Science},
volume = {157},
pages = {388-395},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.230},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311494},
author = {M. Taufik Akbar and M. Nasrul Ilmi and Imanuel V. Rumayar and Jurike Moniaga and Tin-Kai Chen and Andry Chowanda},
keywords = {Dynamic Balancing, Player’s Experiences, Game Technology, Facial Expression Recognition},
abstract = {Player’s Experience in the game has been known to be one of the essential keys for the success of the game. There are several methods exist to enhance the player’s experiences in the games. One of the unexplored methods is a dynamic balancing system using Facial Expression Recognition. The player’s facial expression is captured in real-time while the player is playing the game, and the dynamic balancing system will automatically adjust the game difficulty based on the player’s facial expressions. This research aims to empirically explore the implementation of Facial Expression Recognition for a dynamic balancing system to enhance the player’s experiences in the game. Two action games (2D and 3D) were developed and evaluated with 60 respondents in two groups. Both groups played the game twice, one with facial expression recognition system as dynamic balancing activated and one without. The results demonstrate that they are statistically significant differences (i.e. improvement) between the baseline and enhanced games with p < 0.01.}
}
@article{MARTINEZPERNIA201771,
title = {Using game authoring platforms to develop screen-based simulated functional assessments in persons with executive dysfunction following traumatic brain injury},
journal = {Journal of Biomedical Informatics},
volume = {74},
pages = {71-84},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417301910},
author = {David Martínez-Pernía and Javier Núñez-Huasaf and Ángel {del Blanco} and Amparo Ruiz-Tagle and Juan Velásquez and Mariela Gomez and Carl {Robert Blesius} and Agustin Ibañez and Baltasar Fernández-Manjón and Andrea Slachevsky},
keywords = {Functional evaluation, Naturalistic activities, Executive functions, Prefrontal cortex, Neuropsychological assessment, eAdventure},
abstract = {The assessment of functional status is a critical component of clinical neuropsychological evaluations used for both diagnostic and therapeutic purposes in patients with cognitive brain disorders. There are, however, no widely adopted neuropsychological tests that are both ecologically valid and easily administered in daily clinical practice. This discrepancy is a roadblock to the widespread adoption of functional assessments. In this paper, we propose a novel approach using a serious game authoring platform (eAdventure) for creating screen-based simulated functional assessments. We created a naturalistic functional task that consisted of preparing a cup of tea (SBS-COT) and applied the assessment in a convenience sample of eight dyads of therapists/patients with mild executive dysfunction after traumatic brain injury. We had three main aims. First, we performed a comprehensive review of executive function assessment in activities of daily living. Second, we were interested in measuring the feasibility of this technology with respect to staffing, economic and technical requirements. Third, a serious game was administered to patients to study the feasibility of this technology in the clinical context (pre-screening test). In addition, quantitative (Technology Acceptance Model (TAM) questionnaires) and qualitative (semistructured interviews) evaluations were applied to obtain user input. Our results suggest that the staffing, economic and technical requirements of the SBS-COT are feasible. The outcomes of the pre-screening test provide evidence that this technology is useful in the functional assessment of patients with executive dysfunction. In relation to subjective data, the TAM questionnaire showed good user acceptability from a professional perspective. Interview analyses with professionals and patients showed positive experiences related to the use of the SBS-COT. Our work indicates that the use of these types of authoring platforms could have positive long-term implications for neuropsychological research, opening the door to more reproducible, cooperative and efficient research by allowing the facilitated production, reuse and sharing of neuropsychological assessment tools.}
}
@article{AWADALLAH2024102652,
title = {Remote collaborative framework for real-time structural condition assessment using Augmented Reality},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102652},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102652},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003008},
author = {Omar Awadallah and Katarina Grolinger and Ayan Sadhu},
keywords = {Augmented Reality, Structural Health Monitoring, Building Information Modelling, Real-time Collaboration, Structural Condition Assessment, Microsoft HoloLens},
abstract = {Civil structures worldwide are confronted with a growing threat of structural deterioration, aggravated by various factors such as climate change, population growth, and increased traffic. The latent nature of these issues often leads to undetected vulnerabilities until a catastrophic failure occurs, resulting in substantial losses. To address this challenge, there is a critical need for improved structural monitoring, condition assessment, and maintenance practices. Traditional inspection methods, relying on visual estimation and heavy equipment for inaccessible areas, present formidable obstacles to inspectors. These methods impede the safe and fast examination of structural damage, complicating tracking of structural deterioration, and hindering efficient condition assessment. Recognizing these challenges, this paper proposes a remote collaborative framework to enhance the efficiency of structural inspections by leveraging the capabilities of Augmented Reality (AR), QR code, and 5G network. The proposed framework centers on real-time remote collaboration among on-site and off-site inspectors, aiming to elevate safety, accessibility, and overall inspection efficacy. The integration of real-time data sharing and collaboration facilitates immediate decision-making, enabling inspectors to proactively address structural vulnerabilities and prevent potential failures. This study concludes that the proposed framework effectively facilitates real-time structural condition assessment for on-site AR users. Simultaneously, off-site web users can instantly track the progression of data over time through the utilization of 5G technology. The proposed advanced AR framework effectively demonstrates real-time structural condition assessment through a lab-scale experimental beam and a full-scale bridge.}
}
@article{LOPES2022103489,
title = {A survey on RGB-D datasets},
journal = {Computer Vision and Image Understanding},
volume = {222},
pages = {103489},
year = {2022},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2022.103489},
url = {https://www.sciencedirect.com/science/article/pii/S1077314222000923},
author = {Alexandre Lopes and Roberto Souza and Helio Pedrini},
keywords = {RGB-D data, Monocular depth estimation, Computer vision, Depth datasets},
abstract = {RGB-D data is essential for solving many problems in computer vision. Hundreds of public RGB-D datasets containing various scenes, such as indoor, outdoor, aerial, driving, and medical, have been proposed. These datasets are useful for different applications and are fundamental for addressing classic computer vision tasks, such as monocular depth estimation. This paper reviewed and categorized image datasets that include depth information. We gathered 231 datasets that contain accessible data and grouped them into three categories: scene/objects, body, and medical. We also provided an overview of the different types of sensors, depth applications, and we examined trends and future directions of the usage and creation of datasets containing depth data, and how they can be applied to investigate the development of generalizable machine learning models in the monocular depth estimation field.}
}
@article{JAZIRI20211152,
title = {ORVIPO: An Ontological Prototype for Modeling 3D Scenes in Operating Rooms},
journal = {Procedia Computer Science},
volume = {192},
pages = {1152-1161},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016070},
author = {Faouzi Jaziri and Rim Messaoudi and Achraf Mtibaa and Jonathan Courbon and Mahdi Kilani and Mohamed Mhiri and Antoine Vacavant},
keywords = {Virtual reality, Ontology, Semantic interoperability, Reasoning rules, Interaction},
abstract = {Virtual Operating room characterizes a large artificial environment of interaction in the medical context. It enables a better simulation for different surgical scenarios through 3D (three‐dimensional) objects. Virtual Reality (VR) uses computer technology to develop these virtual simulated applications. It brings valuable innovations in different fields. It is integrated also to enhance healthcare and therapies. To improve and assist on medical VR applications, ontologies would be useful. They can unify the content of these applications and facilitate their implementation via semantic rules. Also, ontologies can be applied to realize an effective VR knowledge modeling. This paper presents a virtual simulation of an operating room taking advantage of the semantic representation. The goal of this study is to propose a novel ontological VR system that models an operating room and its components. The execution of the proposed method was proved by the developed ontology OROnto (Operating Room Ontology). This ontology was useful for modeling hospital scenarios and the construction of a valuable VR system. To demonstrate the proposed approach feasibility and performance, we have implemented the Operating Room Virtual Integration Process Using Ontology (ORVIPO) prototype. Compared to different other ontological methods and related works, our approach have shown interesting findings such as recall (71%), precision (83%), and F-measure (76%).}
}
@article{PRANOTO2019506,
title = {Increase The Interest In Learning By Implementing Augmented Reality: Case studies studying rail transportation.},
journal = {Procedia Computer Science},
volume = {157},
pages = {506-513},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311652},
author = {Hady Pranoto and Francisco Maruli Panggabean},
keywords = {Augmented Reality, learning method, rail transportation},
abstract = {Learn a subject, for some people, might be an uninteresting and boring activity, especially when the subject to learn are difficult subjects to understand. Many methods used to change learning activities become more enjoyable and interested. This study proposed a new method in learning activities, by applied augmented reality technology in the learning process. The case study used in this paper are implementation the augmented reality in studied subjects related to train technology. In this study, author implement augmented reality on learning material, combines real and virtual things in one media, in this case a mobile device. The impact of implementation of augmented studied, at the end of experiment, author can conclude when implement augmented reality technology in learning material helps the learning process and increasing the impressive and fun factor in learning process and make the learning process more interested. Implementation of Augmented Reality in learning material gives more information about the object being studied, information about on shapes, textures, and provide more visualization for the object.}
}
@article{CALZADOMARTINEZ2022e00235,
title = {Accessing interactively the spatio-temporal data-model of an archaeological site through its 3D virtual reconstruction},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {26},
pages = {e00235},
year = {2022},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2022.e00235},
url = {https://www.sciencedirect.com/science/article/pii/S2212054822000248},
author = {Alberto Calzado-Martínez and Ángel-Luis García-Fernández and Lidia M. Ortega-Alvarado},
keywords = {Spatio-temporal database, Virtual 3D archaeological site reconstruction, Topological relationship, 3D navigation and interaction},
abstract = {Information and communication technologies are increasingly used in all archaeological processes. However, archaeologists sometimes consider them as intrusive, too far from the traditional work methodology and even a hindrance. In this article we propose a framework to allow natural and therefore intuitive access to the archaeological record. The information retrieval process is carried out through a three-dimensional virtual reconstruction of the archaeological site. In this system, navigation and interaction with the three-dimensional elements of the environment triggers database queries. To achieve such functionality, a client-server architecture is designed in which the server maintains a spatio-temporal database with heterogeneous information, including the 3D models of the finds. To do this, a virtual replica of the site is created considering spatial restrictions and topological relationships among the finds.}
}
@article{KAJIHARA2019106956,
title = {Non-rigid registration of serial section images by blending transforms for 3D reconstruction},
journal = {Pattern Recognition},
volume = {96},
pages = {106956},
year = {2019},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0031320319302535},
author = {Takehiro Kajihara and Takuya Funatomi and Haruyuki Makishima and Takahito Aoto and Hiroyuki Kubo and Shigehito Yamada and Yasuhiro Mukaigawa},
keywords = {Image registration, Non-rigid deformation, Transformation blending},
abstract = {In this research, we propose a novel registration method for three-dimensional (3D) reconstruction from serial section images. 3D reconstructed data from serial section images provides structural information with high resolution. However, there are three problems in 3D reconstruction: non-rigid deformation, tissue discontinuity, and accumulation of scale change. To solve the non-rigid deformation, we propose a novel non-rigid registration method using blending rigid transforms. To avoid the tissue discontinuity, we propose a target image selection method using the criterion based on the blending of transforms. To solve the scale change of tissue, we propose a scale adjustment method using the tissue area before and after registration. The experimental results demonstrate that our method can represent non-rigid deformation with a small number of control points, and is robust to a variation in staining. The results also demonstrate that our target selection method avoids tissue discontinuity and our scale adjustment reduces scale change.}
}
@article{DIAZ2015205,
title = {How the Type of Content in Educative Augmented Reality Application Affects the Learning Experience},
journal = {Procedia Computer Science},
volume = {75},
pages = {205-212},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.239},
url = {https://www.sciencedirect.com/science/article/pii/S187705091503700X},
author = {Christian Diaz and Mauricio Hincapié and Gustavo Moreno},
keywords = {static and dynamic contents, augmented reality, fundamentals of electronics.},
abstract = {Nowadays, the use of technology to improve teaching and learning experiences in the classroom has been promoted. One of these technologies is augmented reality, which allows overlaying layers of virtual information on real scene with the aim of increasing the perception the user has of reality. In the educational context augmented reality have proved to offer several advantages, i.e. increasing learning engagement and increasing understanding of some topics, especially when spatial skills are involved. Contents deployed in an augmented reality application are of two types, static, i.e. text, or dynamic, i.e. animations. As far as we know no research project has assessed how the type of content, static or dynamic, can affect the student learning experience in augmented reality applications. In this article the development and evaluation of an augmented reality application using static and dynamic content is described. In order to determine how the type of content affects the learning experience of the student, an experimental design in which the student interact with the application, using static and dynamic contents, for learning topics related with an electronic fundamentals course was performed.}
}
@article{BIRCH2018242,
title = {Crowdsourcing with online quantitative design analysis},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {242-251},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617306018},
author = {David Birch and Alvise Simondetti and Yi-ke Guo},
keywords = {Design, Crowdsourcing, Architecture, Cloud, Masterplanning, Optimization},
abstract = {Design is a balancing act between people’s competing concerns, design options and design performance. Recently collecting data on such concerns such as sustainability or aesthetics has become possible through online crowdsourcing, particularly in 3d. However, such systems rarely present more than a single design alternative or allow users to change the design and seldom provide quantitative design analysis to gauge design performance. This precludes a more participatory approach including a wider audience and their insight in the design process. To improve the design process we propose a system to assist the design team in exploring the balance of concerns, design options and their performance. We augment a 3d visualisation crowdsourcing environment with quantitative on-demand assessment of design variants run in the cloud. This enables crowdsourced exploration of the design space and its performance. Automated participant tracking and explicit submitted feedback on design options are collated and presented to aid the design team in balancing the demands of urban master planning. We report application of this system to an urban masterplan with Arup.}
}
@article{KADYR2024341,
title = {Affective computing methods for simulation of action scenarios in video games},
journal = {Procedia Computer Science},
volume = {231},
pages = {341-346},
year = {2024},
note = {14th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 13th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (EUSPN/ICTH 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.214},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923022263},
author = {Sarsenbek Kadyr and Chinibayeva Tolganay},
keywords = {Affective computing, affective loop, video game, procedural content generation, fuzzy logic, artificial neural networks},
abstract = {Video games are becoming a part of the lives of many, many people. They are able to evoke a wide range of emotions, from joy and excitement to fear and empathy, creating deep immersion and memorable impressions for players. How do they manage it and is it possible to use the emotions themselves in the game itself? This article presents the concept of developing a system of dynamic procedural content generation (PCG) with game mechanics of interaction using emotions based on methods affective computing. To achieve this goal, The article developed an emotion recognition system based on fuzzy logic. And PCG performed by Artificial Neural Networks. In the video game industry, affective computing methods, such as interaction with the expression of emotions, can bring great benefits, since the emotional involvement of the player is very important. This will make a new contribution to the diversity of human-computer interaction.}
}
@article{KOSTOV2022896,
title = {Designing a Framework for Collaborative Mixed Reality Training},
journal = {Procedia Computer Science},
volume = {200},
pages = {896-903},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.287},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002964},
author = {Georgi Kostov and Josef Wolfartsberger},
keywords = {multi-device, collaboration, tool, virtual reality, augmented reality, network, human-computer interaction},
abstract = {Training simulations in Virtual Reality (VR) benefit from the technology’s interactive and intuitive nature. Typically, the VR user is isolated in the virtual environment and an external instructor or spectators cannot support the learning progress. Previous research work presents solutions for isolated use cases, but lacks specific guidelines for the implementation of collaborative multi-device systems. We propose a method for expanding collaborative extended reality (XR) applications to multiple platforms to support collaborative learning, but also other types of applications such as remote collaboration and maintenance. In order to test our approach, we expanded an existing application to four different platforms - Steam VR, Microsoft Mixed Reality, smartphone/tablet and desktop computer. Based on lessons learned from our prototype, we propose a solution for multi-device networking and interaction on each platform. The main strengths of our approach are in the decoupling of local and networked objects and the common interface for interaction, which accommodates multiple platforms. We believe our approach can provide useful insights into collaborative training and serve as a good starting point for future projects.}
}
@article{SIVANATHAN2012103,
title = {Temporal Synchronisation of Data Logging in Racing Gameplay},
journal = {Procedia Computer Science},
volume = {15},
pages = {103-110},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008241},
author = {Aparajithan Sivanathan and Theodore Lim and Sandy Louchart and James Ritchie},
keywords = {Data logging, Synchronisation and encoding framework, EEG, telemetry, Psychophysiology, Biofeedback},
abstract = {Contemporary game play is a complex environment where user interacts with many elements. Data logging in a game play is a common practice to study a player's in-game behaviour. Identified behavioural knowledge is then applied to enhance the game elements. Analyzing the user behaviour is a multivariate process and therefore requires more information than mere logging of game context. Multi modal data channels such as biofeedback signals are increasingly used to study the game play so that more detailed understanding of the user behaviour can be established. Precise synchronisation in capturing of multiple streams is essential to produce accurate and meaningful information. This paper presents a generic technique to accurately synchronize multiple data streams captured during a gaming session. It is demonstrated by applying it to a driving game Simulation. Observable correlation between the in-game data and psycho-physiological signals are presented to demonstrate the accuracy and granularity of the synchronisation.}
}
@article{COSINAYERBE202275,
title = {Clustered voxel real-time global illumination},
journal = {Computers & Graphics},
volume = {103},
pages = {75-89},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S009784932200005X},
author = {Alejandro {Cosin Ayerbe} and Gustavo Patow},
keywords = {Real-time, Global illumination, Voxelization, Clustering, Diffuse surfaces},
abstract = {Real-time global illumination is an extremely challenging problem because of its intrinsic complexity due to the interplay between complex geometry, multiple light bounces, and stringent real-time frame-rate requirements. In this paper, we present a new technique that enables the real-time computation of global illumination in generic scenes for diffuse surfaces and static geometry. Our technique combines a voxel-based representation of the scene, a voxel-clustering algorithm, and an iterative light-propagation algorithm based on the resulting clusters. Although our implementation is currently designed to handle the first (and main) bounce, the technique allows for multiple light bounces. Summing up all these components results in a flexible algorithm capable of providing global illumination effects and on-the-fly illumination computations.}
}
@article{BORAWSKA20181616,
title = {The Concept of Virtual Reality System to Study the Media Message Effectiveness of Social Campaigns},
journal = {Procedia Computer Science},
volume = {126},
pages = {1616-1626},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.135},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314133},
author = {Anna Borawska and Mariusz Borawski and Małgorzata Łatuszyńska},
keywords = {Virtual Reality, Social campaign, Multimedia message},
abstract = {Social campaigns are an important tool for promoting positive change in social attitudes (in ecology, health prevention, tolerance promotion, etc.). Improving their effectiveness may therefore have a very tangible effect on many aspects of life – both for individuals and for whole societies. Among the most widespread activities that are undertaken within the framework of social campaigns, one can mention advertising through different types of media – television, radio, internet and print. Assessment of this element is done mostly on the basis of questionnaires and focus groups. Research of this type relies on measures that are proximal, such as perceived effectiveness. Although it is assumed that perceived effectiveness is causally antecedent to actual effectiveness, it would be advisable to find the way of assessing actual effectiveness more directly. One of the approaches, that aim to solve this problem is application of the tools of cognitive neuroscience. The use of cognitive neuroscience techniques for pretesting media messages in social campaigns requires properly designed research. In order to do that, the pivotal stage of each experiment design is the choice of media stimuli that will be presented to the participants. Different media and contents will lead to different patterns of responses in viewers, determining a great deal about how a message is processed, including which parts of the message are attended to, and how the message is evaluated and liked. Stimuli presented during the experiment can be static, like pictures or dynamic, like video. However, such stimuli may not be representative of real-life situations. To avoid problems with picture and video stimuli, one can use virtual environments. The aim of the article is to present a concept of virtual reality system that could enable the research of media messages effectiveness in social campaigns.}
}
@article{ISIK2024483,
title = {Integrating extended reality in industrial maintenance: a game-based framework for compressed air system training},
journal = {Procedia Computer Science},
volume = {232},
pages = {483-492},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924000486},
author = {Birkan Isik and Gulbahar Emir Isik and Miroslav Zilka},
keywords = {Extended reality, Serious game, Maintenance training, Compressed Air System},
abstract = {In industrial environments, maintenance technologies developed under Industry 5.0 are important to ensure trouble-free production and reduce downtime. Compressed Air Systems (CASs) are an integral part of industrial production processes and require competent and correct maintenance. Maintenance personnel should be equipped with continuous and up-to-date training and their knowledge should be tested. There is a significant gap in the field of interactive educational games to improve the technical skills of the care staff, especially in the field of CAS. This research proposes a theoretical framework for a game based on Extended Reality (XR) technologies adapted for CAS maintenance training. The game leverages the trio of Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) to provide immersive and interactive learning encounters aimed at improving technical proficiency, problem-solving skills, and safety awareness among maintenance workers. The aim is to take advantage of the rewards of game-oriented learning, such as enhanced user engagement and knowledge absorption, to increase the effectiveness of training. In this context, the CAS game framework was created with three frameworks: education, game, and CAS. The study concludes by highlighting the potential benefits, research areas, and technologies that underpin the proposed structure.}
}
@article{BOTTANI2021103429,
title = {Wearable and interactive mixed reality solutions for fault diagnosis and assistance in manufacturing systems: Implementation and testing in an aseptic bottling line},
journal = {Computers in Industry},
volume = {128},
pages = {103429},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103429},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000361},
author = {Eleonora Bottani and Francesco Longo and Letizia Nicoletti and Antonio Padovano and Giovanni Paolo Carlo Tancredi and Letizia Tebaldi and Marco Vetrano and Giuseppe Vignali},
keywords = {Augmented reality, Smart technologies, System usability scale, Industry 4.0, Wearable technologies, Industrial safety},
abstract = {Thanks to the spread of technologies stemming from the fourth industrial revolution, also the topic of fault diagnosis and assistance in industrial contexts has benefited. Indeed, several smart tools were developed for assisting with maintenance and troubleshooting, without interfering with operations and facilitating tasks. In line with that, the present manuscript aims at presenting a web smart solution with two possible applications installed on an Android smartphone and Microsoft HoloLens. The solution aims at alerting the operators when an alarm occurs on a machine through notifications, and then at providing the instructions needed for solving the alarm detected. The two devices were tested by the operators of an industrial aseptic bottling line consisting of five machines in real working conditions. The usability of both devices was positively rated by these users based on the System Usability Scale (SUS) and additional appropriate statements. Moreover, the in situ application brought out the main difficulties and interesting issues for the practical implementation of the solutions tested.}
}
@article{BORAWSKI20213777,
title = {The use of a computer game in a social campaign to improve road safety},
journal = {Procedia Computer Science},
volume = {192},
pages = {3777-3786},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.152},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921018913},
author = {Mariusz Borawski and Anna Borawska},
keywords = {social campaign, road safety, cognitive neuroscience techniques},
abstract = {Road traffic deaths and injuries remain a major global concern and current trends suggest that it would not change in the near future. To reverse this tendency, numerous actions are being taken to improve road safety. The implementation and enforcement of legislation on key risk factors are main elements of an integrated strategy to prevent road fatalities in most countries. Legislation should, however, be supported by effective enforcement of rights and social campaigns. Such campaigns appear in a variety of media (TV, radio, newspapers, magazines, etc.). In addition, relatively new communication channels are also used, such as various options of social media. Moreover, social campaigns can also use computer games. The latter medium seems to be particularly interesting and useful in the case of campaigns promoting safe driving, as the age group responsible for the most car accidents to a large extent also leads in the statistics of players. The main difficulty in applying such an approach is to prepare an appropriate game that would be able to interest and engage a potential player and at the same time influence her/his awareness of safe driving. The aim of this article is therefore to present a proposal of a computer game that could be used in a campaign to promote safe driving among members of a specific age group. Additionally, a project of a research experiment was prepared to check the effectiveness of the proposed game in a social campaign.}
}
@article{OYELERE2023100016,
title = {Formative evaluation of immersive virtual reality expedition mini-games to facilitate computational thinking},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100016},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100016},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000107},
author = {Amos Sunday Oyelere and Friday Joseph Agbo and Solomon Sunday Oyelere},
keywords = {Virtual reality, Metaverse, Game-based learning, Computational thinking, Head mounted display, Higher education},
abstract = {Recently, virtual reality (VR) technology has shown great potential in advancing education with many pedagogical benefits for building the 21st-century teaching and learning experience. This study conducted a formative evaluation of an immersive VR expedition application with the aim of understanding users' learning processes and how the application facilitates higher education students' computational thinking skills. Six participants were randomly selected to conduct this evaluation. A mixed research approach consisting of quantitative and qualitative methods was employed. The study quantitatively analyzed users' scores from gameplay to understand how the intervention supported computational thinking skills. Participants were also interviewed to collect data after playing the mini-games to investigate users' experiences. The study showcases players' computational thinking competency, assessed automatically during gameplay. Further, this study used inductive content analysis to demonstrate users' reactions to prototyped VR mini-games. The qualitative findings suggest that users found the VR mini-games interactive and immersive, which provided an opportunity to foster learners' computational thinking skills. The quantitative analysis revealed that student's computational thinking competency can be enhanced through consistent playing of the mini-games. Moreover, the expedition aspect of the VR game stimulated learners' curiosity, which sustained their learning progress. Furthermore, users gained new knowledge and found the mini-games educative. Nevertheless, several aspects of the VR mini-games need improvements, according to users' perceptions. This study contributes to the knowledge in terms of the affordances of VR in education research and provides relevant insights that can shape future studies, for example, the recent hype of metaverse in education.}
}
@article{SCOROLLI2023107910,
title = {Would you rather come to a tango concert in theater or in VR? Aesthetic emotions & social presence in musical experiences, either live, 2D or 3D},
journal = {Computers in Human Behavior},
volume = {149},
pages = {107910},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107910},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002613},
author = {Claudia Scorolli and Eduardo {Naddei Grasso} and Lorenzo Stacchio and Vincenzo Armandi and Giovanni Matteucci and Gustavo Marfia},
keywords = {Musical aesthetic experience, Immersive concert, Virtual reality, Aesthetic emotions, Social presence, Embodied experience, Psychology of art, Empirical aesthetics},
abstract = {This work shows the preliminary results of a pioneering project aimed at comparing the aesthetic experience of a musical concert experienced in different contexts for which the audience's perceived presence is modulated in a continuum ranging from a live concert to a music video, passing through immersive artificial environments. In contrast to previous qualitative investigations of various immersive contexts, our study is unique in both the use of validated scales and the structured comparison of four experimental conditions: 1.live concert (LC), 2.the same concert through a traditional non-immersive music video (MV, analogous to fruition on YouTube), and finally in a virtual reality environment (VR), provided by two different devices, 3. a google cardboard (CVR) and 4. an HTC vive (HVR), allowing respectively for a basic and easily accessible experience, or for a less affordable but more immersive one. Through these manipulations we presumably affected not just the subjective aesthetic experience, but also the perceived presence of the Other/s. Consistently we measured both through the administration of the Aesthetic Emotions Scale (Aesthemos) and the Networked Minds Measure of Social Presence (NMMSP). The NMMSP showed no notable differences between conditions, which instead emerged from the analyses on the Aesthemos. The most liked experience was the Live one. Results also showed that LC experience had a stronger emotional impact only when compared to MV and CVR, but not to HTC since this last manipulation was the one eliciting the greatest interest. Theoretical implications are critically discussed, suggesting novel applications of the proposed approach.}
}
@article{DAI2024103881,
title = {Wavelet-based network for high dynamic range imaging},
journal = {Computer Vision and Image Understanding},
volume = {238},
pages = {103881},
year = {2024},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2023.103881},
url = {https://www.sciencedirect.com/science/article/pii/S1077314223002618},
author = {Tianhong Dai and Wei Li and Xilei Cao and Jianzhuang Liu and Xu Jia and Ales Leonardis and Youliang Yan and Shanxin Yuan},
keywords = {High dynamic range image, Ghosting artifacts, Discrete Wavelet Transform, RAW HDR imaging dataset},
abstract = {High dynamic range (HDR) imaging from multiple low dynamic range (LDR) images has been suffering from ghosting artifacts caused by scene and objects motion. Existing methods, such as optical flow based and end-to-end deep learning based solutions, are error-prone either in detail restoration or ghosting artifacts removal. Comprehensive empirical evidence shows that ghosting artifacts caused by large foreground motion are mainly low-frequency signals and the details are mainly high-frequency signals. In this work, we propose a novel frequency-guided end-to-end deep neural network (FHDRNet) to conduct HDR fusion in the frequency domain, and Discrete Wavelet Transform (DWT) is used to decompose inputs into different frequency bands. The low-frequency signals are used to avoid specific ghosting artifacts, while the high-frequency signals are used for preserving details. Using a U-Net as the backbone, we propose two novel modules: merging module and frequency-guided upsampling module. The merging module applies the attention mechanism to the low-frequency components to deal with the ghost caused by large foreground motion. The frequency-guided upsampling module reconstructs details from multiple frequency-specific components with rich details. In addition, a new RAW dataset is created for training and evaluating multi-frame HDR imaging algorithms in the RAW domain. Extensive experiments are conducted on public datasets and our RAW dataset, showing that the proposed FHDRNet achieves state-of-the-art performance.}
}
@article{TAKAHASHI201531,
title = {A Generic Software Platform for Brain-inspired Cognitive Computing},
journal = {Procedia Computer Science},
volume = {71},
pages = {31-37},
year = {2015},
note = {6th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2015, 6-8 November Lyon, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.185},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036467},
author = {Koichi Takahashi and Kotone Itaya and Masayoshi Nakamura and Moriyoshi Koizumi and Naoya Arakawa and Masaru Tomita and Hiroshi Yamakawa},
keywords = {software platform, cognitive architecture, machine learning, modularity, the whole brain architecture},
abstract = {We have been developing BriCA (Brain-inspired Computing Architecture), the generic software platform that can combine an arbitrary number of machine learning modules to construct higher structures such as cognitive architectures inspired by the brain. We discuss requirements analysis and design principles of this cognitive computing platform, report its implementation, and describe plans for further development.}
}
@article{WANG2024102608,
title = {A deep learning-enhanced Digital Twin framework for improving safety and reliability in human–robot collaborative manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {85},
pages = {102608},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102608},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000832},
author = {Shenglin Wang and Jingqiong Zhang and Peng Wang and James Law and Radu Calinescu and Lyudmila Mihaylova},
keywords = {Safe human–robot collaboration (HRC), Intelligent sensing, Digital Twin, Semi-supervised deep learning framework},
abstract = {In Industry 5.0, Digital Twins bring in flexibility and efficiency for smart manufacturing. Recently, the success of artificial intelligence techniques such as deep learning has led to their adoption in manufacturing and especially in human–robot collaboration. Collaborative manufacturing tasks involving human operators and robots pose significant safety and reliability concerns. In response to these concerns, a deep learning-enhanced Digital Twin framework is introduced through which human operators and robots can be detected and their actions can be classified during the manufacturing process, enabling autonomous decision making by the robot control system. Developed using Unreal Engine 4, our Digital Twin framework complies with the Robotics Operating System specification, and supports synchronous control and communication between the Digital Twin and the physical system. In our framework, a fully-supervised detector based on a faster region-based convolutional neural network is firstly trained on synthetic data generated by the Digital Twin, and then tested on the physical system to demonstrate the effectiveness of the proposed Digital Twin-based framework. To ensure safety and reliability, a semi-supervised detector is further designed to bridge the gap between the twin system and the physical system, and improved performance is achieved by the semi-supervised detector compared to the fully-supervised detector that is simply trained on either synthetic data or real data. The evaluation of the framework in multiple scenarios in which human operators collaborate with a Universal Robot 10 shows that it can accurately detect the human and robot, and classify their actions under a variety of conditions. The data from this evaluation have been made publicly available, and can be widely used for research and operational purposes. Additionally, a semi-automated annotation tool from the Digital Twin framework is published to benefit the collaborative robotics community.}
}
@article{BIERCEWICZ20221509,
title = {The improvements propositions for players’ engagement and sustainable behaviors in managerial games},
journal = {Procedia Computer Science},
volume = {207},
pages = {1509-1518},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.208},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010912},
author = {Konrad Biercewicz and Adam Sulich and Letycja Sołoducho-Pelc},
keywords = {Business Training, E-learing, Gamification, Knowledge Representation, Management},
abstract = {The growing game's popularity is caused by their possibilities in decision-making training. Management games allow avoiding potential mistakes related to the business choices, but they require several updates and improvements in terms of context and new trends. The business environment is constantly changing and requires new creative approaches to be trained. One of them is the business greening and realizing the idea of sustainable behavior and engagement in managerial games. This paper aims to propose managerial game improvements to achieve better game engagement and become more interesting for students because of its real business world connection. The object of the research and analysis is the Marketplace®, which is the educational managerial game. In this paper, the game analysis three levels were presented to propose the recommendations for the Marketplace game development. The main finding is that an assessment survey should occur in the game climax, to engage and immerse students in the game. The survey questions should also be reexamined to develop a more dynamic game analysis. Then the game itself has to consist of sustainable development of practical solutions and enable students’ creativity to raise their engagement.}
}
@article{SCHOFIELD2022200061,
title = {An improved semi-synthetic approach for creating visual-inertial odometry datasets},
journal = {Graphics and Visual Computing},
pages = {200061},
year = {2022},
issn = {2666-6294},
doi = {https://doi.org/10.1016/j.gvc.2022.200061},
url = {https://www.sciencedirect.com/science/article/pii/S2666629422000146},
author = {Sam Schofield and Andrew Bainbridge-Smith and Richard Green},
keywords = {Visual inertial odometry, Semi-synthetic dataset, Robot virtual reality},
abstract = {Capturing outdoor visual-inertial datasets is a challenging yet vital aspect of developing robust visual-inertial odometry (VIO) algorithms. A significant hurdle is that high-accuracy-ground-truth systems (e.g., motion capture) are not practical for outdoor use. One solution is to use a “semi-synthetic” approach that combines rendered images with real IMU data. This approach can produce sequences containing challenging imagery and accurate ground truth but with less simulated data than a fully synthetic sequence. Existing methods (used by popular tools/datasets) record IMU measurements from a visual-inertial system while measuring its trajectory using motion capture, then rendering images along that trajectory. This work identifies a major flaw in that approach, specifically that using motion capture alone to estimate the pose of the robot/system results in the generation of inconsistent visual-inertial data that is not suitable for evaluating VIO algorithms. However, we show that it is possible to generate high-quality semi-synthetic data for VIO algorithm evaluation. We do so using an open-source full-batch optimization tool to incorporate both mocap and IMU measurements when estimating the IMU’s trajectory. We demonstrate that this improved trajectory results in better consistency between the IMU data and rendered images and that the resulting data improves VIO trajectory error by 79% compared to existing methods. Furthermore, we examine the effect of visual-inertial data inconsistency (as a result of trajectory noise) on VIO performance to provide a foundation for future work targeting real-time applications.}
}
@article{ZHOU2023188,
title = {Web-based mixed reality video fusion with remote rendering},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {2},
pages = {188-199},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000274},
author = {Qiang Zhou and Zhong Zhou},
keywords = {Mixed reality, Video fusion, WebRTC, Remote rendering},
abstract = {Background
Mixed reality (MR) video fusion systems merge video imagery with 3D scenes to make the scene more realistic and help users understand the video content and temporal–spatial correlation between them, reducing the user′s cognitive load. MR video fusion are used in various applications; however, video fusion systems require powerful client machines because video streaming delivery, stitching, and rendering are computationally intensive. Moreover, huge bandwidth usage is another critical factor that affects the scalability of video-fusion systems.
Methods
Our framework proposes a fusion method for dynamically projecting video images into 3D models as textures.
Results
Several experiments on different metrics demonstrate the effectiveness of the proposed framework.
Conclusions
The framework proposed in this study can overcome client limitations by utilizing remote rendering. Furthermore, the framework we built is based on browsers. Therefore, the user can test the MR video fusion system with a laptop or tablet without installing any additional plug-ins or application programs.}
}
@article{PRATAMA2023338,
title = {WizardOfMath: A top-down puzzle game with RPG elements to hone the player's arithmetic skills},
journal = {Procedia Computer Science},
volume = {216},
pages = {338-345},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.144},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022220},
author = {Mulia Pratama and Yanfi Yanfi and Pualam Dipa Nusantara},
keywords = {Game, math, Game Development Life Cycle, Game Experience Questionnaire},
abstract = {As one of the important education subjects’ mathematics difficulties can lead to tension and be described as the most hated or feared subject. This study aims to create a puzzle game application with RPG elements called WizardOfMath to increase a user's interest in mathematics subject. The research method includes a method called Game Development Life Cycle (GDLC), which has a pre-production stage that is suitable for game development rather than the Waterfall method. A game application is built based on the prior requirement gathering. The evaluation was arranged using the Game Experience Questionnaire (GEQ) survey which is performed by providing an online form to the public. Reliability test of GEQ modules meets Cronbach's Alpha value above 0.7 and the validity test of the r table is greater than 0.05. The results calculation of the Game Experience Questionnaire (GEQ) from a total of 55 participants and 3 modular structures, which are the Core module, In-game Module, and Post-game Module obtained an average score of 4.06, 3.88, and 3.57 for positive aspects and 2,72, 2.67, and 2,61 for the negative aspect. The contribution of this study shows this puzzle game application with RPG elements decreased user tension and the negative effect of being involved with mathematics subjects.}
}
@article{ALYOUSIFY2022263,
title = {AR-assisted children book for smart teaching and learning of Turkish alphabets},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {3},
pages = {263-277},
year = {2022},
note = {Advances in Wireless Sensor Networks under AI-SG forAugmented Reality Special Issue},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000432},
author = {Ahmed L. Alyousify and Ramadhan J. Mstafa},
keywords = {Human computer interaction, 5G, Augmented reality, IOT, Turkish alphabet, Smart teaching, AR assisted book},
abstract = {Background
Augmented reality (AR), virtual reality (VR), and remote-controlled devices are driving the need for a better 5G infrastructure to support faster data transmission. In this study, mobile AR is emphasized as a viable and widespread solution that can be easily scaled to millions of end-users and educators because it is lightweight and low-cost and can be implemented in a cross-platform manner. Low-efficiency smart devices and high latencies for real-time interactions via regular mobile networks are primary barriers to the use of AR in education. New 5G cellular networks can mitigate some of these issues via network slicing, device-to-device communication, and mobile edge computing.
Methods
In this study, we use a new technology to solve some of these problems. The proposed software monitors image targets on a printed book and renders 3D objects and alphabetic models. In addition, the application considers phonetics. The sound (phonetic) and 3D representation of a letter are played as soon as the image target is detected. 3D models of the Turkish alphabet are created by using Adobe Photoshop with Unity3D and Vuforia SDK.
Results
The proposed application teaches Turkish alphabets and phonetics by using 3D object models, 3D letters, and 3D phrases, including letters and sounds.}
}
@article{KUSIC2023101858,
title = {A digital twin in transportation: Real-time synergy of traffic data streams and simulation for virtualizing motorway dynamics},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101858},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101858},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622003160},
author = {Krešimir Kušić and René Schumann and Edouard Ivanjko},
keywords = {Digital twin, Microscopic traffic simulation, Real-time Big Data analytics, Calibration, Traffic sensors, Smart roads},
abstract = {The introduction of digital twins is expected to fundamentally change the technology in transportation systems, as they appear to be a compelling concept for monitoring the entire life cycle of the transport system. The advent of widespread information technology, particularly the availability of real-time traffic data, provides the foundation for supplementing predominated (offline) microscopic simulation approaches with actual data to create a detailed real-time digital representation of the physical traffic. However, the use of actual traffic data in real-time motorway analysis has not yet been explored. The reason is that there are no supporting models and the applicability of real-time data in the context of microscopic simulations has yet to be recognized. Thus, this article focuses on microscopic motorway simulation with real-time data integration during system run-time. As a result, we propose a novel paradigm in motorway traffic modeling and demonstrate it using the continuously synchronized digital twin model of the Geneva motorway (DT-GM). We analyze the application of the microscopic simulator SUMO in modeling and simulating on-the-fly synchronized digital replicas of real traffic by leveraging fine-grained actual traffic data streams from motorway traffic counters as input to DT-GM. Thus, the detailed methodological process of developing DT-GM is presented, highlighting the calibration features of SUMO that enable (dynamic) continuous calibration of running simulation scenarios. By doing so, the actual traffic data are directly fused into the running DT-GM every minute so that DT-GM is continuously calibrated as the physical equivalent changes. Accordingly, DT-GM raises a technology dimension in motorway traffic simulation to the next level by enabling simulation-based control optimization during system run-time that was previously unattainable. It, thus, forms the foundation for further evolution of real-time predictive analytics as support for safety–critical decisions in traffic management. Simulation results provide a solid basis for the future real-time analysis of an extended Swiss motorway network.}
}
@article{SAS2022111343,
title = {Antipatterns in software classification taxonomies},
journal = {Journal of Systems and Software},
volume = {190},
pages = {111343},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111343},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000826},
author = {Cezar Sas and Andrea Capiluppi},
keywords = {Classification, Software types, Antipattern, Taxonomy, Machine learning, Natural language processing},
abstract = {Empirical results in software engineering have long started to show that findings are unlikely to be applicable to all software systems, or any domain: results need to be evaluated in specified contexts, and limited to the type of systems that they were extracted from. This is a known issue, and requires the establishment of a classification of software types. This paper makes two contributions: the first is to evaluate the quality of the current software classifications landscape. The second is to perform a case study showing how to create a classification of software types using a curated set of software systems. Our contributions show that existing, and very likely even new, classification attempts are deemed to fail for one or more issues, that we named as the ‘antipatterns’ of software classification tasks. We collected 7 of these antipatterns that emerge from both our case study, and the existing classifications. These antipatterns represent recurring issues in a classification, so we discuss practical ways to help researchers avoid these pitfalls. It becomes clear that classification attempts must also face the daunting task of formulating a taxonomy of software types, with the objective of establishing a hierarchy of categories in a classification.}
}
@article{HARWOOD2018363,
title = {Interactive flow simulation using Tegra-powered mobile devices},
journal = {Advances in Engineering Software},
volume = {115},
pages = {363-373},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0965997817307135},
author = {Adrian R.G. Harwood and Alistair J. Revell},
keywords = {Android, Mobile computing, Interactive simulation, Lattice Boltzmann Method, CUDA, Embedded computing},
abstract = {The ability to perform interactive CFD simulations on mobile devices allows the development of portable, affordable simulation tools that can have a significant impact in engineering design as well as teaching and learning. This work extends existing work in the area by developing and implementing a GPU-accelerated, interactive simulation framework suitable for mobile devices. The accuracy, throughput, memory usage and battery consumption of the application is established for a range of problem sizes. The current GPU implementation is found to be over 300 ×  more efficient in terms of combined throughput and power consumption than a comparable CPU implementation. The usability of the simulation is examined through a new ‘interactivity’ metric which identifies the ratio of simulated convection to real world convection of the same problem. This real-time ratio illustrates that large resolutions may increase throughput through parallelisation on the GPU but this only partially offsets the decrease in simulated flow rate due to the necessary shrinking of the time step in the solver with increasing resolution. Therefore, targeting higher throughput configurations of GPU-solvers offer little additional benefit for interactive applications due to ultimately simulations evolving at a too slow a rate to facilitate interaction. The trade-off between accuracy, speed and power consumption are explored with the choice of problem resolution ultimately being characterised by a desired accuracy, flow speed and endurance of a given simulation. With current rates of growth in mobile compute power expected to continue, real-time simulation is expected to be possible at higher resolutions with a reduced energy footprint in the near future.}
}
@article{BEATTIE2015149,
title = {Taking the LEAP with the Oculus HMD and CAD - Plucking at thin Air?},
journal = {Procedia Technology},
volume = {20},
pages = {149-154},
year = {2015},
note = {Proceedings of The 1st International Design Technology Conference, DESTECH2015, Geelong},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2015.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212017315002029},
author = {Nathan Beattie and Ben Horan and Sophie McKenzie},
keywords = {Oculus Rift, LEAP Motion, CAD interaction, HCI},
abstract = {Achieving adequate visualisation of designs within CAD packages remains a challenge for designers with current methods of 3D CAD visualisation requiring either a high level of technical ability, or expensive hardware and software. The recent re-emergence of consumer VR has lowered the barrier for everyday developers wanting to visualise their designs in true 3D. This paper presents the CAD Interaction Lab (CIL) which employs the Oculus Rift Head Mounted Display (HMD) and Leap Motion Controller (LMC) to provide a low cost method enabling users to use their hands to dissect a mechanic model to manipulate and inspect individual components in realistic 3D. Qualitative observations of user interactions with the CIL show that users were able to intuitively manipulate the CAD model using natural hand movements with only minimal instruction.}
}
@article{GARGRISH20201039,
title = {Augmented Reality-Based Learning Environment to Enhance Teaching-Learning Experience in Geometry Education},
journal = {Procedia Computer Science},
volume = {172},
pages = {1039-1046},
year = {2020},
note = {9th World Engineering Education Forum (WEEF 2019) Proceedings : Disruptive Engineering Education for Sustainable Development},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.05.152},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920314824},
author = {Shubham Gargrish and Archana Mantri and Deepti Prit Kaur},
keywords = {Augmented Reality, Augmented Reality-Based Learning Environment, Mobile- Based Learning Environment, 3-D Geometry},
abstract = {Virtual information and physical objects are deployed in regular teaching all over, and until recently, blending these two environments has been a very difficult task at best. Understanding the concepts of geometry and their three-dimensional (3-D) space is still considered a difficult subject area for some students. Therefore, a requirement of a learning innovation arises for learning geometry to overcome the problems faced while understanding geometry by students’. The main objective of the paper is to develop Augmented Reality (AR)- based geometry learning for the android and iOS platforms than to deploy the applications among students for teaching 3-D geometry in high school students. This technology is considered to the realm of science and mathematics classroom and supports theoretical underpinnings in understanding the benefits as well as limitations of augmented reality-based learning environment (ARLE) experiences. One of the topics which are difficult for the students to understand is geometry in mathematics education. To address the problem, the article introduces a framework of mobile- based ARLE system.}
}
@article{ANDRONAS2023102544,
title = {Towards seamless collaboration of humans and high-payload robots: An automotive case study},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {83},
pages = {102544},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102544},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000200},
author = {Dionisis Andronas and Emmanouil Kampourakis and Giorgos Papadopoulos and Katerina Bakopoulou and Panagiotis Stylianos Kotsaris and George Michalos and Sotiris Makris},
keywords = {Human robot collaboration, Interaction, Augmented reality, Safety, Design},
abstract = {Despite the ergonomic challenges that large-sized product assemblies underlay for human operators, related processes are deselected from hybrid automation mostly due to the payload limitations of collaborative robots. Driven by industrial requirements for ergonomic and performance improvement, this paper presents the implementation of a human-high payload robot symbiotic workstation and discusses the key enabling technologies for seamless collaboration, namely: a) a multi-modal human-robot interaction pipeline, b) a gesture based contactless manual guidance module, c) fenceless safety monitoring system and logic, and d) an augmented reality-based training application for operator inclusion. An automotive case study is used for validating the complete hybrid system's usability and performance besides improved operator ergonomics and well-being. This work's findings prove that high-payload robots can support operators through intuitive means of interaction and businesses can rely on collaborative systems for improved performance metrics and job openness.}
}
@article{DEFELICE20231744,
title = {Physical and digital worlds: implications and opportunities of the metaverse},
journal = {Procedia Computer Science},
volume = {217},
pages = {1744-1754},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.374},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922024590},
author = {Fabio {De Felice} and Cristina {De Luca} and Simona Di Chiara and Antonella Petrillo},
keywords = {Metaverse, Immersive Internet, Multi-technology, Sociality, AR, VR},
abstract = {The Metaverse is revolutionizing the world of the internet. It is the new “virtual” universe capable of going beyond the pure three-dimensional and immersive dimension combining the physical and digital worlds. The metaverse, which until recently was an abstract concept, is now assuming great importance and attracting the attention of consumers, investors, brands, and large global players. Certainly, the first sector to adapt to this new immersive reality is e-commerce. Obviously, e-commerce is not the only sector affected by this digital revolution. Interoperability and interconnection will revolutionize current business models. Although the business opportunities seem endless, the scenario is still not entirely clear. Thus, the aim of the present research is to provide an overview on the state of the art, technologies, applications, and challenges of metaverse. Ethical and social implications are also analized. The result is a first detailed scenario analysis on the Metaverse.}
}
@article{PINGTING2025100758,
title = {What drives user churn in serious games? An empirical examination of the TAM, SOR theory, and game quality in Chinese cultural heritage games},
journal = {Entertainment Computing},
volume = {52},
pages = {100758},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100758},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001265},
author = {Mao {Ping Ting} and Cho {Dong Min}},
keywords = {Cultural Heritage, Serious Games, Game Quality, User Churn, TAM, SOR Theory},
abstract = {User churn in serious games related to Chinese cultural heritage is severe due to various game quality issues, leading to a lack of long-term user feedback necessary for optimizing design, which further hampers the development of such games. Through user interviews and literature review, this study identified two dimensions of quality of cultural heritage serious games: factors related to online gaming and those intrinsic to serious games. Within the Stimulus-Organism-Response (SOR) theoretical framework of, this study integrates these quality factors within the Technology Acceptance Model (TAM) to construct a user churn model. This model explores the relationship between quality factors and user churn. Utilizing 534 valid responses gathered from online platforms, this study employs statistical analyses and structural equation modeling to uncover that learning objectives, entertainment experience, system quality, and game design profoundly influence both perceived usefulness and ease of use, which in turn indirectly impacts user churn. Service quality’s influence on user churn is mediated by perceived usefulness, while brand image affects churn through perceived ease of use. Notably, learning objectives and entertainment experience emerge as critical determinants. The insights gained offer valuable guidance for game designers in addressing user needs, enhancing game quality, and reducing churn. Building on previous research, this study uses a mixed-methods approach and identifies factors affecting user churn in the context of cultural heritage serious games and validated the model’s rationality and effectiveness, enriching the foundational theoretical research on serious games.}
}
@article{MOURTZIS2021525,
title = {Equipment Design Optimization Based on Digital Twin Under the Framework of Zero-Defect Manufacturing},
journal = {Procedia Computer Science},
volume = {180},
pages = {525-533},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.271},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003203},
author = {Dimitris Mourtzis and John Angelopoulos and Nikos Panopoulos},
keywords = {Digital Twin, Machine Design, Zero-Defect Manufacturing},
abstract = {A digitalized Smart Factory can be considered as a data island. Moreover, engineers have focused on the development of new technologies and techniques not only for transforming information to data but also to achieve efficient data utilization to further optimize manufacturing processes. However, the Zero-Defect Manufacturing concept has emerged, where the main goal is production optimization. The cornerstone in achieving the factories of the future is to further optimize the design of new assets so as they comply with the unique requirements of the customers. Therefore, this paper proposes the conceptualization, design, and initial development of a platform for the utilization of data derived from industrial environments for the optimization of the equipment design. The main aspects of the proposed framework are the data acquisition, data processing and the simulation. The applicability of the proposed framework has been tested in a laboratory-based machine shop utilizing data from a real-life industrial scenario.}
}
@article{MUKHOPADHYAY202255,
title = {Virtual-reality-based digital twin of office spaces with social distance measurement feature},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {1},
pages = {55-75},
year = {2022},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000043},
author = {Abhishek Mukhopadhyay and GS Rajshekar Reddy and KamalPreet Singh Saluja and Subhankar Ghosh and Anasol Peña-Rios and Gokul Gopal and Pradipta Biswas},
keywords = {Virtual environment, Digital twin, 3D visualization, Convolutional neural network, Object detection, Social distancing},
abstract = {Background
Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
Methods
In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
Results
We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
Conclusions
A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.}
}
@article{RAZUVALOVA2015129,
title = {Virtual Reconstruction of Cultural and Historical Monuments of the Middle Volga},
journal = {Procedia Computer Science},
volume = {75},
pages = {129-136},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.229},
url = {https://www.sciencedirect.com/science/article/pii/S187705091503690X},
author = {Ekaterina Razuvalova and Arthur Nizamutdinov},
keywords = {virtual reconstruction, computer technologies, historical reconstruction, virtual environment},
abstract = {Today people use information technologies in every area of their lives; it makes lives more systematic, clear and definite. This phenomenon appears in humanities too, in such sciences like history and social knowledge. Virtual space is developing, which is, certainly, very helpful and it brings every area of life to a new level. Different virtual environments, simulators and educational resources are created. They let people participate, study and train in inaccessible or impossible areas. In this paper we present practical approach of Virtual reconstruction of cultural and historical monuments of the Middle Volga in which we first create landscape, develop key-systems (such as roads and sound filling editors), then create visual content and finish with optimization and implementing produced assets. This project was created to achieve a lot of aims. First of all, we want to contribute in developing of digital history, save and systematize existing culture-historical monuments and information about them, to help form an image of Tatar Republic. Secondly, we want to expand virtual reality use. We want it to be not only an entertainment, but part of education. Our project expects wide audience of people interested in it. It will be helpful for studying history, because men learn history latently, while they are active in the game. So, this is the benefit of virtual reconstructions: users become part of virtual reality and certain world created in it. They try different roles, do different actions they cannot do in reality. In our educational game “Bolgar XIV” user can try a role of a citizen of ancient city; see defunct architecture and typical activity of that time. This game will attract people, who just want to play a game, but at the same time they will enrich their knowledge and images of this particular historic place. This makes virtual environment more of current interest and multifunctional. Creating virtual environment of ancient city of Bolgar help to understand and create our own technique of creating such culture-historical reconstructions.}
}
@article{BUISSON2013815,
title = {Real-time Collision Avoidance for Pedestrian and Bicyclist Simulation: A Smooth and Predictive Approach},
journal = {Procedia Computer Science},
volume = {19},
pages = {815-820},
year = {2013},
note = {The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.06.108},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913007163},
author = {Jocelyn Buisson and Stéphane Galland and Nicolas Gaud and Mikael Gonçalves and Abderrafiaa Koukam},
keywords = {collision avoidance, pedestrian behavior, crowd simulation, cyclist behavior},
abstract = {This article introduces a new collision avoidance model enabling the design of efficient realistic virtual pedestrian and cyclist behaviors. It is a force-based model using collision prediction with dynamic time-windows to predict future potential collisions with obstacles and other individuals. It introduces a new type of force called sliding force to allow a smooth avoidance of potential collisions while enabling the pedestrian to continue to progress towards its goal. Unlike most existing models, our forces are not scaled according to the distance to the obstacle but depending on the estimate of the collision time with this obstacle. This inherently integrates obstacles’ velocity. This greatly reduces the compu- tational complexity of the model while ensuring a smooth avoidance. This model is oscillation-free except for concave obstacles. It enables the reproduction of inherent emergent properties of real crowds such as spontaneous organizations of pedestrians into lane lines, etc. This model is computationally efficient and designed for real time simulation of large crowds.}
}
@article{NOWLAN2023100012,
title = {Higher-order thinking skills assessment in 3D virtual learning environments using motifs and expert data},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100012},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100012},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000065},
author = {Nuket Nowlan and Ali Arya and Hossain Samar Qorbani and Maryam Abdinejad},
keywords = {Higher-order thinking skills, Virtual learning environments, Motif, Assessment, Process metric},
abstract = {The research reported in this paper addresses the problem of assessing higher-order thinking skills, such as reflective and creative thinking, within the context of virtual learning environments. Assessment of these skills requires process-based observations and evaluation, as the output-based methods have been found to be insufficient. Virtual learning environments offer a wealth of data on the process, which makes them good candidates for process-based evaluation, but the existing assessment methods in these environments have shortcomings, such as reliance on large data sets, inability to offer specific feedback on actions, and the lack of consideration for how actions are integrated into bigger tasks. Demonstrating and confirming the ability of three-dimensional virtual learning environments to work with process metrics for assessment, we propose and evaluate the use of motifs as an assessment tool. Motifs are short and meaningful combination of metrics. Combining time-ordered motifs with a similarity analysis between expert and learner data, our proposed approach can potentially offer feedback on specific actions that the learner takes, as opposed to single output-based feedback. It can do so without the use of large training datasets due to reliance on expert data and similarity analysis. Through a user study, we found out that such a motif-based approach can be effective in the assessment of higher-order thinking skills while addressing the identified shortcomings of previous work. We also address the limited research on similarity-based analysis methods, compare their effectiveness, and show that utilizing different similarity measures for different tasks may be a more effective approach. Our proposed method facilitates and encourages the involvement of instructors and course designers through the definition of motifs and expert problem-solving paths.}
}
@article{FATHI20242670,
title = {Unveiling the Potential of Mixed Reality: Enhancing Time Measurement and Operator Support in Manual Assembly Processes},
journal = {Procedia Computer Science},
volume = {232},
pages = {2670-2679},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.084},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002618},
author = {Masood Fathi and Ingemar Karlsson and Göran Grahn and Andreas Björnsson},
keywords = {Mixed Reality, Operator Support, Time study, Manual Assembly},
abstract = {This study investigates the potential of Mixed Reality (MR) in the manual assembly processes and conducts a case study at a pump manufacturing plant in Sweden. An MR solution is developed to assist operators through visual instructions and guiding aides. The solution also captures the operator's motions using advanced hand and eye tracking features for real-time guidance and accurate time measurement. The proposed MR solution uses the build feature of HoLolens and a workstation editor, which facilitates the use of the solution in diverse assembly environments. The results of the experiments show that the developed MR solution can improve operator support, reduce errors, and enhance the overall efficiency of manual assembly processes. Moreover, it is shown to be an efficient tool for time measurement of the manual assembly process that has promising potential to replace sophisticated and time-consuming traditional time study methods.}
}
@article{TAMBA2023670,
title = {The Effect of Educational Platformer Game "Loving Ma"},
journal = {Procedia Computer Science},
volume = {227},
pages = {670-679},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.571},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017386},
author = {Nikolaus Hasudungan Tamba and Billy Andrian and  Vincenzo and Yanfi Yanfi and Pualam Dipa Nusantara},
keywords = {game application, platformer, Extreme Programming, Eight Golden Rules, Game Experience Questionnaire},
abstract = {This study aimed to create an educational game application based on the theme of motherly roles in the platformer genre. The game application is designed to feature three levels consisting of multiple-choice questions about the roles of a mother. The Extreme Programming (XP) method is utilized to create the game application. Extreme Programming (XP) is a branch of the Agile Development method used to develop systems with uncertain or rapid requirements changes. The steps of the XP method consist of planning, designing, coding, and testing. The game application is evaluated both internally and outside. Black-box testing and eight golden rules are used for internal review. The findings of black-box testing can be used to determine the viability of a game application. The evaluation findings of the eight golden rules are required to evaluate the game application's user interface. The Game Experience Questionnaire is used for external review (GEQ). The GEQ was distributed through Google Forms, and 67 respondents completed it. The result of the GEQ was then tested for validity and reliability, calculated to find the mean number and standard deviation, and analyzed. Therefore, it was concluded that the game application provides a good user experience.}
}
@article{WATTANASOONTORN2012293,
title = {The Framework of a Life Support Simulation Application},
journal = {Procedia Computer Science},
volume = {15},
pages = {293-294},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008459},
author = {Voravika Wattanasoontorn and Imma Boada and Carles Blavi and Mateu Sbert},
keywords = {Life support, Serious Games, Medical Simulation, Cardiopulmonary Respiration, Resuscitation},
abstract = {In this paper we present the framework of a LIfe Support Simulation Application (LISSA) designed to teach and learn cardiopulmonary resuscitation (CPR) skills. DLISSA exploits video game technology to link in a single framework computer-based simulations of CPR emergencies with the functionalities of e-learning platforms. DEmergency situations are presented as problems that the learner has to solve in a game mode. Learner actions are registered in a database. DThis information is used to present new problems to the learner in an adaptive learning mode.DLISSA can be used as a substitute or a complement for traditional CPR classroom-based instruction Dor to refresh and improve CPR skill retention over time.}
}
@article{LIU2021200020,
title = {Virtual reality game level layout design for real environment constraints},
journal = {Graphics and Visual Computing},
volume = {4},
pages = {200020},
year = {2021},
issn = {2666-6294},
doi = {https://doi.org/10.1016/j.gvc.2021.200020},
url = {https://www.sciencedirect.com/science/article/pii/S2666629421000036},
author = {Huimin Liu and Zhiquan Wang and Angshuman Mazumdar and Christos Mousas},
keywords = {Game level, Level layout design, Virtual reality, Real environment constraints},
abstract = {This paper presents an optimization-based approach for designing virtual reality game level layouts, based on the layout of a real environment. Our method starts by asking the user to define the shape of the real environment and the obstacles (e.g., furniture) located in it. Then, by representing a game level as an assembly of chunks and defining the game level layout design decisions in cost terms (mapping, fitting, variations, and accessibility) in a total cost function, our system automatically synthesizes a game level layout that fulfills the real environment layout and its constraints as well as the user-defined design decisions. To evaluate the proposed method, a user study was conducted. The results indicated that the proposed method: (1) enhanced the levels of presence; (2) enhanced the levels of involvement of participants in the virtual environment; and (3) reduced the fear of collision with the real environment and its constraints. Limitations and future research directions are also discussed.}
}
@article{BAADEN2022324,
title = {Deep inside molecules — digital twins at the nanoscale},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {4},
pages = {324-341},
year = {2022},
note = {Virtual-reality and intelligent hardware in digital twins A)},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000171},
author = {Marc Baaden},
keywords = {Digital twins, Molecular simulation, Virtual reality},
abstract = {Background
Digital twins offer rich potential for exploration in virtual reality (VR). Using interactive molecular simulation approaches, they enable a human operator to access the physical properties of molecular objects and to build, manipulate, and study their assemblies. Integrative modeling and drug design are important applications of this technology.
Methods
In this study, head-mounted virtual reality displays connected to molecular simulation engines were used to create interactive and immersive digital twins. They were used to perform tasks relevant to specific use cases.
Results
Three areas were investigated, including model building, rational design, and tangible models. Here, we report several membrane-embedded systems of ion channels, viral components, and artificial water channels. We were able to improve and create molecular designs based on digital twins.
Conclusions
The molecular application domain offers great opportunities, and most of the technical and technological aspects have been solved. Wider adoption is expected once the onboarding of VR is simplified and the technology gains wider acceptance.}
}
@article{CHARCO2021104182,
title = {Camera pose estimation in multi-view environments: From virtual scenarios to the real world},
journal = {Image and Vision Computing},
volume = {110},
pages = {104182},
year = {2021},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2021.104182},
url = {https://www.sciencedirect.com/science/article/pii/S0262885621000871},
author = {Jorge L. Charco and Angel D. Sappa and Boris X. Vintimilla and Henry O. Velesaca},
keywords = {Relative camera pose estimation, Domain adaptation, Siamese architecture, Synthetic data, Multi-view environments},
abstract = {This paper presents a domain adaptation strategy to efficiently train network architectures for estimating the relative camera pose in multi-view scenarios. The network architectures are fed by a pair of simultaneously acquired images, hence in order to improve the accuracy of the solutions, and due to the lack of large datasets with pairs of overlapped images, a domain adaptation strategy is proposed. The domain adaptation strategy consists on transferring the knowledge learned from synthetic images to real-world scenarios. For this, the networks are firstly trained using pairs of synthetic images, which are captured at the same time by a pair of cameras in a virtual environment; and then, the learned weights of the networks are transferred to the real-world case, where the networks are retrained with a few real images. Different virtual 3D scenarios are generated to evaluate the relationship between the accuracy on the result and the similarity between virtual and real scenarios—similarity on both geometry of the objects contained in the scene as well as relative pose between camera and objects in the scene. Experimental results and comparisons are provided showing that the accuracy of all the evaluated networks for estimating the camera pose improves when the proposed domain adaptation strategy is used, highlighting the importance on the similarity between virtual-real scenarios.}
}
@article{MONIAGA2018361,
title = {Facial Expression Recognition as Dynamic Game Balancing System},
journal = {Procedia Computer Science},
volume = {135},
pages = {361-368},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.185},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314741},
author = {Jurike V. Moniaga and Andry Chowanda and Agus Prima and  Oscar and M. Dimas {Tri Rizqi}},
keywords = {Facial Expression Recognition, Dynamic Balancing, Player Experience, Immersive Experience Questionnaire},
abstract = {This research proposes a dynamic game balancing by using Facial Expression Recognition to enhance the player’s experience when playing the game. Research has shown that players generally express their emotions when playing the game. This allow us to capture the player’s expression from their face and use it to dynamically adjust the game difficulties. A preliminary study was conducted to capture what kind of game that would be suitable to test dynamic balancing. The game with a dynamic balancing system then was developed using Scrum methodology as the software development methodology. Furthermore, the dynamic balancing in the game was simulated in the computer and evaluated, with some of items from Immersive Experience Questionnaire, with players played two versions of game, one with dynamic balancing activated and the other with-out dynamic balancing activated. The results evidently show that there was some statistically significant enhancement in the game with Facial Expression Recognition activated as a game dynamic game balancing compare to the one with-out Facial Expression Recognition.}
}
@article{ZARZUELA2013382,
title = {Educational Tourism through a Virtual Reality Platform},
journal = {Procedia Computer Science},
volume = {25},
pages = {382-388},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.047},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012520},
author = {Mario Martínez Zarzuela and Francisco J. Díaz Pernas and Sergio Martín Calzón and David González Ortega and Miriam Antón Rodríguez},
keywords = {Serious game, Virtual Reality, Educational Tourism},
abstract = {This article presents a Virtual Reality Serious Game that allows the user to increase the knowledge about the city of Valladolid in Spain. With this goal, the Main Square and some of the historic buildings in the downtown have been virtually recreated. We have taken advantage of the characteristic tiled floor of the town hall square to represent a game board. Different tiled floors are squares which hide questions behind. The user plays using a Natural User Interface based on Microsoft® Kinect.}
}
@article{MONTEIRO20231,
title = {Exploring the user experience of hands-free VR interaction methods during a Fitts’ task},
journal = {Computers & Graphics},
volume = {117},
pages = {1-12},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S009784932300242X},
author = {Pedro Monteiro and Hugo Coelho and Guilherme Gonçalves and Miguel Melo and Maximino Bessa},
keywords = {Hands-free, HCI, Immersive Virtual Reality, Interaction, Usability},
abstract = {Despite advancements in interaction with immersive Virtual Reality (VR) systems, using hand gestures for all interactions still imposes some challenges, especially in interactions with graphical user interfaces that are usually performed with point-and-click interfaces. Therefore, exploring the use of alternative hands-free methods for selection is essential to overcome usability problems and provide natural interaction for users. The results and insights gained from this exploration can lead to enhanced user experiences in VR applications. This study aims to contribute to the literature with the evaluation of the usability of the most commonly used hands-free methods for selection and system control tasks in immersive VR and their impact on standard and validated experience and usability metrics, namely the sense of presence, cybersickness, system usability, workload, and user satisfaction. A Fitts’ selection task was performed using a within-subjects design by nine participants experienced in VR. The methods evaluated were the handheld controllers, the head gaze, eye gaze, and voice commands for pointing at the targets, and dwell time and voice commands to confirm the selections. Results show that the methods provide similar levels of sense of presence and low cybersickness while showing low workload values and high user satisfaction, matching the experience of traditional handheld controllers for non-multimodal approaches. The assisted eye gaze with dwell was the preferred hands-free method and the one with the highest values of usability. Still, developers should minimize the number of gaze movements to reduce fatigue. The evaluation also showed that using a multimodal approach for selections, especially using the voice, decreases user satisfaction and increases users’ frustration.}
}
@article{STATHAM2022100476,
title = {Game environment art with modular architecture},
journal = {Entertainment Computing},
volume = {41},
pages = {100476},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2021.100476},
url = {https://www.sciencedirect.com/science/article/pii/S1875952121000732},
author = {Wilhelmina Zoe Statham and João Jacob and Mikael Fridenfalk},
keywords = {Art Fatigue, Game environment art, Game development lifecycle, Level design, Modular architecture, Modular assets, Modular kits},
abstract = {3D games routinely turn to modular architecture as an optimization technique for environment art and level design, although there is little documentation of how to implement it. As a result, many games struggle to apply it effectively, contributing to problems such as art fatigue and production delays. This investigation proposes a set of unified principles of modular architecture for 3D game environment art based on traditional architecture and game examples. It finds that the use of a uniform grid and standard measurements are key as well as extensive planning, and that assets organized as modular kits can be more cost-effective and facilitate flexible level design. It concludes with a series of steps per development phase of a game’s lifecycle aimed at game environment art, with emphasis on early planning and testing, and suggests that close collaboration between environment artists and level designers is key to develop effective modular assets.}
}
@article{GOLUBEV2016152,
title = {Dijkstra-based Terrain Generation Using Advanced Weight Functions},
journal = {Procedia Computer Science},
volume = {101},
pages = {152-160},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916326862},
author = {Kirill Golubev and Aleksander Zagarskikh and Andrey Karsakov},
keywords = {terrain generation, weight functions, virtual worlds, visualization},
abstract = {Due to the growing popularity of consumer virtual reality devices, the new surge in the use of computer-generated terrain is already happening. That leads to the need to develop new fast and efficient methods of landscapes generation. However, lack of flexibility of user control over terrain generation in most popular modern terrain generation algorithms is a big part of terrain generation problem. In this paper, we present a new method for generating three-dimensional landscapes based on modified Dijkstra algorithm. The proposed method allows a user to set the initial location of landscape features and select or create weight functions that determine the appearance of the generated terrain. It has a lower computational cost compared to the closest analogs giving the equal quality of results and allows users to create various types of terrain, as well as to combine them together in one landscape.}
}
@article{LI2019468,
title = {Immersive VR Theater with Multi-device Interaction and Efficient Multi-user Collaboration},
journal = {Procedia Computer Science},
volume = {147},
pages = {468-472},
year = {2019},
note = {2018 International Conference on Identification, Information and Knowledge in the Internet of Things},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.274},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919302972},
author = {Huiyu Li and Shisheng Zhou and Fan Zhang and Chenglei Yang},
keywords = {Multi-device Interaction, Multi-user Collaboration, Immersive Virtual Environment, VR Theater},
abstract = {The virtual reality (VR) theater typically supports users to interact in an immersive virtual environment. In this paper, we propose the architecture and implementation of a VR theater system supporting multi-device interaction and multi-user collaboration. In our study, the system is equipped with a large cylindrical screen to display immersive stereo virtual scene and several types of interactive devices, including multiple degrees of freedom platform, simulation gun and smart phone, to support user interaction. To improve efficiency of user collaboration, the users are divided into two groups assigned with different interactive tasks, allowing them to collaborate with each other.}
}
@article{YLIPULLI2023594,
title = {Public libraries as a partner in digital innovation project: Designing a virtual reality experience to support digital literacy},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {594-605},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003011},
author = {Johanna Ylipulli and Matti Pouke and Nils Ehrenberg and Turkka Keinonen},
keywords = {Public libraries, Participatory design, Virtual reality, Digital literacy, Digital social innovation, Research-through-design},
abstract = {We introduce a project titled as Our Shared Virtual World which aims at increasing public libraries’ capability to provide knowledge on digital technology to general public. The practical goal of the project has been to produce a functional prototype of a virtual reality (VR) application that could be utilized freely in all the public libraries in Finland. In many countries worldwide, libraries’ role is expanding from providers of traditional books to providers of information technologies and related new forms of literacy, and this development provides the broader backdrop for the project. The contribution of the article is two-fold: First, we describe how an immersive VR application can be collaboratively developed within this specific research context, namely within a network of public libraries, and introduce the tangible outcome of the project, the VR application called Forest Elf. Secondly, we scrutinize how results of such a design work can be sustained over time: through participatory design (PD), we aimed at creating conditions which would enable public libraries to continue developing and using the artefact also after the project. We provide insights on how to tackle the challenge of research prototypes ending up being abandoned, and what factors in the context of library partnership support or hamper sustainable digital innovation — digital innovation that is inclusive and equitable but also has a long-lasting impact.}
}
@article{VARGASMOLANO201973,
title = {Parametric Facial Animation for Affective Interaction Workflow for Avatar Retargeting},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {343},
pages = {73-88},
year = {2019},
note = {The proceedings of AmI, the 2018 European Conference on Ambient Intelligence.},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2019.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S1571066119300131},
author = {Juan Sebastián {Vargas Molano} and Gloria Mercedes Díaz and Wilson J. Sarmiento},
keywords = {Animation, Avatar, Candide, Emotion},
abstract = {Virtual avatars/characters are essential in the interaction with virtual environments and 3D video games. While these avatars can be controlled by a bot or a human, they need a suitable model that allows emotion representation regardless of what or who is controlling them, since it increases the interaction and immersion. Provide facial expressions to any avatar require an animation process to emulate a particular gesture or emotion before to include it in a virtual environment. This process is specific to every single facial expression required in the avatar. This paper presents an initial approximation to contribute to the solve this problem with a workflow to apply a well-known parameterized face model called Candide. The proposal aims to adapt Candide into any avatar previously modeled through automatic morphological adjustment and texture mapping. This work includes the application of this workflow into six characters licensed under the Creative Commons copyright. We also present examples of facial emotion animation, as well as subjective assessment by five experts in audiovisual and animation production.}
}
@article{LIU2024100205,
title = {Cooperative multi-agent game based on reinforcement learning},
journal = {High-Confidence Computing},
volume = {4},
number = {1},
pages = {100205},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2667295224000084},
author = {Hongbo Liu},
keywords = {Collaborative multi-agent, Reinforcement learning, Credit distribution, Multi-agent communication, Reward shaping},
abstract = {Multi-agent reinforcement learning holds tremendous potential for revolutionizing intelligent systems across diverse domains. However, it is also concomitant with a set of formidable challenges, which include the effective allocation of credit values to each agent, real-time collaboration among heterogeneous agents, and an appropriate reward function to guide agent behavior. To handle these issues, we propose an innovative solution named the Graph Attention Counterfactual Multiagent Actor–Critic algorithm (GACMAC). This algorithm encompasses several key components: First, it employs a multi-agent actor–critic framework along with counterfactual baselines to assess the individual actions of each agent. Second, it integrates a graph attention network to enhance real-time collaboration among agents, enabling heterogeneous agents to effectively share information during handling tasks. Third, it incorporates prior human knowledge through a potential-based reward shaping method, thereby elevating the convergence speed and stability of the algorithm. We tested our algorithm on the StarCraft Multi-Agent Challenge (SMAC) platform, which is a recognized platform for testing multi-agent algorithms, and our algorithm achieved a win rate of over 95% on the platform, comparable to the current state-of-the-art multi-agent controllers.}
}
@article{PASANISI2023816,
title = {On Domain Randomization for Object Detection in real industrial scenarios using Synthetic Images},
journal = {Procedia Computer Science},
volume = {217},
pages = {816-825},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.278},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023560},
author = {Davide Pasanisi and Emanuele Rota and Michele Ermidoro and Luca Fasanotti},
keywords = {Deep Learning, Domain Randomization, Object Detection},
abstract = {Fine-tuning a pre-trained deep learning model is commonly preferred since it significantly lowers the training effort while still enabling state-of-the-art performances for downstream tasks. Nevertheless, for many industrial applications it is not always possible to collect the required real-world images. In this scenario, Domain Randomization is a promising technique for training a deep learning model on synthetic images. In this work, two real industrial applications are illustrated. In the first application, CAD models are exploited to generate a realistic 3D render of the objects of interest to investigate the feasibility of fine-tuning a model using a fully synthetic dataset but only qualitative results are shown. For the second application, semantic fidelity is favored over visual fidelity: objects of interest are drawn as a composition of primitive shapes and textures, allowing the generation of a synthetic dataset without using a dedicated 3D rendering software while preserving the flexibility to achieve enough variability in the dataset. A model fine-tuned on a synthetic dataset generated with this approach achieved an estimation error below 1% for the measurement of the diameter of barrels used in the Food and Beverage industry.}
}
@article{MONTE20131402,
title = {Estimation of Volume Rendering Efficiency with GPU in a Parallel Distributed Environment},
journal = {Procedia Computer Science},
volume = {18},
pages = {1402-1411},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.307},
url = {https://www.sciencedirect.com/science/article/pii/S187705091300450X},
author = {Cristian Federico Perez Monte and Fabiana Piccoli and Cristian Luciano and Silvio Rizzi and Germán Bianchini and Paola Caymes Scutari},
keywords = {Volume Rendering, Monte Carlo Method, Ray Tracing, GPU, Scientific Visualization, Cluster Computing, Parallel Processing, efficiency},
abstract = {Visualization methods of medical imagery based on volumetric data constitute a fundamental tool for medical diagnosis, training and pre-surgical planning. Often, large volume sizes and/or the complexity of the required computations present se- rious obstacles for reaching higher levels of realism and real-time performance. Performance and efficiency are two critical aspects in traditional algorithms based on complex lighting models. To overcome these problems, a volume rendering algo- rithm, PD-Render intra for individual networked nodes in a parallel distributed architecture with a single GPU per node is presented in this paper. The implemented algorithm is able to achieve photorealistic rendering as well as a high signal-to- noise ratio at interactive frame rates. Experiments show excellent results in terms of efficiency and performance for rendering medical volumes in real time.}
}
@article{TIWARI2022887,
title = {Dimensionality and Angular Disparity Influence Mental Rotation in Computer Gaming},
journal = {Computers, Materials and Continua},
volume = {72},
number = {1},
pages = {887-905},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.023886},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822010852},
author = {Akanksha Tiwari and Ram Bilas Pachori and Premjit Khanganba Sanjram},
keywords = {Computer-based games, angular disparity, dimensionality, mental rotation, EEG, hemispheric laterality},
abstract = {Computer gaming is one of the most common activities that individuals are indulged in their usual activities concerning interactive system-based entertainment. Visuospatial processing is an essential aspect of mental rotation (MR) in playing computer-games. Previous studies have explored how objects’ features affect the MR process; however, non-isomorphic 2D and 3D objects lack a fair comparison. In addition, the effects of these features on brain activation during the MR in computer-games have been less investigated. This study investigates how dimensionality and angular disparity affect brain activation during MR in computer-games. EEG (electroencephalogram) data were recorded from sixty healthy adults while playing an MR-based computer game. Isomorphic 2D and 3D visual objects with convex and reflex angular disparity were presented in the game. Cluster-based permutation tests were applied on EEG spectral power for frequency range 3.5–30 Hz to identify significant spatio-spectral changes. Also, the band-specific hemispheric lateralization was evaluated to investigate task-specific asymmetry. The results indicated higher alpha desynchronization in the left hemisphere during MR compared to baseline. The fronto-parietal areas showed neural activations during the game with convex angular disparities and 3D objects, for a frequency range of 7.8–14.2 Hz and 7.8–10.5 Hz, respectively. These areas also showed activations during the game with reflex angular disparities and 2D objects, but for narrower frequency bands, i.e., 8.0–10.0 Hz and 11.0–11.7 Hz, respectively. Left hemispheric dominance was observed for alpha and beta frequencies. However, the right parietal region was notably more dominant for convex angular disparity and 3D objects. Overall, the results showed higher neural activities elicited by convex angular disparities and 3D objects in the game compared to the reflex angles and 2D objects. The findings suggest future applications, such as cognitive modeling and controlled MR training using computer games.}
}
@article{DREZEWSKI20211914,
title = {The application of selected modern artificial intelligence techniques in an exemplary strategy game},
journal = {Procedia Computer Science},
volume = {192},
pages = {1914-1923},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.197},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016938},
author = {Rafał Dreżewski and Jakub Solawa},
keywords = {artificial intelligence, computer games, strategy games, team coordination, decision-making},
abstract = {In the paper requirements for artificial intelligence algorithms designed for modern strategy computer games are analyzed. The selected techniques used to fulfill those requirements are described in detail. Then the exemplary game, which was designed and implemented using the selected algorithms for path-finding, decision-making, tactical and strategic reasoning, and team coordination is presented. Finally, the results of experiments conducted with the use of the created game are analyzed. The results prove the efficiency of selected techniques in creating a strategically challenging game.}
}
@article{MOLONEY2015212,
title = {Videogame Technology Re-Purposed: Towards Interdisciplinary Design Environments for Engineering and Architecture},
journal = {Procedia Technology},
volume = {20},
pages = {212-218},
year = {2015},
note = {Proceedings of The 1st International Design Technology Conference, DESTECH2015, Geelong},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2015.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S221201731500211X},
author = {Jules Moloney},
keywords = {architecture and design engineering education, simulation environments, videogame technology, serious games.},
abstract = {The author and associated researchers have in previous projects adapted videogame technology for design in the context of architectural design education. This paper reflects on this body of research: the original motivations and aspirations; what threads may be productively revisited; how contemporary shifts to parametric design and building information modelling may be incorporated; and considers how some aspects of game play, in particular competition, may seedInterdisciplinary Design environments for Engineering and Architecture (IDeEA).}
}
@article{JULIANTINO2023310,
title = {The development of virtual healing environment in VR platform},
journal = {Procedia Computer Science},
volume = {216},
pages = {310-318},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.141},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022190},
author = {Chris Juliantino and Mega Putri Nathania and Religiana Hendarti and Herru Darmadi and Bonny A Suryawinata},
keywords = {Android, Environment, Smartphone, Virtual Reality, Virtual space},
abstract = {The purpose of this research is to build a healing environment that can be experienced through virtual reality (VR) to reduce stress levels of the user because of the lengthy period of COVID-19 pandemic that forced people to stay at home. The healing environment is created based on several theories on the principles of Healing Environments related to Architectural design. And as for the development of the simulation software, it is using the method of Extreme Programming that is based on Agile principle that combined with Architectural design flow. The VR is targeted to run optimally in low-end devices with cardboard and common Bluetooth controller so that it can be accessed inclusively. The evaluation is conducted using a user acceptance test with expert judgment and survey to 32 respondents. The findings are that they enjoyed the simulation because of the guidance is clear and the virtual environment looks more real compared to others that looks cartoonish.}
}
@article{SALGADO2019665,
title = {The Effect of Cybersickness of an Immersive Wheelchair Simulator},
journal = {Procedia Computer Science},
volume = {160},
pages = {665-670},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317302},
author = {Débora P. Salgado and Thiago B. Rodrigues and Felipe R. Martins and Eduardo L.M. Naves and Ronan Flynn and Niall Murray},
keywords = {Cybersickness, Immersive Technologies, Wheelchair Simulator, Assistive Technologies},
abstract = {A key challenge that Immersive applications have to overcome is cybersickness. Cybersickness is particularly prevalent in dynamic applications such as vehicles simulators. The work presented here aims to understand the cause of cybersickness symptoms in an assistive technology (AT) application, the virtual wheelchair training simulator. This evaluation is performed in terms of errors made during experience and post-experience Simulator Sickness Questionnaire (SSQ). The performance metrics analyzed are time to complete the proposal task and number of collisions (errors/mistakes). The post-experience questionnaires (subjective measurements) collected the user’s experience in terms of simulator sickness by applying the Simulator Sickness Questionnaire (SSQ) and immersion questions. The experiments were conducted with 10 participants. In terms of results, analysis of human factors reveals that the average cybersickness score is slightly higher for women compared to men. However, these differences were not statistically significant. There was an inverse correlation between cybersickness symptoms and task performance as well as between cybersickness symptoms and immersion.}
}
@article{PEREZ2024111440,
title = {Generation of probabilistic synthetic data for serious games: A case study on cyberbullying},
journal = {Knowledge-Based Systems},
volume = {286},
pages = {111440},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111440},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124000753},
author = {Jaime Pérez and Mario Castro and Edmond Awad and Gregorio López},
keywords = {Synthetic data, Serious games, Cyberbullying, Item response theory, Bayesian network, Hierarchical Bayesian model, Computational social science},
abstract = {Synthetic data generation has been a growing area of research in recent years. However, its potential applications in serious games have yet to be thoroughly explored. Advances in this field could anticipate data modeling and analysis, as well as speed up the development process. To fill this gap in the literature, we propose a simulator architecture for generating probabilistic synthetic data for decision-based serious games. This architecture is designed to be versatile and modular so that it can be used by other researchers on similar problems (e.g., multiple choice exams, political surveys, any type of questionnaire). To simulate the interaction of synthetic players with the game, we use a cognitive testing model based on the Item Response Theory framework. We also show how probabilistic graphical models (in particular, Bayesian networks) can introduce expert knowledge and external data into the simulation. Finally, we apply the proposed architecture and methods in the case of a serious game focused on cyberbullying. We perform Bayesian inference experiments using a hierarchical model to demonstrate the identifiability and robustness of the generated data.}
}
@article{WANG2024100073,
title = {Learning cleanroom microfabrication operations in virtual reality – An immersive and guided learning experience},
journal = {Computers & Education: X Reality},
volume = {5},
pages = {100073},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100073},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000230},
author = {Fang Wang and Xinhao Xu and Shangman Li and Weiyu Feng and Mahmoud Almasri},
keywords = {Virtual reality, Microfabrication, Immersive, Learning, Education, Semiconductor, Training, Lab},
abstract = {This paper details the design and development of an immersive and self-guided Virtual Reality training system (iSGVRTS) for learning cleanroom microfabrication operations, with a specific emphasis on the photolithography process, within a college-level semiconductor laboratory curriculum. It presents the thorough construction of the iSGVRTS environment as well as the incorporation of integrated instructional methodologies. To assess the impact of the iSGVRTS intervention, pre-and post-tests were administered to evaluate learners' performance. The implementation of iSGVRTS yielded a notable enhancement for learners in laboratory operational proficiency, evidenced by improvements in task correct rates, reduction in procedural errors, and knowledge acquisition. Moreover, post-session interviews revealed learners’ reported increased confidence, a heightened sense of presence, manageable cognitive load, and positive feedback regarding the immersive learning experience.}
}
@article{RAZZAK2024100234,
title = {Using virtual reality to enhance attention for autistic spectrum disorder with eye tracking},
journal = {High-Confidence Computing},
pages = {100234},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100234},
url = {https://www.sciencedirect.com/science/article/pii/S2667295224000370},
author = {Rehma Razzak and Yi {(Joy) Li} and Jing {(Selena) He} and Sungchul Jung and Chao Mei and Yan Huang},
keywords = {Virtual reality, Attention training, Autistic spectrum disorders, Human–computer interaction},
abstract = {Attention deficit disorder is a frequently observed symptom in individuals with autism spectrum disorder (ASD). This condition can present significant obstacles for those affected, manifesting in challenges such as sustained focus, task completion, and the management of distractions. These issues can impede learning, social interactions, and daily functioning. This complexity of symptoms underscores the need for tailored approaches in both educational and therapeutic settings to support individuals with ASD effectively. In this study, we have expanded upon our initial virtual reality (VR) prototype, originally created for attention therapy, to conduct a detailed statistical analysis. Our objective was to precisely identify and measure any significant differences in attention-related outcomes between sessions and groups. Our study found that heart rate (HR) and electrodermal activity (EDA) were more responsive to attention shifts than temperature. The ‘Noise’ and ‘Score’ strategies significantly affected eye openness, with the ASD group showing more responsiveness. The control group had smaller pupil sizes, and the ASD group’s pupil size increased notably when switching strategies in Session 1. Distraction log data showed that both ‘Noise’ and ‘Object Opacity’ strategies influenced attention patterns, with the ‘Red Vignette’ strategy showing a significant effect only in the ASD group. The responsiveness of HR and EDA to attention shifts and the changes in pupil size could serve as valuable physiological markers to monitor and guide these interventions. These findings further support evidence that VR has positive implications for helping those with ASD, allowing for more tailored personalized interventions with meaningful impact.}
}
@article{DIIORIO2005129,
title = {What's the Name of the Game? Formal Specification of Artificial Intelligence Games},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {130},
pages = {129-150},
year = {2005},
note = {Proceedings of the Brazilian Symposium on Formal Methods (SBMF 2004)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1571066105002185},
author = {Vladimir {Di Iorio} and Roberto S. Bigonha and Mariza A.S. Bigonha and Alcione Oliveira and Eliseu Miguel},
keywords = {Artificial Intelligence, computer games, Abstract State Machines},
abstract = {Artificial intelligence games are a very interesting tool for teaching Artificial Intelligence techniques. Competitors write programs for agents, which are supposed to complete a given task or fight against other agents. In order to achieve the best performance, programs may have to use advanced Artificial Intelligence methods. In this paper, we present a framework to build artificial intelligence games, using Abstract State Machines (ASM) for the specification of the rules of the games. Choosing ASM, we expect that the competitors will be able to understand clearly the semantics of the rules. The framework includes a compiler for an ASM-based language, allows complete control of the order of execution of agents and easy integration with graphical libraries.}
}
@article{MOLINA2023101183,
title = {Two-step techniques for accurate selection of small elements in VR environments},
journal = {Graphical Models},
volume = {128},
pages = {101183},
year = {2023},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2023.101183},
url = {https://www.sciencedirect.com/science/article/pii/S1524070323000139},
author = {Elena Molina and Pere-Pau Vázquez},
keywords = {3D interaction, Selection, Molecular visualization},
abstract = {One of the key interactions in 3D environments is target acquisition, which can be challenging when targets are small or in cluttered scenes. Here, incorrect elements may be selected, leading to frustration and wasted time. The accuracy is further hindered by the physical act of selection itself, typically involving pressing a button. This action reduces stability, increasing the likelihood of erroneous target acquisition. We focused on molecular visualization and on the challenge of selecting atoms, rendered as small spheres. We present two techniques that improve upon previous progressive selection techniques. They facilitate the acquisition of neighbors after an initial selection, providing a more comfortable experience compared to using classical ray-based selection, particularly with occluded elements. We conducted a pilot study followed by two formal user studies. The results indicated that our approaches were highly appreciated by the participants. These techniques could be suitable for other crowded environments as well.}
}
@article{DELIMA2023100590,
title = {Managing the plot structure of character-based interactive narratives in games},
journal = {Entertainment Computing},
volume = {47},
pages = {100590},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100590},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000459},
author = {Edirlei Soares {de Lima} and Bruno Feijó and Antonio L. Furtado},
keywords = {Interactive storytelling, Narrative generation, Drama management, Plot structure, Automated planning},
abstract = {The use of narrative generation methods in games is a complex challenge that involves multiple problems of plot-based processes integrated with character-based methods. Examples of these problems are the high computational complexity of many story generation algorithms, the difficulties associated with the generation of interactive narratives that are compelling and emotionally impactful, the complex interactions among characters, and the need for tools and methods to support story writers in the process of creating and managing the narrative structure of interactive stories. In this work, we present and evaluate a new approach to generate and manage the plot structure of character-based interactive narratives in games, which combines multi-agent planning with a drama management strategy based on narrative structures. The proposed method is supported by an authoring tool that allows authors to create and test interactive narratives using graphical interfaces and intuitive diagrams. The results of our study suggest the effectiveness of our approach in generating interactive narratives for highly interactive game environments. In addition, a user study of the proposed authoring tool indicates that it can successfully support the development of character-based interactive narratives without requiring programming knowledge.}
}
@article{HAUGE2012210,
title = {Evaluation of Simulation Games for Teaching Engineering and Manufacturing},
journal = {Procedia Computer Science},
volume = {15},
pages = {210-220},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.073},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008356},
author = {Jannicke Baalsrud Hauge and Johann C.K.H. Riedel},
keywords = {Serious Games, Evaluation, Evaluation Methods, Engineering, Manufacturing.},
abstract = {This paper reports on the evaluation methods and findings from serious games for teaching engineering and manufacturing. Two serious games are considered: Cosiga, a new product development simulation game and Beware, a risk management simulation game. These two games cover the front and middle parts of the engineering process – from design to manufacture to sale. For the Cosiga simulation evaluations of the communication and cognitive change were performed. For the Beware game evaluation of communication, risk awareness and improvement of risk management skills were performed The findings from the evaluations showed that serious games deliver learning outcomes. However, there are drawbacks to their use that need to be taken into account. Principally the high cost of development and the need for expert facilitators for running game sessions.}
}
@article{BULBUL2023101170,
title = {Procedural generation of semantically plausible small-scale towns},
journal = {Graphical Models},
volume = {126},
pages = {101170},
year = {2023},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2023.101170},
url = {https://www.sciencedirect.com/science/article/pii/S1524070323000012},
author = {Abdullah Bulbul},
keywords = {Procedural modeling, 3D city modeling, Semantic control},
abstract = {Procedural techniques have been successfully utilized for generating various kinds of 3D models. In this study, we propose a procedural method to build 3D towns that can be manipulated by a set of high-level semantic principles namely security, privacy, sustainability, social-life, economy, and beauty. Based on the user defined weights of these principles, our method generates a 3D settlement to accommodate a desired population over a given terrain. Our approach firstly determines where to establish the settlement over the large terrain which is followed by iteratively constructing the town. In both steps, the principles guide the decisions and our method generates natural looking small-scale 3D residential regions similar to the cities of pre-industrial era. We demonstrate the effectiveness of the proposed approach to build semantically plausible town models by presenting sample results over real world based terrains.}
}
@article{DEHLAGHIGHADIM2023103906,
title = {ICSSIM — A framework for building industrial control systems security testbeds},
journal = {Computers in Industry},
volume = {148},
pages = {103906},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103906},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000568},
author = {Alireza Dehlaghi-Ghadim and Ali Balador and Mahshid Helali Moghadam and Hans Hansson and Mauro Conti},
keywords = {Cybersecurity, Industrial control system, Testbed, Network emulation, Cyberattack},
abstract = {With the advent of the smart industry, Industrial Control Systems (ICS) moved from isolated environments to connected platforms to meet Industry 4.0 targets. The inherent connectivity in these services exposes such systems to increased cybersecurity risks. To protect ICSs against cyberattacks, intrusion detection systems (IDS) empowered by machine learning are used to detect abnormal behavior of the systems. Operational ICSs are not safe environments to research IDSs due to the possibility of catastrophic risks. Therefore, realistic ICS testbeds enable researchers to analyze and validate their IDSs in a controlled environment. Although various ICS testbeds have been developed, researchers’ access to a low-cost, extendable, and customizable testbed that can accurately simulate ICSs and suits security research is still an important issue. In this paper, we present ICSSIM, a framework for building customized virtual ICS security testbeds in which various cyber threats and network attacks can be effectively and efficiently investigated. This framework contains base classes to simulate control system components and communications. Simulated components are deployable on actual hardware such as Raspberry Pis, containerized environments like Docker, and simulation environments such as GNS-3. ICSSIM also offers physical process modeling using software and hardware in the loop simulation. This framework reduces the time for developing ICS components and aims to produce extendable, versatile, reproducible, low-cost, and comprehensive ICS testbeds with realistic details and high fidelity. We demonstrate ICSSIM by creating a testbed and validating its functionality by showing how different cyberattacks can be applied.}
}
@article{MEMER2024116670,
title = {Robust numerical integration of embedded solids described in boundary representation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {419},
pages = {116670},
year = {2024},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116670},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523007934},
author = {Manuel Meßmer and Stefan Kollmannsberger and Roland Wüchner and Kai-Uwe Bletzinger},
keywords = {Embedded finite elements, Boundary representation (B-rep), Moment fitting equations, Clipping and intersection algorithms, Flawed geometries},
abstract = {Embedded and immersed methods have become essential tools in computational mechanics, as they allow discretizing arbitrarily complex geometries without the need for boundary-fitted meshes. One of their main challenges is the accurate numerical integration of cut elements. Among the various integration schemes developed for this purpose, moment fitting has proven to be a powerful technique that provides highly efficient and accurate integration rules. This publication presents a framework for the robust and efficient numerical integration of embedded solids described by oriented boundary meshes using moment fitting. The developments include an intersection algorithm that aims to drastically accelerate the computation of the necessary moments while achieving high accuracy. A closed surface parameterization of each cut domain is computed to facilitate the direct application of the divergence theorem. The algorithm is subject to a single quality criterion that guarantees the accurate evaluation of boundary integrals. At the same time, it allows to disregard classical mesh criteria, such as high aspect ratios, strongly varying angles, etc., resulting in extremely fast runtimes. In addition, an existing robust flood fill-based element classification scheme is further developed to initiate filling from arbitrary seed elements and to enable parallel execution, increasing its flexibility and efficiency. The successful application of all proposed algorithms to 4948 valid and flawed STLs from the Thingi10K database (Zhou and Jacobson, 2016) demonstrates their extraordinary robustness. In all cases, the wall-clock time scales at most linearly with the number of elements in the background mesh. We show that higher-order quadrature rules on the boundary elements enable efficient computation of the moments via the divergence theorem with near-machine precision. Finally, the presented methodologies are used to perform direct FE analyses on clean and flawed B-Rep models. All proposed algorithms are publicly available in the open-source C++ framework QuESo – Quadrature for Embedded Solids (https://github.com/manuelmessmer/QuESo), where the moment fitting equations are assembled and solved.}
}
@article{SANTAMARIAVAZQUEZ2023107357,
title = {MEDUSA©: A novel Python-based software ecosystem to accelerate brain-computer interface and cognitive neuroscience research},
journal = {Computer Methods and Programs in Biomedicine},
volume = {230},
pages = {107357},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107357},
url = {https://www.sciencedirect.com/science/article/pii/S016926072300024X},
author = {Eduardo Santamaría-Vázquez and Víctor Martínez-Cagigal and Diego Marcos-Martínez and Víctor Rodríguez-González and Sergio Pérez-Velasco and Selene Moreno-Calderón and Roberto Hornero},
keywords = {Brain-computer interfaces, Neurotechnology, Neuroscience, Electroencephalography},
abstract = {Background and objective: Neurotechnologies have great potential to transform our society in ways that are yet to be uncovered. The rate of development in this field has increased significantly in recent years, but there are still barriers that need to be overcome before bringing neurotechnologies to the general public. One of these barriers is the difficulty of performing experiments that require complex software, such as brain-computer interfaces (BCI) or cognitive neuroscience experiments. Current platforms have limitations in terms of functionality and flexibility to meet the needs of researchers, who often need to implement new experimentation settings. This work was aimed to propose a novel software ecosystem, called MEDUSA©, to overcome these limitations. Methods: We followed strict development practices to optimize MEDUSA© for research in BCI and cognitive neuroscience, making special emphasis in the modularity, flexibility and scalability of our solution. Moreover, it was implemented in Python, an open-source programming language that reduces the development cost by taking advantage from its high-level syntax and large number of community packages. Results: MEDUSA© provides a complete suite of signal processing functions, including several deep learning architectures or connectivity analysis, and ready-to-use BCI and neuroscience experiments, making it one of the most complete solutions nowadays. We also put special effort in providing tools to facilitate the development of custom experiments, which can be easily shared with the community through an app market available in our website to promote reproducibility. Conclusions: MEDUSA© is a novel software ecosystem for modern BCI and neurotechnology experimentation that provides state-of-the-art tools and encourages the participation of the community to make a difference for the progress of these fields. Visit the official website at https://www.medusabci.com/ to know more about this project.}
}
@article{VANOOSTEROM2022119,
title = {Organizing and visualizing point clouds with continuous levels of detail},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {194},
pages = {119-131},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622002647},
author = {Peter {van Oosterom} and Simon {van Oosterom} and Haicheng Liu and Rod Thompson and Martijn Meijers and Edward Verbree},
keywords = {nD point clouds, Continuous level of detail (cLoD), Space Filling Curve (SFC), Perspective view selection},
abstract = {Point clouds contain high detail and high accuracy geometry representation of the scanned Earth surface parts. To manage the huge amount of data, the point clouds are traditionally organized on location and map-scale; e.g. in an octree structure, where top-levels of the tree contain few points suitable for small scale overviews and lower levels of the tree contain more points suitable for large scale detailed views. The drawback of this solution is that it is based on discrete levels, causing visual artifacts in the form of data density shocks when creating the commonly used perspective views. This paper presents a method based on an optimized distribution of points over continuous levels, avoiding the visualization shocks. The traditional distribution ratio’s of data amounts over discrete levels of raster or vector data is considered the reference. How to convert this to point clouds with continuous levels (still benefiting from the proven advantages of the data distribution in discrete levels for efficient access at a wide range of scales)? In our solution, for each point a cLoD (continuous Level of Detail) value is computed and added as dimension to the point. A SFC (Space Filling Curve)-based nD data clustering technique can be used to organize the points, so that they can be efficiently queried. It should be noted that also other multi-dimensional indexing and clustering techniques could be applied to realize continuous levels based on the cLoD value. Besides the mathematical foundation of the approach also several implementations are described, varying from a 3D web-browser based solution to an augmented reality point cloud app in a mobile phone. The cLoD enables interactive real-time visualization using perspective views without data density shocks, while supporting continuous zoom-in/out and progressive data streaming between server and client. The described cLoD based approach is generic and supports different types of point clouds: from airborne, terrestrial, mobile and indoor laser scanning, but also from dense matching optical imagery or multi-beam echo soundings.}
}
@article{KUSUMA2018385,
title = {Analysis of Gamification Models in Education Using MDA Framework},
journal = {Procedia Computer Science},
volume = {135},
pages = {385-392},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.187},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314765},
author = {Gede Putra Kusuma and Evan Kristia Wigati and Yesun Utomo and Louis Khrisna {Putera Suryapranata}},
keywords = {Gamification in education, Gamification models, Student motivation, Student achievement},
abstract = {Gamification nowadays is being one of techniques that can increase motivation and encourage the involvement of users, particularly in education domain that requires teaching and learning activities to be more fun and interesting. This paper surveys some analysis of gamification models. MDA framework is used to identify surveyed papers by breaking them down into three categories: mechanics, dynamics and aesthetics. Findings from the survey show there are many gamification models in education domain. However, there are some very representative gamification models could be used as a method to increase motivation, achievement and engagement in learning activities. By knowing the latest gamification models in education domain stated in this paper, it could help gamification practitioners to make new strategies in learning activities to increase students’ motivation, achievement and involvement. We also suggest some gamification strategies, which combine several mechanics in such a way to create dynamics that results in all types of aesthetics outputs.}
}
@article{BUTTUSSI2020103590,
title = {A virtual reality methodology for cardiopulmonary resuscitation training with and without a physical mannequin},
journal = {Journal of Biomedical Informatics},
volume = {111},
pages = {103590},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103590},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420302197},
author = {Fabio Buttussi and Luca Chittaro and Francesca Valent},
keywords = {Virtual reality, Training, Cardiopulmonary resuscitation, Medical education, Mannequin, Experimental evaluation},
abstract = {Background
Cardiopulmonary resuscitation (CPR) is an emergency procedure that can increase survival after a cardiac arrest. Performing CPR effectively requires both procedural knowledge and manual skills. Traditional CPR training methodology includes lessons led by instructors and supervised practice on mannequins, thus requiring considerable resources.
Objective
This paper proposes a new methodology for low-cost CPR training based on virtual reality (VR) with and without the addition of a physical mannequin. Moreover, it describes an experimental evaluation of the methodology that assessed gain in manual skills during training, transfer of procedural knowledge and manual skills in a final assessment, and changes in self-efficacy with three measurements over time (pre-training, post-training, and post-assessment).
Methods
We implemented a VR application that supports the proposed methodology, and can thus be used with or without a mannequin. The experimental evaluation involved 30 participants who tried CPR in VR twice, performing two repetitions of 30 chest compressions per trial. Half participants tried the VR application with the mannequin and half without it. Final assessment required all participants to perform CPR on the mannequin without the assistance of VR. To assess self-efficacy, participants filled in a questionnaire at the three times of measurement.
Results
Mixed-design ANOVAs showed effects of repetition, effects of group, or interaction between the two variables on manual skills assessed during training. In the final assessment, participants in both groups correctly remembered most of the steps of the procedure. ANOVAs revealed differences between the two groups only in pressure-related skills (better with mannequin) and in the number of wrong steps added to the procedure (better without mannequin). Mixed-design ANOVA showed a self-efficacy increase in both groups after training, which was maintained after final assessment.
Conclusions
The proposed VR methodology for CPR training has a positive effect on procedural knowledge, manual skills, and self-efficacy, with as well as without the physical mannequin. Trials on a mannequin are required to understand the correct pressure for chest compression. This supports the adoption of the proposed VR methodology to reduce instructor and mannequin time required to teach CPR to trainees.}
}
@article{SLUPINSKA20213123,
title = {Planning an experiment in a virtual environment reality as a place of research on human behaviour using methods of neuroscience measurement – bibliometric analysis and methodological approach},
journal = {Procedia Computer Science},
volume = {192},
pages = {3123-3133},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.085},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921018226},
author = {Kamila Słupińska and Jarosław Duda and Konrad Biercewicz},
keywords = {wirtualna rzeczywistość (VR), EEG, eye traking, metodyka},
abstract = {The methods used in cognitive neuroscience allow through measurement methods to analyze the behavior of subjects. In the opinion of the authors, the application of the experiment in a virtual environment and the measurement of brain activity can obtain a detailed analysis of the information and knowledge of the subject. For this to happen, however, it is important that the entire course of the study is carried out in accordance with the assumptions, without errors that could affect the incorrect interpretation of the results obtained. Planning an experiment focuses on planning all the details involved in carrying out the study (Hicks, 1973, Goodwin, 2008). Therefore, in the course of the research, the authors sought information on the methodology of experimental research carried out in a virtual reality environment with the use of selected tools of cognitive neuroscience, including in particular the use of virtual reality (VR), electroencephalograph (EEG) and eye tracking. Valuable knowledge and information on this topic can be found, among others, in the publication of B Rey, M Alcañiz in the article "Research in neuroscience and virtual reality" [1] in which the importance of the implementation of research in virtual reality is highlighted. The authors believe that VR is an ideal tool to "generate controlled environments the can be used to observe human behavior. Different aspects can be analyzed from a neuroscience perspective, including perception, control of movement, learning, memory, and emotional aspects. " However, in order to assess the extent to which the experimental method in combination with technology allows for reliable information and knowledge management, it was necessary to look at considerations of the research methodology. Therefore, as the first stage, a bibliometric study was adopted, the analysis of the content contained in three databases such as WoS, Scopus, and Google Scholar. The analyses carried out showed a small percentage of publications presenting the methodology of implementation of an experimental study in VR treated as an environment in which research is carried out, within which the aforementioned methods of measurement from the field of neuroscience were applied. Based on the analysis, the authors, on the basis of selected materials in which the research processes have been well-grounded in methodology, as well as on their own research experience, will present the stages of design and implementation of an experimental study carried out in the virtual reality environment using the indicated methods of cognitive neuroscience. Based on a case study, they presented the activities carried out in the course of analyzed events, as well as pointed to potential errors that may occur in the course of research. A supermarket was taken as the virtual location of the study.}
}
@article{DEPAULA201839,
title = {Playing Beowulf: Bridging computational thinking, arts and literature through game-making},
journal = {International Journal of Child-Computer Interaction},
volume = {16},
pages = {39-46},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300247},
author = {Bruno Henrique {de Paula} and Andrew Burn and Richard Noss and José Armando Valente},
keywords = {Game-making, Computational thinking, Arts, Humanities},
abstract = {Preparing younger generations to engage meaningfully with digital technology is often seen as one of the goals of 21st century education. JeanetteWing’s seminal work on Computational Thinking (CT) is an important landmark for this goal: CT represents a way of thinking, a set of problem-solving skills which can be valuable when interacting with digital technologies, and with different fields of knowledge, such as Arts and Humanities. Even if this cross-areas relevance has been celebrated and acknowledged in theoretical research, there has been a lack of practical projects exploring these links between CT and non-STEM fields. This research develops these links. We present a specific case – a game produced by two 14 years-old boys – within Playing Beowulf, a collaboration with the British library’s Young Researchers programme, in which students aged 13–14 from an inner-London (UK) school have developed games based on their own readings of the Anglo-Saxon poem Beowulf during an after-school club. The game was produced using MissionMaker, a software (currently under development at UCL Knowledge Lab) that allows users to create and code their own first-person 3D games in a simple way, using pre-made 3D assets, such as rooms, props, characters and weapons and a simplified programming language manipulated through drop-down lists. We argue that MissionMaker, by simplifying the development process (low floor), can be a means to foster the building of knowledge in both STEM (CT) and Arts and Humanities, building bridges between these two areas inside and outside traditional schooling.}
}
@article{BOBOU2020e00164,
title = {Archive Archaeology in palmyra, Syria a new 3D reconstruction of the tomb of Ḥairan},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {19},
pages = {e00164},
year = {2020},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2020.e00164},
url = {https://www.sciencedirect.com/science/article/pii/S2212054820300631},
author = {Olympia Bobou and Nathalia B. Kristensen and Scott McAvoy and Rubina Raja},
keywords = {Palmyra, Cultural heritage destruction and preservation, Harald ingholt, Tomb of H, airan, 3D reconstructions, Hypogeum, Fresco},
abstract = {Since 2010, the conflict in Syria has made it almost impossible to conduct fieldwork in the country. The Palmyra Portrait Project at Aarhus University has since 2012 been compiling the now largest corpus of funerary portraits in the ancient world, outside of Rome, which were produced in the Syrian city of Palmyra in the first three centuries CE. During the project, fieldwork diaries of the Danish archaeologist Kai Harald Ingholt, who worked in Palmyra between 1924 and 1935, are also being digitized and the information in these can be used to reconstruct in-situ contexts, which no longer exist. In this paper, we, for the first time, present the sketches and information given in the diaries, along with archival photos, paintings, inscriptions, and sculptures in an interactive web-based digital model of a monumental underground tomb, the so-called tomb of Ḥairan.}
}
@article{GALVANDEBARBA2018142,
title = {Self-attribution of distorted reaching movements in immersive virtual reality},
journal = {Computers & Graphics},
volume = {76},
pages = {142-152},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318301353},
author = {Henrique {Galvan Debarba} and Ronan Boulic and Roy Salomon and Olaf Blanke and Bruno Herbelin},
keywords = {Virtual reality, Self-attribution, Evaluation},
abstract = {This study explores the extent to which individuals embodied in Virtual Reality tend to self-attribute the movements of their avatar. More specifically, we tested subjects performing goal-directed movements and distorted the mapping between user and avatar movements by decreasing or increasing the amplitude of the avatar hand movement required to reach for a target, while maintaining the apparent amplitude – visual distance – fixed. In two experiments, we asked subjects to report whether the movement that they have seen matched the movement that they have performed, or asked them to classify whether a distortion was making the task easier or harder to complete. Our results show that subjects perform poorly in detecting discrepancies when the nature of the distortion is not made explicit and that subjects are biased to self-attributing distorted movements that make the task easier. These findings, in line with previous accounts on the sense of agency, demonstrate the flexibility of avatar embodiment and open new perspectives for the design of guided interactions in Virtual Reality.}
}
@article{CHATZITOFIS2015340,
title = {HeartHealth: A Cardiovascular Disease Home-based Rehabilitation System},
journal = {Procedia Computer Science},
volume = {63},
pages = {340-347},
year = {2015},
note = {The 6th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2015)/ The 5th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2015)/ Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.352},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915024874},
author = {Anargyros Chatzitofis and David Monaghan and Edmond Mitchell and Freddie Honohan and Dimitrios Zarpalas and Noel E. O’Connor and Petros Daras},
keywords = {home-based rehabilitation platform, rehabilitation, CVD patients, CVD exercises, motion capture, serious games ;},
abstract = {The increasing pressure on medical institutions around the world requires health care professionals to be prescribing home- based exercise rehabilitation treatments to empower patients to self-monitor their rehabilitation journey. Home-based exercise rehabilitation has shown to be highly effective in treating conditions such as Cardiovascular Disease (CVD). However, adherence to home-based exercise rehabilitation remains low. Possible causes for this are that patients are not monitored, they cannot be con- fident that they are performing the exercise correctly or accurately and they receive no feedback. This paper proposes HeartHealth, a novel patient-centric gamified exercise rehabilitation platform that can help address the issue of adherence to these programmes. The key functionality is the ability to record the patient movements and compare them against the exercises that have been pre- scribed in order to return feedback to the patient and to the health care professional, as well. In order to synthesize a compact fully operational system able to work in real life scenarios, tools and services from FI-PPP projects, FIWARE 1 and FI-STAR 2, were exploited and a new FI-STAR component, Motion Evaluation Specific Enabler (SE), was designed and developed. The HeartHealth system brings together real-time cloud-based motion evaluation coupled with accurate low-cost motion capture, a personalised ex- ercise rehabilitation programme and an intuitive and fun serious game interface, designed specifically with a Cardiac Rehabilitation population in mind.}
}
@article{RIEGER2019175,
title = {Towards the definitive evaluation framework for cross-platform app development approaches},
journal = {Journal of Systems and Software},
volume = {153},
pages = {175-199},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300743},
author = {Christoph Rieger and Tim A. Majchrzak},
keywords = {Mobile app, Mobile computing, Cross-platform, Multi-platform, Development framework},
abstract = {Mobile app development is hindered by device fragmentation and vendor-specific modifications. Boundaries between devices blur with PC-tablet hybrids on the one side and wearables on the other. Future apps need to support a host of app-enabled devices with differing capabilities, along with their software ecosystems. Prior work on cross-platform app development concerned concepts and prototypes, and compared approaches that target smartphones. To aid choosing an appropriate framework and to support the scientific assessment of approaches, an up-to-date comparison framework is needed. Extending work on a holistic, weighted set of assessment criteria, we propose what could become the definitive framework for evaluating cross-platform approaches. We have based it on sound abstract concepts that allow extensions. The weighting capabilities offer customisation to avoid the proverbial comparison of apples and oranges lurking in the variety of available frameworks. Moreover, it advises on multiple development situations based on a single assessment. In this article, we motivate and describe our evaluation criteria. We then present a study that assesses several frameworks and compares them to Web Apps and native development. Our findings suggest that cross-platform development has seen much progress but the challenges are ever growing. Therefore, additional support for app developers is warranted.}
}
@article{PIENIMAKI2021100283,
title = {Finding fun in non-formal technology education},
journal = {International Journal of Child-Computer Interaction},
volume = {29},
pages = {100283},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100283},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000234},
author = {Maija Pienimäki and Marianne Kinnula and Netta Iivari},
keywords = {Non-formal education, Informal education, Technology education, After-school clubs, Out-of-school learning, Robotics, Programming, Making, Children, Teenagers, Youth, Fun, Enjoyment},
abstract = {In this exploratory study into the world of 8–17-year-old children’s non-formal technology education, two different types of technology education with varying levels of non-formality were investigated to see how participants find fun in these situations as it is apparent that if something is non-mandatory to attend to, there should be some type of enjoyment found in the process. The results of the analysis suggest that there are three main ways children and teenagers have fun in non-formal education: fun from the tasks they are doing, social fun by sharing with other attendants, and pedagogical fun that has been embedded in the learning process. Based on our findings, we offer suggestions for how to add elements of fun in the non-formal technology education, to make it more motivating and enjoyable to the participants.}
}
@article{ZDUN2004131,
title = {Supporting incremental and experimental software evolution by runtime method transformations},
journal = {Science of Computer Programming},
volume = {52},
number = {1},
pages = {131-163},
year = {2004},
note = {Special Issue on Program Transformation},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2004.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167642304000498},
author = {Uwe Zdun},
keywords = {Runtime method transformation, Software evolution, Software adaptation, Patterns},
abstract = {Transformations of object-oriented methods are a prevalent object-oriented programming technique, but in many languages they are not supported at runtime. Therefore it can be hard to apply method transformations for incremental or experimental software evolution, or other problems that require runtime software behavior adaptation. The goal of the work presented in this paper is to provide a better conceptual and technical support for runtime method transformations. A non-intrusive model for method transformations and a set of runtime method transformation primitives are presented. We also present a pattern language for implementing dynamic method abstractions and combining them with languages that do not support dynamic methods natively. As a case study we introduce a runtime transformation framework for the dynamic configuration and composition language Frag, its connection to Java, and an end user programming example.}
}
@article{GENTILE20221,
title = {The role of mental rotation in TetrisTM gameplay: An ACT-R computational cognitive model},
journal = {Cognitive Systems Research},
volume = {73},
pages = {1-11},
year = {2022},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2021.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000991},
author = {Manuel Gentile and Antonio Lieto},
keywords = {Cognitive architectures, ACT-R, Mental rotation, Tetris, Serious games design},
abstract = {The mental rotation ability is an essential spatial reasoning skill in human cognition and has proven to be an essential predictor of mathematical and STEM skills, critical and computational thinking. Despite its importance, little is known about when and how mental rotation processes are activated in games explicitly targeting spatial reasoning tasks. In particular, the relationship between spatial abilities and TetrisTM has been analysed several times in the literature. However, these analyses have shown contrasting results between the effectiveness of Tetris-based training activities to improve mental rotation skills. In this work, we studied whether, and under what conditions, such ability is used in the TetrisTM game by explicitly modelling mental rotation via an ACT-R based cognitive model controlling a virtual agent. The obtained results show meaningful insights into the activation of mental rotation during game dynamics. The study suggests the necessity to adapt game dynamics in order to force the activation of this process and, therefore, can be of inspiration to design learning activities based on TetrisTM or re-design the game itself to improve its educational effectiveness.}
}
@article{TSARKOV2021771,
title = {Toward a socially acceptable model of emotional artificial intelligence},
journal = {Procedia Computer Science},
volume = {190},
pages = {771-788},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.090},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013454},
author = {Vladimir S. Tsarkov and Vladislav A. Enikeev and Alexei V. Samsonovich},
keywords = {affective computing, emotional intelligence, eBICA, believable social agents},
abstract = {The framework of emotional Biologically Inspired Cognitive Architecture (eBICA) is used to define a cognitive model, producing believable socially emotional behavior in a social interaction paradigm in a virtual environment. The paradigm selected for this study is a virtual pet interacting with a human. Empirical results indicate that the combination of somatic factors, moral schemas and rational constraints in one model has the potential to make behavior of a virtual actor more believable, humanlike and socially acceptable. Implications concern future intelligent collaborative robots and virtual assistants.}
}
@article{VALLS20181039,
title = {Urban data and urban design: A data mining approach to architecture education},
journal = {Telematics and Informatics},
volume = {35},
number = {4},
pages = {1039-1052},
year = {2018},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2017.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0736585317303416},
author = {Francesc Valls and Ernesto Redondo and David Fonseca and Ricardo Torres-Kompen and Sergi Villagrasa and Nuria Martí},
keywords = {Data mining, Urban data, Architecture education, Informal learning},
abstract = {The configuration of urban projects using Information and Communication Technologies is an essential aspect in the education of future architects. Students must know the technologies that will facilitate their academic and professional development, as well as anticipating the needs of the citizens and the requirements of their designs. In this paper, a data mining approach was used to outline the strategic requirements for an urban design project in an architecture course using a Project-Based Learning strategy. Informal data related to an award-winning public space (Gillett Square in London, UK) was retrieved from two social networks (Flickr and Twitter), and from its official website. The analysis focused on semantic, temporal and spatial patterns, aspects generally overlooked in traditional approaches. Text-mining techniques were used to relate semantic and temporal data, focusing on seasonal and weekly (work-leisure) cycles, and the geographic patterns were extracted both from geotagged pictures and by geocoding user locations. The results showed that it is possible to obtain and extract valuable data and information in order to determine the different uses and architectural requirements of an urban space, but such data and information can be challenging to retrieve, structure, analyze and visualize. The main goal of the paper is to outline a strategy and present a visualization of the results, in a way designed to be attractive and informative for both students and professionals – even without a technical background – so the conducted analysis may be reproducible in other urban data contexts.}
}
@article{IPSITA2024100074,
title = {Authoring instructional flow in iVR learning units to promote outcome-oriented learning},
journal = {Computers & Education: X Reality},
volume = {5},
pages = {100074},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100074},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000242},
author = {Ananya Ipsita and Mayank Patel and Asim Unmesh and Karthik Ramani},
keywords = {Virtual reality training, Backward design, Welding simulator, Instruction flow, Curriculum planning, Skill learning},
abstract = {Despite the recognized efficacy of immersive Virtual Reality (iVR) in skill learning, the design of iVR-based learning units by subject matter experts (SMEs) based on target requirements is severely restricted. This is partly due to a lack of flexible ways of authoring instruction flows to arrange the learning activities in alignment with the desired learning objectives. Our research provides a workflow design enabling SMEs to author the flow of learning activities developed by the Virtual Reality (VR) developers, with an aim to enable learners achieve desired goals progressively in a virtual environment. Additionally, this outcome-oriented flow authoring utilizes a scalable learning framework that categorizes learning activities into four instructional phases: Introduction, Presentation, Practice, and Application. Such frameworks can be easily integrated into the instruction to plan a class or a series of classes to cover an entire concept or chapter. Using a welding use case, our user study evaluation with 12 experienced welders indicated positive ratings about the usefulness of such workflows for flexible planning of training scenarios. We envision adoption of such methods could facilitate greater and more efficient adoption of the iVR technologies in pedagogical settings.}
}
@article{HAMM2019103135,
title = {Enabling older adults to carry out paperless falls-risk self-assessments using guidetomeasure-3D: A mixed methods study},
journal = {Journal of Biomedical Informatics},
volume = {92},
pages = {103135},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103135},
url = {https://www.sciencedirect.com/science/article/pii/S153204641930053X},
author = {Julian Hamm and Arthur G. Money and Anita Atwal},
keywords = {Health informatics, Falls, Occupational therapy, Assistive equipment, Self-assessment, Measurement guidance, Extrinsic risk factors, 3D mobile visualisation, Technology-based systems},
abstract = {Background
The home environment falls-risk assessment process (HEFAP) is a widely used falls prevention intervention strategy which involves a clinician using paper-based measurement guidance to ensure that appropriate information and measurements are taken and recorded accurately. Despite the current use of paper-based guidance, over 30% of all assistive devices installed within the home are abandoned by patients. This is in part due to poor fit between the device, the patient, and the environment in which it is installed. Currently HEFAP is a clinician-led process, however, older adult patients are increasingly being expected to collect HEFAP measurements themselves as part of the personalisation agenda. Without appropriate patient-centred guidance, levels of device abandonment to are likely to rise to unprecedented levels. This study presents guidetomeasure-3D, a mobile 3D measurement guidance application designed to support patients in carrying out HEFAP self-assessments.
Aim
The aim of this study is to present guidetomeasure-3D, a web-enabled 3D mobile application that enables older-adult patients to carry out self-assessment measurement tasks, and to carry out a mixed-methods evaluation of its performance, and associated user perceptions of the application, compared with a 2D paper-based equivalent.
Methods
Thirty-four older adult participants took part in a mixed-methods within-subjects repeated measures study set within a living lab. A series of HEFAP self-assessment tasks were carried out according to two treatment conditions: (1) using the 3D guidetomeasure-3D application; (2) using a 2D paper-based guide. SUS questionnaires and semi-structured interviews were completed at the end of the task. A comparative statistical analysis explored performance with regards to measurement accuracy, accuracy consistency, task efficiency, and system usability. Interview transcripts were analysed using inductive and deductive thematic analysis (informed by UTAUT).
Results
The guidetomeasure-3D application outperformed the 2D paper-based guidance in terms of accuracy (smaller mean error difference in 11 out of 12 items), accuracy consistency (p < 0.05, for 6 out of 12 items), task efficiency (p = 0.003), system usability (p < 0.00625, for two out of 10 SUS items), and clarity of guidance (p < 0.0125, for three out of four items). Three high-level themes emerged from interviews: Performance Expectancy, Effort Expectancy, and Social Influence. Participants reported that guidetomeasure-3D provided improved visual quality, clarity, and more precise guidance overall. Real-time audio instruction was reported as being particularly useful, as was the use of the object rotation and zoom functions which were associated with improving user confidence particularly when carrying out more challenging tasks.
Conclusions
This study reveals that older adults using guidetomeasure-3D achieved improved levels of accuracy and efficiency along with improved satisfaction and increased levels of confidence compared with the 2D paper-based equivalent. These results are significant and promising for overcoming HEFAP equipment abandonment issue. Furthermore they constitute an important step towards overcoming challenges associated with older adult patients, the digitisation of healthcare, and realising the enablement of patient self-care and management via the innovative use of mobile technologies. Numerous opportunities for the generalisability and transferability of the findings of this research are also proposed. Future research will explore the extent to which mobile 3D visualisation technologies may be utilised to optimise the clinical utility of HEFAP when deployed by clinicians.}
}
@article{LEHIKKO2024100066,
title = {Exploring interactivity effects on learners’ sense of agency, cognitive load, and learning outcomes in immersive virtual reality: A mixed methods study},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100066},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100066},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000163},
author = {Anu Lehikko and Mikko Nykänen and Kristian Lukander and Jose Uusitalo and Heli Ruokamo},
keywords = {Immersive virtual reality, Interactivity, Sense of agency, Cognitive load, Learning Outcomes},
abstract = {This study explored the effects of IVR interactivity on learners’ sense of agency, cognitive load, and learning outcomes. The research questions were: 1. “How does interactivity influence the learners’ sense of agency?” and 2. “How does interactivity influence the learners’ cognitive load and learning outcomes?” A mixed-methods experimental design was applied. Safety training interventions, including individually performed IVR scenarios, were held for 68 participants in groups of two to four persons in two work organizations. Single- and repeated-measure questionnaires were the main source of the quantitative data. Qualitative data collection by video recordings and stimulated recall interviews was carried out on 23 persons in total. The results indicate that high interactivity enables a stronger sense of agency for the learners and yields learning benefits by supporting generative cognitive processing. Based on the results, interactivity and learner involvement may be particularly important for achieving affective training goals. Considering the sociocultural and individual factors in training design and pre-briefing the learners are also recommended.}
}
@article{EVANGELISTA20231470,
title = {Advanced visualization of ergonomic assessment data through industrial Augmented Reality},
journal = {Procedia Computer Science},
volume = {217},
pages = {1470-1478},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.346},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922024310},
author = {Alessandro Evangelista and Vito Modesto Manghisi and Sara Romano and Vito {De Giglio} and Lorenzo Cipriani and Antonio Emmanuele Uva},
keywords = {Ergonomics, Augmented Reality, RULA, Human-Centered, Kinect v2, Hololens 2},
abstract = {The industrial transition to the 4.0 paradigm defines new scenarios in which the operator plays a central role within the industrial ecosystem. Thanks to the enabling technologies of Industry 4.0, it is possible to effectively improve operators' working conditions by applying the Human-Centered approach. Nowadays, one of the main challenges is to reduce work-related musculoskeletal disorders resulting from ergonomically incorrect working conditions in order to prevent the occurrence of occupational diseases. To this end, we developed a software tool that leverages a low-cost D-RGB camera (Kinect v2) to track the human body and an Augmented Reality (AR) visualization system based on Microsoft HoloLens 2. The tool assesses postural ergonomic risk in real-time according to the Rapid Upper Limb Assessment (RULA) metric. The proposed AR application allows a three-dimensional visualization of postures, which can be observed directly superimposed on the operator's body in the real scene. This approach aims to optimize the understanding of postures by creating a link between real information (operator's body) and virtual information (virtual skeleton, RULA score, and angles) by providing a simple and immediate user interface for ergonomists.}
}
@article{HUBAL2006532,
title = {Informed consent procedures: An experimental test using a virtual character in a dialog systems training application},
journal = {Journal of Biomedical Informatics},
volume = {39},
number = {5},
pages = {532-540},
year = {2006},
note = {Dialog Systems for Health Communications},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2005.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1532046405001401},
author = {Robert C. Hubal and Ruth S. Day},
keywords = {Informed consent procedures, Intelligent agents, Discourse processes, Effectiveness evaluation, Virtual reality},
abstract = {Researchers are generally trained to administer informed consent by studying approved guidelines, but still can fail to satisfactorily answer questions from potential participants. An application using a virtual character allowed novice participants to practice administering informed consent. This character was designed to behave as a potential participant for a study and asked many of the questions research participants typically ask, such as queries about the study itself, the sponsor, timing, selection procedures, confidentiality, voluntariness, benefits and risks, and contact information. The user responded to the character’s queries as if speaking with a true potential research participant. The application was effective even after only brief usage. In a laboratory experiment, novice participants who practiced with the virtual character were later more effective in conducting informed consent interviews with a human interviewee than those who were trained only with written materials. Thus, simulated learning-by-doing improved informed consent skills. Implications for related health dialog applications are discussed.}
}
@article{BIRRELL2022103843,
title = {Urban air mobility infrastructure design: Using virtual reality to capture user experience within the world's first urban airport},
journal = {Applied Ergonomics},
volume = {105},
pages = {103843},
year = {2022},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103843},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022001661},
author = {Stewart Birrell and William Payre and Katie Zdanowicz and Paul Herriotts},
keywords = {Urban air mobility, User experience, Virtual reality, User-centred design, Human factors},
abstract = {Human factors research can play an important role in the successful design of infrastructure to support future mobility. Through engaging users and stakeholders early in the design process we can gain insights before the physical environments are built. This paper presents data from a truly novel application of Virtual Reality (VR), where user experience and wayfinding were evaluated within an emerging future transport infrastructure to support urban air mobility (UAM) – the urban airport (aka vertiports). Urban airports are located in city centres where drones or ‘flying cars’ would land and take off from. Previous quantitative studies have investigated passenger experience in traditional airports using field observation and surveys, but this paper is the first to present qualitative research on user experience in this emerging mobility infrastructure using an immersive VR environment. Twenty participants completed a series of six scenarios aimed at understanding customer ‘exciters’ and ‘pain points’ within an urban airport. Results and recommendations from this empirical research will help inform the design of all future mobility infrastructure solutions, through improving user experience before the infrastructure is physically deployed. Finally, this paper highlights the benefits of engaging users at an early stage of the design process to ensure that future transport infrastructure will be accessible, easy to navigate and a pleasure to use.}
}
@article{GOERTZEL2014158,
title = {A Software Architecture for Generally Intelligent Humanoid Robotics},
journal = {Procedia Computer Science},
volume = {41},
pages = {158-163},
year = {2014},
note = {5th Annual International Conference on Biologically Inspired Cognitive Architectures, 2014 BICA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.11.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914015440},
author = {Ben Goertzel and David Hanson and Gino Yu},
keywords = {Consciousness, Information Integration, Global Workspace, Nonlinear Dynamics},
abstract = {This paper summarizes the authors’ thinking regarding the design of a software framework for interfacing between general-intelligence-oriented software systems and complex mobile robots, including humanoid robots. The framework describes incorporates perception synthesis, action orchestration, and high level control, and is designed to effectively leverage existing relevant software frameworks such as ROS and Blender. An initial case study motivating this work is the use of the OpenCog AGI (Artificial General Intelligence) software framework to help control humanoid robots created by Hanson Robotics.}
}
@article{PINHEIRO2020102418,
title = {Mutating code annotations: An empirical evaluation on Java and C# programs},
journal = {Science of Computer Programming},
volume = {191},
pages = {102418},
year = {2020},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2020.102418},
url = {https://www.sciencedirect.com/science/article/pii/S0167642320300290},
author = {Pedro Pinheiro and José Carlos Viana and Márcio Ribeiro and Leo Fernandes and Fabiano Ferrari and Rohit Gheyi and Baldoino Fonseca},
keywords = {Mutation testing, Code annotations, Mining bugs},
abstract = {Mutation testing injects code changes to check whether tests can detect them. Mutation testing tools use mutation operators that modify program elements such as operators, names, and entire statements. Most existing mutation operators focus on imperative and object-oriented language constructs. However, many current projects use meta-programming through code annotations. In a previous work, we have proposed nine mutation operators for code annotations focused on the Java programming language. In this article, we extend our previous work by mapping the operators to the C# language. Moreover, we enlarge the empirical evaluation. In particular, we mine Java and C# projects that make heavy use of annotations to identify annotation-related faults. We analyzed 200 faults and categorized them as “misuse,” when the developer did not appear to know how to use the code annotations properly, and “wrong annotation parsing” when the developer incorrectly parsed annotation code (by using reflection, for example). Our operators mimic 95% of the 200 mined faults. In particular, three operators can mimic 82% of the faults in Java projects and 84% of the faults in C# projects. In addition, we provide an extended and improved repository hosted on GitHub with the 200 code annotation faults we analyzed. We organize the repository according to the type of errors made by the programmers while dealing with code annotations, and to the mutation operator that can mimic the faults. Last but not least, we also provide a mutation engine, based on these operators, which is publicly available and can be incorporated into existing or new mutation tools. The engine works for Java and C#. As implications for practice, our operators can help developers to improve test suites and parsers of annotated code.}
}
@article{HOU2023106066,
title = {HINNet: Inertial navigation with head-mounted sensors using a neural network},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106066},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106066},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623002506},
author = {Xinyu Hou and Jeroen H.M. Bergmann},
keywords = {Machine learning, Inertial navigation, Pedestrian Dead Reckoning, Deep neural network, Inertial measurement unit, Wearable sensors},
abstract = {Human inertial navigation systems have been developing rapidly in recent years, and it has shown great potential for applications within healthcare, smart homes, sports, and emergency services. Placing inertial measurement units on the head for localisation is relatively new. However, it provides a very interesting option, as there are several everyday head-worn items that could easily be equipped with sensors. Yet, there remains a lack of research in this area and currently no localisation solutions have been offered that allow for free head-rotations during long periods of walking. To solve this problem, we present HINNet, the first deep neural network (DNN) pedestrian inertial navigation system allowing free head movements with head-mounted inertial measurement units (IMUs), which deploys a 2-layer bi-directional LSTM. A new ’peak ratio’ feature is introduced and utilised as part of the input to the neural network. This information can be leveraged to solve the issue of differentiating between changes in movements related to the head and those that are associated with the walking pattern. A dataset with 8 subjects totalling 528 min has been collected on three different tracks for training and verification. The HINNet could effectively distinguish head rotations and changes in walking direction with a distance percentage error of 0.46%, a relative trajectory error of 3.88 m, and a absolute trajectory error of 5.98 m, which outperforms the current best head-mounted Pedestrian Dead Reckoning (PDR) method.}
}
@article{KASURINEN2017341,
title = {Usability Issues of Virtual Reality Learning Simulator in Healthcare and Cybersecurity},
journal = {Procedia Computer Science},
volume = {119},
pages = {341-349},
year = {2017},
note = {6th International Young Scientist Conference on Computational Science, YSC 2017, 01-03 November 2017, Kotka, Finland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.193},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917324031},
author = {Jussi Kasurinen},
keywords = {Virtual reality, usability, user experience, case study, cybersecurity},
abstract = {The virtualization and the digital environments are common learning platforms in several different domains, such as in flying airplanes or controlling nuclear power plants. However, virtual reality is no longer expensive special hardware; the basic installations for virtual and augmented reality can be done within household budgets and with common customer products. In this paper, we study the aspect of usability issues in a scenario, where a new virtual learning environment is built to teach correct prevention mechanics and strategies against common physical and cybersecurity threats in healthcare, namely in a hospital. Our proof-of-concept studies indicate that the concept is functional and that on hardware level components exist. The problems are in the usability and user immersion aspects, which are discussed in this paper and further studied in the proposed research setting.}
}
@article{HASENBEIN2022107282,
title = {Learning with simulated virtual classmates: Effects of social-related configurations on students’ visual attention and learning experiences in an immersive virtual reality classroom},
journal = {Computers in Human Behavior},
volume = {133},
pages = {107282},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2022.107282},
url = {https://www.sciencedirect.com/science/article/pii/S0747563222001042},
author = {Lisa Hasenbein and Philipp Stark and Ulrich Trautwein and Anna Carolina Muller Queiroz and Jeremy Bailenson and Jens-Uwe Hahn and Richard Göllner},
keywords = {Immersive virtual reality, Classroom simulation, Peer effects, Visual attention, Network analysis, Eye-tracking},
abstract = {Immersive virtual reality (IVR) provides great potential to experimentally investigate effects of peers on student learning in class and to strategically deploy virtual peer learners to improve learning. The present study examined how three social-related classroom configurations (i.e., students' position in the classroom, visualization style of virtual avatars, and virtual classmates' performance-related behavior) affect students' visual attention toward information presented in the IVR classroom using a large-scale eye-tracking data set of N = 274 sixth graders. ANOVA results showed that the IVR configurations were systematically associated with differences in learners' visual attention on classmates or the instructional content and their overall gaze distribution in the IVR classroom (Cohen's d ranging from 0.28 to 2.04 for different IVR configurations and gaze features). Gaze-based attention on classmates was negatively related to students' interest in the IVR lesson (d = 0.28); specifically, the more boys were among the observed peers, the lower students' situational self-concept (d = 0.24). In turn, gaze-based attention on the instructional content was positively related to students' performance after the IVR lesson (d = 0.26). Implications for the future use of IVR classrooms in educational research and practice are discussed.}
}
@article{LINAKER201817,
title = {Motivating the contributions: An Open Innovation perspective on what to share as Open Source Software},
journal = {Journal of Systems and Software},
volume = {135},
pages = {17-36},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302169},
author = {J. Linåker and H. Munir and K. Wnuk and C.E. Mols},
keywords = {Open innovation, Open Source Software, Software ecosystem, Contribution strategy, Product planning, Product strategy},
abstract = {Open Source Software (OSS) ecosystems have reshaped the ways how software-intensive firms develop products and deliver value to customers. However, firms still need support for strategic product planning in terms of what to develop internally and what to share as OSS. Existing models accurately capture commoditization in software business, but lack operational support to decide what contribution strategy to employ in terms of what and when to contribute. This study proposes a Contribution Acceptance Process (CAP) model from which firms can adopt contribution strategies that align with product strategies and planning. In a design science influenced case study executed at Sony Mobile, the CAP model was iteratively developed in close collaboration with the firm’s practitioners. The CAP model helps classify artifacts according to business impact and control complexity so firms may estimate and plan whether an artifact should be contributed or not. Further, an information meta-model is proposed that helps operationalize the CAP model at the organization. The CAP model provides an operational OI perspective on what firms involved in OSS ecosystems should share, by helping them motivate contributions through the creation of contribution strategies. The goal is to help maximize return on investment and sustain needed influence in OSS ecosystems.}
}
@article{GILL2024100070,
title = {Implementing Universal Design through augmented-reality game-based learning},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100070},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000205},
author = {Amarpreet Gill and Derek Irwin and Dave Towey and Yanhui Zhang and Pinzhuang Long and Linjing Sun and Wanling Yu and Yaxin Zheng},
keywords = {Augmented reality, Digital game-based learning, Microlearning, Collaborative learning, Engineering education, Educational technology},
abstract = {Technology integration in higher education (HE) provides educators with the opportunity to design stimulating learning environments, especially in design and engineering education (DEE) where it plays a unique role in nurturing creativity, problem-solving and innovation. This study investigates the integration of an augmented reality (AR) educational game in DEE, focusing on the teaching of Design for Manufacturing and Assembly (DfMA) principles. The primary aim of this research is to enhance traditional DfMA teaching and learning (T&L) practices by applying innovative T&L strategies. The resulting AR DfMA game, developed using digital game-based learning, microlearning and collaborative learning techniques, aligns with the Universal Design for Learning framework to create an inclusive learning environment that encourages participation from all students and supports several learning styles. The study tests the individual features designed for personal study opportunities to discover which elements are optimal in that environment, and then on the proposed in-class group components of the AR game, involving 68 participants from the appropriate program, though it is not currently implemented as part of a module. We utilize a mixed methods approach to examine the students’ experience and identify key design features that contribute to an inclusive educational experience. The findings highlight positive student experiences, and preferences for hands-on engagement, multimodal content, and collaborative activities. The AR DfMA game also has the potential to enhance intrinsic motivation and create an active and inclusive learning environment. Challenges and areas for improvement are also discussed.}
}
@article{LANG2019118,
title = {Mixed reality in production and logistics: Discussing the application potentials of Microsoft HoloLensTM},
journal = {Procedia Computer Science},
volume = {149},
pages = {118-129},
year = {2019},
note = {ICTE in Transportation and Logistics 2018 (ICTE 2018)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.115},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930122X},
author = {Sebastian Lang and Mohammed Saif Sheikh {Dastagir Kota} and David Weigert and Fabian Behrendt},
keywords = {Mixed Reality, Microsoft HoloLens, Production, Logistics, Manufacturing},
abstract = {In 2016, Microsoft released the mixed reality (MR) head-mounted display (HMD) HoloLensTM. As augmented reality (AR) devices, the HoloLensTM can enrich the user’s perceived environment with virtual information. However, Microsoft does not consider the HoloLensTM as AR device, but as the first MR device, since it has some additional features compared to competing AR devices. Especially to mention is the possibility to interact with virtual objects and vice versa. This paper shall provide an insight about the application potentials of MR in the field of production and logistics. For this purpose, we present the findings of a literature review about the current state of research concerning the application of Microsoft HoloLensTM in the field of production and logistics. Furthermore, we present a small HoloLensTM application, which we have developed to evaluate the capabilities of the device. Several persons tested our application and gave us feedback concerning the utility and usability of the HoloLensTM. We provide an evaluation of the user experiences at the end of the paper.}
}
@article{KRAUSEGLAU2022107007,
title = {Collaborative program comprehension via software visualization in extended reality},
journal = {Information and Software Technology},
volume = {151},
pages = {107007},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107007},
url = {https://www.sciencedirect.com/science/article/pii/S095058492200132X},
author = {Alexander Krause-Glau and Malte Hansen and Wilhelm Hasselbring},
keywords = {Program comprehension, Software visualization, City metaphor, Extended reality, Virtual reality, Augmented reality},
abstract = {Context:
In software visualization research, various approaches strive to create immersive environments by employing extended reality devices. In that context, only few research has been conducted on the effect of collaborative, i.e., multi-user, extended reality environments.
Objective:
We present our journey toward a web-based approach to enable (location-independent) collaborative program comprehension using desktop, virtual reality, and mobile augmented reality devices.
Method:
We designed and implemented three multi-user modes in our web-based live trace visualization tool ExplorViz. Users can employ desktop, mobile, and virtual reality devices to collaboratively explore software visualizations. We conducted two preliminary user studies in which subjects evaluated our VR and AR modes after solving common program comprehension tasks.
Results:
The VR and AR environments can be suitable for collaborative work in the context of program comprehension. The analyzed feedback revealed problems regarding the usability, e.g., readability of visualized entities and performance issues. Nonetheless, our approach can be seen as a blueprint for other researchers to replicate or build upon these modes and results.
Conclusions:
ExplorViz’s multi-user modes are our approach to enable heterogeneous collaborative software visualizations. The preliminary results indicate the need for more research regarding effectiveness, usability, and acceptance. Unlike related work, we approach the latter by introducing a multi-user augmented reality environment for software visualizations based on off-the-shelf mobile devices.}
}
@article{MANTZIOU2015241,
title = {Do Children in the Spectrum of Autism Interact with Real-time Emotionally Expressive Human Controlled Avatars?},
journal = {Procedia Computer Science},
volume = {67},
pages = {241-251},
year = {2015},
note = {Proceedings of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.268},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915031142},
author = {Olga Mantziou and Ioannis Vrellis and Tassos A. Mikropoulos},
keywords = {Autism Spectrum Disorder, Facial Emotion Recognition, low and high functioning autism, avatar},
abstract = {Children in Autism Spectrum Disorder (ASD) are characterized by impairments in social skills and they usually face difficulties in recognizing facial emotion expressions. Early intervention and treatment is of major concern and therefore a number of detection and teaching methods have been developed, including the use of ICT. This article presents a brief but extensive literature review on the way tutors are represented in digital environments. The results showed that there is a need for further investigation on the effectiveness of the methods used for the interaction of children on ASD with technology tools, as well as on knowledge transfer. Since there is a lack of empirical data concerning the preference of different interaction modalities by children with ASD, this article also reports on an exploratory study conducted to investigate the acceptance and preference of three different real-time modalities used in facial emotion recognition by two children with ASD (low and high functioning autism). The results indicated a discrepancy between the two children which can be mainly attributed to the differences accompanied the categorization of children with ASD in low and high functioning autism.}
}
@article{DAVID2019646,
title = {Development of Escape Room Game using VR Technology},
journal = {Procedia Computer Science},
volume = {157},
pages = {646-652},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311421},
author = {David David and  Edwin and Edward Arman and  Hikari and Natalia Chandra and Nadia Nadia},
keywords = {Virtual Reality, Presence, Prototype, Unity, Samsung Gear VR},
abstract = {Escape room is one of the media games that can improve the logic of thinking. Puzzles in the escape room traditionally have disadvantages because the type of puzzle that is made requires a lot of material. The purpose of this research is to produce a game with Escape Room as the basic theme with Virtual Reality technology. Virtual Reality technology is used to develop presence in users, attendance is about the intimacy of users with the gaming world. By using Virtual Reality, the puzzle elements that are created can be replaced regularly without the need to change the building’s skeleton. The development method used is a prototype model using Unity game machines. The research method was carried out using a questionnaire for user analysis. The application generated from this research is the Escape Room VR game that can be played on an Android smartphone that is compatible with Samsung Gear VR. The application can be used as an additional means for traditional Escape Room games.}
}
@article{SADEGHIESFAHLANI2018150,
title = {Validity of the Kinect and Myo armband in a serious game for assessing upper limb movement},
journal = {Entertainment Computing},
volume = {27},
pages = {150-156},
year = {2018},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1875952117300952},
author = {Shabnam {Sadeghi Esfahlani} and Bogdan Muresan and Alireza Sanaei and George Wilson},
keywords = {Kinect v2, Monte Carlo Tree Search (MCTS), Myo armband, FootPedal},
abstract = {A cost-effective, easily-accessible neuro-motor rehabilitation solution is proposed that can determine the range of motion and the kinematic ability of participants. A serious game comprising four-scenarios are developed in which the players control an avatar that mirrors the rotations of the upper-limb joints through multi-channel-input devices (Kinect, Myo, FootPedal). Administered functional reach tests (FRT) challenge the player to interact with a 3D-environment while standing or sitting and using the FootPedal which simulates the action of walking whilst body movement is measured concurrently. The FRT’s complexity level is adapted using a Monte Carlo Tree Search algorithm which determines a virtual object’s position based on the proved ability of the user. Twenty-three volunteers were recruited to play the game in 45-min sessions. The data show that the system has a more positive impact on players performance and is more motivating than formal therapy. The visual representation of the trajectory of the objects is shown to increase the perception of the participants voluntary/involuntary upper extremity movement, and the results show a comparable inter-session reliability (acceptable-good) over two repeated sessions. A high Pearson correlation demonstrates the validity of using Kinect and Myo devices in assessing upper-limb rehabilitation, and the timing and the clinically relevant movement data have a higher accuracy when the devices are paired.}
}
@article{FUENTESREYES202374,
title = {A 2D/3D multimodal data simulation approach with applications on urban semantic segmentation, building extraction and change detection},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {205},
pages = {74-97},
year = {2023},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2023.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S092427162300254X},
author = {Mario {Fuentes Reyes} and Yuxing Xie and Xiangtian Yuan and Pablo d’Angelo and Franz Kurz and Daniele Cerra and Jiaojiao Tian},
keywords = {3D change detection, Building extraction, Urban semantic segmentation, Synthetic datasets},
abstract = {Advances in remote sensing image processing techniques have further increased the demand for annotated datasets. However, preparing annotated multi-temporal 2D/3D multimodal data is especially challenging, both for the increased costs of the annotation step and the lack of multimodal acquisitions available on the same area. We introduce the Simulated Multimodal Aerial Remote Sensing (SMARS) dataset, a synthetic dataset aimed at the tasks of urban semantic segmentation, change detection, and building extraction, along with a description of the pipeline to generate them and the parameters required to set our rendering. Samples in the form of orthorectified photos, digital surface models and ground truth for all the tasks are provided. Unlike existing datasets, orthorectified images and digital surface models are derived from synthetic images using photogrammetry, yielding more realistic simulations of the data. The increased size of SMARS, compared to available datasets of this kind, facilitates both traditional and deep learning algorithms. Reported experiments from state-of-the-art algorithms on SMARS scenes yield satisfactory results, in line with our expectations. Both benefits of the SMARS datasets and constraints imposed by its use are discussed. Specifically, building detection on the SMARS-real Potsdam cross-domain test demonstrates the quality and the advantages of proposed synthetic data generation workflow. SMARS is published as an ISPRS benchmark dataset and can be downloaded from https://www2.isprs.org/commissions/comm1/wg8/benchmark_smars/.}
}
@article{RYDZEWSKI201722,
title = {A distributed system for conducting chess games in parallel},
journal = {Procedia Computer Science},
volume = {119},
pages = {22-29},
year = {2017},
note = {6th International Young Scientist Conference on Computational Science, YSC 2017, 01-03 November 2017, Kotka, Finland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917323669},
author = {Aleksander Rydzewski and Paweł Czarnul},
keywords = {chess engine, Universal Chess Interface protocol, Elo ranking, cloud computing, loosely coupled architecture},
abstract = {This paper proposes a distributed and scalable cloud based system designed to play chess games in parallel. Games can be played between chess engines alone or between clusters created by combined chess engines. The system has a built-in mechanism that compares engines, based on Elo ranking which finally presents the strength of each tested approach. If an approach needs more computational power, the design of the system allows it to scale. The system was designed using a loosely coupled architecture approach and the master-slave pattern. It works under Unix or MacOS operating systems. In order to split chess engine processing between every CPU in the system the Akka technology with the Scala language was used while the other part was written in Java. We tested many free chess engines connected to the system by the UCI protocol supported by the proposed system. CloudAMQP is an implementation of Advanced Message Queue Protocol and was used as a message-oriented middleware. This layer was created to split games between every available processing node connected to the system. This element also contributes to greater fault tolerance. We present results of games played between many available chess engines.}
}
@article{MATHEWS20167,
title = {A Virtual Reality Environment for Rehabilitation of Prospective Memory in Stroke Patients},
journal = {Procedia Computer Science},
volume = {96},
pages = {7-15},
year = {2016},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 20th International Conference KES-2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.081},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916318695},
author = {Moffat Mathews and Antonija Mitrovic and Stellan Ohlsson and Jay Holland and Audrey McKinley},
keywords = {virtual reality environment, rehabilitation of prospective memory, evaluation},
abstract = {Prospective Memory (PM), or remembering to perform actions in the future, is of crucial importance for everyday life. This kind of memory is often impaired in stroke survivors and can interfere with independent living. We have developed a computer-based treatment which uses visual imagery to teach participants how to remember time- and event-based prospective memory tasks better. After the treatment, participants practiced their PM skills using videos first, and later in a Virtual Reality (VR) environment. The VR environment uses Constraint-Based Modeling (CBM) to track the user actions and provide individual feedback. We report on a study with 15 stroke survivors, which shows that our treatment is highly effective.}
}
@article{KARABELNIKOVA2021414,
title = {Virtual partner dance as a paradigm for empirical study of cognitive models of emotional intelligence},
journal = {Procedia Computer Science},
volume = {190},
pages = {414-433},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.050},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012953},
author = {Yuliana Karabelnikova and Alexei V. Samsonovich},
keywords = {emotional intelligence, cognitive model, social virtual agent, affective computing, moral schema},
abstract = {This work presents a new paradigm for empirical study of cognitive models of emotional intelligence and preliminary results obtained within it. Among computational approaches to modeling human emotions, the eBICA cognitive architecture takes a special place, because it provides a framework for unification of cognitive and physiological models of emotional intelligence, dimensional and componential representations of emotions, and more. Selected paradigm involves spontaneous interactions among three dancing avatars, one or two of which can be controlled by human participants. Available behaviors include selection of a partner, changing facial expressions and dance patterns. Several types of virtual dance partners are implemented based on the eBICA model: “timid”, “ringleader”, “dancer”, “naïve”, and intermediate cases. Simulations and empirical data show that the selected approach is useful and can potentially lead to a socially attractive, emotionally intelligent behavior.}
}
@article{VANKIPURAM2011432,
title = {Toward automated workflow analysis and visualization in clinical environments},
journal = {Journal of Biomedical Informatics},
volume = {44},
number = {3},
pages = {432-440},
year = {2011},
note = {Biomedical Complexity and Error},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2010.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S1532046410000845},
author = {Mithra Vankipuram and Kanav Kahol and Trevor Cohen and Vimla L. Patel},
keywords = {Clinical workflow, Complex systems, Medical errors, Radio identification, Virtual reality, Visualization},
abstract = {Lapses in patient safety have been linked to unexpected perturbations in clinical workflow. The effectiveness of workflow analysis becomes critical to understanding the impact of these perturbations on patient outcome. The typical methods used for workflow analysis, such as ethnographic observations and interviewing, are limited in their ability to capture activities from different perspectives simultaneously. This limitation, coupled with the complexity and dynamic nature of clinical environments makes understanding the nuances of clinical workflow difficult. The methods proposed in this research aim to provide a quantitative means of capturing and analyzing workflow. The approach taken utilizes recordings of motion and location of clinical teams that are gathered using radio identification tags and observations. This data is used to model activities in critical care environments. The detected activities can then be replayed in 3D virtual reality environments for further analysis and training. Using this approach, the proposed system augments existing methods of workflow analysis, allowing for capture of workflow in complex and dynamic environments. The system was tested with a set of 15 simulated clinical activities that when combined represent workflow in trauma units. A mean recognition rate of 87.5% was obtained in automatically recognizing the activities.}
}
@article{PENTANGELO2024107512,
title = {SENEM: A software engineering-enabled educational metaverse},
journal = {Information and Software Technology},
volume = {174},
pages = {107512},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107512},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001174},
author = {Viviana Pentangelo and Dario {Di Dario} and Stefano Lambiase and Filomena Ferrucci and Carmine Gravino and Fabio Palomba},
keywords = {Metaverse engineering, Virtual learning environments, Human-centered studies, Software engineering in practice},
abstract = {Context:
The term metaverse refers to a persistent, virtual, three-dimensional environment where individuals may communicate, engage, and collaborate. One of the most multifaceted and challenging use cases of the metaverse is education, where educators and learners may require multiple technical, social, psychological, and interaction instruments to accomplish their learning objectives. While the characteristics of the metaverse might nicely fit the problem’s needs, our research points out a noticeable lack of knowledge into (1) the specific requirements that an educational metaverse should actually fulfill to let educators and learners successfully interact towards their objectives and (2) how to design an appropriate educational metaverse for both educators and learners.
Objective:
In this paper, we aim to bridge this knowledge gap by proposing SENEM, a novel software engineering-enabled educational metaverse. We first elicit a set of functional requirements that an educational metaverse should fulfill.
Method:
In this respect, we conduct a literature survey to extract the currently available knowledge on the matter discussed by the research community, and afterward, we assess and complement such knowledge through semi-structured interviews with educators and learners. Upon completing the requirements elicitation stage, we then build our prototype implementation of SENEM, a metaverse that makes available to educators and learners the features identified in the previous stage. Finally, we evaluate the tool in terms of learnability, efficiency, and satisfaction through a Rapid Iterative Testing and Evaluation research approach, leading us to the iterative refinement of our prototype.
Results:
Through our survey strategy, we extracted nine requirements that guided the tool development that the study participants positively evaluated.
Conclusion:
Our study reveals that the target audience appreciates the elicited design strategy. Our work has the potential to form a solid contribution that other researchers can use as a basis for further improvements.}
}
@article{OLAVERRIMONREAL2019880,
title = {Collaborative approach for a safe driving distance using stereoscopic image processing},
journal = {Future Generation Computer Systems},
volume = {95},
pages = {880-889},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.050},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17307124},
author = {Cristina Olaverri-Monreal and Gerd Ch. Krizek and Florian Michaeler and Rene Lorenz and Matthias Pichler},
keywords = {Cooperative systems, Tailgating, Safety distance, Image capturing},
abstract = {Disregard for the rules regarding the minimum safety distance can make the avoidance of a rear-end collision nearly impossible. In a joint effort to enhance safety and improve the decision making processes on an individual level, we contribute to the state of the art with an innovative and affordable system that identifies vehicles and provides a rear-end distance warning system capable of recognizing dangerous situations, and which can also inform other vehicles of the danger, independent of their communication capabilities or equipment. Vision sensors garner information through the stereoscopic capturing and processing of images by rear cameras to calculate the distance between the leading and following vehicles. Visual data related to the safety distance is provided to the following vehicle in real-time, relying on an asynchronous collaborative process. A detailed error analysis of the distance calculation is provided based on the measurement procedure and roadway geometry. Relying on the communication between the two vehicles, an in-vehicle system was compared to the rear-mounted distance warning system under lab-controlled conditions. Both human–machine interaction paradigms were evaluated in terms of their impact on driver response. Results showed that both systems influenced the driver in keeping a time gap of two seconds.}
}
@article{FAN2021501,
title = {Detection of scene-irrelevant head movements via eye-head coordination information},
journal = {Virtual Reality & Intelligent Hardware},
volume = {3},
number = {6},
pages = {501-514},
year = {2021},
note = {Special Issue on Locomotion Perception and Redirection},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2021.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2096579621000917},
author = {Xiaoxiong Fan and Yun Cai and Yufei Yang and Tianxing Xu and Yike Li and Songhai Zhang and Fanglue Zhang},
keywords = {Virtual reality, Human-centered computing, Human-computer interaction (HCI), Interaction paradigms, HCI design and evaluation methods, User models},
abstract = {Background
Accurate motion tracking in head-mounted displays (HMDs) has been widely used in immersive VR interaction technologies. However, tracking the head motion of users at all times is not always desirable. During a session of HMD usage, users may make scene-irrelevant head rotations, such as adjusting the head position to avoid neck pain or responding to distractions from the physical world. To the best of our knowledge, this is the first study that addresses the problem of scene-irrelevant head movements.
Methods
We trained a classifier to detect scene-irrelevant motions using temporal eyehead-coordinated information sequences. To investigate the usefulness of the detection results, we propose a technique to suspend motion tracking in HMDs where scene-irrelevant motions are detected.
Results/Conclusions
Experimental results demonstrate that the scene-relevancy of movements can be detected using eye-head coordination information, and that ignoring scene-irrelevant head motions in HMDs improves user continuity without increasing sickness or breaking immersion.}
}
@article{ESCUDEIRO2015252,
title = {Virtual Sign – A Real Time Bidirectional Translator of Portuguese Sign Language},
journal = {Procedia Computer Science},
volume = {67},
pages = {252-262},
year = {2015},
note = {Proceedings of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.269},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915031154},
author = {Paula Escudeiro and Nuno Escudeiro and Rosa Reis and Jorge Lopes and Marcelo Norberto and Ana Bela Baltasar and Maciel Barbosa and José Bidarra},
keywords = {Portuguese Sign Language, Education, Deaf, Kinect, Sensor Gloves, Translator, Machine Learning, OpenCv, Support Vector Machines, Dynamic Time Warping, Avatar.},
abstract = {Promoting equity, equal opportunities to all and social inclusion of people with disabilities is a concern of modern societies at large and a key topic in the agenda of European Higher Education. Despite all the progress, we cannot ignore the fact that the conditions provided by the society for the deaf are still far from being perfect. The communication with deaf by means of written text is not as efficient as it might seem at first. In fact, there is a very deep gap between sign language and spoken/written language. The vocabulary, the sentence construction and the grammatical rules are quite different among these two worlds. These facts bring significant difficulties in reading and understanding the meaning of text for deaf people and, on the other hand, make it quite difficult for people with no hearing disabilities to understand sign language. The deployment of tools to assist the daily communication, in schools, in public services, in museums and other, between deaf people and the rest may be a significant contribution to the social inclusion of the deaf community. The work described in this paper addresses the development of a bidirectional translator between Portuguese Sign Language and Portuguese text. The translator from sign language to text resorts to two devices, namely the Microsoft Kinect and 5DT Sensor Gloves in order to gather data about the motion and shape of the hands. The hands configurations are classified using Support Vector Machines. The classification of the movement and orientation of the hands are achieved through the use of Dynamic Time Warping algorithm. The translator exhibits a precision higher than 90%. In the other direction, the translation of Portuguese text to Portuguese Sign Language is supported by a 3D avatar which interprets the entered text and performs the corresponding animations.}
}
@article{OLIVEIRA2012274,
title = {Serious Game in Security: A Solution for Security Trainees},
journal = {Procedia Computer Science},
volume = {15},
pages = {274-282},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.079},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008411},
author = {Vítor Oliveira and António Coelho and Rui Guimarães and Carlos Rebelo},
keywords = {Serious Game, Security, Safety, Multiplayer, Cooperation},
abstract = {Serious games have been used with success for training field operatives in tasks where there is a danger of injury or life threatening situations. This paper presents the development of a serious game aimed at the areas of security and safety, supporting the training of specialists through supervised situational scenarios. DDThe training plans involve security against third parties, focusing on social level security at a corporate level, and also safety actions on events such as floods and fires in buildings/facilities. The game provides a 3D virtual environment of the real location/facility to be secured and a multiplayer platform to allow collaborative training and supervising.DD.}
}
@article{RIVERAFLOR2019641,
title = {Evaluation of Task Workload and Intrinsic Motivation in a Virtual Reality Simulator of Electric-Powered Wheelchairs},
journal = {Procedia Computer Science},
volume = {160},
pages = {641-646},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931734X},
author = {Hamilton Rivera-Flor and Kevin A. Hernandez-Ossa and Berthil Longo and Teodiano Bastos},
keywords = {Electric-Powered Wheelchair, Driving Training, Virtual Reality Simulator},
abstract = {For some people with severe physical disabilities, the immediate driving of an Electric-Powered Wheelchair (EPW) appears as a safety problem, which can be solved by the use of a virtual reality (VR) simulator for safe-driving learning purposes. There are several VR environment approaches in the literature applied to EPW driving training, including several tests that were performed to validate these simulators. This work evaluates the influence of different display devices on task workload and intrinsic motivation of participants, using the Simcadrom EPW Simulator developed at UFES/Brazil. Results from two qualitative tests: Intrinsic Motivation Inventory (IMI) and NASA Task Load Index (NASA-TLX) were compared for three displays (Head Mounted Display – HMD, desktop screen, and video projector). The results show that the HMD provided the highest usefulness score. On the other hand, the desktop screen reported the lowest task workload.}
}
@article{ZHAO2022101828,
title = {Comparing self-navigation and video mode in a choice experiment to measure public space preferences},
journal = {Computers, Environment and Urban Systems},
volume = {95},
pages = {101828},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2022.101828},
url = {https://www.sciencedirect.com/science/article/pii/S0198971522000722},
author = {Yuwen Zhao and Pauline E.W. {van den Berg} and Ioulia V. Ossokina and Theo A. Arentze},
keywords = {3D dynamic visualization, Stated choice experiment, Neighborhood public spaces, Virtual environment, Self-navigation},
abstract = {3D dynamic visualization technologies are increasingly used in studying residents' preferences for urban planning and design scenarios. The techniques help concentrate respondent attention and improve the measurement quality of environmental preferences. However, little is known about differences in measurement quality between different 3D dynamic visualization modes. This paper applies two modes – a self-navigation mode and a video mode - in a virtual environment-based stated choice experiment with the aim of measuring neighborhood public spaces preferences. Based on data from 276 experiment participants and applying conditional and mixed logit techniques, we find no statistically significant differences in the model fit between the two modes. Out of six public space attributes, only one shows statistically significant differences in valuation between modes, namely ‘vertical green’. Our results suggest that the choice between video and self-navigation modes can be based on other (secondary) considerations, i.e., required sample size, respondents' experiences with navigation interfaces, the specific goals of the study or application in practice, and the costs and effort needed to conduct the experiment.}
}
@article{BUREIKO2017182,
title = {Multiscale dynamic visualization of signal transduction processes with detailing of target-genes activation in three-dimensional genome structure},
journal = {Procedia Computer Science},
volume = {119},
pages = {182-189},
year = {2017},
note = {6th International Young Scientist Conference on Computational Science, YSC 2017, 01-03 November 2017, Kotka, Finland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.175},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917323852},
author = {Kseniia Bureiko and Maria Semashko and Ksenia D. Mukhina and Andrey Karsakov},
keywords = {Bioinformatics visualization, signaling pathways, spatial genome structure},
abstract = {Given the complexity of modern biological data it is essentially crucial to accord a consistent expounding. Interpreting such data into complex networks and visualizing them can reveal understanding of various processes in a cell. A consequence mapping of signal transduction processes to the spatial genome structure can benefit new insights in interaction detection in the spatial arrangement of genes. We present an approach for multiscale dynamic visualization of signal transduction processes with detailing of target-genes activation in spatial genome structure. The usage of this approach is demonstrated for the WNT signaling pathway in a human cell. We conclude with suggesting future research questions to improve our approach by considering new available data.}
}
@article{MANGHISI20221338,
title = {A Virtual Reality Approach for Assisting Sustainable Human-Centered Ergonomic Design: The ErgoVR tool},
journal = {Procedia Computer Science},
volume = {200},
pages = {1338-1346},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.335},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003441},
author = {Vito M. Manghisi and Alessandro Evangelista and Antonio E. Uva},
keywords = {Ergonomics, RULA, Virtual Reality, User-centered, Collaborative design, Kinect V2},
abstract = {Industry 4.0 is characterized by great potential for innovation impacting the operator’s role, increasingly engaged in smart activities of a decision-making nature. In such a working scenario, operators’ working conditions can be effectively improved by applying a user-centered collaborative design approach. To this end, we developed a Virtual Reality-based multiplayer tool exploiting low-cost body tracking technology to evaluate ergonomic postural risk. The tool allows evaluating both in real-time and off-line the ergonomic postural risk according to the Rapid Upper Limb Assessment metrics. By applying this approach, a twofold advantage can be achieved. On the one hand, ergonomic experts can have an immersive three-dimensional visualization of postures even in off-line observations. On the other hand, it is possible to evaluate the ergonomics of workstations in the design phase by having the operator work on virtual mock-ups of workstations, thus allowing a sustainable approach to user-centered collaborative design.}
}
@article{BIERCEWICZ20232057,
title = {VR educational game in public awareness campaign preventing the spread of COVID-19 – a pilot study},
journal = {Procedia Computer Science},
volume = {225},
pages = {2057-2066},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.196},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013546},
author = {Konrad Biercewicz and Anna Borawska and Mariusz Borawski and Jarosław Duda},
keywords = {virtual reality games, public awareness campaign, EEG, engagement},
abstract = {Within a few years, virtual reality (VR) has generated a lot of interest. With the development of this technology, many opportunities for its use have arisen. Among the most popular applications, one can also mention education. The typical VR approach in education involves applying games to increase the learner's involvement and motivation. The question is whether engaging in a virtual reality game designed for a public awareness campaign can help the participants enhance their assessment of social distancing measured in meters, potentially aiding in the reduction of COVID-19 transmission. The Sustainable Development Goals have been greatly affected by the COVID-19 pandemic, impacting almost all countries around the world. This has led to changes in political and health priorities as well as research strategies. For this purpose, a pilot study was conducted on 4 people in the game created in the Unity engine. The respondents’ reactions were examined with the use of the following EEG indices - Engagement, Arousal, Valence, and Approach-Withdrawal were used. As a result, it was obtained that the created game can improve the assessment of distance by the respondents. Therefore, we can conclude that applying virtual reality games can teach people to follow certain rules and contribute to slowing down the spread of COVID-19.}
}
@article{MARUYAMA2016250,
title = {Motion-capture-based walking simulation of digital human adapted to laser-scanned 3D as-is environments for accessibility evaluation},
journal = {Journal of Computational Design and Engineering},
volume = {3},
number = {3},
pages = {250-265},
year = {2016},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2016.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2288430015300142},
author = {Tsubasa Maruyama and Satoshi Kanai and Hiroaki Date and Mitsunori Tada},
keywords = {Digital human model, Walking simulation, Laser-scanning, Accessibility evaluation, Motion-capture, Human behavior simulation},
abstract = {Owing to our rapidly aging society, accessibility evaluation to enhance the ease and safety of access to indoor and outdoor environments for the elderly and disabled is increasing in importance. Accessibility must be assessed not only from the general standard aspect but also in terms of physical and cognitive friendliness for users of different ages, genders, and abilities. Meanwhile, human behavior simulation has been progressing in the areas of crowd behavior analysis and emergency evacuation planning. However, in human behavior simulation, environment models represent only “as-planned” situations. In addition, a pedestrian model cannot generate the detailed articulated movements of various people of different ages and genders in the simulation. Therefore, the final goal of this research was to develop a virtual accessibility evaluation by combining realistic human behavior simulation using a digital human model (DHM) with “as-is” environment models. To achieve this goal, we developed an algorithm for generating human-like DHM walking motions, adapting its strides, turning angles, and footprints to laser-scanned 3D as-is environments including slopes and stairs. The DHM motion was generated based only on a motion-capture (MoCap) data for flat walking. Our implementation constructed as-is 3D environment models from laser-scanned point clouds of real environments and enabled a DHM to walk autonomously in various environment models. The difference in joint angles between the DHM and MoCap data was evaluated. Demonstrations of our environment modeling and walking simulation in indoor and outdoor environments including corridors, slopes, and stairs are illustrated in this study.}
}
@article{JAFARI201622,
title = {PTRebeca: Modeling and analysis of distributed and asynchronous systems},
journal = {Science of Computer Programming},
volume = {128},
pages = {22-50},
year = {2016},
note = {Special issue on Automated Verification of Critical Systems (AVoCS’14)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2016.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167642316000800},
author = {Ali Jafari and Ehsan Khamespanah and Marjan Sirjani and Holger Hermanns and Matteo Cimini},
keywords = {Probabilistic Timed Automata, Timed Markov Decision Process, IMCA model checker, Probabilistic Timed Rebeca, Model checking, Performance analysis},
abstract = {Distributed systems exhibit probabilistic and non-deterministic behaviors and may have time constraints. Probabilistic Timed Rebeca (PTRebeca) is introduced as a timed and probabilistic actor-based language for modeling distributed real-time systems with asynchronous message passing. The semantics of PTRebeca is a Timed Markov Decision Process. In this paper, we provide SOS rules for PTRebeca, introduce a new tool-set and describe the corresponding mappings. The tool-set automatically generates a Markov Automaton from a PTRebeca model in the form of the input language of the Interactive Markov Chain Analyzer (IMCA). The IMCA can be used as a back-end model checker for performance analysis of PTRebeca models against expected reachability and probabilistic reachability properties. Comparing to the existing tool-set, proposed in the conference paper, we now have the ability of analyzing significantly larger models, and we also can add different rewards to the model. We show the applicability of our approach and efficiency of our tool by analyzing a Network on Chip architecture as a real-world case study.}
}
@article{HEATON2019172,
title = {Design and development of BIM models to support operations and maintenance},
journal = {Computers in Industry},
volume = {111},
pages = {172-186},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519300557},
author = {James Heaton and Ajith Kumar Parlikad and Jennifer Schooling},
keywords = {Building information modelling, Asset management, Asset information model, Asset management systems, Asset classification},
abstract = {Building Information Modelling (BIM) is one of the most significant technological advancements in recent years that has been adopted by the design and construction industry. While BIM adoption is growing, it can be witnessed that adoption is relatively weak within operational and maintenance (O&M) organisations such as Estate and Infrastructure Management, who would ultimately gain the highest value from utilising BIM. While the challenges of BIM adoption are multifaceted, there is a recurring theme of poor data integration between BIM and existing information management systems. There is a clear gap of knowledge on how to structure a BIM model that allows its efficient use in the O&M phase. Furthermore, there is a lack of claritiy on how to exchange information from a BIM model into an Asset Information Model (AIM). This paper outlines a methodology that enables extraction of BIM-related data directly from a model into a relational database for integration with existing asset management systems. The paper describes the BIM model requirements, development of the extraction platform, database architecture and framework. Furthermore, a case study is presented to demonstrate the methodology. The case study demonstrates that if the BIM model is designed from the start with consideration for the O&M requirements, it can be exploited for development into an AIM. It also shows that a structured approach to object classification within a BIM model supports the efficient exchange of data directly from the BIM model.}
}
@article{CUTUMISU200732,
title = {ScriptEase: A generative/adaptive programming paradigm for game scripting},
journal = {Science of Computer Programming},
volume = {67},
number = {1},
pages = {32-58},
year = {2007},
note = {Special Issue on Aspects of Game Programming},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2007.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167642307000536},
author = {Maria Cutumisu and Curtis Onuczko and Matthew McNaughton and Thomas Roy and Jonathan Schaeffer and Allan Schumacher and Jeff Siegel and Duane Szafron and Kevin Waugh and Mike Carbonaro and Harvey Duff and Stephanie Gillis},
keywords = {Generative pattern, Computer game, Scripting language, Adaptive programming, Game scripting, Game authoring, Game agent},
abstract = {The traditional approach to implementing interactions between a player character (PC) and objects in computer games is to write scripts in a procedural scripting language. These scripts are usually so complex that they must be written by a computer programmer rather than by the author of the game story. This interruption in the game story authoring process has two distinct disadvantages: it increases the cost of game production and it introduces a disconnect between the author’s intentions and the interactions produced from the programmer’s written scripts. We introduce a mechanism to solve these problems. We show that game authors (non-programmers) can generate the necessary scripts for implementing meaningful interactions between the PC and game objects using a three-step process. In the first step, the author uses a generative pattern (concept) to create a high-level description of a commonly occurring game scenario. In the second step, the author uses a standard set of adaptation operations to customize the high-level description to the particular circumstances of the story that is being told. In the third step, the author presses a button that automatically generates scripting code from the adapted pattern. We describe the results of three studies in which a combined total of 56 game story authors used this three-step process to construct Neverwinter Nights game stories, using a tool called ScriptEase. We believe that this generative/adaptive process is the key to future game story scripting. More generally, this article advocates the development of adaptive programming as an alternative to current constructive programming techniques, as well as the application of adaptive programming in many domains.}
}
@article{KUMAR2020100077,
title = {Use of mixed reality for surgery planning: Assessment and development workflow},
journal = {Journal of Biomedical Informatics},
volume = {112},
pages = {100077},
year = {2020},
note = {Articles initially published in Journal of Biomedical Informatics: X 5-8, 2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.yjbinx.2020.100077},
url = {https://www.sciencedirect.com/science/article/pii/S2590177X20300111},
author = {Rahul Prasanna Kumar and Egidijus Pelanis and Robin Bugge and Henrik Brun and Rafael Palomar and Davit L. Aghayan and Åsmund Avdem Fretland and Bjørn Edwin and Ole Jakob Elle},
keywords = {Mixed reality, Surgery, Planning, Segmentation},
abstract = {Meticulous preoperative planning is an important part of any surgery to achieve high levels of precision and avoid complications. Conventional medical 2D images and their corresponding three-dimensional (3D) reconstructions are the main components of an efficient planning system. However, these systems still use flat screens for visualisation of 3D information, thus losing depth information which is crucial for 3D spatial understanding. Currently, cutting-edge mixed reality systems have shown to be a worthy alternative to provide 3D information to clinicians. In this work, we describe development details of the different steps in the workflow for the clinical use of mixed reality, including results from a qualitative user evaluation and clinical use-cases in laparoscopic liver surgery and heart surgery. Our findings indicate a very high general acceptance of mixed reality devices with our applications and they were consistently rated high for device, visualisation and interaction areas in our questionnaire. Furthermore, our clinical use-cases demonstrate that the surgeons perceived the HoloLens to be useful, recommendable to other surgeons and also provided a definitive answer at a multi-disciplinary team meeting.}
}
@article{KAO2022103216,
title = {Coupled Rigid-Block Analysis: Stability-Aware Design of Complex Discrete-Element Assemblies},
journal = {Computer-Aided Design},
volume = {146},
pages = {103216},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2022.103216},
url = {https://www.sciencedirect.com/science/article/pii/S0010448522000161},
author = {Gene Ting-Chun Kao and Antonino Iannuzzo and Bernhard Thomaszewski and Stelian Coros and Tom {Van Mele} and Philippe Block},
keywords = {3D assembly, Computational fabrication, Stability-aware design, Concave shapes, Friction, Contact mechanics},
abstract = {The rigid-block equilibrium (RBE) method uses a penalty formulation to measure structural infeasibility or to guide the design of stable discrete-element assemblies from unstable geometry. However, RBE is a purely force-based formulation, and it incorrectly describes stability when complex interface geometries are involved. To overcome this issue, this paper introduces the coupled rigid-block analysis (CRA) method, a more robust approach building upon RBE’s strengths. The CRA method combines equilibrium and kinematics in a penalty formulation in a nonlinear programming problem. An extensive benchmark campaign is used to show how CRA enables accurate modelling of complex three-dimensional discrete-element assemblies formed by rigid blocks. In addition, an interactive stability-aware design process to guide user design towards structurally-sound assemblies is proposed. Finally, the potential of our method for real-world problems are demonstrated by designing complex and scaffolding-free physical models.}
}
@article{GONZALEZGONZALEZ2019103266,
title = {Serious games for rehabilitation: Gestural interaction in personalized gamified exercises through a recommender system},
journal = {Journal of Biomedical Informatics},
volume = {97},
pages = {103266},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103266},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419301856},
author = {Carina S. González-González and Pedro A. Toledo-Delgado and Vanesa Muñoz-Cruz and Pablo V. Torres-Carrion},
keywords = {Recommender systems, Exergames, Rehabilitation, Gamification, Serious games},
abstract = {One of the principal problems of rehabilitation is that therapy sessions can be boring due the repetition of exercises. Serious games, and in particular exergames in rehabilitation, can motivate, engage and increase patients’ adherence to their treatment. Also, the automatic personalization of exercises to each patient can help therapists. Thus, the main objective of this work is to build an intelligent exergame-based rehabilitation system consisting of a platform with an exergame player and a designer tool. The intelligent platform includes a recommender system which analyzes user interactions, along with the user’s history, to select new gamified exercises for the user. The main contributions of this paper focus, first, on defining a recommender system based on different difficulty levels and user skills. The recommender system offers the ability to provide the user with a personalized game mode based on his own history and preferences. The results of a triple validation with experts, users and rehabilitation center professionals reveal a positive impact on gestural interaction and rehabilitation uses. Also, different methods are presented for testing the rehabilitation recommender system.}
}
@article{MORENOLUMBRERAS2024111985,
title = {The influence of the city metaphor and its derivates in software visualization},
journal = {Journal of Systems and Software},
volume = {210},
pages = {111985},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.111985},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000281},
author = {David Moreno-Lumbreras and Jesus M. Gonzalez-Barahona and Gregorio Robles and Valerio Cosentino},
keywords = {Software visualization, City metaphor, Software comprehension, Systematic mapping study, Visualizations, State of the art, Extended reality},
abstract = {Context:
The city metaphor is widely used in software visualization to represent complex systems as buildings and structures, providing an intuitive way for developers to understand software components. Various software visualization tools have utilized this approach.
Objective:
Identify the influence of the city metaphor on software visualization research, determine its state-of-the-art status, and identify derived tools and their main characteristics.
Method:
Conduct a systematic mapping study of 406 publications that reference the first paper on the use of the city metaphor in software visualization and/or the main paper of the CodeCity tool. Analyze the 168 publications from which valuable information could be extracted, and build a complete categoric analysis.
Results:
The field has grown considerably, with an increasing number of publications since 2001, and a changing research community with evolving interconnections between groups. Researchers have developed more tools that support the city metaphor, but less than 50% of the tools were referenced in their papers. Moreover, 85% of the tools did not use extended reality environments, indicating an opportunity for further exploration.
Conclusion:
The study demonstrates the active and continually growing presence of the city metaphor in research and its impact on software visualization and its derivatives. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{MORIARTY2012200,
title = {Utilizing Depth Based Sensors and Customizable Software Frameworks for Experiential Application},
journal = {Procedia Computer Science},
volume = {12},
pages = {200-205},
year = {2012},
note = {Complex Adaptive Systems 2012},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.09.054},
url = {https://www.sciencedirect.com/science/article/pii/S187705091200645X},
author = {Brian Moriarty and Elizabeth Lennon and Frank DiCola and Kyle Buzby and Morisa Manzella and Emily Hromada},
keywords = {Kinect, Natural Gesture Interfaces, STEM education, Process Simulator, Depth Sensors, Skeletal Tracking, Artificial Intelligence},
abstract = {Using depth sensor cameras such as the Kinect and highly customizable software development frameworks in conjunction with artificial intelligence methodologies offer significant opportunities in a variety of applications, such as undergraduate science, technology, engineering, and math (STEM) education, professional or military training simulation, and individually-tailored cultural and media arts immersion. Designing a participatory educational experience where users are able to actively interface and experience given subject matter in a practical experiential manner can enhance the user's ability to learn and retain presented information. Such natural gesture user interfaces have potential for broad application in disciplines ranging from systems engineering education to process simulation. This paper will discuss progress on the development of testing environments for interactive educational methods in conjunction with artificial intelligent systems that have the ability to adjust the educational user experience based on individual user identification. This will be achieved through depth sensor skeletal tracking, allowing experience adaptation based on the nature and effectiveness of the interactive educational experience.}
}
@article{KHAN20225741,
title = {Deep Reinforcement Learning Based Unmanned Aerial Vehicle (UAV) Control Using 3D Hand Gestures},
journal = {Computers, Materials and Continua},
volume = {72},
number = {3},
pages = {5741-5759},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.024927},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822009444},
author = {Fawad Salam Khan and Mohd Norzali {Haji Mohd} and Saiful Azrin B. M. Zulkifli and Ghulam E {Mustafa Abro} and Suhail Kazi and Dur Muhammad Soomro},
keywords = {Deep reinforcement learning, UAV, 3D hand gestures, obstacle detection, polar mask},
abstract = {The evident change in the design of the autopilot system produced massive help for the aviation industry and it required frequent upgrades. Reinforcement learning delivers appropriate outcomes when considering a continuous environment where the controlling Unmanned Aerial Vehicle (UAV) required maximum accuracy. In this paper, we designed a hybrid framework, which is based on Reinforcement Learning and Deep Learning where the traditional electronic flight controller is replaced by using 3D hand gestures. The algorithm is designed to take the input from 3D hand gestures and integrate with the Deep Deterministic Policy Gradient (DDPG) to receive the best reward and take actions according to 3D hand gestures input. The UAV consist of a Jetson Nano embedded testbed, Global Positioning System (GPS) sensor module, and Intel depth camera. The collision avoidance system based on the polar mask segmentation technique detects the obstacles and decides the best path according to the designed reward function. The analysis of the results has been observed providing best accuracy and computational time using novel design framework when compared with traditional Proportional Integral Derivatives (PID) flight controller. There are six reward functions estimated for 2500, 5000, 7500, and 10000 episodes of training, which have been normalized between 0 to −4000. The best observation has been captured on 2500 episodes where the rewards are calculated for maximum value. The achieved training accuracy of polar mask segmentation for collision avoidance is 86.36%.}
}
@article{QUISHPEARMAS2015413,
title = {An Immersive 3D Virtual Learning Environment for Analyzing the Atomic Structure of MEMS-Relevant Materials},
journal = {Procedia Computer Science},
volume = {75},
pages = {413-416},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.265},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037266},
author = {Jorge A. Quishpe-Armas and Luis D. Cedeño-Viveros and Javier Meléndez-Campos and Carlos A. Suárez-Mora and Sergio Camacho-Leon},
keywords = {Immersive, Learning Environment, Virtual Reality, MEMS, NEMS},
abstract = {In this paper, an immersive three-dimensional (3D) virtual learning environment based on the Oculus head-mounted display is presented for analyzing the atomic structure of monocrystalline materials relevant to Micro-Electro-Mechanical Systems (MEMS), e.g. Silicon (Si), Chromium (Cr), Titanium (Ti) and Copper (Cu). This environment allows the real-time visualization and interactive analysis of key crystal lattice parameters, e.g. number of atoms in the lattice, atomic packing factor, linear atomic density, surface atomic density, volumetric density, etc.}
}
@article{OHMOTO2016607,
title = {A Support System to Accumulate Interpretations of Multiple Story Timelines},
journal = {Procedia Computer Science},
volume = {96},
pages = {607-616},
year = {2016},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 20th International Conference KES-2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.241},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916320518},
author = {Yoshimasa Ohmoto and Takashi Ookaki and Toyoaki Nishida},
keywords = {Knowledge archives, group learning assistance, theatrical role play, narrative technology ;},
abstract = {The story base interpretation is subjectively summarised and segmented from the first-person viewpoint. However, we often need to objectively represent an entire image by integrated knowledge. Yet, this is a difficult task. We proposed a novel approach, named the synthetic evidential study (SES), for understanding and augmenting collective thought processes through substantiated thought by interactive media. In this study, we investigated the kind of data that can be obtained through the SES sessions as interpretation archives and whether the database is useful to understand multiple story timelines. For the purpose, we designed a machine-readable interpretation data format and developed support systems to create and provide data that are easy to understand. We conducted an experiment using the simulation of the projection phase in SES sessions. From the results, we suggested that a “meta comment” which was deepened interpretation comment by the others in the interpretation archives to have been posted when it was necessary to consider other participants’ interpretation to broaden their horizons before posting the comment. In addition, the construction of networks to represent the relationships between the interpretation comments enabled us to suggest the important comments by using the degree centrality.}
}
@article{KUSUMA2021886,
title = {Enhancing Historical Learning Using Role-Playing Game on Mobile Platform},
journal = {Procedia Computer Science},
volume = {179},
pages = {886-893},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.078},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921001083},
author = {Gede Putra Kusuma and Louis Khrisna {Putera Suryapranata} and Evan Kristia Wigati and Yesun Utomo},
keywords = {Game-Based Learning, Historical Learning, Role-Playing Game, Mobile Platform, Statistical Analysis},
abstract = {Learning history means to learn what caused the world around us to become the way it is right now. For decades, historical events were recorded and taught to younger generations to learn from them. However, most students found that learning history is boring because they could not feel or understand the moment of those historical events. This paper aims to investigate how a game-based learning approach influences the achievement and motivation of historical learning through a mobile learning environment. We propose game-based historical learning using the role-playing game on a mobile platform. The implementation is based on the history of the first army general in Indonesia, General Sudirman. The game was tested to 63 Junior High School students in Jakarta. Based on the evaluation, using the game can increase student learning motivation and learning achievement.}
}
@article{SCHALBETTER2023102003,
title = {From board games to immersive urban imaginaries: Visualization fidelity's impact on stimulating discussions on urban transformation},
journal = {Computers, Environment and Urban Systems},
volume = {104},
pages = {102003},
year = {2023},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2023.102003},
url = {https://www.sciencedirect.com/science/article/pii/S0198971523000662},
author = {Laura Schalbetter and Nicolas Salliou and Ralph Sonderegger and Adrienne Grêt-Regamey},
keywords = {Serious Game, 3D point clouds, Aesthetics, Leverage point, Place},
abstract = {Given the increasing importance of public participation in urban transformation processes, serious games have emerged as a promising tool for fostering active engagement and constructive dialogue. By providing insights into opposing arguments and helping navigate complex trade-offs, serious games can support informed decision-making and promote collaborative solutions. Although digital media are increasingly used in games, applications in serious games are still in their infancy. Furthermore, previous research has primarily focused on resource use rather than on the impact of media on the debate itself. Additionally, there has been little investigation into the role of soft factors, such as aesthetics and interests, in multi-stakeholder discussions, especially as they may be influenced by the medium in immersive serious games. In this contribution, we demonstrate how a three-dimensional realistic serious game triggers different argumentation patterns compared to a board game. The serious game engages the players in a case study in Hochdorf, a Swiss suburban municipality that is stuck in urban transformation. The serious game is based on point-cloud LiDAR data that can be modified using three-dimensional hand-drawn sketches to visualize urban transformation measures. We found that aesthetical issues are discussed and emotional factors are expressed more often in the closer-to-reality game than in the board game. Immersive visualizations influence the discussion about urban transformations as the expression of visions and world views is more straightforward for stakeholders, and the patterns of argumentation appear more diverse.}
}
@article{CECIL2018128,
title = {An IoMT based cyber training framework for orthopedic surgery using Next Generation Internet technologies},
journal = {Informatics in Medicine Unlocked},
volume = {12},
pages = {128-137},
year = {2018},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352914818300285},
author = {J. Cecil and Avinash Gupta and Miguel Pirela-Cruz and Parmesh Ramanathan},
keywords = {Cyber physical systems, Internet of Things, Surgical training, Telemedicine, Virtual reality, Internet of Medical Things},
abstract = {Internet of Things based approaches and frameworks hold significant potential in changing the way in which engineering activities are accomplished. The information centric revolution underway has served as a catalyst in the design of innovative methods and practices in several engineering and other domains. In this paper, an Internet of Medical Things based framework for surgical training is discussed in the broader context of Next Generation frameworks. The design and development of this Internet of Medical Things based framework involving adoption of Global Environment for Network Innovations based networking principles is elaborated. The Virtual Reality based simulation environments incorporate haptic based interfaces which support collaborative training and interactions among expert surgeons and residents in orthopedic surgery from distributed locations. The impact of using this Internet of Medical Things based framework for medical education has also been studied; the outcomes underscore the potential of adopting such Internet of Medical Things based approaches for medical education.}
}
@article{DROZDZ20214886,
title = {Virtual Reality Training System as a comprehensive and effective method for delivering technical hands-on training in the field of Distribution System Operators},
journal = {Procedia Computer Science},
volume = {192},
pages = {4886-4899},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.267},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020068},
author = {Wojciech Drożdż},
keywords = {virtual reality system, virtual reality training, hand-on training in virtual reality, live-line work in virtual reality, commutating operation in virtual reality},
abstract = {The Paper presents the possibilities of using virtual reality techniques in interactive training of technical employees of a Distribution System Operator (DSO). It also describes the concept, assumptions and implementation method of the created Flexible Training System. Thanks to its capabilities, employees can participate in sophisticated training conducted in an innovative way eliminating the current disadvantages and limitations of classic – “real” training approaches. The System enables practice in two main categories of training used in the DSO environment: commutating operations and live-line works. Benefits and risks associated with system implementation in DSO structures are presented}
}
@article{DELAMO2022107954,
title = {Hybrid recommendations and dynamic authoring for AR knowledge capture and re-use in diagnosis applications},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {107954},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107954},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121010868},
author = {Iñigo Fernández {del Amo} and John Ahmet Erkoyuncu and Maryam Farsi and Dedy Ariansyah},
keywords = {Augmented Reality, Failure diagnosis, Authoring systems, Knowledge capture, Ontology-based reporting},
abstract = {In Industry 4.0, integrated data management is an important challenge due to heterogeneity and the lack of structure of numerous existing data sources. A relevant research gap involves human knowledge integration, especially in maintenance operations. Augmented Reality (AR) can bridge this gap, but it requires improved augmented content to enable effective and efficient knowledge capture. This paper proposes dynamic authoring and hybrid recommender methods for accurate AR-based reporting. These methods aim to provide maintainers with augmented data input formats and recommended datasets for enhancing the efficiency and effectiveness of their reporting tasks. The proposed contributions have been validated through experiments and surveys in two failure diagnosis reporting scenarios. Experimental results indicated that the proposed reporting solution can reduce reporting errors by 50% and reporting time by 20% compared to alternative recommender and AR tools. Besides, survey results suggested that testers perceived the proposed reporting solution as more effective and satisfactory for reporting tasks than alternative tools. Thus, proving that the proposed methods can improve the effectiveness and efficiency of diagnosis reporting applications. Finally, this paper proposes future works towards a framework for automatic adaptive authoring in AR knowledge transfer and capture applications for human knowledge integration in the context of Industry 4.0.}
}
@article{IVAN2017309,
title = {Help The Math Town: Adaptive Multiplayer Math-Science Games using Fuzzy Logic},
journal = {Procedia Computer Science},
volume = {116},
pages = {309-317},
year = {2017},
note = {Discovery and innovation of computer science technology in artificial intelligence era: The 2nd International Conference on Computer Science and Computational Intelligence (ICCSCI 2017)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.10.080},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917321300},
author = {Charles Ivan and Pingkan C.B. Rumondor and Michael Yoseph Ricky and Emny Harna Yossy and Widodo Budiharto},
keywords = {adaptive games, fuzzy, android, psychological aspects},
abstract = {The mobile games for education and entertainment are rapidly developed. Unfortunately the development of interesting and entertaining adaptive multiplayer game based learning for math and science that consider psychological aspects and game balancing for students on mobile platforms that have game balancing is very limited. This paper proposes an integrated model of adaptive Math and Science game based learning for elementary school students with adjustable level of difficulty based on user’s ability and additional option for “Free” and “Adventures” modes. We propose a method where level of difficulty can be adjusted based on previous score, using fuzzy two inputs in the form of percentage correct and the speed of answer to produce output. The experimental results are presented and show the adaptive games are running well on mobile devices based on Android platform and well received by elementary school students grade 3 to 6.}
}
@article{CASSOLA2014130,
title = {Online-Gym: A 3D Virtual Gymnasium Using Kinect Interaction},
journal = {Procedia Technology},
volume = {13},
pages = {130-138},
year = {2014},
note = {SLACTIONS 2013: Research conference on virtual worlds – Learning with simulations},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314000279},
author = {Fernando Cassola and Leonel Morgado and Fausto {de Carvalho} and Hugo Paredes and Benjamim Fonseca and Paulo Martins},
keywords = {Virtual Environments, Second Life, OpenSimulator, Virtual Worlds, Kinect, Motion Capture, Human Computer Interaction, Natural User Interfaces, online gymnastics, rehabilitation},
abstract = {Synchronized online gymnastics may provide new possibilities for enhancing the physical and social well-being of people with restricted mobility. We propose a prototype platform for this – Online-Gym – which allows users to interact using a Microsoft Kinect and participate in on-line gymnastics sessions. In this paper we present the Online-Gym concept and a first iteration of the platform architecture that allows interaction in OpenSimulator or Second Life virtual worlds with movement captured by a Kinect device. The exploratory work done so far provides evidence that this approach is viable and that such scenarios may be pursued.}
}
@article{ADAO2018441,
title = {A rapid prototyping tool to produce 360° video-based immersive experiences enhanced with virtual/multimedia elements},
journal = {Procedia Computer Science},
volume = {138},
pages = {441-453},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316958},
author = {Telmo Adão and Luís Pádua and Miguel Fonseca and Luís Agrellos and Joaquim J. Sousa and Luís Magalhães and Emanuel Peres},
keywords = {Virtual Reality, 360° Videos Editing, Immersive Digital Panoramas, Multimedia, 3D model, 3D text, Spatialized Sound, Prototyping},
abstract = {While the popularity of virtual reality (VR) grows in a wide range of application contexts – e.g. entertainment, training, cultural heritage and medicine –, its economic impact is expected to reach around 15bn USD, by the year of 2020. Within VR field, 360° video has been sparking the interest of development and research communities. However, editing tools supporting 360° panoramas are usually expensive and/or demand programming skills and/or advanced user knowledge. Besides, application approaches to quickly and intuitively set up such 360° video-based VR environments complemented with diverse types of parameterizable virtual assets and multimedia elements are still hard to find. Thereby, this paper aims to propose a system specification to simply and rapidly configure immersive VR environments composed of surrounding 360° video spheres that can be complemented with parameterizable multimedia contents – namely 3D models, text and spatial sound –, whose behavior can be either time-range or user-interaction dependent. Moreover, a preliminary prototype that follows a substantial part of the previously mentioned specification and implements the enhancement of 360° videos with time-range dependent virtual assets is presented. Preliminary tests evaluating usability and user satisfaction were also carried out with 30 participants, from which encouraging results were achieved.}
}
@article{TADEJA2023134,
title = {Exploring the repair process of a 3D printer using augmented reality-based guidance},
journal = {Computers & Graphics},
volume = {117},
pages = {134-144},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002546},
author = {Sławomir K. Tadeja and Luca O. {Solari Bozzi} and Kerr D.G. Samson and Sebastian W. Pattinson and Thomas Bohné},
keywords = {Augmented reality, AR, AR-guided repair, Immersive interface, 3D printing, 3D printer repair, Right to repair},
abstract = {In recent years, additive manufacturing (AM) techniques have transcended their typical rapid prototyping role and become viable methods to directly manufacture end products in a highly versatile manner. Due to its low cost and relative ease of use, fused deposition modeling (FDM) has become the most universally applied AM technology. Nonetheless, skilled operators are often still required to perform maintenance, diagnostic, and repair tasks. Such operators need to be adequately trained. Here, Augmented reality (AR) technology could be used to automate this training and help to promptly provide new operators with the necessary skills to perform specific tasks as required. However, the most effective approach to designing such AR-based assistance systems has not yet been fully explored. Consequently, we address this need by reporting on how to design such guiding systems using well-known design engineering methodologies. We then further assess the applicability of our approach through a user study with domain experts. In addition, we complete our assessment with heuristical verification of system expressiveness to reason about the influence of cognitively important components of the AR interface on the operators.}
}
@article{RIOS2013161,
title = {A Mobile Solution to Enhance Training and Execution of Troubleshooting Techniques of the Engine Air Bleed System on Boeing 737},
journal = {Procedia Computer Science},
volume = {25},
pages = {161-170},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012258},
author = {Horacio Rios and Eduardo González and Ciro Rodriguez and Hector R. Siller and Manuel Contero},
keywords = {Augmented Reality, Troubleshooting, Aircraft, Complex Assembly},
abstract = {The process of troubleshooting an aircraft engine requires highly skilled and trained personnel who must be able to respond effectively to any circumstance; therefore, new methods of training to accelerate the cognitive processes of technicians must be integrated in the industry. In this matter the Augmented Reality technology represents an innovative tool that can ensure the efficient and correct transfer of knowledge. The numbers of errors during maintenance tasks can be reduced, AR provides information that is generally not easily available during maintenance operations because, in general, the troubleshooting process for airplane engine is a highly complex task and the diagnosis of a failure is critical for the passengers’ safety. This research focuses on training and execution of tasks where an aviation technician must be familiarized with a wide variety of technical data, physical components of mechanical systems and the regulations that must be followed to release an airplane for flight, the specialist must develop a correct mind map of the system and should be able to troubleshoot if necessary. The case of study is the 737 Engine Bleed Air System that is designed to provide engine compressed air to air conditioning pack with the purpose of air pressurization during flight; engine air from the compressor is used, from the 5° and the 9° stage in a safe an economical way, knowledge of the correct function of the components will increase safety and considerably reduce cost of maintenance operations. The purpose of the investigation was to develop an ergonomic tool than improves the cognitive process of technician during training for the troubleshooting techniques of the aircraft, but it also can be used to the everyday task by capturing the know-how and helpful tips from more experienced operators. A mobile solution that functions on regular tablets was delivered to enhance the troubleshooting techniques and maintenance procedures of the Engine Air Bleed System, the software can function on two aspects for training and in situ operations. A commercial aeronautical training kit was used to validate the Fault Isolation Software; the results showed that the augmented reality technique takes 17% less time and a quality increment of 24% for this complex assembly system.}
}
@article{HO202275,
title = {Vision based crown loss estimation for individual trees with remote aerial robots},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {188},
pages = {75-88},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622000971},
author = {Boon Ho and Basaran Bahadir Kocer and Mirko Kovac},
keywords = {Aerial robots, Unmanned aerial vehicles, Crown loss estimation, Convolutional neural network, Variational autoencoder, Foliar sampling},
abstract = {With the capability of capturing high-resolution imagery data and the ease of accessing remote areas, aerial robots are becoming increasingly popular for forest health monitoring applications. For example, forestry tasks such as field surveys and foliar sampling which are generally manual and labour intensive can be automated with remotely controlled aerial robots. In this study, we propose two new online frameworks to quantify and rank the severity of individual tree crown loss. The real-time crown loss estimation (RTCLE) model localises and classifies individual trees into their respective crown loss percentage bins. Experiments are conducted to investigate if synthetically generated tree images can be used to train the RTCLE model as real images with diverse viewpoints are generally expensive to collect. Results have shown that synthetic data training helps to achieve a satisfactory baseline mean average precision (mAP) which can be further improved with just some additional real imagery data. We showed that the mAP can be increased approximately from 60% to 78% by mixing the real dataset with the generated synthetic data. For individual tree crown loss ranking, a two-step crown loss ranking (TSCLR) framework is developed to handle the inconsistently labelled crown loss data. The TSCLR framework detects individual trees before ranking them based on some relative crown loss severity measures. The tree detection model is trained with the combined dataset used in the RTCLE model training where we achieved an mAP of approximately 95% suggesting that the model generalises well to unseen datasets. The relative crown loss severity of each tree is estimated, with deep representation learning, by a probabilistic encoder from a fully trained variational autoencoder (VAE) model. The VAE is trained end-to-end to reconstruct tree images in a background agnostic way. Based on a conservative evaluation, the estimated crown loss severity from the probabilistic encoder generally showed moderate agreement with the expert’s estimation across all species of trees present in the dataset. All the software pipelines, the dataset, and the synthetic dataset generation can be found in the GitHub link.}
}
@article{ALMEIDA202351,
title = {SIT6: Indirect touch-based object manipulation for DeskVR},
journal = {Computers & Graphics},
volume = {117},
pages = {51-60},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002509},
author = {Diogo Almeida and Daniel Mendes and Rui Rodrigues},
keywords = {Virtual reality, Object manipulation, DeskVR},
abstract = {Virtual reality (VR) has the potential to significantly boost productivity in professional settings, especially those that can benefit from immersive environments that allow a better and more thorough way of visualizing information. However, the physical demands of mid-air movements make it difficult to use VR for extended periods. DeskVR offers a solution that allows users to engage in VR while seated at a desk, minimizing physical exhaustion. However, developing appropriate motion techniques for this context is challenging due to limited mobility and space constraints. This work focuses on object manipulation techniques, exploring touch-based and mid-air-based approaches to design a suitable solution for DeskVR, hypothesizing that touch-based object manipulation techniques could be as effective as mid-air object manipulation in a DeskVR scenario while less physically demanding. Thus, we propose Scaled Indirect Touch 6-DOF (SIT6), an indirect touch-based object manipulation technique incorporating scaled input mapping to address precision and out-of-reach manipulation issues. The implementation of our solution consists of a state machine with error-handling mechanisms and visual indicators to enhance interaction. User experiments were conducted to compare the SIT6 technique with a baseline mid-air approach, revealing comparable effectiveness while demanding less physical exertion. These results validated our hypothesis and established SIT6 as a viable option for object manipulation in DeskVR scenarios.}
}
@article{HUHNT2022101790,
title = {Modeling bounded and unbounded space with polyhedra: Topology and operators for manifold cell complexes},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101790},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101790},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002488},
author = {Wolfgang Huhnt and Maximilian Sternal and Peter Jan Pahl},
abstract = {This paper proposes polyhedral space partitioning as an alternative to component assembly of digital models of objects with complex linear shapes. A partition is specified with a path-connected user model, where each object is bounded by n-manifolds. Faces and cells can be non-convex, multiply-connected and unbounded. The user interacts with the user model and specifies work steps. Each work step splits one edge, face or cell of the partition, or merges two neighboring objects of equal dimension. As a consequence, only a small subset of the model objects, consisting of the user specified objects and their neighbors, are affected by a work step. The user model is automatically mapped to a core model containing methods for topological relations and navigation. The topological structure is described by bundles of twin arrows of opposite direction arranged in polygons, twin facets with normal vectors of opposite direction and dihedral facet cycles at the edges. Imaginary topological objects are introduced to define unbounded cells, faces and edges. The approach guarantees that there is no overlap or gap between any pair of neighboring objects. It supports modelling of non-convex and multiply connected bounded and unbounded objects. For verification, several example models are presented and visualized. The paper ends with conclusions and an outlook to ongoing and planned further research in this field.}
}
@article{PFOUGA201854,
title = {Leveraging 3D geometric knowledge in the product lifecycle based on industrial standards},
journal = {Journal of Computational Design and Engineering},
volume = {5},
number = {1},
pages = {54-67},
year = {2018},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2288430017300441},
author = {Alain Pfouga and Josip Stjepandić},
keywords = {3D shape representation, PDF, 3D file format, Visualization, Engineering collaboration, Data exchange},
abstract = {With their practical introduction by the 1970s, virtual product data have emerged to a primary technical source of intelligence in manufacturing. Modern organization have since then deployed and continuously improved strategies, methods and tools to feed the individual needs of their business domains, multidisciplinary teams, and supply chain, mastering the growing complexity of virtual product development. As far as product data are concerned, data exchange, 3D visualization, and communication are crucial processes for reusing manufacturing intelligence across lifecycle stages. Research and industry have developed several CAD interoperability, and visualization formats to uphold these product development strategies. Most of them, however, have not yet provided sufficient integration capabilities required for current digital transformation needs, mainly due to their lack of versatility in the multi-domains of the product lifecycle and primary focus on individual product descriptions. This paper analyses the methods and tools used in virtual product development to leverage 3D CAD data in the entire life cycle based on industrial standards. It presents a set of versatile concepts for mastering exchange, aware and unaware visualization and collaboration from single technical packages fit purposely for various domains and disciplines. It introduces a 3D master document utilizing PDF techniques, which fulfills requirements for electronic discovery and enables multi-domain collaboration and long-term data retention for the digital enterprise.}
}
@article{LUKKA2024100706,
title = {Measuring digital intervention user experience with a novel ecological momentary assessment (EMA) method, CORTO},
journal = {Internet Interventions},
volume = {35},
pages = {100706},
year = {2024},
issn = {2214-7829},
doi = {https://doi.org/10.1016/j.invent.2023.100706},
url = {https://www.sciencedirect.com/science/article/pii/S2214782923001069},
author = {Lauri Lukka and Veli-Matti Karhulahti and Vilma-Reetta Bergman and J. Matias Palva},
keywords = {Digital interventions, Ecological momentary assessment, Engagement, Evaluation methods, Formative evaluation, Interviewing, Methodology, Mental health, Mixed methods, Qualitative study, Questionnaire, Remote study, Serious games, User experience},
abstract = {Digital interventions often suffer from low usage, which may reflect insufficient attention to user experience. Moreover, the existing evaluation methods have limited applicability in the remote study of user experience of complex interventions that have expansive content and that are used over an extensive period of time. To alleviate these challenges, we describe here a novel qualitative Ecological Momentary Assessment (EMA) method: the CORTO method (Contextual, One-item, Repeated, Timely, Open-ended). We used it to gather digital intervention user experience data from Finnish adults (n = 184) who lived with interview-confirmed major depressive disorder (MDD) and took part in a randomized controlled trial (RCT) that studied the efficacy of a novel 12-week game-based digital intervention for depression. A second dataset on user experience was gathered with retrospective interviews (n = 22). We inductively coded the CORTO method and retrospective interview data, which led to four user experience categories: (1) contextual use, (2) interaction-elicited emotional experience, (3) usability, and (4) technical issues. Then, we used the created user experience categories and Template Analysis to analyze both datasets together, and reported the results qualitatively. Finally, we compared the two datasets with each other. We found that the data generated with the CORTO method offered more insights into usability and technical categories than the interview data that particularly illustrated the contextual use. The emotional valence of the interview data was more positive compared with the CORTO data. Both the CORTO and interview data detected 55 % of the micro-level categories; 20 % of micro-level categories were only detected by the CORTO data and 25 % only by the interview data. We found that the during-intervention user experience measurement with the CORTO method can provide intervention-specific insights, and thereby further the iterative user-centered intervention development. Overall, these findings highlight the impact of evaluation methods on the categories and qualities of insights acquired in intervention research.}
}
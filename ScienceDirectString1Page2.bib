@article{ARJUN202249,
title = {Interactive Sensor Dashboard for Smart Manufacturing},
journal = {Procedia Computer Science},
volume = {200},
pages = {49-61},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002137},
author = {Somnath Arjun and LRD Murthy and Pradipta Biswas},
keywords = {Information Visualization, Eye Tracking, Sensor netwrok, Interaction, Virtual Reality},
abstract = {This paper presents a smart sensor dashboard for a digital twin of a smart manufacturing workshop. We described the development of the digital twin followed by three user studies on the visualization and interaction aspects of the smart sensor dashboard. The first two user studies evaluated ocular parameters and users’ response for different 2D and 3D graphs rendered on 2D screen and VR headset. The bar chart found to generate most accurate users’ response in both 2D and 3D case. The third study recreated the Fitts’ Law task in 3D and compared visual and haptic feedback. We found that haptic feedback significantly improved quantitative metrics of interaction than a no-feedback case, whereas multimodal feedback is significantly improved qualitative metrics of the interaction. Results from the study can be utilized to design VR environments with interactive graphs.}
}
@article{ZHAO2023101992,
title = {The time course of spatial knowledge acquisition for different digital navigation aids},
journal = {Computers, Environment and Urban Systems},
volume = {103},
pages = {101992},
year = {2023},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2023.101992},
url = {https://www.sciencedirect.com/science/article/pii/S0198971523000558},
author = {Hantao Zhao and Lisa Frese and Claudio Venzin and Daniel Kaszás and Raphael P. Weibel and Christoph Hölscher and Victor R. Schinazi and Tyler Thrash},
keywords = {Virtual reality, CAVE, Wayfinding, Map, Spatial learning, Eye tracking},
abstract = {Digital maps on personal devices (e.g., phones) are common tools used to aid navigation. Different types of digital maps can influence spatial knowledge acquisition, and this effect might depend on whether the user interacts with an forward-up or north-up map. In spatial cognition theory, these differences can be used to support either sequential or continuous theories of spatial knowledge acquisition. To test these hypotheses, we compared spatial learning of participants (N = 67) after navigation in a virtual city with either a forward-up map, a north-up map, or a guiding arrow (i.e., a control) as a navigation aid. Critically, participants were tested on landmark recognition tasks and judgments of relative direction (JRDs) after each of four navigation blocks. We also examined mental workload during map usage using eye tracking in terms of the distributions of fixations on the maps. The results indicated that, regardless of navigation aid, participants improved on both landmark recognition and JRD tasks over blocks of trials. In addition, we found an interaction between block and navigation aid. This interaction suggests that participants in the forward-up map group initially produced more JRD errors than the north-up map group but that the two groups performed similarly in later blocks as they became more familiar with the environment. These findings are consistent with the eye-tracking data, which suggested a decrease in mental workload as evidenced by an increase in fixation distributions over blocks of trials. Together, these results suggest that participants with any of these navigation aids perform similarly on some tasks (supporting sequential theory), although the time course of learning may differ between map types (supporting continuous theory).}
}
@article{CIEZA2018352,
title = {Educational Mobile Application of Augmented Reality Based on Markers to Improve the Learning of Vowel Usage and Numbers for Children of a Kindergarten in Trujillo},
journal = {Procedia Computer Science},
volume = {130},
pages = {352-358},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918304046},
author = {Edwin Cieza and David Lujan},
keywords = {Mobile app, Learning, Augmented reality, Markers, Development tools},
abstract = {The main objective of this research was to improve the level of understanding of the usage of vowels and numbers for children over 4 years of age in the Juana Alarco de Dammert Nursery School in Trujillo through an educational mobile application that is composed of the unit development platform, monodevelopment, Andriod Studio, Vuforia using the programming language C# that was made based on the development methodology of extreme software programming. The research design is a pre-experimental experiment grade which was composed of 10 children over the age of 4 of the nursery school and was used as a method of data analysis Student T test. In addition, with the implemented application it was possible to increase the level of academic performance of vowel usage by 27.60% and the use of numbers by 22.60%. It was concluded that with the implementation of the educational mobile application of augmented reality, the level of understanding of vowel usage and numbers had improved in the Juana Alarco de Dammart nursery school children.}
}
@article{FERNANDEZCHAVES2021107440,
title = {ViMantic, a distributed robotic architecture for semantic mapping in indoor environments},
journal = {Knowledge-Based Systems},
volume = {232},
pages = {107440},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107440},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007024},
author = {D. Fernandez-Chaves and J.R. Ruiz-Sarmiento and N. Petkov and J. Gonzalez-Jimenez},
keywords = {Semantic maps, Robotic architecture, Mobile robots, Unity 3D, ROS, Object detection, Detectron2, Robot@Home},
abstract = {Semantic maps augment traditional representations of robot workspaces, typically based on their geometry and/or topology, with meta-information about the properties, relations and functionalities of their composing elements. A piece of such information could be: fridges are appliances typically found in kitchens and employed to keep food in good condition. Thereby, semantic maps allow for the execution of high-level robotic tasks in an efficient way, e.g. “Hey robot, Store the leftover salad”. This paper presents ViMantic, a novel semantic mapping architecture for the building and maintenance of such maps, which brings together a number of features as demanded by modern mobile robotic systems, including: (i) a formal model, based on ontologies, which defines the semantics of the problem at hand and establishes mechanisms for its manipulation; (ii) techniques for processing sensory information and automatically populating maps with, for example, objects detected by cutting-edge CNNs; (iii) distributed execution capabilities through a client–server design, making the knowledge in the maps accessible and extendable to other robots/agents; (iv) a user interface that allows for the visualization and interaction with relevant parts of the maps through a virtual environment; (v) public availability, hence being ready to use in robotic platforms. The suitability of ViMantic has been assessed using Robot@Home, a vast repository of data collected by a robot in different houses. The experiments carried out consider different scenarios with one or multiple robots, from where we have extracted satisfactory results regarding automatic population, execution times, and required size in memory of the resultant semantic maps.}
}
@article{FERNANDO2024356,
title = {Game-based Activity Design in Primary School Students’ Learning Style Detection},
journal = {Procedia Computer Science},
volume = {239},
pages = {356-363},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.182},
url = {https://www.sciencedirect.com/science/article/pii/S187705092401425X},
author = {Pumudu A. Fernando and H.K. Salinda Premadasa},
keywords = {learning style, game-based learning, game design, fedler silverman, generation Alpha},
abstract = {Generation Alpha, the present primary school cohort born after 2010, has significant exposure to mobile devices and gaming. Adopting a "One Size Fits All" approach in modern teaching methods may not be effective, as it overlooks individual learning preferences. Personalized learning can be facilitated by identifying a student’s learning style (LS). Adaptive learning based on LS has been found to have positive effects in several studies. However, traditional learning style detection techniques such as questionnaires and self-assessments can be time-consuming and demotivating for primary school students. This study aims to propose a game-based activity framework as an alternative to the Index of Learning Style (ILS) questionnaire linked with Felder Silverman Learning Style Model for LS detection. The proposed game was evaluated with a sample of sixty students, and preliminary results indicate that the game outperforms the original ILS questionnaire in terms of student engagement and motivation to complete LS activities, achieving an overall satisfaction rate of 87.5%. The second phase of the research will focus on evaluating the accuracy of LS prediction using the designed game, which is currently ongoing.}
}
@article{MANH20222698,
title = {G2L: A Global to Local Alignment Method for Unsupervised Domain Adaptive Semantic Segmentation},
journal = {Procedia Computer Science},
volume = {207},
pages = {2698-2707},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.328},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012170},
author = {Nguyen Viet Manh and Kieu Dang Nam and Dinh Viet Sang and Thi-Oanh Nguyen},
keywords = {Unsupervised domain adaptation, semantic segmentation, adversarial training, self training, pseudo-label denoising, style transfer},
abstract = {Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a source dataset with dense pixel-level annotations to an unlabeled target dataset. However, the performance of UDA methods often suffers from the domain shift, which is the discrepancy between the feature distributions of the two domains. There have been several attempts to match these distributions at the image level marginally. However, due to the so-called category-level domain shift, such global alignments do not guarantee a good separability of deep features extracted from different categories in the target domain. As a result, the generated pseudo-labels can be noisy and thus poison the learning process on the target domain. Some recent methods focus on denoising the pseudo-labels online using category-wise information. This paper introduces a novel UDA method called Global-to-Local alignment (G2L) that leverages fine-grained adversarial training and a newly proposed chromatic Fourier transform to address the image-level domain shift from a global perspective. Next, our method deals with the category-level domain shift under a local view. Specifically, we propose a long-tail category rating strategy as well as apply dynamic confidence thresholds and category-wise priority weights when generating and denoising the pseudo-labels to favor rare categories. Finally, self-distillation is used to boost the final segmentation results. Experiments on popular benchmarks GTA5 → Cityscapes and SYNTHIA → Cityscapes show that our method yields superior accuracy performance than other state-of-the-art methods.}
}
@article{KHANAL201449,
title = {Collaborative virtual reality based advanced cardiac life support training simulator using virtual reality principles},
journal = {Journal of Biomedical Informatics},
volume = {51},
pages = {49-59},
year = {2014},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414000902},
author = {Prabal Khanal and Akshay Vankipuram and Aaron Ashby and Mithra Vankipuram and Ashish Gupta and Denise Drumm-Gurnee and Karen Josey and Linda Tinker and Marshall Smith},
keywords = {Computer uses in education – , Multimedia information systems – , Serious games, Computer applications in medicine, Advanced cardiac life support, Medical team training},
abstract = {Background
Advanced Cardiac Life Support (ACLS) is a series of team-based, sequential and time constrained interventions, requiring effective communication and coordination of activities that are performed by the care provider team on a patient undergoing cardiac arrest or respiratory failure. The state-of-the-art ACLS training is conducted in a face-to-face environment under expert supervision and suffers from several drawbacks including conflicting care provider schedules and high cost of training equipment.
Objective
The major objective of the study is to describe, including the design, implementation, and evaluation of a novel approach of delivering ACLS training to care providers using the proposed virtual reality simulator that can overcome the challenges and drawbacks imposed by the traditional face-to-face training method.
Methods
We compare the efficacy and performance outcomes associated with traditional ACLS training with the proposed novel approach of using a virtual reality (VR) based ACLS training simulator. One hundred and forty-eight (148) ACLS certified clinicians, translating into 26 care provider teams, were enrolled for this study. Each team was randomly assigned to one of the three treatment groups: control (traditional ACLS training), persuasive (VR ACLS training with comprehensive feedback components), or minimally persuasive (VR ACLS training with limited feedback components). The teams were tested across two different ACLS procedures that vary in the degree of task complexity: ventricular fibrillation or tachycardia (VFib/VTach) and pulseless electric activity (PEA).
Results
The difference in performance between control and persuasive groups was not statistically significant (P=.37 for PEA and P=.1 for VFib/VTach). However, the difference in performance between control and minimally persuasive groups was significant (P=.05 for PEA and P=.02 for VFib/VTach). The pre-post comparison of performances of the groups showed that control (P=.017 for PEA, P=.01 for VFib/VTach) and persuasive (P=.02 for PEA, P=.048 for VFib/VTach) groups improved their performances significantly, whereas minimally persuasive group did not (P=.45 for PEA, P=.46 for VFib/VTach). Results also suggest that the benefit of persuasiveness is constrained by the potentially interruptive nature of these features.
Conclusions
Our results indicate that the VR-based ACLS training with proper feedback components can provide a learning experience similar to face-to-face training, and therefore could serve as a more easily accessed supplementary training tool to the traditional ACLS training. Our findings also suggest that the degree of persuasive features in VR environments have to be designed considering the interruptive nature of the feedback elements.}
}
@article{PEDRAZAHUESO2015161,
title = {Rehabilitation Using Kinect-based Games and Virtual Reality},
journal = {Procedia Computer Science},
volume = {75},
pages = {161-168},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.233},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036947},
author = {Miguel Pedraza-Hueso and Sergio Martín-Calzón and Francisco Javier Díaz-Pernas and Mario Martínez-Zarzuela},
keywords = {Serious games, Virtual Reality, Kinect},
abstract = {This paper introduces the development of a customized virtual reality system based on a serious game which allows the user to carry out physical and cognitive rehabilitation therapies using a natural user interface based on Microsoft© Kinect. Within these serious games you can find the exergames. It is a type of serious game which aims to stimulate body mobility through an immersive experience that situates the user inside virtual interactive landscapes. This type of game has become popular in recent years thanks to the creation of consoles like Nintendo Wii, Playstation or Xbox, which use gestural interaction game interfaces. Likewise, these technologies have become extremely useful tools in rehabilitation, and they are expected to permit a reduction of costs in socio-sanitary environments. The proposed virtual reality platform consists of different types of exercises by which the user is able to train or rehabilitate several aspects such as strength, aerobic or cognitive capacities. The system has been modelled so that the physical presence of a therapist is not required during the course of the session and there is no need to wear any kind of marker or sensor. Moreover, all parameters of the different exercises can be configured without the physical presence of a therapist. The reports of each session can also be read offline, therefore, the therapist will always know if a user has performed the session in a good way and act accordingly modifying whatever he deems necessary in the patient's therapy. Due to these facts the system developed and presented in this article is a rehabilitation system based on remote assistance. It is important that this type of serious games accomplish all the functionalities a videogame fulfils at the same time that accomplish specific functionalities in its therapeutic environment. Remarkably, it is required an adaptation to the patient's abilities to avoid frustration and provide immediate feedback to the user during the exercises. In our system, the users are monitored and receive an audio-visual feedback during the session, so that they know in real-time if they are correctly doing the exercises of the specific therapy that was designed for them.}
}
@article{MIYATA2018295,
title = {Modeling emotion and inference as a value calculation system},
journal = {Procedia Computer Science},
volume = {123},
pages = {295-301},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.046},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300474},
author = {Masahiro Miyata and Takashi Omori},
keywords = {Emotion, Decision Making, Value Calculation System, Model, Probabilistic Inference},
abstract = {There have been many studies on the modeling of a relation between emotion and decision making. Though the effect of emotion on inference in making a decision is evident, its computational mechanism, especially for intuitive inference, is not yet clear. Therefore, in this paper, we discuss the possibility of the computational modeling of an intuitive inference guided by emotion in which random-seeming neural excitation plays the role of a probability-based parallel search of values. First, we show a possible architecture of the intuitive inference in which the system of multiple values affects the process of action decision. Then, we focus on an effect of the value-control mechanism for intuitive inference in a path-finding task. In a computer simulation, we aimed to simulate a model of value management in which multiple value components of the brainstem are controlled for an action search. Though the brainstem seems simple, the model includes the essence of the resolution of conflicts between multiple values.}
}
@article{CYRINO2022551,
title = {An Intuitive VR-based Environment for Monitoring and Control of Electrical Power Substations},
journal = {Procedia Computer Science},
volume = {201},
pages = {551-558},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004847},
author = {Gabriel F. Cyrino and Camilo L. Barreto and Leandro R. Mattioli and Alexandre Cardoso and Edgard A. Lamounier and Gerson F.M. Lima and Daniel S. Ramos},
keywords = {Substations Operation, Operator Training, Virtual Reality, 3D interfaces},
abstract = {A Virtual Reality System provides an intuitive tridimensional user interface, with distinct interaction when compared to a traditional 2D layout, allowing the user to manipulate data similar to the real world. Such systems give the users physical and cognitive immersion, by granting a mental model compatible with field operation. In this work, we present the development aspects and some results of a Virtual Reality based solution to provide a more natural and intuitive environment for controlling electrical operation centers. The solution intends to minimize the issues caused by the operation of electric power substations due to the lack of spatial and functional information on the traditional operation interfaces. The research is being carried out with the collaboration between the energy electric company of Minas Gerais–Brazil (CEMIG) and the Federal University of Uberlândia. Besides operating generation and transmission actives, the environment presents great potential for the training of operators and other professionals of support and maintenance.}
}
@article{VERHULST2021106951,
title = {Do VR and AR versions of an immersive cultural experience engender different user experiences?},
journal = {Computers in Human Behavior},
volume = {125},
pages = {106951},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106951},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221002740},
author = {Isabelle Verhulst and Andy Woods and Laryssa Whittaker and James Bennett and Polly Dalton},
keywords = {Virtual reality, Augmented reality, User experience, Presence, Enjoyment, Engagement},
abstract = {Although Virtual Reality (VR) and Augmented Reality (AR) user experiences have received large amounts of recent research interest, a direct comparison of different immersive technologies' user experiences has not often been conducted. This study compared user experiences of one VR and two AR versions of an immersive gallery experience ‘Virtual Veronese’, measuring multiple aspects of user experience, including enjoyment, presence, cognitive, emotional and behavioural engagement, using a between-subjects design, at the National Gallery in London, UK. Analysis of the self-reported survey data (N = 368) showed that enjoyment was high on all devices, with the Oculus Quest (VR) receiving higher mean scores than both AR devices, Magic Leap and Mira Prism. In relation to presence, the elements ‘spatial presence’, ‘involvement’, and ‘sense of being there’ received a higher mean score on the Oculus Quest than on both AR devices, and on ‘realism’ the Oculus Quest scored significantly higher than the Magic Leap. Cognitive engagement was similar between the three devices, with only ‘I knew what to do’ being rated higher for Quest than Mira Prism. Emotional engagement was similar between the devices. Behavioural engagement was high on all devices, with only ‘I would like to see more experiences like this’ being higher for Oculus Quest than Mira Prism. Negative effects including nausea were rarely reported. Differences in user experiences were likely partly driven by differences in immersion levels between the devices.}
}
@article{OUALI2022158,
title = {Augmented Reality for Scene Text Recognition, Visualization and Reading to Assist Visually Impaired People},
journal = {Procedia Computer Science},
volume = {207},
pages = {158-167},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009218},
author = {Imene OUALI and Mohamed BEN HALIMA and Ali WALI},
keywords = {Text Visualization, Text detection, Text recognition, Natural Scene, Augmented Reality, VGG19},
abstract = {Reading traffic signs while driving a car for visually impaired people and people with visual problems is a very difficult task for them. This task is encountered every day, sometimes incorrect reading of traffic signs can lead to very serious results. In particular, the Arabic language is very difficult, making recognizing and viewing Arabic text a difficult task. In this context, we are looking for an effective solution to remove errors and results that can sometimes end someone's life. This article aims to correctly read traffic signs with Arabic text using augmented reality technology. Our system is composed of three modules. The first is text detection and recognition. The second is Text visualization. The third is Text to speech methods conversion. With this system, the user can have two different results. The first result is visual with much-improved text and enhancement. The second result is sound, he can hear the text aloud. This system is very applicable and effective for daily life. To assess the effectiveness of our work, we offer a survey to a group of visually impaired people to give their opinion on the use of our application. The results have been good for most people.}
}
@article{LEE2020443,
title = {VEGO: A novel design towards customizable and adjustable head-mounted display for VR},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {5},
pages = {443-453},
year = {2020},
note = {VR/AR research and commercial applications in Singapore},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300759},
author = {Jia Ming Lee and Xinxing Xia and Clemen OW and Felix Chua and Yunqing Guan},
keywords = {Virtual reality, Head mounted display, Vergence accommodation conflict, Inter-pupil distance},
abstract = {Background
Virtual Reality (VR) technologies have advanced fast and have been applied to a wide spectrum of sectors in the past few years. VR can provide an immersive experience to users by generating virtual images and displaying the virtual images to the user with a head-mounted display (HMD) which is a primary component of VR. Normally, an HMD contains a list of hardware components, e.g., housing pack, micro LCD display, microcontroller, optical lens, etc. Settings of VR HMD to accommodate the user's inter-pupil distance (IPD) and the user's eye focus power are important for the user's experience with VR.
Methods
Although various methods have been developed towards IPD and focus adjustments for VR HMD, the increased cost and complexity impede the possibility for users who wish to assemble their own VR HMD for various purposes, e.g., DIY teaching, etc. In our paper, we present a novel design towards building a customizable and adjustable HMD for VR in a cost-effective manner. Modular design methodology is adopted, and the VR HMD can be easily printed with 3D printers. The design also features adjustable IPD and variable distance between the optical lens and the display. It can help to mitigate the vergence and accommodation conflict issue.
Results
A prototype of the customizable and adjustable VR HMD has been successfully built up with off-the-shelf components. A VR software program running on Raspberry Pi board has been developed and can be utilized to show the VR effects. A user study with 20 participants is conducted with positive feedback on our novel design.
Conclusions
Modular design can be successfully applied for building up VR HMD with 3D printing. It helps to promote the wide application of VR at affordable costs while featuring flexibility and adjustability.}
}
@article{BOGES202012,
title = {Virtual reality framework for editing and exploring medial axis representations of nanometric scale neural structures},
journal = {Computers & Graphics},
volume = {91},
pages = {12-24},
year = {2020},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2020.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0097849320300789},
author = {Daniya Boges and Marco Agus and Ronell Sicat and Pierre J. Magistretti and Markus Hadwiger and Corrado Calì},
keywords = {Ultrastructural analysis, Medial axis representation, Immersive environments, Virtual reality in neuroscience},
abstract = {We present a novel virtual reality (VR) based framework for the exploratory analysis of nanoscale 3D reconstructions of cellular structures acquired from rodent brain samples through serial electron microscopy. The system is specifically targeted on medial axis representations (skeletons) of branched and tubular structures of cellular shapes, and it is designed for providing to domain scientists: i) effective and fast semi-automatic interfaces for tracing skeletons directly on surface-based representations of cells and structures, ii) fast tools for proofreading, i.e., correcting and editing of semi-automatically constructed skeleton representations, and iii) natural methods for interactive exploration, i.e., measuring, comparing, and analyzing geometric features related to cellular structures based on medial axis representations. Neuroscientists currently use the system for performing morphology studies on sparse reconstructions of glial cells and neurons extracted from a sample of the somatosensory cortex of a juvenile rat. The framework runs in a standard PC and has been tested on two different display and interaction setups: PC-tethered stereoscopic head-mounted display (HMD) with 3D controllers and tracking sensors, and a large display wall with a standard gamepad controller. We report on a user study that we carried out for analyzing user performance on different tasks using these two setups.}
}
@article{PARAMARTHA2023874,
title = {Multimedia Application based on Virtual Reality to Introduce College Majors in Universities},
journal = {Procedia Computer Science},
volume = {227},
pages = {874-883},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.594},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017611},
author = {Damar Harip Paramartha and M.Afdhal Arief Malik and Selvi Dian Pertiwi and Reinert Yosua Rumagit},
keywords = {Virtual Reality, College Major, Universities},
abstract = {The goal of this project is to create virtual reality-based multimedia programs that can introduce users to fresh university experiences and information. Many prospective students are currently selecting the incorrect major, which could have negative effects on those prospective students. The Game Development Life Cycle, often known as the GDLC approach, is the development process method employed in this application. The GDLC process begins with initiation and progresses through pre-production, testing, beta, and release. Several respondents have tested the application. According to the survey's findings, 74.5% of respondents said that this application may encourage students to select the majors they are passionate about. According to the testimonial results, 77.1% of respondents thought this application was simple to understand.}
}
@article{SERRANOLAGUNA2012203,
title = {Tracing a Little for Big Improvements: Application of Learning Analytics and Videogames for Student Assessment},
journal = {Procedia Computer Science},
volume = {15},
pages = {203-209},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008344},
author = {Ángel Serrano-Laguna and Javier Torrente and Pablo Moreno-Ger and Baltasar Fernández-Manjón},
keywords = {Learning Analytics, Educational games, Data mining, Assessment},
abstract = {Assessment is essential to establish the failure or success of any educational activity. To measure the acquisition of the knowledge covered by the activity and also to determine the effectiveness of the activity itself. The increasing adoption of new technologies is promoting the use of new types of activities in schools, like educational video games that in some cases are developed by the teachers themselves. In this kind of activity, interactivity increases compared to traditional activities (e.g. reading a document), which can be a powerful source of data to feed learning analytics systems that infer knowledge about the effectiveness of the educational process. In this paper, we discuss how a part of the students’ assessment can be achieved semi-automatically by logging the interaction with educational video games. We conclude that even the application of rather simple tracking techniques means an advantage compared to other systems that are fed with less quality data.}
}
@article{SUGIANTO2023623,
title = {3D Modelling Building of District Johar Baru Using ArcGIS Pro and CityEngine},
journal = {Procedia Computer Science},
volume = {227},
pages = {623-631},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.566},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017337},
author = {Edi Sugianto and Johan Fernando Hosea and Bakti Amirul Jabar and Edy Irwansyah and Devi Fitrianah},
keywords = {3D Modelling, Level of Detail, ArcGIS, CityEngine},
abstract = {Various image-based approaches are currently available for 3 Dimensional (3D) modelling. Among them, two main geomatics techniques are photogrammetry and laser scanning. Owing to the limited availability of photographs to the general populace, the employment of satellite and aerial photogrammetry is rather intricate. In light of Jakarta's absence of a comprehensive three-dimensional portrayal, there is an impetus to generate accurate, precise, and detailed tri-dimensional visualizations of a particular sector within Jakarta, specifically Johar Baru. This endeavor shall be undertaken through a technique termed LoD, with ArcGIS and CityEngine serving as the fundamental instruments for this initiative. To do this, we provided a framework that leverages existing technologies and data sources to reduce the time and work needed for prospective 3D indoor routing applications. Our indoor and building 3D models were created using the CityEngine procedural modelling approach from CAD files and building footprints. This work does not address data delivery or administration; it primarily focuses on 3D visualization. Cadastral, visualization, and non-functional requirements are the three main areas of 3D visualization requirements against which CityEngine was assessed. A case study corresponding to a real issue that has already been recognized in Portugal is used to evaluate the problem. The results are encouraging. The CityEngine won't be the best choice for all users because of the steep learning curve. Using CityEngine, we can show and design a 3D reality in Johar Baru.}
}
@article{CLARKE201238,
title = {PR:EPARe: A Game-Based Approach to Relationship Guidance for Adolescents},
journal = {Procedia Computer Science},
volume = {15},
pages = {38-44},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.056},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008186},
author = {Samantha Clarke and Sylvester Arnab and Ian Dunwell and Katherine Brown},
keywords = {Relationship education, Participatory design, Serious games, Blended learning, Game-based intervention},
abstract = {Ensuring adolescents are equipped with the necessary skills to handle coercion and pressure from peers is a central component of effective relationship education. However, for teachers attempting to convey these principles, didactic methods have been shown to meet with limited success, as the highest-risk students may fail to engage with the subject matter in a meaningful fashion. In this paper, the potential a digital game may hold as a component of a blended learning solution to this problem is explored though the development of PR:EPARe (Positive Relationships: Eliminating Coercion and Pressure in Adolescent Relationships). Adopting a participatory design approach, designers considered relevant input from stakeholders, subject experts, teachers and students in the development of PR:EPARe. Participatory involvement has allowed the game to be developed in such a way that draws focus on the role of the end user to extend from the traditional concern of the student's learning needs to consider that of the practitioner's needs as another primary condition of successful game based learning. An examination of the first section of the PR:EPARe game is undertaken through a cluster randomized control trial of 507 students across three UK schools. Using ANOVA to demonstrating significant differences between control and game groups (p<0.05) for responses to a range of questions on preparedness and self-efficacy. An overall significant positive effect of the game over time when compared to the control (p<0.001) is observed. Based on these preliminary findings, the participatory approach to development is shown to lead to a developed game which is well- received by students, offering the potential to provide a valuable resource for teachers attempting to address this difficult subject within a classroom-based context.}
}
@article{DAVID2024103890,
title = {The Salient360! toolbox: Handling gaze data in 3D made easy},
journal = {Computers & Graphics},
volume = {119},
pages = {103890},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103890},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000177},
author = {Erwan David and Jesús Gutiérrez and Melissa Lè-Hoa Võ and Antoine Coutrot and Matthieu {Perreira Da Silva} and Patrick {Le Callet}},
keywords = {Toolbox, Gaze tracking, 360°stimuli, Processing, Comparison, Visualisation},
abstract = {Eye tracking has historically been a very popular tool. The data it records allow us to understand how people behave and what they attend to within our visual world; under this perspective the experiments, applications and use-cases are endless. Therefore, it is not surprising to witness a strong rise in the use of eXtended Reality (XR) devices with embedded eye trackers in research. These devices allow for less obtrusive experimenting conditions, and a significantly higher experimental control compared to traditional desktop testing. The use of eye tracking in XR is increasing and so is the need for a toolbox enabling consensus about eye tracking methods in 3D. We present the Salient360! toolbox: it implements functions to identify saccades and fixations and output gaze features (e.g., saccade directions) to generate saliency maps, fixation maps, and scanpath data. It implements comparisons of gaze data with methods adapted to 3D. We plan continuous improvements of the toolbox as the community develops new tools and methods dedicated to 360°gaze tracking. We hope that this toolbox will spark discussions about the methodology of 3D gaze processing, facilitate running experiments, and improve studying gaze in 3D. https://github.com/David-Ef/salient360Toolbox}
}
@article{SLOB2023107815,
title = {Virtual reality-based digital twins for greenhouses: A focus on human interaction},
journal = {Computers and Electronics in Agriculture},
volume = {208},
pages = {107815},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.107815},
url = {https://www.sciencedirect.com/science/article/pii/S016816992300203X},
author = {Naftali Slob and William Hurst and Rick {van de Zedde} and Bedir Tekinerdogan},
keywords = {Digital twin, Virtual reality, Greenhouse, Simulation sickness, Cyber sickness},
abstract = {The agricultural domain is experiencing an increasing use of digital twin technologies in greenhouse horticulture for the betterment of monitoring production. Research in this domain has started leading towards more advanced ways of visually interacting with these digital twins, and this could be by means of immersive technologies (such as virtual reality) to better allow farmers to feel a sense of presence when exploring these digital copies. Yet there are many remaining challenges for the technology’s integration and more studies are needed into user interaction; specifically regarding simulation sickness to cater for comfort during prolonged and regular use. Thus, this article documents the survey of 30 participants by means of the Simulator Sickness Questionnaire (before and after interaction) when using an immersive digital greenhouse twin environment. Findings indicate that users who experience simulation sickness tend to provide a lower evaluation score of the digital twin and their prior experience of gaming (on varied devices) affects the overall evaluation of the digital environment. Further, the level of playability and realism (referred to as convenience), statistically affects the end users’ level of sickness. For tracking crop growth by means of digital greenhouse twins, where prolonged user interaction may take place in a virtual environment, these are notable considerations for digital twin developers.}
}
@article{YU201837,
title = {Space-based Collision Avoidance Framework for Autonomous Vehicles},
journal = {Procedia Computer Science},
volume = {140},
pages = {37-45},
year = {2018},
note = {Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.290},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831963X},
author = {Jinke Yu and Leonard Petnga},
keywords = {Collision avoidance, Autonomous vehicles, Artificial intelligence, Spatio-temporal algorithms, AirSim},
abstract = {High confidence in the safe operation of autonomous systems remains a critical hurdle on their path to becoming ubiquitous. Recent accidents of Uber and Google driverless cars illustrate the difficulty ahead. Leading collision avoidance framework for autonomous systems fail to properly capture and account for the high variability of geometries, shapes, and sizes of the agents (e.g., 18 wheels truck vs. 4 doors sedan), capabilities that are critical in situations with high risk of accident (e.g., intersection crossing). We introduce a simple and efficient multi-agent collision avoidance framework for Autonomous Vehicles (AV) in various collision configurations (i.e., glancing, away, clipping). Machine learning techniques are proposed to properly train the autonomous systems involved. Vehicle-to-Vehicle (V2V) communication technologies and shape-based spatial-temporal collision avoidance algorithms are leveraged to ensure the accurate prediction of the collision and correct decision on the appropriate steps to avoid its occurrence. A prototype implementation and simulation is currently under development for a clipping collision problem at a lightless intersection crossing using the AirSim platform.}
}
@article{DOWNS2021102113,
title = {Assessing Industrial Robot agility through international competitions},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {70},
pages = {102113},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102113},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520303239},
author = {Anthony Downs and Zeid Kootbally and William Harrison and Pavel Pilliptchak and Brian Antonishek and Murat Aksu and Craig Schlenoff and Satyandra K. Gupta},
keywords = {Performance evaluation, Industrial Robotics, Artificial intelligence, Agility, Competitions},
abstract = {Manufacturing and Industrial Robotics have reached a point where to be more useful to small and medium sized manufacturers, the systems must become more agile and must be able to adapt to changes in the environment. This paper describes the process for creating and the lessons learned over multiple years of the Agile Robotics for Industrial Automation Competition (ARIAC) being run by the National Institute of Standards and Technology.}
}
@article{BRUNNSTROM2020116005,
title = {Latency impact on Quality of Experience in a virtual reality simulator for remote control of machines},
journal = {Signal Processing: Image Communication},
volume = {89},
pages = {116005},
year = {2020},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2020.116005},
url = {https://www.sciencedirect.com/science/article/pii/S0923596520301648},
author = {Kjell Brunnström and Elijs Dima and Tahir Qureshi and Mathias Johanson and Mattias Andersson and Mårten Sjöström},
keywords = {Quality of Experience (QoE), Virtual reality, Latency, Head-Mounted Displays (HMD), Forestry crane},
abstract = {In this article, we have investigated a VR simulator of a forestry crane used for loading logs onto a truck. We have mainly studied the Quality of Experience (QoE) aspects that may be relevant for task completion, and whether there are any discomfort related symptoms experienced during the task execution. QoE experiments were designed to capture the general subjective experience of using the simulator, and to study task performance. The focus was to study the effects of latency on the subjective experience, with regards to delays in the crane control interface. Subjective studies were performed with controlled delays added to the display update and hand controller (joystick) signals. The added delays ranged from 0 to 30 ms for the display update, and from 0 to 800 ms for the hand controller. We found a strong effect on latency in the display update and a significant negative effect for 800 ms added delay on latency in the hand controller (in total approx. 880 ms latency including the system delay). The Simulator Sickness Questionnaire (SSQ) gave significantly higher scores after the experiment compared to before the experiment, but a majority of the participants reported experiencing only minor symptoms. Some test subjects ceased the test before finishing due to their symptoms, particularly due to the added latency in the display update.}
}
@article{HAMM2019349,
title = {Guidetomeasure-OT: A mobile 3D application to improve the accuracy, consistency, and efficiency of clinician-led home-based falls-risk assessments},
journal = {International Journal of Medical Informatics},
volume = {129},
pages = {349-365},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618304337},
author = {Julian Hamm and Arthur Money and Anita Atwal},
keywords = {Technology for health, Health informatics, Falls prevention, Occupational therapy, 3D mobile visualisation, Measurement guidance, Falls risk factors},
abstract = {Background
A key falls prevention intervention delivered within occupational therapy is the home environment falls-risk assessment process. This involves the clinician visiting the patient’s home and using a 2D paper-based measurement guidance booklet to ensure that all measurements are taken and recorded accurately. However, 30% of all assistive devices installed within the home are abandoned by patients, in part as a result of the inaccurate measurements being recorded as part of the home environment falls-risk assessment process. In the absence of more appropriate and effective guidance, high levels of device abandonment are likely to persist.
Aim
This study presents guidetomeasure-OT, a mobile 3D measurement guidance application designed to support occupational therapists in carrying out home environment falls-risk assessments. Furthermore, this study aims to empirically evaluate the performance of guidetomeasure-OT compared with an equivalent paper-based measurement guidance booklet.
Methods
Thirty-five occupational therapists took part in this within-subjects repeated measures study, delivered within a living lab setting. Participants carried out the home environment falls-risk assessment process under two counterbalanced treatment conditions; using 3D guidetomeasure-OT; and using a 2D paper-based guide. Systems Usability Scale questionnaires and semi-structured interviews were completed at the end of both task. A comparative statistical analysis explored performance relating to measurement accuracy, measurement accuracy consistency, task completion time, and overall system usability, learnability, and effectiveness of guidance. Interview transcripts were analysed using inductive and deductive thematic analysis, the latter was informed by the Unified Theory of Acceptance and Use of Technology model.
Results
The guidetomeasure-OT application significantly outperformed the 2D paper-based guidance in terms task efficiency (p <  0.001), learnability (p <  0.001), system usability (p <  0.001), effectiveness of guidance (p =  0.001). Regarding accuracy, in absolute terms, guidetomeasure-OT produced lower mean error differences for 11 out of 12 items and performed significantly better for six out of 12 items (p = < 0.05). In terms of SUS, guidetomeasure-OT scored 83.7 compared with 70.4 achieved by the booklet. Five high-level themes emerged from interviews: Performance Expectancy, Effort Expectancy, Social Influence, Clinical Benefits, and Augmentation of Clinical Practice. Participants reported that guidetomeasure-OT delivered clearer measurement guidance that was more realistic, intuitive, precise and usable than the paper-based equivalent. Audio instructions and animated prompts were seen as being helpful in reducing the learning overhead required to comprehend measurement guidance and maintain awareness of task progression.
Conclusions
This study reveals that guidetomeasure-OT enables occupational therapists to carry out significantly more accurate and efficient home environment falls-risk assessments, whilst also providing a measurement guide tool that is considered more usable compared with the paper-based measurement guide that is currently used by clinicians in practice. These results are significant as they indicate that mobile 3D visualisation technologies can be effectively deployed to improve clinical practice, particularly within the home environment falls-risk assessment context. Furthermore, the empirical findings constitute overcoming the challenges associated with the digitisation of health care and delivery of new innovative and enabling technological solutions that health providers and policy makers so urgently need to ease the ever-increasing burden on existing public resources. Future work will focus on the development and empirical evaluation of a mobile 3D application for patient self-assessment and automated assistive equipment prescription. Furthermore, broader User Experience aspects of the application design and the interaction mechanisms that are made available to the user could be considered so as to minimize the effect of cognitive overloading and optimise user performance.}
}
@article{DENHAAN2020104855,
title = {The Virtual River Game: Gaming using models to collaboratively explore river management complexity},
journal = {Environmental Modelling & Software},
volume = {134},
pages = {104855},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104855},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220309129},
author = {R.J. {den Haan} and M.C. {van der Voort} and F. Baart and K.D. Berends and M.C. {van den Berg} and M.W. Straatsma and A.J.P. Geenen and S.J.M.H. Hulscher},
keywords = {Serious gaming, Social learning, Water management, Stakeholder participation, Participatory decision-making, Tangible interaction},
abstract = {Serious games are increasingly used as tools to facilitate stakeholder participation and stimulate social learning in environmental management. We present the Virtual River Game that aims to support stakeholders in collaboratively exploring the complexity of a changed river management paradigm in the Netherlands. The game uses a novel, hybrid interface design that features a bidirectional coupling of a physical game board to computer models. We ran five game sessions involving both domain experts and non-experts to assess the game's value as a participatory tool. The results show that the game was effective in enabling participants to collaboratively experiment with various river interventions and in stimulating social learning. As a participatory tool, the game appears to be valuable to introduce non-expert stakeholders to Dutch river management. We further discuss how the hybrid interface combines qualities usually found in board and computer games that are beneficial in engaging stakeholders and stimulating learning.}
}
@article{BEKTAS2024103909,
title = {Gaze-enabled activity recognition for augmented reality feedback},
journal = {Computers & Graphics},
volume = {119},
pages = {103909},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103909},
url = {https://www.sciencedirect.com/science/article/pii/S009784932400044X},
author = {Kenan Bektaş and Jannis Strecker and Simon Mayer and Kimberly Garcia},
keywords = {Pervasive eye tracking, Augmented reality, Attention, Human activity recognition, Context-awareness, Ubiquitous computing},
abstract = {Head-mounted Augmented Reality (AR) displays overlay digital information on physical objects. Through eye tracking, they provide insights into user attention, intentions, and activities, and allow novel interaction methods based on this information. However, in physical environments, the implications of using gaze-enabled AR for human activity recognition have not been explored in detail. In an experimental study with the Microsoft HoloLens 2, we collected gaze data from 20 users while they performed three activities: Reading a text, Inspecting a device, and Searching for an object. We trained machine learning models (SVM, Random Forest, Extremely Randomized Trees) with extracted features and achieved up to 89.6% activity-recognition accuracy. Based on the recognized activity, our system—GEAR—then provides users with relevant AR feedback. Due to the sensitivity of the personal (gaze) data GEAR collects, the system further incorporates a novel solution based on the Solid specification for giving users fine-grained control over the sharing of their data. The provided code and anonymized datasets may be used to reproduce and extend our findings, and as teaching material.}
}
@article{LI2022418,
title = {RADepthNet: Reflectance-aware monocular depth estimation},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {5},
pages = {418-431},
year = {2022},
note = {Computer graphics for metaverse},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000808},
author = {Chuxuan Li and Ran Yi and Saba Ghazanfar Ali and Lizhuang Ma and Enhua Wu and Jihong Wang and Lijuan Mao and Bin Sheng},
keywords = {Monocular depth estimation, Deep learning, Intrinsic image decomposition},
abstract = {Background
Monocular depth estimation aims to predict a dense depth map from a single RGB image, and has important applications in 3D reconstruction, automatic driving, and augmented reality. However, existing methods directly feed the original RGB image into the model to extract depth features without avoiding the interference of depth-irrelevant information on depth-estimation accuracy, which leads to inferior performance.
Methods
To remove the influence of depth-irrelevant information and improve the depth-prediction accuracy, we propose RADepthNet, a novel reflectance-guided network that fuses boundary features. Specifically, our method predicts depth maps using the following three steps: (1) Intrinsic Image Decomposition. We propose a reflectance extraction module consisting of an encoder-decoder structure to extract the depth-related reflectance. Through an ablation study, we demonstrate that the module can reduce the influence of illumination on depth estimation. (2) Boundary Detection. A boundary extraction module, consisting of an encoder, refinement block, and upsample block, was proposed to better predict the depth at object boundaries utilizing gradient constraints. (3) Depth Prediction Module. We use an encoder different from (2) to obtain depth features from the reflectance map and fuse boundary features to predict depth. In addition, we proposed FIFADataset, a depth-estimation dataset applied in soccer scenarios.
Results
Extensive experiments on a public dataset and our proposed FIFADataset show that our method achieves state-of-the-art performance.}
}
@article{TASFI2023110401,
title = {Dynamic Successor Features for transfer learning and guided exploration},
journal = {Knowledge-Based Systems},
volume = {267},
pages = {110401},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110401},
url = {https://www.sciencedirect.com/science/article/pii/S095070512300151X},
author = {Norman Tasfi and Eder Santana and Luisa Liboni and Miriam Capretz},
keywords = {Reinforcement learning, Successor features, Transfer learning, Deep learning},
abstract = {The Successor Feature framework for Reinforcement Learning algorithms improves task transfer by decomposing the learned state–action value function. The decomposition involves two components, one that captures future-expected state features and the other that models the task-related reward structure. However, successful transfer between tasks depends heavily on how the reward function changes, possibly leading to failure of the original Successor Feature formulation. This paper proposes the Dynamic Successor Feature framework, DynSF, by extending the mathematical formulation of the original Successor Feature framework to center around a learned state-transition model. Under this formulation, the state-transition model dynamically induces the acting policy. The flexibility of DynSF also extends to the architecture, requiring only a state-transition model and a small vector of parameters. This architecture provides immense flexibility in the choice of the model used to learn the state-transition model. The DynSF framework is evaluated and compared to other baseline algorithms through several experiments in a continuous grid world environment, a robotic Reacher, and pixels in the Doom environment.}
}
@article{TADEJA2024103919,
title = {Immersive presentations of real-world medical equipment through interactive VR environment populated with the high-fidelity 3D model of mobile MRI unit},
journal = {Computers & Graphics},
volume = {120},
pages = {103919},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103919},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000542},
author = {Sławomir Konrad Tadeja and Thomas Bohné and Kacper Godula and Artur Cybulski and Magdalena Maria Woźniak},
keywords = {Virtual reality, VR, Immersive interface, Magnetic resonance imaging, MRI, Virtual presentation},
abstract = {The primary goal behind the system presented in this paper is to investigate the efficacy of using virtual reality (VR) for showcasing sizable medical equipment. Specifically, we focused on a mobile magnetic resonance imaging (MRI) scanner mounted on a truck trailer. The latter is integral to the mobile MRI setup and must be presented as part of the immersive experience. Therefore, we not only have to depict the medical apparatus but also provide the means of understanding its surroundings. This is especially important to radiologists and other medical personnel to ascertain if a given mobile medical facility fulfills their needs and wants. Furthermore, despite such MRI devices being designed for mobility, their long-distance transportation can be time-consuming, troublesome and expensive. Therefore, we can observe the need for showcasing such mobile MRI units without additional cost and burden related to transportation. To achieve this, we designed an immersive environment in which the users can interact with the real-life scale 3D model of a mobile MRI. In addition, we also verified the usability and expressiveness of our system using established heuristical approaches.}
}
@article{GREIPL2021106946,
title = {When the brain comes into play: Neurofunctional correlates of emotions and reward in game-based learning},
journal = {Computers in Human Behavior},
volume = {125},
pages = {106946},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106946},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221002697},
author = {S. Greipl and E. Klein and A. Lindstedt and K. Kiili and K. Moeller and H.-O. Karnath and J. Bahnmueller and J. Bloechle and M. Ninaus},
keywords = {Emotions, Game-based learning, Human-computer interaction, Brain, FMRI, Reward},
abstract = {Accumulating evidence identifies emotions as drivers of effective learning. In parallel, game-based learning was found to emotionally engage learners, allegedly harnessing the fundamental tie between emotions and cognition. Questioning further whether and how game-based learning elicit emotional processes, the current fMRI study examined the neurofunctional correlates of game-based learning by directly comparing a game-based and a non-game-based version of a digital learning task. We evaluated neurofunctional activation patterns within a comprehensive set of brain areas involved in emotional and reward processes (e.g. amygdala or ventral tegmental area) when participants received feedback. With only a few exceptions, decoding of these brain areas’ activation patterns indicated predominantly stronger relative activation in the game-based task version. As such, our results substantiate on a neurofunctional level that game-based learning leads to an invigoration of learning processes through processes of reward and emotional engagement.}
}
@article{HUBER2023107948,
title = {Game elements enhance engagement and mitigate attrition in online learning tasks},
journal = {Computers in Human Behavior},
volume = {149},
pages = {107948},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107948},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002996},
author = {Stefan E. Huber and Rodolpho Cortez and Kristian Kiili and Antero Lindstedt and Manuel Ninaus},
keywords = {Educational games, Game-based learning, Games, Distance education and online learning, Media in education, Human-computer interface, Gamification},
abstract = {A growing body of literature suggests that adding game elements to learning tasks indirectly influences the learning process by increasing engagement with the tasks. The present study aims to advance learning engagement research by examining an often neglected subcomponent of behavioral engagement, attrition. Implementing two equivalent versions of a learning task, differing solely in the presence of game elements, allowed unequivocal attribution of any effect on the presence of game elements. Conducting the study in an online learning environment allowed further a highly unconstrained examination of the effects of game elements on attrition. We found that game elements affected both participant attrition and engagement of participants who completed the learning task. Participants with low self-efficacy were particularly prone to drop out in the non-game condition. Game elements also affected both learning efficacy and efficiency. We further found task attractivity to partially mediate the effect of game elements on learning outcomes. The results suggest that by facilitating engagement via task attractivity game elements can compensate to some extent for the increased cognitive demand that the game elements induce. We finally discuss the importance of considering the interrelations between learner characteristics, game elements, and engagement for interpreting results on learning performance measures.}
}
@article{SCHOFIELD2023101172,
title = {An improved semi-synthetic approach for creating visual-inertial odometry datasets},
journal = {Graphical Models},
volume = {126},
pages = {101172},
year = {2023},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2023.101172},
url = {https://www.sciencedirect.com/science/article/pii/S1524070323000036},
author = {Sam Schofield and Andrew Bainbridge-Smith and Richard Green},
keywords = {Visual inertial odometry, Semi-synthetic dataset, Robot virtual reality},
abstract = {Capturing outdoor visual-inertial datasets is a challenging yet vital aspect of developing robust visual-inertial odometry (VIO) algorithms. A significant hurdle is that high-accuracy-ground-truth systems (e.g., motion capture) are not practical for outdoor use. One solution is to use a “semi-synthetic” approach that combines rendered images with real IMU data. This approach can produce sequences containing challenging imagery and accurate ground truth but with less simulated data than a fully synthetic sequence. Existing methods (used by popular tools/datasets) record IMU measurements from a visual-inertial system while measuring its trajectory using motion capture, then rendering images along that trajectory. This work identifies a major flaw in that approach, specifically that using motion capture alone to estimate the pose of the robot/system results in the generation of inconsistent visual-inertial data that is not suitable for evaluating VIO algorithms. However, we show that it is possible to generate high-quality semi-synthetic data for VIO algorithm evaluation. We do so using an open-source full-batch optimisation tool to incorporate both mocap and IMU measurements when estimating the IMU’s trajectory. We demonstrate that this improved trajectory results in better consistency between the IMU data and rendered images and that the resulting data improves VIO trajectory error by 79% compared to existing methods. Furthermore, we examine the effect of visual-inertial data inconsistency (as a result of trajectory noise) on VIO performance to provide a foundation for future work targeting real-time applications.}
}
@article{MELEIRO201466,
title = {Natural User Interfaces in the Motor Development of Disabled Children},
journal = {Procedia Technology},
volume = {13},
pages = {66-75},
year = {2014},
note = {SLACTIONS 2013: Research conference on virtual worlds – Learning with simulations},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314000206},
author = {Pedro Meleiro and Rui Rodrigues and João Jacob and Tiago Marques},
keywords = {Natural User Interfaces, motor disability, skeleton tracking, motion sensor, assisted rehabilitation},
abstract = {This study describes a framework based upon body tracking devices and aimed at assisting children with motor impairments and aims at understanding what positive contribute it can deliver for their rehabilitation process. A case study is defined featuring two motor disorders that take advantage of the technological specifications, as well as the types of exercise appropriate for this context. The developed framework collects motricity data by asking the user to mimic the movements of a previously recorded exercise, and is thoroughly detailed in this paper. The results obtained evidence the data collected regarding the user performance denotes certain motor patterns of the disorder, making it apt to be applied as an auxiliary tool for impairments diagnosis. A few tracking issues indicate that the technologies selected can be applied in a real context to assist in rehabilitation sessions, but require additional evaluation metrics to support its conclusions.}
}
@article{SETIONO2021781,
title = {Enhancing Player Experience in Game With Affective Computing},
journal = {Procedia Computer Science},
volume = {179},
pages = {781-788},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921000843},
author = {Doyo Setiono and David Saputra and Kaleb Putra and Jurike V. Moniaga and Andry Chowanda},
keywords = {Affective Computing, Facial Expression Recognition, Virtual Pet Simulation, Game Experiences},
abstract = {This research aims to implement Affective Computing as part of game design by capturing, processing, and interpreting the player’s emotions to enhance the player’s experience in the game. We argue by implementing affective computing in the game will statistically enhance the players game experiences. Two almost identical games designed and developed to prove the claim. One game imbued with the affective computing system by capturing players emotions from their facial expressions as the game input, when the other game only implement a touch screen system for the input. Both games then were evaluated in two groups of 50 respondents in each group. A combination of Game Experiences Questionnaire and Immersiveness Game Questionnaire used to evaluate the game player experiences in this research. The result concludes that most of the game experiences score in the game with FER system implemented was statistically increased compared to the one with-out the FER system implemented, except for Q3 (p = 0.06), and Q9 (p = 0.08).}
}
@article{SVATON20141445,
title = {Improving Strategy in Robot Soccer Game by Sequence Extraction},
journal = {Procedia Computer Science},
volume = {35},
pages = {1445-1454},
year = {2014},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.08.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914011697},
author = {Václav Svatoň and Jan Martinovič and Kateřina Slaninová and Tomáš Bureš},
keywords = {robot soccer, strategy, rule, sequence},
abstract = {Robot Soccer is a very attractive platform in terms of research. It contains a number of challenges in the areas of robot control, artificial intelligence and image analysis. This article presents a look at the overall architecture of the game and describes some results of our experiments in analysis and optimization of strategies using sequence extraction. We have extracted sequences of game situations from the log of a game played in our simulator, as they occurred during the game. Afterwards, these sequences were compared by methods LCS, LCSS and T-WLCS, which are usually used for sequence comparison in the sequence alignment area. Using these methods, we are able to visualize the relations between the sequences of game situations and clusters of similar game situations in a graph. In conclusion, a possible description improvement of these game situations is introduced. Therefore, a possible strategy improvement to ensure a smoother and faster performing of actions defined by these situations is described.}
}
@article{AKBAR2019388,
title = {Enhancing Game Experience with Facial Expression Recognition as Dynamic Balancing},
journal = {Procedia Computer Science},
volume = {157},
pages = {388-395},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.230},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311494},
author = {M. Taufik Akbar and M. Nasrul Ilmi and Imanuel V. Rumayar and Jurike Moniaga and Tin-Kai Chen and Andry Chowanda},
keywords = {Dynamic Balancing, Player’s Experiences, Game Technology, Facial Expression Recognition},
abstract = {Player’s Experience in the game has been known to be one of the essential keys for the success of the game. There are several methods exist to enhance the player’s experiences in the games. One of the unexplored methods is a dynamic balancing system using Facial Expression Recognition. The player’s facial expression is captured in real-time while the player is playing the game, and the dynamic balancing system will automatically adjust the game difficulty based on the player’s facial expressions. This research aims to empirically explore the implementation of Facial Expression Recognition for a dynamic balancing system to enhance the player’s experiences in the game. Two action games (2D and 3D) were developed and evaluated with 60 respondents in two groups. Both groups played the game twice, one with facial expression recognition system as dynamic balancing activated and one without. The results demonstrate that they are statistically significant differences (i.e. improvement) between the baseline and enhanced games with p < 0.01.}
}
@article{MARTINEZPERNIA201771,
title = {Using game authoring platforms to develop screen-based simulated functional assessments in persons with executive dysfunction following traumatic brain injury},
journal = {Journal of Biomedical Informatics},
volume = {74},
pages = {71-84},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417301910},
author = {David Martínez-Pernía and Javier Núñez-Huasaf and Ángel {del Blanco} and Amparo Ruiz-Tagle and Juan Velásquez and Mariela Gomez and Carl {Robert Blesius} and Agustin Ibañez and Baltasar Fernández-Manjón and Andrea Slachevsky},
keywords = {Functional evaluation, Naturalistic activities, Executive functions, Prefrontal cortex, Neuropsychological assessment, eAdventure},
abstract = {The assessment of functional status is a critical component of clinical neuropsychological evaluations used for both diagnostic and therapeutic purposes in patients with cognitive brain disorders. There are, however, no widely adopted neuropsychological tests that are both ecologically valid and easily administered in daily clinical practice. This discrepancy is a roadblock to the widespread adoption of functional assessments. In this paper, we propose a novel approach using a serious game authoring platform (eAdventure) for creating screen-based simulated functional assessments. We created a naturalistic functional task that consisted of preparing a cup of tea (SBS-COT) and applied the assessment in a convenience sample of eight dyads of therapists/patients with mild executive dysfunction after traumatic brain injury. We had three main aims. First, we performed a comprehensive review of executive function assessment in activities of daily living. Second, we were interested in measuring the feasibility of this technology with respect to staffing, economic and technical requirements. Third, a serious game was administered to patients to study the feasibility of this technology in the clinical context (pre-screening test). In addition, quantitative (Technology Acceptance Model (TAM) questionnaires) and qualitative (semistructured interviews) evaluations were applied to obtain user input. Our results suggest that the staffing, economic and technical requirements of the SBS-COT are feasible. The outcomes of the pre-screening test provide evidence that this technology is useful in the functional assessment of patients with executive dysfunction. In relation to subjective data, the TAM questionnaire showed good user acceptability from a professional perspective. Interview analyses with professionals and patients showed positive experiences related to the use of the SBS-COT. Our work indicates that the use of these types of authoring platforms could have positive long-term implications for neuropsychological research, opening the door to more reproducible, cooperative and efficient research by allowing the facilitated production, reuse and sharing of neuropsychological assessment tools.}
}
@article{AWADALLAH2024102652,
title = {Remote collaborative framework for real-time structural condition assessment using Augmented Reality},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102652},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102652},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003008},
author = {Omar Awadallah and Katarina Grolinger and Ayan Sadhu},
keywords = {Augmented Reality, Structural Health Monitoring, Building Information Modelling, Real-time Collaboration, Structural Condition Assessment, Microsoft HoloLens},
abstract = {Civil structures worldwide are confronted with a growing threat of structural deterioration, aggravated by various factors such as climate change, population growth, and increased traffic. The latent nature of these issues often leads to undetected vulnerabilities until a catastrophic failure occurs, resulting in substantial losses. To address this challenge, there is a critical need for improved structural monitoring, condition assessment, and maintenance practices. Traditional inspection methods, relying on visual estimation and heavy equipment for inaccessible areas, present formidable obstacles to inspectors. These methods impede the safe and fast examination of structural damage, complicating tracking of structural deterioration, and hindering efficient condition assessment. Recognizing these challenges, this paper proposes a remote collaborative framework to enhance the efficiency of structural inspections by leveraging the capabilities of Augmented Reality (AR), QR code, and 5G network. The proposed framework centers on real-time remote collaboration among on-site and off-site inspectors, aiming to elevate safety, accessibility, and overall inspection efficacy. The integration of real-time data sharing and collaboration facilitates immediate decision-making, enabling inspectors to proactively address structural vulnerabilities and prevent potential failures. This study concludes that the proposed framework effectively facilitates real-time structural condition assessment for on-site AR users. Simultaneously, off-site web users can instantly track the progression of data over time through the utilization of 5G technology. The proposed advanced AR framework effectively demonstrates real-time structural condition assessment through a lab-scale experimental beam and a full-scale bridge.}
}
@article{LOPES2022103489,
title = {A survey on RGB-D datasets},
journal = {Computer Vision and Image Understanding},
volume = {222},
pages = {103489},
year = {2022},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2022.103489},
url = {https://www.sciencedirect.com/science/article/pii/S1077314222000923},
author = {Alexandre Lopes and Roberto Souza and Helio Pedrini},
keywords = {RGB-D data, Monocular depth estimation, Computer vision, Depth datasets},
abstract = {RGB-D data is essential for solving many problems in computer vision. Hundreds of public RGB-D datasets containing various scenes, such as indoor, outdoor, aerial, driving, and medical, have been proposed. These datasets are useful for different applications and are fundamental for addressing classic computer vision tasks, such as monocular depth estimation. This paper reviewed and categorized image datasets that include depth information. We gathered 231 datasets that contain accessible data and grouped them into three categories: scene/objects, body, and medical. We also provided an overview of the different types of sensors, depth applications, and we examined trends and future directions of the usage and creation of datasets containing depth data, and how they can be applied to investigate the development of generalizable machine learning models in the monocular depth estimation field.}
}
@article{JAZIRI20211152,
title = {ORVIPO: An Ontological Prototype for Modeling 3D Scenes in Operating Rooms},
journal = {Procedia Computer Science},
volume = {192},
pages = {1152-1161},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016070},
author = {Faouzi Jaziri and Rim Messaoudi and Achraf Mtibaa and Jonathan Courbon and Mahdi Kilani and Mohamed Mhiri and Antoine Vacavant},
keywords = {Virtual reality, Ontology, Semantic interoperability, Reasoning rules, Interaction},
abstract = {Virtual Operating room characterizes a large artificial environment of interaction in the medical context. It enables a better simulation for different surgical scenarios through 3D (three‐dimensional) objects. Virtual Reality (VR) uses computer technology to develop these virtual simulated applications. It brings valuable innovations in different fields. It is integrated also to enhance healthcare and therapies. To improve and assist on medical VR applications, ontologies would be useful. They can unify the content of these applications and facilitate their implementation via semantic rules. Also, ontologies can be applied to realize an effective VR knowledge modeling. This paper presents a virtual simulation of an operating room taking advantage of the semantic representation. The goal of this study is to propose a novel ontological VR system that models an operating room and its components. The execution of the proposed method was proved by the developed ontology OROnto (Operating Room Ontology). This ontology was useful for modeling hospital scenarios and the construction of a valuable VR system. To demonstrate the proposed approach feasibility and performance, we have implemented the Operating Room Virtual Integration Process Using Ontology (ORVIPO) prototype. Compared to different other ontological methods and related works, our approach have shown interesting findings such as recall (71%), precision (83%), and F-measure (76%).}
}
@article{PRANOTO2019506,
title = {Increase The Interest In Learning By Implementing Augmented Reality: Case studies studying rail transportation.},
journal = {Procedia Computer Science},
volume = {157},
pages = {506-513},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311652},
author = {Hady Pranoto and Francisco Maruli Panggabean},
keywords = {Augmented Reality, learning method, rail transportation},
abstract = {Learn a subject, for some people, might be an uninteresting and boring activity, especially when the subject to learn are difficult subjects to understand. Many methods used to change learning activities become more enjoyable and interested. This study proposed a new method in learning activities, by applied augmented reality technology in the learning process. The case study used in this paper are implementation the augmented reality in studied subjects related to train technology. In this study, author implement augmented reality on learning material, combines real and virtual things in one media, in this case a mobile device. The impact of implementation of augmented studied, at the end of experiment, author can conclude when implement augmented reality technology in learning material helps the learning process and increasing the impressive and fun factor in learning process and make the learning process more interested. Implementation of Augmented Reality in learning material gives more information about the object being studied, information about on shapes, textures, and provide more visualization for the object.}
}
@article{CALZADOMARTINEZ2022e00235,
title = {Accessing interactively the spatio-temporal data-model of an archaeological site through its 3D virtual reconstruction},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {26},
pages = {e00235},
year = {2022},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2022.e00235},
url = {https://www.sciencedirect.com/science/article/pii/S2212054822000248},
author = {Alberto Calzado-Martínez and Ángel-Luis García-Fernández and Lidia M. Ortega-Alvarado},
keywords = {Spatio-temporal database, Virtual 3D archaeological site reconstruction, Topological relationship, 3D navigation and interaction},
abstract = {Information and communication technologies are increasingly used in all archaeological processes. However, archaeologists sometimes consider them as intrusive, too far from the traditional work methodology and even a hindrance. In this article we propose a framework to allow natural and therefore intuitive access to the archaeological record. The information retrieval process is carried out through a three-dimensional virtual reconstruction of the archaeological site. In this system, navigation and interaction with the three-dimensional elements of the environment triggers database queries. To achieve such functionality, a client-server architecture is designed in which the server maintains a spatio-temporal database with heterogeneous information, including the 3D models of the finds. To do this, a virtual replica of the site is created considering spatial restrictions and topological relationships among the finds.}
}
@article{KAJIHARA2019106956,
title = {Non-rigid registration of serial section images by blending transforms for 3D reconstruction},
journal = {Pattern Recognition},
volume = {96},
pages = {106956},
year = {2019},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0031320319302535},
author = {Takehiro Kajihara and Takuya Funatomi and Haruyuki Makishima and Takahito Aoto and Hiroyuki Kubo and Shigehito Yamada and Yasuhiro Mukaigawa},
keywords = {Image registration, Non-rigid deformation, Transformation blending},
abstract = {In this research, we propose a novel registration method for three-dimensional (3D) reconstruction from serial section images. 3D reconstructed data from serial section images provides structural information with high resolution. However, there are three problems in 3D reconstruction: non-rigid deformation, tissue discontinuity, and accumulation of scale change. To solve the non-rigid deformation, we propose a novel non-rigid registration method using blending rigid transforms. To avoid the tissue discontinuity, we propose a target image selection method using the criterion based on the blending of transforms. To solve the scale change of tissue, we propose a scale adjustment method using the tissue area before and after registration. The experimental results demonstrate that our method can represent non-rigid deformation with a small number of control points, and is robust to a variation in staining. The results also demonstrate that our target selection method avoids tissue discontinuity and our scale adjustment reduces scale change.}
}
@article{DIAZ2015205,
title = {How the Type of Content in Educative Augmented Reality Application Affects the Learning Experience},
journal = {Procedia Computer Science},
volume = {75},
pages = {205-212},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.239},
url = {https://www.sciencedirect.com/science/article/pii/S187705091503700X},
author = {Christian Diaz and Mauricio Hincapié and Gustavo Moreno},
keywords = {static and dynamic contents, augmented reality, fundamentals of electronics.},
abstract = {Nowadays, the use of technology to improve teaching and learning experiences in the classroom has been promoted. One of these technologies is augmented reality, which allows overlaying layers of virtual information on real scene with the aim of increasing the perception the user has of reality. In the educational context augmented reality have proved to offer several advantages, i.e. increasing learning engagement and increasing understanding of some topics, especially when spatial skills are involved. Contents deployed in an augmented reality application are of two types, static, i.e. text, or dynamic, i.e. animations. As far as we know no research project has assessed how the type of content, static or dynamic, can affect the student learning experience in augmented reality applications. In this article the development and evaluation of an augmented reality application using static and dynamic content is described. In order to determine how the type of content affects the learning experience of the student, an experimental design in which the student interact with the application, using static and dynamic contents, for learning topics related with an electronic fundamentals course was performed.}
}
@article{BIRCH2018242,
title = {Crowdsourcing with online quantitative design analysis},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {242-251},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617306018},
author = {David Birch and Alvise Simondetti and Yi-ke Guo},
keywords = {Design, Crowdsourcing, Architecture, Cloud, Masterplanning, Optimization},
abstract = {Design is a balancing act between people’s competing concerns, design options and design performance. Recently collecting data on such concerns such as sustainability or aesthetics has become possible through online crowdsourcing, particularly in 3d. However, such systems rarely present more than a single design alternative or allow users to change the design and seldom provide quantitative design analysis to gauge design performance. This precludes a more participatory approach including a wider audience and their insight in the design process. To improve the design process we propose a system to assist the design team in exploring the balance of concerns, design options and their performance. We augment a 3d visualisation crowdsourcing environment with quantitative on-demand assessment of design variants run in the cloud. This enables crowdsourced exploration of the design space and its performance. Automated participant tracking and explicit submitted feedback on design options are collated and presented to aid the design team in balancing the demands of urban master planning. We report application of this system to an urban masterplan with Arup.}
}
@article{KADYR2024341,
title = {Affective computing methods for simulation of action scenarios in video games},
journal = {Procedia Computer Science},
volume = {231},
pages = {341-346},
year = {2024},
note = {14th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 13th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (EUSPN/ICTH 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.214},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923022263},
author = {Sarsenbek Kadyr and Chinibayeva Tolganay},
keywords = {Affective computing, affective loop, video game, procedural content generation, fuzzy logic, artificial neural networks},
abstract = {Video games are becoming a part of the lives of many, many people. They are able to evoke a wide range of emotions, from joy and excitement to fear and empathy, creating deep immersion and memorable impressions for players. How do they manage it and is it possible to use the emotions themselves in the game itself? This article presents the concept of developing a system of dynamic procedural content generation (PCG) with game mechanics of interaction using emotions based on methods affective computing. To achieve this goal, The article developed an emotion recognition system based on fuzzy logic. And PCG performed by Artificial Neural Networks. In the video game industry, affective computing methods, such as interaction with the expression of emotions, can bring great benefits, since the emotional involvement of the player is very important. This will make a new contribution to the diversity of human-computer interaction.}
}
@article{KOSTOV2022896,
title = {Designing a Framework for Collaborative Mixed Reality Training},
journal = {Procedia Computer Science},
volume = {200},
pages = {896-903},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.287},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002964},
author = {Georgi Kostov and Josef Wolfartsberger},
keywords = {multi-device, collaboration, tool, virtual reality, augmented reality, network, human-computer interaction},
abstract = {Training simulations in Virtual Reality (VR) benefit from the technology’s interactive and intuitive nature. Typically, the VR user is isolated in the virtual environment and an external instructor or spectators cannot support the learning progress. Previous research work presents solutions for isolated use cases, but lacks specific guidelines for the implementation of collaborative multi-device systems. We propose a method for expanding collaborative extended reality (XR) applications to multiple platforms to support collaborative learning, but also other types of applications such as remote collaboration and maintenance. In order to test our approach, we expanded an existing application to four different platforms - Steam VR, Microsoft Mixed Reality, smartphone/tablet and desktop computer. Based on lessons learned from our prototype, we propose a solution for multi-device networking and interaction on each platform. The main strengths of our approach are in the decoupling of local and networked objects and the common interface for interaction, which accommodates multiple platforms. We believe our approach can provide useful insights into collaborative training and serve as a good starting point for future projects.}
}
@article{SIVANATHAN2012103,
title = {Temporal Synchronisation of Data Logging in Racing Gameplay},
journal = {Procedia Computer Science},
volume = {15},
pages = {103-110},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008241},
author = {Aparajithan Sivanathan and Theodore Lim and Sandy Louchart and James Ritchie},
keywords = {Data logging, Synchronisation and encoding framework, EEG, telemetry, Psychophysiology, Biofeedback},
abstract = {Contemporary game play is a complex environment where user interacts with many elements. Data logging in a game play is a common practice to study a player's in-game behaviour. Identified behavioural knowledge is then applied to enhance the game elements. Analyzing the user behaviour is a multivariate process and therefore requires more information than mere logging of game context. Multi modal data channels such as biofeedback signals are increasingly used to study the game play so that more detailed understanding of the user behaviour can be established. Precise synchronisation in capturing of multiple streams is essential to produce accurate and meaningful information. This paper presents a generic technique to accurately synchronize multiple data streams captured during a gaming session. It is demonstrated by applying it to a driving game Simulation. Observable correlation between the in-game data and psycho-physiological signals are presented to demonstrate the accuracy and granularity of the synchronisation.}
}
@article{COSINAYERBE202275,
title = {Clustered voxel real-time global illumination},
journal = {Computers & Graphics},
volume = {103},
pages = {75-89},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S009784932200005X},
author = {Alejandro {Cosin Ayerbe} and Gustavo Patow},
keywords = {Real-time, Global illumination, Voxelization, Clustering, Diffuse surfaces},
abstract = {Real-time global illumination is an extremely challenging problem because of its intrinsic complexity due to the interplay between complex geometry, multiple light bounces, and stringent real-time frame-rate requirements. In this paper, we present a new technique that enables the real-time computation of global illumination in generic scenes for diffuse surfaces and static geometry. Our technique combines a voxel-based representation of the scene, a voxel-clustering algorithm, and an iterative light-propagation algorithm based on the resulting clusters. Although our implementation is currently designed to handle the first (and main) bounce, the technique allows for multiple light bounces. Summing up all these components results in a flexible algorithm capable of providing global illumination effects and on-the-fly illumination computations.}
}
@article{BORAWSKA20181616,
title = {The Concept of Virtual Reality System to Study the Media Message Effectiveness of Social Campaigns},
journal = {Procedia Computer Science},
volume = {126},
pages = {1616-1626},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.135},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314133},
author = {Anna Borawska and Mariusz Borawski and Małgorzata Łatuszyńska},
keywords = {Virtual Reality, Social campaign, Multimedia message},
abstract = {Social campaigns are an important tool for promoting positive change in social attitudes (in ecology, health prevention, tolerance promotion, etc.). Improving their effectiveness may therefore have a very tangible effect on many aspects of life – both for individuals and for whole societies. Among the most widespread activities that are undertaken within the framework of social campaigns, one can mention advertising through different types of media – television, radio, internet and print. Assessment of this element is done mostly on the basis of questionnaires and focus groups. Research of this type relies on measures that are proximal, such as perceived effectiveness. Although it is assumed that perceived effectiveness is causally antecedent to actual effectiveness, it would be advisable to find the way of assessing actual effectiveness more directly. One of the approaches, that aim to solve this problem is application of the tools of cognitive neuroscience. The use of cognitive neuroscience techniques for pretesting media messages in social campaigns requires properly designed research. In order to do that, the pivotal stage of each experiment design is the choice of media stimuli that will be presented to the participants. Different media and contents will lead to different patterns of responses in viewers, determining a great deal about how a message is processed, including which parts of the message are attended to, and how the message is evaluated and liked. Stimuli presented during the experiment can be static, like pictures or dynamic, like video. However, such stimuli may not be representative of real-life situations. To avoid problems with picture and video stimuli, one can use virtual environments. The aim of the article is to present a concept of virtual reality system that could enable the research of media messages effectiveness in social campaigns.}
}
@article{ISIK2024483,
title = {Integrating extended reality in industrial maintenance: a game-based framework for compressed air system training},
journal = {Procedia Computer Science},
volume = {232},
pages = {483-492},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924000486},
author = {Birkan Isik and Gulbahar Emir Isik and Miroslav Zilka},
keywords = {Extended reality, Serious game, Maintenance training, Compressed Air System},
abstract = {In industrial environments, maintenance technologies developed under Industry 5.0 are important to ensure trouble-free production and reduce downtime. Compressed Air Systems (CASs) are an integral part of industrial production processes and require competent and correct maintenance. Maintenance personnel should be equipped with continuous and up-to-date training and their knowledge should be tested. There is a significant gap in the field of interactive educational games to improve the technical skills of the care staff, especially in the field of CAS. This research proposes a theoretical framework for a game based on Extended Reality (XR) technologies adapted for CAS maintenance training. The game leverages the trio of Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) to provide immersive and interactive learning encounters aimed at improving technical proficiency, problem-solving skills, and safety awareness among maintenance workers. The aim is to take advantage of the rewards of game-oriented learning, such as enhanced user engagement and knowledge absorption, to increase the effectiveness of training. In this context, the CAS game framework was created with three frameworks: education, game, and CAS. The study concludes by highlighting the potential benefits, research areas, and technologies that underpin the proposed structure.}
}
@article{BOTTANI2021103429,
title = {Wearable and interactive mixed reality solutions for fault diagnosis and assistance in manufacturing systems: Implementation and testing in an aseptic bottling line},
journal = {Computers in Industry},
volume = {128},
pages = {103429},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103429},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000361},
author = {Eleonora Bottani and Francesco Longo and Letizia Nicoletti and Antonio Padovano and Giovanni Paolo Carlo Tancredi and Letizia Tebaldi and Marco Vetrano and Giuseppe Vignali},
keywords = {Augmented reality, Smart technologies, System usability scale, Industry 4.0, Wearable technologies, Industrial safety},
abstract = {Thanks to the spread of technologies stemming from the fourth industrial revolution, also the topic of fault diagnosis and assistance in industrial contexts has benefited. Indeed, several smart tools were developed for assisting with maintenance and troubleshooting, without interfering with operations and facilitating tasks. In line with that, the present manuscript aims at presenting a web smart solution with two possible applications installed on an Android smartphone and Microsoft HoloLens. The solution aims at alerting the operators when an alarm occurs on a machine through notifications, and then at providing the instructions needed for solving the alarm detected. The two devices were tested by the operators of an industrial aseptic bottling line consisting of five machines in real working conditions. The usability of both devices was positively rated by these users based on the System Usability Scale (SUS) and additional appropriate statements. Moreover, the in situ application brought out the main difficulties and interesting issues for the practical implementation of the solutions tested.}
}
@article{BORAWSKI20213777,
title = {The use of a computer game in a social campaign to improve road safety},
journal = {Procedia Computer Science},
volume = {192},
pages = {3777-3786},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.152},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921018913},
author = {Mariusz Borawski and Anna Borawska},
keywords = {social campaign, road safety, cognitive neuroscience techniques},
abstract = {Road traffic deaths and injuries remain a major global concern and current trends suggest that it would not change in the near future. To reverse this tendency, numerous actions are being taken to improve road safety. The implementation and enforcement of legislation on key risk factors are main elements of an integrated strategy to prevent road fatalities in most countries. Legislation should, however, be supported by effective enforcement of rights and social campaigns. Such campaigns appear in a variety of media (TV, radio, newspapers, magazines, etc.). In addition, relatively new communication channels are also used, such as various options of social media. Moreover, social campaigns can also use computer games. The latter medium seems to be particularly interesting and useful in the case of campaigns promoting safe driving, as the age group responsible for the most car accidents to a large extent also leads in the statistics of players. The main difficulty in applying such an approach is to prepare an appropriate game that would be able to interest and engage a potential player and at the same time influence her/his awareness of safe driving. The aim of this article is therefore to present a proposal of a computer game that could be used in a campaign to promote safe driving among members of a specific age group. Additionally, a project of a research experiment was prepared to check the effectiveness of the proposed game in a social campaign.}
}
@article{OYELERE2023100016,
title = {Formative evaluation of immersive virtual reality expedition mini-games to facilitate computational thinking},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100016},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100016},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000107},
author = {Amos Sunday Oyelere and Friday Joseph Agbo and Solomon Sunday Oyelere},
keywords = {Virtual reality, Metaverse, Game-based learning, Computational thinking, Head mounted display, Higher education},
abstract = {Recently, virtual reality (VR) technology has shown great potential in advancing education with many pedagogical benefits for building the 21st-century teaching and learning experience. This study conducted a formative evaluation of an immersive VR expedition application with the aim of understanding users' learning processes and how the application facilitates higher education students' computational thinking skills. Six participants were randomly selected to conduct this evaluation. A mixed research approach consisting of quantitative and qualitative methods was employed. The study quantitatively analyzed users' scores from gameplay to understand how the intervention supported computational thinking skills. Participants were also interviewed to collect data after playing the mini-games to investigate users' experiences. The study showcases players' computational thinking competency, assessed automatically during gameplay. Further, this study used inductive content analysis to demonstrate users' reactions to prototyped VR mini-games. The qualitative findings suggest that users found the VR mini-games interactive and immersive, which provided an opportunity to foster learners' computational thinking skills. The quantitative analysis revealed that student's computational thinking competency can be enhanced through consistent playing of the mini-games. Moreover, the expedition aspect of the VR game stimulated learners' curiosity, which sustained their learning progress. Furthermore, users gained new knowledge and found the mini-games educative. Nevertheless, several aspects of the VR mini-games need improvements, according to users' perceptions. This study contributes to the knowledge in terms of the affordances of VR in education research and provides relevant insights that can shape future studies, for example, the recent hype of metaverse in education.}
}
@article{SCOROLLI2023107910,
title = {Would you rather come to a tango concert in theater or in VR? Aesthetic emotions & social presence in musical experiences, either live, 2D or 3D},
journal = {Computers in Human Behavior},
volume = {149},
pages = {107910},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107910},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002613},
author = {Claudia Scorolli and Eduardo {Naddei Grasso} and Lorenzo Stacchio and Vincenzo Armandi and Giovanni Matteucci and Gustavo Marfia},
keywords = {Musical aesthetic experience, Immersive concert, Virtual reality, Aesthetic emotions, Social presence, Embodied experience, Psychology of art, Empirical aesthetics},
abstract = {This work shows the preliminary results of a pioneering project aimed at comparing the aesthetic experience of a musical concert experienced in different contexts for which the audience's perceived presence is modulated in a continuum ranging from a live concert to a music video, passing through immersive artificial environments. In contrast to previous qualitative investigations of various immersive contexts, our study is unique in both the use of validated scales and the structured comparison of four experimental conditions: 1.live concert (LC), 2.the same concert through a traditional non-immersive music video (MV, analogous to fruition on YouTube), and finally in a virtual reality environment (VR), provided by two different devices, 3. a google cardboard (CVR) and 4. an HTC vive (HVR), allowing respectively for a basic and easily accessible experience, or for a less affordable but more immersive one. Through these manipulations we presumably affected not just the subjective aesthetic experience, but also the perceived presence of the Other/s. Consistently we measured both through the administration of the Aesthetic Emotions Scale (Aesthemos) and the Networked Minds Measure of Social Presence (NMMSP). The NMMSP showed no notable differences between conditions, which instead emerged from the analyses on the Aesthemos. The most liked experience was the Live one. Results also showed that LC experience had a stronger emotional impact only when compared to MV and CVR, but not to HTC since this last manipulation was the one eliciting the greatest interest. Theoretical implications are critically discussed, suggesting novel applications of the proposed approach.}
}
@article{DAI2024103881,
title = {Wavelet-based network for high dynamic range imaging},
journal = {Computer Vision and Image Understanding},
volume = {238},
pages = {103881},
year = {2024},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2023.103881},
url = {https://www.sciencedirect.com/science/article/pii/S1077314223002618},
author = {Tianhong Dai and Wei Li and Xilei Cao and Jianzhuang Liu and Xu Jia and Ales Leonardis and Youliang Yan and Shanxin Yuan},
keywords = {High dynamic range image, Ghosting artifacts, Discrete Wavelet Transform, RAW HDR imaging dataset},
abstract = {High dynamic range (HDR) imaging from multiple low dynamic range (LDR) images has been suffering from ghosting artifacts caused by scene and objects motion. Existing methods, such as optical flow based and end-to-end deep learning based solutions, are error-prone either in detail restoration or ghosting artifacts removal. Comprehensive empirical evidence shows that ghosting artifacts caused by large foreground motion are mainly low-frequency signals and the details are mainly high-frequency signals. In this work, we propose a novel frequency-guided end-to-end deep neural network (FHDRNet) to conduct HDR fusion in the frequency domain, and Discrete Wavelet Transform (DWT) is used to decompose inputs into different frequency bands. The low-frequency signals are used to avoid specific ghosting artifacts, while the high-frequency signals are used for preserving details. Using a U-Net as the backbone, we propose two novel modules: merging module and frequency-guided upsampling module. The merging module applies the attention mechanism to the low-frequency components to deal with the ghost caused by large foreground motion. The frequency-guided upsampling module reconstructs details from multiple frequency-specific components with rich details. In addition, a new RAW dataset is created for training and evaluating multi-frame HDR imaging algorithms in the RAW domain. Extensive experiments are conducted on public datasets and our RAW dataset, showing that the proposed FHDRNet achieves state-of-the-art performance.}
}
@article{TAKAHASHI201531,
title = {A Generic Software Platform for Brain-inspired Cognitive Computing},
journal = {Procedia Computer Science},
volume = {71},
pages = {31-37},
year = {2015},
note = {6th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2015, 6-8 November Lyon, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.185},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036467},
author = {Koichi Takahashi and Kotone Itaya and Masayoshi Nakamura and Moriyoshi Koizumi and Naoya Arakawa and Masaru Tomita and Hiroshi Yamakawa},
keywords = {software platform, cognitive architecture, machine learning, modularity, the whole brain architecture},
abstract = {We have been developing BriCA (Brain-inspired Computing Architecture), the generic software platform that can combine an arbitrary number of machine learning modules to construct higher structures such as cognitive architectures inspired by the brain. We discuss requirements analysis and design principles of this cognitive computing platform, report its implementation, and describe plans for further development.}
}
@article{WANG2024102608,
title = {A deep learning-enhanced Digital Twin framework for improving safety and reliability in human–robot collaborative manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {85},
pages = {102608},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102608},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000832},
author = {Shenglin Wang and Jingqiong Zhang and Peng Wang and James Law and Radu Calinescu and Lyudmila Mihaylova},
keywords = {Safe human–robot collaboration (HRC), Intelligent sensing, Digital Twin, Semi-supervised deep learning framework},
abstract = {In Industry 5.0, Digital Twins bring in flexibility and efficiency for smart manufacturing. Recently, the success of artificial intelligence techniques such as deep learning has led to their adoption in manufacturing and especially in human–robot collaboration. Collaborative manufacturing tasks involving human operators and robots pose significant safety and reliability concerns. In response to these concerns, a deep learning-enhanced Digital Twin framework is introduced through which human operators and robots can be detected and their actions can be classified during the manufacturing process, enabling autonomous decision making by the robot control system. Developed using Unreal Engine 4, our Digital Twin framework complies with the Robotics Operating System specification, and supports synchronous control and communication between the Digital Twin and the physical system. In our framework, a fully-supervised detector based on a faster region-based convolutional neural network is firstly trained on synthetic data generated by the Digital Twin, and then tested on the physical system to demonstrate the effectiveness of the proposed Digital Twin-based framework. To ensure safety and reliability, a semi-supervised detector is further designed to bridge the gap between the twin system and the physical system, and improved performance is achieved by the semi-supervised detector compared to the fully-supervised detector that is simply trained on either synthetic data or real data. The evaluation of the framework in multiple scenarios in which human operators collaborate with a Universal Robot 10 shows that it can accurately detect the human and robot, and classify their actions under a variety of conditions. The data from this evaluation have been made publicly available, and can be widely used for research and operational purposes. Additionally, a semi-automated annotation tool from the Digital Twin framework is published to benefit the collaborative robotics community.}
}
@article{BIERCEWICZ20221509,
title = {The improvements propositions for players’ engagement and sustainable behaviors in managerial games},
journal = {Procedia Computer Science},
volume = {207},
pages = {1509-1518},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.208},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010912},
author = {Konrad Biercewicz and Adam Sulich and Letycja Sołoducho-Pelc},
keywords = {Business Training, E-learing, Gamification, Knowledge Representation, Management},
abstract = {The growing game's popularity is caused by their possibilities in decision-making training. Management games allow avoiding potential mistakes related to the business choices, but they require several updates and improvements in terms of context and new trends. The business environment is constantly changing and requires new creative approaches to be trained. One of them is the business greening and realizing the idea of sustainable behavior and engagement in managerial games. This paper aims to propose managerial game improvements to achieve better game engagement and become more interesting for students because of its real business world connection. The object of the research and analysis is the Marketplace®, which is the educational managerial game. In this paper, the game analysis three levels were presented to propose the recommendations for the Marketplace game development. The main finding is that an assessment survey should occur in the game climax, to engage and immerse students in the game. The survey questions should also be reexamined to develop a more dynamic game analysis. Then the game itself has to consist of sustainable development of practical solutions and enable students’ creativity to raise their engagement.}
}
@article{SCHOFIELD2022200061,
title = {An improved semi-synthetic approach for creating visual-inertial odometry datasets},
journal = {Graphics and Visual Computing},
pages = {200061},
year = {2022},
issn = {2666-6294},
doi = {https://doi.org/10.1016/j.gvc.2022.200061},
url = {https://www.sciencedirect.com/science/article/pii/S2666629422000146},
author = {Sam Schofield and Andrew Bainbridge-Smith and Richard Green},
keywords = {Visual inertial odometry, Semi-synthetic dataset, Robot virtual reality},
abstract = {Capturing outdoor visual-inertial datasets is a challenging yet vital aspect of developing robust visual-inertial odometry (VIO) algorithms. A significant hurdle is that high-accuracy-ground-truth systems (e.g., motion capture) are not practical for outdoor use. One solution is to use a “semi-synthetic” approach that combines rendered images with real IMU data. This approach can produce sequences containing challenging imagery and accurate ground truth but with less simulated data than a fully synthetic sequence. Existing methods (used by popular tools/datasets) record IMU measurements from a visual-inertial system while measuring its trajectory using motion capture, then rendering images along that trajectory. This work identifies a major flaw in that approach, specifically that using motion capture alone to estimate the pose of the robot/system results in the generation of inconsistent visual-inertial data that is not suitable for evaluating VIO algorithms. However, we show that it is possible to generate high-quality semi-synthetic data for VIO algorithm evaluation. We do so using an open-source full-batch optimization tool to incorporate both mocap and IMU measurements when estimating the IMU’s trajectory. We demonstrate that this improved trajectory results in better consistency between the IMU data and rendered images and that the resulting data improves VIO trajectory error by 79% compared to existing methods. Furthermore, we examine the effect of visual-inertial data inconsistency (as a result of trajectory noise) on VIO performance to provide a foundation for future work targeting real-time applications.}
}
@article{ZHOU2023188,
title = {Web-based mixed reality video fusion with remote rendering},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {2},
pages = {188-199},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000274},
author = {Qiang Zhou and Zhong Zhou},
keywords = {Mixed reality, Video fusion, WebRTC, Remote rendering},
abstract = {Background
Mixed reality (MR) video fusion systems merge video imagery with 3D scenes to make the scene more realistic and help users understand the video content and temporal–spatial correlation between them, reducing the user′s cognitive load. MR video fusion are used in various applications; however, video fusion systems require powerful client machines because video streaming delivery, stitching, and rendering are computationally intensive. Moreover, huge bandwidth usage is another critical factor that affects the scalability of video-fusion systems.
Methods
Our framework proposes a fusion method for dynamically projecting video images into 3D models as textures.
Results
Several experiments on different metrics demonstrate the effectiveness of the proposed framework.
Conclusions
The framework proposed in this study can overcome client limitations by utilizing remote rendering. Furthermore, the framework we built is based on browsers. Therefore, the user can test the MR video fusion system with a laptop or tablet without installing any additional plug-ins or application programs.}
}
@article{PRATAMA2023338,
title = {WizardOfMath: A top-down puzzle game with RPG elements to hone the player's arithmetic skills},
journal = {Procedia Computer Science},
volume = {216},
pages = {338-345},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.144},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022220},
author = {Mulia Pratama and Yanfi Yanfi and Pualam Dipa Nusantara},
keywords = {Game, math, Game Development Life Cycle, Game Experience Questionnaire},
abstract = {As one of the important education subjects’ mathematics difficulties can lead to tension and be described as the most hated or feared subject. This study aims to create a puzzle game application with RPG elements called WizardOfMath to increase a user's interest in mathematics subject. The research method includes a method called Game Development Life Cycle (GDLC), which has a pre-production stage that is suitable for game development rather than the Waterfall method. A game application is built based on the prior requirement gathering. The evaluation was arranged using the Game Experience Questionnaire (GEQ) survey which is performed by providing an online form to the public. Reliability test of GEQ modules meets Cronbach's Alpha value above 0.7 and the validity test of the r table is greater than 0.05. The results calculation of the Game Experience Questionnaire (GEQ) from a total of 55 participants and 3 modular structures, which are the Core module, In-game Module, and Post-game Module obtained an average score of 4.06, 3.88, and 3.57 for positive aspects and 2,72, 2.67, and 2,61 for the negative aspect. The contribution of this study shows this puzzle game application with RPG elements decreased user tension and the negative effect of being involved with mathematics subjects.}
}
@article{ALYOUSIFY2022263,
title = {AR-assisted children book for smart teaching and learning of Turkish alphabets},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {3},
pages = {263-277},
year = {2022},
note = {Advances in Wireless Sensor Networks under AI-SG forAugmented Reality Special Issue},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000432},
author = {Ahmed L. Alyousify and Ramadhan J. Mstafa},
keywords = {Human computer interaction, 5G, Augmented reality, IOT, Turkish alphabet, Smart teaching, AR assisted book},
abstract = {Background
Augmented reality (AR), virtual reality (VR), and remote-controlled devices are driving the need for a better 5G infrastructure to support faster data transmission. In this study, mobile AR is emphasized as a viable and widespread solution that can be easily scaled to millions of end-users and educators because it is lightweight and low-cost and can be implemented in a cross-platform manner. Low-efficiency smart devices and high latencies for real-time interactions via regular mobile networks are primary barriers to the use of AR in education. New 5G cellular networks can mitigate some of these issues via network slicing, device-to-device communication, and mobile edge computing.
Methods
In this study, we use a new technology to solve some of these problems. The proposed software monitors image targets on a printed book and renders 3D objects and alphabetic models. In addition, the application considers phonetics. The sound (phonetic) and 3D representation of a letter are played as soon as the image target is detected. 3D models of the Turkish alphabet are created by using Adobe Photoshop with Unity3D and Vuforia SDK.
Results
The proposed application teaches Turkish alphabets and phonetics by using 3D object models, 3D letters, and 3D phrases, including letters and sounds.}
}
@article{KUSIC2023101858,
title = {A digital twin in transportation: Real-time synergy of traffic data streams and simulation for virtualizing motorway dynamics},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101858},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101858},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622003160},
author = {Krešimir Kušić and René Schumann and Edouard Ivanjko},
keywords = {Digital twin, Microscopic traffic simulation, Real-time Big Data analytics, Calibration, Traffic sensors, Smart roads},
abstract = {The introduction of digital twins is expected to fundamentally change the technology in transportation systems, as they appear to be a compelling concept for monitoring the entire life cycle of the transport system. The advent of widespread information technology, particularly the availability of real-time traffic data, provides the foundation for supplementing predominated (offline) microscopic simulation approaches with actual data to create a detailed real-time digital representation of the physical traffic. However, the use of actual traffic data in real-time motorway analysis has not yet been explored. The reason is that there are no supporting models and the applicability of real-time data in the context of microscopic simulations has yet to be recognized. Thus, this article focuses on microscopic motorway simulation with real-time data integration during system run-time. As a result, we propose a novel paradigm in motorway traffic modeling and demonstrate it using the continuously synchronized digital twin model of the Geneva motorway (DT-GM). We analyze the application of the microscopic simulator SUMO in modeling and simulating on-the-fly synchronized digital replicas of real traffic by leveraging fine-grained actual traffic data streams from motorway traffic counters as input to DT-GM. Thus, the detailed methodological process of developing DT-GM is presented, highlighting the calibration features of SUMO that enable (dynamic) continuous calibration of running simulation scenarios. By doing so, the actual traffic data are directly fused into the running DT-GM every minute so that DT-GM is continuously calibrated as the physical equivalent changes. Accordingly, DT-GM raises a technology dimension in motorway traffic simulation to the next level by enabling simulation-based control optimization during system run-time that was previously unattainable. It, thus, forms the foundation for further evolution of real-time predictive analytics as support for safety–critical decisions in traffic management. Simulation results provide a solid basis for the future real-time analysis of an extended Swiss motorway network.}
}
@article{SAS2022111343,
title = {Antipatterns in software classification taxonomies},
journal = {Journal of Systems and Software},
volume = {190},
pages = {111343},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111343},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000826},
author = {Cezar Sas and Andrea Capiluppi},
keywords = {Classification, Software types, Antipattern, Taxonomy, Machine learning, Natural language processing},
abstract = {Empirical results in software engineering have long started to show that findings are unlikely to be applicable to all software systems, or any domain: results need to be evaluated in specified contexts, and limited to the type of systems that they were extracted from. This is a known issue, and requires the establishment of a classification of software types. This paper makes two contributions: the first is to evaluate the quality of the current software classifications landscape. The second is to perform a case study showing how to create a classification of software types using a curated set of software systems. Our contributions show that existing, and very likely even new, classification attempts are deemed to fail for one or more issues, that we named as the ‘antipatterns’ of software classification tasks. We collected 7 of these antipatterns that emerge from both our case study, and the existing classifications. These antipatterns represent recurring issues in a classification, so we discuss practical ways to help researchers avoid these pitfalls. It becomes clear that classification attempts must also face the daunting task of formulating a taxonomy of software types, with the objective of establishing a hierarchy of categories in a classification.}
}
@article{HARWOOD2018363,
title = {Interactive flow simulation using Tegra-powered mobile devices},
journal = {Advances in Engineering Software},
volume = {115},
pages = {363-373},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0965997817307135},
author = {Adrian R.G. Harwood and Alistair J. Revell},
keywords = {Android, Mobile computing, Interactive simulation, Lattice Boltzmann Method, CUDA, Embedded computing},
abstract = {The ability to perform interactive CFD simulations on mobile devices allows the development of portable, affordable simulation tools that can have a significant impact in engineering design as well as teaching and learning. This work extends existing work in the area by developing and implementing a GPU-accelerated, interactive simulation framework suitable for mobile devices. The accuracy, throughput, memory usage and battery consumption of the application is established for a range of problem sizes. The current GPU implementation is found to be over 300 ×  more efficient in terms of combined throughput and power consumption than a comparable CPU implementation. The usability of the simulation is examined through a new ‘interactivity’ metric which identifies the ratio of simulated convection to real world convection of the same problem. This real-time ratio illustrates that large resolutions may increase throughput through parallelisation on the GPU but this only partially offsets the decrease in simulated flow rate due to the necessary shrinking of the time step in the solver with increasing resolution. Therefore, targeting higher throughput configurations of GPU-solvers offer little additional benefit for interactive applications due to ultimately simulations evolving at a too slow a rate to facilitate interaction. The trade-off between accuracy, speed and power consumption are explored with the choice of problem resolution ultimately being characterised by a desired accuracy, flow speed and endurance of a given simulation. With current rates of growth in mobile compute power expected to continue, real-time simulation is expected to be possible at higher resolutions with a reduced energy footprint in the near future.}
}
@article{BEATTIE2015149,
title = {Taking the LEAP with the Oculus HMD and CAD - Plucking at thin Air?},
journal = {Procedia Technology},
volume = {20},
pages = {149-154},
year = {2015},
note = {Proceedings of The 1st International Design Technology Conference, DESTECH2015, Geelong},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2015.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212017315002029},
author = {Nathan Beattie and Ben Horan and Sophie McKenzie},
keywords = {Oculus Rift, LEAP Motion, CAD interaction, HCI},
abstract = {Achieving adequate visualisation of designs within CAD packages remains a challenge for designers with current methods of 3D CAD visualisation requiring either a high level of technical ability, or expensive hardware and software. The recent re-emergence of consumer VR has lowered the barrier for everyday developers wanting to visualise their designs in true 3D. This paper presents the CAD Interaction Lab (CIL) which employs the Oculus Rift Head Mounted Display (HMD) and Leap Motion Controller (LMC) to provide a low cost method enabling users to use their hands to dissect a mechanic model to manipulate and inspect individual components in realistic 3D. Qualitative observations of user interactions with the CIL show that users were able to intuitively manipulate the CAD model using natural hand movements with only minimal instruction.}
}
@article{GARGRISH20201039,
title = {Augmented Reality-Based Learning Environment to Enhance Teaching-Learning Experience in Geometry Education},
journal = {Procedia Computer Science},
volume = {172},
pages = {1039-1046},
year = {2020},
note = {9th World Engineering Education Forum (WEEF 2019) Proceedings : Disruptive Engineering Education for Sustainable Development},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.05.152},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920314824},
author = {Shubham Gargrish and Archana Mantri and Deepti Prit Kaur},
keywords = {Augmented Reality, Augmented Reality-Based Learning Environment, Mobile- Based Learning Environment, 3-D Geometry},
abstract = {Virtual information and physical objects are deployed in regular teaching all over, and until recently, blending these two environments has been a very difficult task at best. Understanding the concepts of geometry and their three-dimensional (3-D) space is still considered a difficult subject area for some students. Therefore, a requirement of a learning innovation arises for learning geometry to overcome the problems faced while understanding geometry by students’. The main objective of the paper is to develop Augmented Reality (AR)- based geometry learning for the android and iOS platforms than to deploy the applications among students for teaching 3-D geometry in high school students. This technology is considered to the realm of science and mathematics classroom and supports theoretical underpinnings in understanding the benefits as well as limitations of augmented reality-based learning environment (ARLE) experiences. One of the topics which are difficult for the students to understand is geometry in mathematics education. To address the problem, the article introduces a framework of mobile- based ARLE system.}
}
@article{ANDRONAS2023102544,
title = {Towards seamless collaboration of humans and high-payload robots: An automotive case study},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {83},
pages = {102544},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102544},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000200},
author = {Dionisis Andronas and Emmanouil Kampourakis and Giorgos Papadopoulos and Katerina Bakopoulou and Panagiotis Stylianos Kotsaris and George Michalos and Sotiris Makris},
keywords = {Human robot collaboration, Interaction, Augmented reality, Safety, Design},
abstract = {Despite the ergonomic challenges that large-sized product assemblies underlay for human operators, related processes are deselected from hybrid automation mostly due to the payload limitations of collaborative robots. Driven by industrial requirements for ergonomic and performance improvement, this paper presents the implementation of a human-high payload robot symbiotic workstation and discusses the key enabling technologies for seamless collaboration, namely: a) a multi-modal human-robot interaction pipeline, b) a gesture based contactless manual guidance module, c) fenceless safety monitoring system and logic, and d) an augmented reality-based training application for operator inclusion. An automotive case study is used for validating the complete hybrid system's usability and performance besides improved operator ergonomics and well-being. This work's findings prove that high-payload robots can support operators through intuitive means of interaction and businesses can rely on collaborative systems for improved performance metrics and job openness.}
}
@article{DEFELICE20231744,
title = {Physical and digital worlds: implications and opportunities of the metaverse},
journal = {Procedia Computer Science},
volume = {217},
pages = {1744-1754},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.374},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922024590},
author = {Fabio {De Felice} and Cristina {De Luca} and Simona Di Chiara and Antonella Petrillo},
keywords = {Metaverse, Immersive Internet, Multi-technology, Sociality, AR, VR},
abstract = {The Metaverse is revolutionizing the world of the internet. It is the new “virtual” universe capable of going beyond the pure three-dimensional and immersive dimension combining the physical and digital worlds. The metaverse, which until recently was an abstract concept, is now assuming great importance and attracting the attention of consumers, investors, brands, and large global players. Certainly, the first sector to adapt to this new immersive reality is e-commerce. Obviously, e-commerce is not the only sector affected by this digital revolution. Interoperability and interconnection will revolutionize current business models. Although the business opportunities seem endless, the scenario is still not entirely clear. Thus, the aim of the present research is to provide an overview on the state of the art, technologies, applications, and challenges of metaverse. Ethical and social implications are also analized. The result is a first detailed scenario analysis on the Metaverse.}
}
@article{PINGTING2025100758,
title = {What drives user churn in serious games? An empirical examination of the TAM, SOR theory, and game quality in Chinese cultural heritage games},
journal = {Entertainment Computing},
volume = {52},
pages = {100758},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100758},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001265},
author = {Mao {Ping Ting} and Cho {Dong Min}},
keywords = {Cultural Heritage, Serious Games, Game Quality, User Churn, TAM, SOR Theory},
abstract = {User churn in serious games related to Chinese cultural heritage is severe due to various game quality issues, leading to a lack of long-term user feedback necessary for optimizing design, which further hampers the development of such games. Through user interviews and literature review, this study identified two dimensions of quality of cultural heritage serious games: factors related to online gaming and those intrinsic to serious games. Within the Stimulus-Organism-Response (SOR) theoretical framework of, this study integrates these quality factors within the Technology Acceptance Model (TAM) to construct a user churn model. This model explores the relationship between quality factors and user churn. Utilizing 534 valid responses gathered from online platforms, this study employs statistical analyses and structural equation modeling to uncover that learning objectives, entertainment experience, system quality, and game design profoundly influence both perceived usefulness and ease of use, which in turn indirectly impacts user churn. Service quality’s influence on user churn is mediated by perceived usefulness, while brand image affects churn through perceived ease of use. Notably, learning objectives and entertainment experience emerge as critical determinants. The insights gained offer valuable guidance for game designers in addressing user needs, enhancing game quality, and reducing churn. Building on previous research, this study uses a mixed-methods approach and identifies factors affecting user churn in the context of cultural heritage serious games and validated the model’s rationality and effectiveness, enriching the foundational theoretical research on serious games.}
}
@article{MOURTZIS2021525,
title = {Equipment Design Optimization Based on Digital Twin Under the Framework of Zero-Defect Manufacturing},
journal = {Procedia Computer Science},
volume = {180},
pages = {525-533},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.271},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003203},
author = {Dimitris Mourtzis and John Angelopoulos and Nikos Panopoulos},
keywords = {Digital Twin, Machine Design, Zero-Defect Manufacturing},
abstract = {A digitalized Smart Factory can be considered as a data island. Moreover, engineers have focused on the development of new technologies and techniques not only for transforming information to data but also to achieve efficient data utilization to further optimize manufacturing processes. However, the Zero-Defect Manufacturing concept has emerged, where the main goal is production optimization. The cornerstone in achieving the factories of the future is to further optimize the design of new assets so as they comply with the unique requirements of the customers. Therefore, this paper proposes the conceptualization, design, and initial development of a platform for the utilization of data derived from industrial environments for the optimization of the equipment design. The main aspects of the proposed framework are the data acquisition, data processing and the simulation. The applicability of the proposed framework has been tested in a laboratory-based machine shop utilizing data from a real-life industrial scenario.}
}
@article{MUKHOPADHYAY202255,
title = {Virtual-reality-based digital twin of office spaces with social distance measurement feature},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {1},
pages = {55-75},
year = {2022},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000043},
author = {Abhishek Mukhopadhyay and GS Rajshekar Reddy and KamalPreet Singh Saluja and Subhankar Ghosh and Anasol Peña-Rios and Gokul Gopal and Pradipta Biswas},
keywords = {Virtual environment, Digital twin, 3D visualization, Convolutional neural network, Object detection, Social distancing},
abstract = {Background
Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
Methods
In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
Results
We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
Conclusions
A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.}
}
@article{RAZUVALOVA2015129,
title = {Virtual Reconstruction of Cultural and Historical Monuments of the Middle Volga},
journal = {Procedia Computer Science},
volume = {75},
pages = {129-136},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.229},
url = {https://www.sciencedirect.com/science/article/pii/S187705091503690X},
author = {Ekaterina Razuvalova and Arthur Nizamutdinov},
keywords = {virtual reconstruction, computer technologies, historical reconstruction, virtual environment},
abstract = {Today people use information technologies in every area of their lives; it makes lives more systematic, clear and definite. This phenomenon appears in humanities too, in such sciences like history and social knowledge. Virtual space is developing, which is, certainly, very helpful and it brings every area of life to a new level. Different virtual environments, simulators and educational resources are created. They let people participate, study and train in inaccessible or impossible areas. In this paper we present practical approach of Virtual reconstruction of cultural and historical monuments of the Middle Volga in which we first create landscape, develop key-systems (such as roads and sound filling editors), then create visual content and finish with optimization and implementing produced assets. This project was created to achieve a lot of aims. First of all, we want to contribute in developing of digital history, save and systematize existing culture-historical monuments and information about them, to help form an image of Tatar Republic. Secondly, we want to expand virtual reality use. We want it to be not only an entertainment, but part of education. Our project expects wide audience of people interested in it. It will be helpful for studying history, because men learn history latently, while they are active in the game. So, this is the benefit of virtual reconstructions: users become part of virtual reality and certain world created in it. They try different roles, do different actions they cannot do in reality. In our educational game “Bolgar XIV” user can try a role of a citizen of ancient city; see defunct architecture and typical activity of that time. This game will attract people, who just want to play a game, but at the same time they will enrich their knowledge and images of this particular historic place. This makes virtual environment more of current interest and multifunctional. Creating virtual environment of ancient city of Bolgar help to understand and create our own technique of creating such culture-historical reconstructions.}
}
@article{BUISSON2013815,
title = {Real-time Collision Avoidance for Pedestrian and Bicyclist Simulation: A Smooth and Predictive Approach},
journal = {Procedia Computer Science},
volume = {19},
pages = {815-820},
year = {2013},
note = {The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.06.108},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913007163},
author = {Jocelyn Buisson and Stéphane Galland and Nicolas Gaud and Mikael Gonçalves and Abderrafiaa Koukam},
keywords = {collision avoidance, pedestrian behavior, crowd simulation, cyclist behavior},
abstract = {This article introduces a new collision avoidance model enabling the design of efficient realistic virtual pedestrian and cyclist behaviors. It is a force-based model using collision prediction with dynamic time-windows to predict future potential collisions with obstacles and other individuals. It introduces a new type of force called sliding force to allow a smooth avoidance of potential collisions while enabling the pedestrian to continue to progress towards its goal. Unlike most existing models, our forces are not scaled according to the distance to the obstacle but depending on the estimate of the collision time with this obstacle. This inherently integrates obstacles’ velocity. This greatly reduces the compu- tational complexity of the model while ensuring a smooth avoidance. This model is oscillation-free except for concave obstacles. It enables the reproduction of inherent emergent properties of real crowds such as spontaneous organizations of pedestrians into lane lines, etc. This model is computationally efficient and designed for real time simulation of large crowds.}
}
@article{NOWLAN2023100012,
title = {Higher-order thinking skills assessment in 3D virtual learning environments using motifs and expert data},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100012},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100012},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000065},
author = {Nuket Nowlan and Ali Arya and Hossain Samar Qorbani and Maryam Abdinejad},
keywords = {Higher-order thinking skills, Virtual learning environments, Motif, Assessment, Process metric},
abstract = {The research reported in this paper addresses the problem of assessing higher-order thinking skills, such as reflective and creative thinking, within the context of virtual learning environments. Assessment of these skills requires process-based observations and evaluation, as the output-based methods have been found to be insufficient. Virtual learning environments offer a wealth of data on the process, which makes them good candidates for process-based evaluation, but the existing assessment methods in these environments have shortcomings, such as reliance on large data sets, inability to offer specific feedback on actions, and the lack of consideration for how actions are integrated into bigger tasks. Demonstrating and confirming the ability of three-dimensional virtual learning environments to work with process metrics for assessment, we propose and evaluate the use of motifs as an assessment tool. Motifs are short and meaningful combination of metrics. Combining time-ordered motifs with a similarity analysis between expert and learner data, our proposed approach can potentially offer feedback on specific actions that the learner takes, as opposed to single output-based feedback. It can do so without the use of large training datasets due to reliance on expert data and similarity analysis. Through a user study, we found out that such a motif-based approach can be effective in the assessment of higher-order thinking skills while addressing the identified shortcomings of previous work. We also address the limited research on similarity-based analysis methods, compare their effectiveness, and show that utilizing different similarity measures for different tasks may be a more effective approach. Our proposed method facilitates and encourages the involvement of instructors and course designers through the definition of motifs and expert problem-solving paths.}
}
@article{FATHI20242670,
title = {Unveiling the Potential of Mixed Reality: Enhancing Time Measurement and Operator Support in Manual Assembly Processes},
journal = {Procedia Computer Science},
volume = {232},
pages = {2670-2679},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.084},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002618},
author = {Masood Fathi and Ingemar Karlsson and Göran Grahn and Andreas Björnsson},
keywords = {Mixed Reality, Operator Support, Time study, Manual Assembly},
abstract = {This study investigates the potential of Mixed Reality (MR) in the manual assembly processes and conducts a case study at a pump manufacturing plant in Sweden. An MR solution is developed to assist operators through visual instructions and guiding aides. The solution also captures the operator's motions using advanced hand and eye tracking features for real-time guidance and accurate time measurement. The proposed MR solution uses the build feature of HoLolens and a workstation editor, which facilitates the use of the solution in diverse assembly environments. The results of the experiments show that the developed MR solution can improve operator support, reduce errors, and enhance the overall efficiency of manual assembly processes. Moreover, it is shown to be an efficient tool for time measurement of the manual assembly process that has promising potential to replace sophisticated and time-consuming traditional time study methods.}
}
@article{TAMBA2023670,
title = {The Effect of Educational Platformer Game "Loving Ma"},
journal = {Procedia Computer Science},
volume = {227},
pages = {670-679},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.571},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017386},
author = {Nikolaus Hasudungan Tamba and Billy Andrian and  Vincenzo and Yanfi Yanfi and Pualam Dipa Nusantara},
keywords = {game application, platformer, Extreme Programming, Eight Golden Rules, Game Experience Questionnaire},
abstract = {This study aimed to create an educational game application based on the theme of motherly roles in the platformer genre. The game application is designed to feature three levels consisting of multiple-choice questions about the roles of a mother. The Extreme Programming (XP) method is utilized to create the game application. Extreme Programming (XP) is a branch of the Agile Development method used to develop systems with uncertain or rapid requirements changes. The steps of the XP method consist of planning, designing, coding, and testing. The game application is evaluated both internally and outside. Black-box testing and eight golden rules are used for internal review. The findings of black-box testing can be used to determine the viability of a game application. The evaluation findings of the eight golden rules are required to evaluate the game application's user interface. The Game Experience Questionnaire is used for external review (GEQ). The GEQ was distributed through Google Forms, and 67 respondents completed it. The result of the GEQ was then tested for validity and reliability, calculated to find the mean number and standard deviation, and analyzed. Therefore, it was concluded that the game application provides a good user experience.}
}
@article{WATTANASOONTORN2012293,
title = {The Framework of a Life Support Simulation Application},
journal = {Procedia Computer Science},
volume = {15},
pages = {293-294},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008459},
author = {Voravika Wattanasoontorn and Imma Boada and Carles Blavi and Mateu Sbert},
keywords = {Life support, Serious Games, Medical Simulation, Cardiopulmonary Respiration, Resuscitation},
abstract = {In this paper we present the framework of a LIfe Support Simulation Application (LISSA) designed to teach and learn cardiopulmonary resuscitation (CPR) skills. DLISSA exploits video game technology to link in a single framework computer-based simulations of CPR emergencies with the functionalities of e-learning platforms. DEmergency situations are presented as problems that the learner has to solve in a game mode. Learner actions are registered in a database. DThis information is used to present new problems to the learner in an adaptive learning mode.DLISSA can be used as a substitute or a complement for traditional CPR classroom-based instruction Dor to refresh and improve CPR skill retention over time.}
}
@article{LIU2021200020,
title = {Virtual reality game level layout design for real environment constraints},
journal = {Graphics and Visual Computing},
volume = {4},
pages = {200020},
year = {2021},
issn = {2666-6294},
doi = {https://doi.org/10.1016/j.gvc.2021.200020},
url = {https://www.sciencedirect.com/science/article/pii/S2666629421000036},
author = {Huimin Liu and Zhiquan Wang and Angshuman Mazumdar and Christos Mousas},
keywords = {Game level, Level layout design, Virtual reality, Real environment constraints},
abstract = {This paper presents an optimization-based approach for designing virtual reality game level layouts, based on the layout of a real environment. Our method starts by asking the user to define the shape of the real environment and the obstacles (e.g., furniture) located in it. Then, by representing a game level as an assembly of chunks and defining the game level layout design decisions in cost terms (mapping, fitting, variations, and accessibility) in a total cost function, our system automatically synthesizes a game level layout that fulfills the real environment layout and its constraints as well as the user-defined design decisions. To evaluate the proposed method, a user study was conducted. The results indicated that the proposed method: (1) enhanced the levels of presence; (2) enhanced the levels of involvement of participants in the virtual environment; and (3) reduced the fear of collision with the real environment and its constraints. Limitations and future research directions are also discussed.}
}
@article{BAADEN2022324,
title = {Deep inside molecules — digital twins at the nanoscale},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {4},
pages = {324-341},
year = {2022},
note = {Virtual-reality and intelligent hardware in digital twins A)},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096579622000171},
author = {Marc Baaden},
keywords = {Digital twins, Molecular simulation, Virtual reality},
abstract = {Background
Digital twins offer rich potential for exploration in virtual reality (VR). Using interactive molecular simulation approaches, they enable a human operator to access the physical properties of molecular objects and to build, manipulate, and study their assemblies. Integrative modeling and drug design are important applications of this technology.
Methods
In this study, head-mounted virtual reality displays connected to molecular simulation engines were used to create interactive and immersive digital twins. They were used to perform tasks relevant to specific use cases.
Results
Three areas were investigated, including model building, rational design, and tangible models. Here, we report several membrane-embedded systems of ion channels, viral components, and artificial water channels. We were able to improve and create molecular designs based on digital twins.
Conclusions
The molecular application domain offers great opportunities, and most of the technical and technological aspects have been solved. Wider adoption is expected once the onboarding of VR is simplified and the technology gains wider acceptance.}
}
@article{CHARCO2021104182,
title = {Camera pose estimation in multi-view environments: From virtual scenarios to the real world},
journal = {Image and Vision Computing},
volume = {110},
pages = {104182},
year = {2021},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2021.104182},
url = {https://www.sciencedirect.com/science/article/pii/S0262885621000871},
author = {Jorge L. Charco and Angel D. Sappa and Boris X. Vintimilla and Henry O. Velesaca},
keywords = {Relative camera pose estimation, Domain adaptation, Siamese architecture, Synthetic data, Multi-view environments},
abstract = {This paper presents a domain adaptation strategy to efficiently train network architectures for estimating the relative camera pose in multi-view scenarios. The network architectures are fed by a pair of simultaneously acquired images, hence in order to improve the accuracy of the solutions, and due to the lack of large datasets with pairs of overlapped images, a domain adaptation strategy is proposed. The domain adaptation strategy consists on transferring the knowledge learned from synthetic images to real-world scenarios. For this, the networks are firstly trained using pairs of synthetic images, which are captured at the same time by a pair of cameras in a virtual environment; and then, the learned weights of the networks are transferred to the real-world case, where the networks are retrained with a few real images. Different virtual 3D scenarios are generated to evaluate the relationship between the accuracy on the result and the similarity between virtual and real scenarios—similarity on both geometry of the objects contained in the scene as well as relative pose between camera and objects in the scene. Experimental results and comparisons are provided showing that the accuracy of all the evaluated networks for estimating the camera pose improves when the proposed domain adaptation strategy is used, highlighting the importance on the similarity between virtual-real scenarios.}
}
@article{MONIAGA2018361,
title = {Facial Expression Recognition as Dynamic Game Balancing System},
journal = {Procedia Computer Science},
volume = {135},
pages = {361-368},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.185},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314741},
author = {Jurike V. Moniaga and Andry Chowanda and Agus Prima and  Oscar and M. Dimas {Tri Rizqi}},
keywords = {Facial Expression Recognition, Dynamic Balancing, Player Experience, Immersive Experience Questionnaire},
abstract = {This research proposes a dynamic game balancing by using Facial Expression Recognition to enhance the player’s experience when playing the game. Research has shown that players generally express their emotions when playing the game. This allow us to capture the player’s expression from their face and use it to dynamically adjust the game difficulties. A preliminary study was conducted to capture what kind of game that would be suitable to test dynamic balancing. The game with a dynamic balancing system then was developed using Scrum methodology as the software development methodology. Furthermore, the dynamic balancing in the game was simulated in the computer and evaluated, with some of items from Immersive Experience Questionnaire, with players played two versions of game, one with dynamic balancing activated and the other with-out dynamic balancing activated. The results evidently show that there was some statistically significant enhancement in the game with Facial Expression Recognition activated as a game dynamic game balancing compare to the one with-out Facial Expression Recognition.}
}
@article{ZARZUELA2013382,
title = {Educational Tourism through a Virtual Reality Platform},
journal = {Procedia Computer Science},
volume = {25},
pages = {382-388},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.047},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012520},
author = {Mario Martínez Zarzuela and Francisco J. Díaz Pernas and Sergio Martín Calzón and David González Ortega and Miriam Antón Rodríguez},
keywords = {Serious game, Virtual Reality, Educational Tourism},
abstract = {This article presents a Virtual Reality Serious Game that allows the user to increase the knowledge about the city of Valladolid in Spain. With this goal, the Main Square and some of the historic buildings in the downtown have been virtually recreated. We have taken advantage of the characteristic tiled floor of the town hall square to represent a game board. Different tiled floors are squares which hide questions behind. The user plays using a Natural User Interface based on Microsoft® Kinect.}
}
@article{MONTEIRO20231,
title = {Exploring the user experience of hands-free VR interaction methods during a Fitts’ task},
journal = {Computers & Graphics},
volume = {117},
pages = {1-12},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S009784932300242X},
author = {Pedro Monteiro and Hugo Coelho and Guilherme Gonçalves and Miguel Melo and Maximino Bessa},
keywords = {Hands-free, HCI, Immersive Virtual Reality, Interaction, Usability},
abstract = {Despite advancements in interaction with immersive Virtual Reality (VR) systems, using hand gestures for all interactions still imposes some challenges, especially in interactions with graphical user interfaces that are usually performed with point-and-click interfaces. Therefore, exploring the use of alternative hands-free methods for selection is essential to overcome usability problems and provide natural interaction for users. The results and insights gained from this exploration can lead to enhanced user experiences in VR applications. This study aims to contribute to the literature with the evaluation of the usability of the most commonly used hands-free methods for selection and system control tasks in immersive VR and their impact on standard and validated experience and usability metrics, namely the sense of presence, cybersickness, system usability, workload, and user satisfaction. A Fitts’ selection task was performed using a within-subjects design by nine participants experienced in VR. The methods evaluated were the handheld controllers, the head gaze, eye gaze, and voice commands for pointing at the targets, and dwell time and voice commands to confirm the selections. Results show that the methods provide similar levels of sense of presence and low cybersickness while showing low workload values and high user satisfaction, matching the experience of traditional handheld controllers for non-multimodal approaches. The assisted eye gaze with dwell was the preferred hands-free method and the one with the highest values of usability. Still, developers should minimize the number of gaze movements to reduce fatigue. The evaluation also showed that using a multimodal approach for selections, especially using the voice, decreases user satisfaction and increases users’ frustration.}
}
@article{STATHAM2022100476,
title = {Game environment art with modular architecture},
journal = {Entertainment Computing},
volume = {41},
pages = {100476},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2021.100476},
url = {https://www.sciencedirect.com/science/article/pii/S1875952121000732},
author = {Wilhelmina Zoe Statham and João Jacob and Mikael Fridenfalk},
keywords = {Art Fatigue, Game environment art, Game development lifecycle, Level design, Modular architecture, Modular assets, Modular kits},
abstract = {3D games routinely turn to modular architecture as an optimization technique for environment art and level design, although there is little documentation of how to implement it. As a result, many games struggle to apply it effectively, contributing to problems such as art fatigue and production delays. This investigation proposes a set of unified principles of modular architecture for 3D game environment art based on traditional architecture and game examples. It finds that the use of a uniform grid and standard measurements are key as well as extensive planning, and that assets organized as modular kits can be more cost-effective and facilitate flexible level design. It concludes with a series of steps per development phase of a game’s lifecycle aimed at game environment art, with emphasis on early planning and testing, and suggests that close collaboration between environment artists and level designers is key to develop effective modular assets.}
}
@article{GOLUBEV2016152,
title = {Dijkstra-based Terrain Generation Using Advanced Weight Functions},
journal = {Procedia Computer Science},
volume = {101},
pages = {152-160},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916326862},
author = {Kirill Golubev and Aleksander Zagarskikh and Andrey Karsakov},
keywords = {terrain generation, weight functions, virtual worlds, visualization},
abstract = {Due to the growing popularity of consumer virtual reality devices, the new surge in the use of computer-generated terrain is already happening. That leads to the need to develop new fast and efficient methods of landscapes generation. However, lack of flexibility of user control over terrain generation in most popular modern terrain generation algorithms is a big part of terrain generation problem. In this paper, we present a new method for generating three-dimensional landscapes based on modified Dijkstra algorithm. The proposed method allows a user to set the initial location of landscape features and select or create weight functions that determine the appearance of the generated terrain. It has a lower computational cost compared to the closest analogs giving the equal quality of results and allows users to create various types of terrain, as well as to combine them together in one landscape.}
}
@article{LI2019468,
title = {Immersive VR Theater with Multi-device Interaction and Efficient Multi-user Collaboration},
journal = {Procedia Computer Science},
volume = {147},
pages = {468-472},
year = {2019},
note = {2018 International Conference on Identification, Information and Knowledge in the Internet of Things},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.274},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919302972},
author = {Huiyu Li and Shisheng Zhou and Fan Zhang and Chenglei Yang},
keywords = {Multi-device Interaction, Multi-user Collaboration, Immersive Virtual Environment, VR Theater},
abstract = {The virtual reality (VR) theater typically supports users to interact in an immersive virtual environment. In this paper, we propose the architecture and implementation of a VR theater system supporting multi-device interaction and multi-user collaboration. In our study, the system is equipped with a large cylindrical screen to display immersive stereo virtual scene and several types of interactive devices, including multiple degrees of freedom platform, simulation gun and smart phone, to support user interaction. To improve efficiency of user collaboration, the users are divided into two groups assigned with different interactive tasks, allowing them to collaborate with each other.}
}
@article{YLIPULLI2023594,
title = {Public libraries as a partner in digital innovation project: Designing a virtual reality experience to support digital literacy},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {594-605},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003011},
author = {Johanna Ylipulli and Matti Pouke and Nils Ehrenberg and Turkka Keinonen},
keywords = {Public libraries, Participatory design, Virtual reality, Digital literacy, Digital social innovation, Research-through-design},
abstract = {We introduce a project titled as Our Shared Virtual World which aims at increasing public libraries’ capability to provide knowledge on digital technology to general public. The practical goal of the project has been to produce a functional prototype of a virtual reality (VR) application that could be utilized freely in all the public libraries in Finland. In many countries worldwide, libraries’ role is expanding from providers of traditional books to providers of information technologies and related new forms of literacy, and this development provides the broader backdrop for the project. The contribution of the article is two-fold: First, we describe how an immersive VR application can be collaboratively developed within this specific research context, namely within a network of public libraries, and introduce the tangible outcome of the project, the VR application called Forest Elf. Secondly, we scrutinize how results of such a design work can be sustained over time: through participatory design (PD), we aimed at creating conditions which would enable public libraries to continue developing and using the artefact also after the project. We provide insights on how to tackle the challenge of research prototypes ending up being abandoned, and what factors in the context of library partnership support or hamper sustainable digital innovation — digital innovation that is inclusive and equitable but also has a long-lasting impact.}
}
@article{VARGASMOLANO201973,
title = {Parametric Facial Animation for Affective Interaction Workflow for Avatar Retargeting},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {343},
pages = {73-88},
year = {2019},
note = {The proceedings of AmI, the 2018 European Conference on Ambient Intelligence.},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2019.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S1571066119300131},
author = {Juan Sebastián {Vargas Molano} and Gloria Mercedes Díaz and Wilson J. Sarmiento},
keywords = {Animation, Avatar, Candide, Emotion},
abstract = {Virtual avatars/characters are essential in the interaction with virtual environments and 3D video games. While these avatars can be controlled by a bot or a human, they need a suitable model that allows emotion representation regardless of what or who is controlling them, since it increases the interaction and immersion. Provide facial expressions to any avatar require an animation process to emulate a particular gesture or emotion before to include it in a virtual environment. This process is specific to every single facial expression required in the avatar. This paper presents an initial approximation to contribute to the solve this problem with a workflow to apply a well-known parameterized face model called Candide. The proposal aims to adapt Candide into any avatar previously modeled through automatic morphological adjustment and texture mapping. This work includes the application of this workflow into six characters licensed under the Creative Commons copyright. We also present examples of facial emotion animation, as well as subjective assessment by five experts in audiovisual and animation production.}
}
@article{LIU2024100205,
title = {Cooperative multi-agent game based on reinforcement learning},
journal = {High-Confidence Computing},
volume = {4},
number = {1},
pages = {100205},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2667295224000084},
author = {Hongbo Liu},
keywords = {Collaborative multi-agent, Reinforcement learning, Credit distribution, Multi-agent communication, Reward shaping},
abstract = {Multi-agent reinforcement learning holds tremendous potential for revolutionizing intelligent systems across diverse domains. However, it is also concomitant with a set of formidable challenges, which include the effective allocation of credit values to each agent, real-time collaboration among heterogeneous agents, and an appropriate reward function to guide agent behavior. To handle these issues, we propose an innovative solution named the Graph Attention Counterfactual Multiagent Actor–Critic algorithm (GACMAC). This algorithm encompasses several key components: First, it employs a multi-agent actor–critic framework along with counterfactual baselines to assess the individual actions of each agent. Second, it integrates a graph attention network to enhance real-time collaboration among agents, enabling heterogeneous agents to effectively share information during handling tasks. Third, it incorporates prior human knowledge through a potential-based reward shaping method, thereby elevating the convergence speed and stability of the algorithm. We tested our algorithm on the StarCraft Multi-Agent Challenge (SMAC) platform, which is a recognized platform for testing multi-agent algorithms, and our algorithm achieved a win rate of over 95% on the platform, comparable to the current state-of-the-art multi-agent controllers.}
}
@article{PASANISI2023816,
title = {On Domain Randomization for Object Detection in real industrial scenarios using Synthetic Images},
journal = {Procedia Computer Science},
volume = {217},
pages = {816-825},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.278},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023560},
author = {Davide Pasanisi and Emanuele Rota and Michele Ermidoro and Luca Fasanotti},
keywords = {Deep Learning, Domain Randomization, Object Detection},
abstract = {Fine-tuning a pre-trained deep learning model is commonly preferred since it significantly lowers the training effort while still enabling state-of-the-art performances for downstream tasks. Nevertheless, for many industrial applications it is not always possible to collect the required real-world images. In this scenario, Domain Randomization is a promising technique for training a deep learning model on synthetic images. In this work, two real industrial applications are illustrated. In the first application, CAD models are exploited to generate a realistic 3D render of the objects of interest to investigate the feasibility of fine-tuning a model using a fully synthetic dataset but only qualitative results are shown. For the second application, semantic fidelity is favored over visual fidelity: objects of interest are drawn as a composition of primitive shapes and textures, allowing the generation of a synthetic dataset without using a dedicated 3D rendering software while preserving the flexibility to achieve enough variability in the dataset. A model fine-tuned on a synthetic dataset generated with this approach achieved an estimation error below 1% for the measurement of the diameter of barrels used in the Food and Beverage industry.}
}
@article{MONTE20131402,
title = {Estimation of Volume Rendering Efficiency with GPU in a Parallel Distributed Environment},
journal = {Procedia Computer Science},
volume = {18},
pages = {1402-1411},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.307},
url = {https://www.sciencedirect.com/science/article/pii/S187705091300450X},
author = {Cristian Federico Perez Monte and Fabiana Piccoli and Cristian Luciano and Silvio Rizzi and Germán Bianchini and Paola Caymes Scutari},
keywords = {Volume Rendering, Monte Carlo Method, Ray Tracing, GPU, Scientific Visualization, Cluster Computing, Parallel Processing, efficiency},
abstract = {Visualization methods of medical imagery based on volumetric data constitute a fundamental tool for medical diagnosis, training and pre-surgical planning. Often, large volume sizes and/or the complexity of the required computations present se- rious obstacles for reaching higher levels of realism and real-time performance. Performance and efficiency are two critical aspects in traditional algorithms based on complex lighting models. To overcome these problems, a volume rendering algo- rithm, PD-Render intra for individual networked nodes in a parallel distributed architecture with a single GPU per node is presented in this paper. The implemented algorithm is able to achieve photorealistic rendering as well as a high signal-to- noise ratio at interactive frame rates. Experiments show excellent results in terms of efficiency and performance for rendering medical volumes in real time.}
}
@article{TIWARI2022887,
title = {Dimensionality and Angular Disparity Influence Mental Rotation in Computer Gaming},
journal = {Computers, Materials and Continua},
volume = {72},
number = {1},
pages = {887-905},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.023886},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822010852},
author = {Akanksha Tiwari and Ram Bilas Pachori and Premjit Khanganba Sanjram},
keywords = {Computer-based games, angular disparity, dimensionality, mental rotation, EEG, hemispheric laterality},
abstract = {Computer gaming is one of the most common activities that individuals are indulged in their usual activities concerning interactive system-based entertainment. Visuospatial processing is an essential aspect of mental rotation (MR) in playing computer-games. Previous studies have explored how objects’ features affect the MR process; however, non-isomorphic 2D and 3D objects lack a fair comparison. In addition, the effects of these features on brain activation during the MR in computer-games have been less investigated. This study investigates how dimensionality and angular disparity affect brain activation during MR in computer-games. EEG (electroencephalogram) data were recorded from sixty healthy adults while playing an MR-based computer game. Isomorphic 2D and 3D visual objects with convex and reflex angular disparity were presented in the game. Cluster-based permutation tests were applied on EEG spectral power for frequency range 3.5–30 Hz to identify significant spatio-spectral changes. Also, the band-specific hemispheric lateralization was evaluated to investigate task-specific asymmetry. The results indicated higher alpha desynchronization in the left hemisphere during MR compared to baseline. The fronto-parietal areas showed neural activations during the game with convex angular disparities and 3D objects, for a frequency range of 7.8–14.2 Hz and 7.8–10.5 Hz, respectively. These areas also showed activations during the game with reflex angular disparities and 2D objects, but for narrower frequency bands, i.e., 8.0–10.0 Hz and 11.0–11.7 Hz, respectively. Left hemispheric dominance was observed for alpha and beta frequencies. However, the right parietal region was notably more dominant for convex angular disparity and 3D objects. Overall, the results showed higher neural activities elicited by convex angular disparities and 3D objects in the game compared to the reflex angles and 2D objects. The findings suggest future applications, such as cognitive modeling and controlled MR training using computer games.}
}
@article{DREZEWSKI20211914,
title = {The application of selected modern artificial intelligence techniques in an exemplary strategy game},
journal = {Procedia Computer Science},
volume = {192},
pages = {1914-1923},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.197},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016938},
author = {Rafał Dreżewski and Jakub Solawa},
keywords = {artificial intelligence, computer games, strategy games, team coordination, decision-making},
abstract = {In the paper requirements for artificial intelligence algorithms designed for modern strategy computer games are analyzed. The selected techniques used to fulfill those requirements are described in detail. Then the exemplary game, which was designed and implemented using the selected algorithms for path-finding, decision-making, tactical and strategic reasoning, and team coordination is presented. Finally, the results of experiments conducted with the use of the created game are analyzed. The results prove the efficiency of selected techniques in creating a strategically challenging game.}
}
@article{MOLONEY2015212,
title = {Videogame Technology Re-Purposed: Towards Interdisciplinary Design Environments for Engineering and Architecture},
journal = {Procedia Technology},
volume = {20},
pages = {212-218},
year = {2015},
note = {Proceedings of The 1st International Design Technology Conference, DESTECH2015, Geelong},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2015.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S221201731500211X},
author = {Jules Moloney},
keywords = {architecture and design engineering education, simulation environments, videogame technology, serious games.},
abstract = {The author and associated researchers have in previous projects adapted videogame technology for design in the context of architectural design education. This paper reflects on this body of research: the original motivations and aspirations; what threads may be productively revisited; how contemporary shifts to parametric design and building information modelling may be incorporated; and considers how some aspects of game play, in particular competition, may seedInterdisciplinary Design environments for Engineering and Architecture (IDeEA).}
}
@article{JULIANTINO2023310,
title = {The development of virtual healing environment in VR platform},
journal = {Procedia Computer Science},
volume = {216},
pages = {310-318},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.141},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022190},
author = {Chris Juliantino and Mega Putri Nathania and Religiana Hendarti and Herru Darmadi and Bonny A Suryawinata},
keywords = {Android, Environment, Smartphone, Virtual Reality, Virtual space},
abstract = {The purpose of this research is to build a healing environment that can be experienced through virtual reality (VR) to reduce stress levels of the user because of the lengthy period of COVID-19 pandemic that forced people to stay at home. The healing environment is created based on several theories on the principles of Healing Environments related to Architectural design. And as for the development of the simulation software, it is using the method of Extreme Programming that is based on Agile principle that combined with Architectural design flow. The VR is targeted to run optimally in low-end devices with cardboard and common Bluetooth controller so that it can be accessed inclusively. The evaluation is conducted using a user acceptance test with expert judgment and survey to 32 respondents. The findings are that they enjoyed the simulation because of the guidance is clear and the virtual environment looks more real compared to others that looks cartoonish.}
}
@article{SALGADO2019665,
title = {The Effect of Cybersickness of an Immersive Wheelchair Simulator},
journal = {Procedia Computer Science},
volume = {160},
pages = {665-670},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317302},
author = {Débora P. Salgado and Thiago B. Rodrigues and Felipe R. Martins and Eduardo L.M. Naves and Ronan Flynn and Niall Murray},
keywords = {Cybersickness, Immersive Technologies, Wheelchair Simulator, Assistive Technologies},
abstract = {A key challenge that Immersive applications have to overcome is cybersickness. Cybersickness is particularly prevalent in dynamic applications such as vehicles simulators. The work presented here aims to understand the cause of cybersickness symptoms in an assistive technology (AT) application, the virtual wheelchair training simulator. This evaluation is performed in terms of errors made during experience and post-experience Simulator Sickness Questionnaire (SSQ). The performance metrics analyzed are time to complete the proposal task and number of collisions (errors/mistakes). The post-experience questionnaires (subjective measurements) collected the user’s experience in terms of simulator sickness by applying the Simulator Sickness Questionnaire (SSQ) and immersion questions. The experiments were conducted with 10 participants. In terms of results, analysis of human factors reveals that the average cybersickness score is slightly higher for women compared to men. However, these differences were not statistically significant. There was an inverse correlation between cybersickness symptoms and task performance as well as between cybersickness symptoms and immersion.}
}
@article{PEREZ2024111440,
title = {Generation of probabilistic synthetic data for serious games: A case study on cyberbullying},
journal = {Knowledge-Based Systems},
volume = {286},
pages = {111440},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111440},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124000753},
author = {Jaime Pérez and Mario Castro and Edmond Awad and Gregorio López},
keywords = {Synthetic data, Serious games, Cyberbullying, Item response theory, Bayesian network, Hierarchical Bayesian model, Computational social science},
abstract = {Synthetic data generation has been a growing area of research in recent years. However, its potential applications in serious games have yet to be thoroughly explored. Advances in this field could anticipate data modeling and analysis, as well as speed up the development process. To fill this gap in the literature, we propose a simulator architecture for generating probabilistic synthetic data for decision-based serious games. This architecture is designed to be versatile and modular so that it can be used by other researchers on similar problems (e.g., multiple choice exams, political surveys, any type of questionnaire). To simulate the interaction of synthetic players with the game, we use a cognitive testing model based on the Item Response Theory framework. We also show how probabilistic graphical models (in particular, Bayesian networks) can introduce expert knowledge and external data into the simulation. Finally, we apply the proposed architecture and methods in the case of a serious game focused on cyberbullying. We perform Bayesian inference experiments using a hierarchical model to demonstrate the identifiability and robustness of the generated data.}
}
@article{WANG2024100073,
title = {Learning cleanroom microfabrication operations in virtual reality – An immersive and guided learning experience},
journal = {Computers & Education: X Reality},
volume = {5},
pages = {100073},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100073},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000230},
author = {Fang Wang and Xinhao Xu and Shangman Li and Weiyu Feng and Mahmoud Almasri},
keywords = {Virtual reality, Microfabrication, Immersive, Learning, Education, Semiconductor, Training, Lab},
abstract = {This paper details the design and development of an immersive and self-guided Virtual Reality training system (iSGVRTS) for learning cleanroom microfabrication operations, with a specific emphasis on the photolithography process, within a college-level semiconductor laboratory curriculum. It presents the thorough construction of the iSGVRTS environment as well as the incorporation of integrated instructional methodologies. To assess the impact of the iSGVRTS intervention, pre-and post-tests were administered to evaluate learners' performance. The implementation of iSGVRTS yielded a notable enhancement for learners in laboratory operational proficiency, evidenced by improvements in task correct rates, reduction in procedural errors, and knowledge acquisition. Moreover, post-session interviews revealed learners’ reported increased confidence, a heightened sense of presence, manageable cognitive load, and positive feedback regarding the immersive learning experience.}
}
@article{RAZZAK2024100234,
title = {Using virtual reality to enhance attention for autistic spectrum disorder with eye tracking},
journal = {High-Confidence Computing},
pages = {100234},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100234},
url = {https://www.sciencedirect.com/science/article/pii/S2667295224000370},
author = {Rehma Razzak and Yi {(Joy) Li} and Jing {(Selena) He} and Sungchul Jung and Chao Mei and Yan Huang},
keywords = {Virtual reality, Attention training, Autistic spectrum disorders, Human–computer interaction},
abstract = {Attention deficit disorder is a frequently observed symptom in individuals with autism spectrum disorder (ASD). This condition can present significant obstacles for those affected, manifesting in challenges such as sustained focus, task completion, and the management of distractions. These issues can impede learning, social interactions, and daily functioning. This complexity of symptoms underscores the need for tailored approaches in both educational and therapeutic settings to support individuals with ASD effectively. In this study, we have expanded upon our initial virtual reality (VR) prototype, originally created for attention therapy, to conduct a detailed statistical analysis. Our objective was to precisely identify and measure any significant differences in attention-related outcomes between sessions and groups. Our study found that heart rate (HR) and electrodermal activity (EDA) were more responsive to attention shifts than temperature. The ‘Noise’ and ‘Score’ strategies significantly affected eye openness, with the ASD group showing more responsiveness. The control group had smaller pupil sizes, and the ASD group’s pupil size increased notably when switching strategies in Session 1. Distraction log data showed that both ‘Noise’ and ‘Object Opacity’ strategies influenced attention patterns, with the ‘Red Vignette’ strategy showing a significant effect only in the ASD group. The responsiveness of HR and EDA to attention shifts and the changes in pupil size could serve as valuable physiological markers to monitor and guide these interventions. These findings further support evidence that VR has positive implications for helping those with ASD, allowing for more tailored personalized interventions with meaningful impact.}
}
@article{DIIORIO2005129,
title = {What's the Name of the Game? Formal Specification of Artificial Intelligence Games},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {130},
pages = {129-150},
year = {2005},
note = {Proceedings of the Brazilian Symposium on Formal Methods (SBMF 2004)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1571066105002185},
author = {Vladimir {Di Iorio} and Roberto S. Bigonha and Mariza A.S. Bigonha and Alcione Oliveira and Eliseu Miguel},
keywords = {Artificial Intelligence, computer games, Abstract State Machines},
abstract = {Artificial intelligence games are a very interesting tool for teaching Artificial Intelligence techniques. Competitors write programs for agents, which are supposed to complete a given task or fight against other agents. In order to achieve the best performance, programs may have to use advanced Artificial Intelligence methods. In this paper, we present a framework to build artificial intelligence games, using Abstract State Machines (ASM) for the specification of the rules of the games. Choosing ASM, we expect that the competitors will be able to understand clearly the semantics of the rules. The framework includes a compiler for an ASM-based language, allows complete control of the order of execution of agents and easy integration with graphical libraries.}
}
@article{MOLINA2023101183,
title = {Two-step techniques for accurate selection of small elements in VR environments},
journal = {Graphical Models},
volume = {128},
pages = {101183},
year = {2023},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2023.101183},
url = {https://www.sciencedirect.com/science/article/pii/S1524070323000139},
author = {Elena Molina and Pere-Pau Vázquez},
keywords = {3D interaction, Selection, Molecular visualization},
abstract = {One of the key interactions in 3D environments is target acquisition, which can be challenging when targets are small or in cluttered scenes. Here, incorrect elements may be selected, leading to frustration and wasted time. The accuracy is further hindered by the physical act of selection itself, typically involving pressing a button. This action reduces stability, increasing the likelihood of erroneous target acquisition. We focused on molecular visualization and on the challenge of selecting atoms, rendered as small spheres. We present two techniques that improve upon previous progressive selection techniques. They facilitate the acquisition of neighbors after an initial selection, providing a more comfortable experience compared to using classical ray-based selection, particularly with occluded elements. We conducted a pilot study followed by two formal user studies. The results indicated that our approaches were highly appreciated by the participants. These techniques could be suitable for other crowded environments as well.}
}
@article{DELIMA2023100590,
title = {Managing the plot structure of character-based interactive narratives in games},
journal = {Entertainment Computing},
volume = {47},
pages = {100590},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100590},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000459},
author = {Edirlei Soares {de Lima} and Bruno Feijó and Antonio L. Furtado},
keywords = {Interactive storytelling, Narrative generation, Drama management, Plot structure, Automated planning},
abstract = {The use of narrative generation methods in games is a complex challenge that involves multiple problems of plot-based processes integrated with character-based methods. Examples of these problems are the high computational complexity of many story generation algorithms, the difficulties associated with the generation of interactive narratives that are compelling and emotionally impactful, the complex interactions among characters, and the need for tools and methods to support story writers in the process of creating and managing the narrative structure of interactive stories. In this work, we present and evaluate a new approach to generate and manage the plot structure of character-based interactive narratives in games, which combines multi-agent planning with a drama management strategy based on narrative structures. The proposed method is supported by an authoring tool that allows authors to create and test interactive narratives using graphical interfaces and intuitive diagrams. The results of our study suggest the effectiveness of our approach in generating interactive narratives for highly interactive game environments. In addition, a user study of the proposed authoring tool indicates that it can successfully support the development of character-based interactive narratives without requiring programming knowledge.}
}
@article{HAUGE2012210,
title = {Evaluation of Simulation Games for Teaching Engineering and Manufacturing},
journal = {Procedia Computer Science},
volume = {15},
pages = {210-220},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.073},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008356},
author = {Jannicke Baalsrud Hauge and Johann C.K.H. Riedel},
keywords = {Serious Games, Evaluation, Evaluation Methods, Engineering, Manufacturing.},
abstract = {This paper reports on the evaluation methods and findings from serious games for teaching engineering and manufacturing. Two serious games are considered: Cosiga, a new product development simulation game and Beware, a risk management simulation game. These two games cover the front and middle parts of the engineering process – from design to manufacture to sale. For the Cosiga simulation evaluations of the communication and cognitive change were performed. For the Beware game evaluation of communication, risk awareness and improvement of risk management skills were performed The findings from the evaluations showed that serious games deliver learning outcomes. However, there are drawbacks to their use that need to be taken into account. Principally the high cost of development and the need for expert facilitators for running game sessions.}
}
@article{BULBUL2023101170,
title = {Procedural generation of semantically plausible small-scale towns},
journal = {Graphical Models},
volume = {126},
pages = {101170},
year = {2023},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2023.101170},
url = {https://www.sciencedirect.com/science/article/pii/S1524070323000012},
author = {Abdullah Bulbul},
keywords = {Procedural modeling, 3D city modeling, Semantic control},
abstract = {Procedural techniques have been successfully utilized for generating various kinds of 3D models. In this study, we propose a procedural method to build 3D towns that can be manipulated by a set of high-level semantic principles namely security, privacy, sustainability, social-life, economy, and beauty. Based on the user defined weights of these principles, our method generates a 3D settlement to accommodate a desired population over a given terrain. Our approach firstly determines where to establish the settlement over the large terrain which is followed by iteratively constructing the town. In both steps, the principles guide the decisions and our method generates natural looking small-scale 3D residential regions similar to the cities of pre-industrial era. We demonstrate the effectiveness of the proposed approach to build semantically plausible town models by presenting sample results over real world based terrains.}
}
@article{DEHLAGHIGHADIM2023103906,
title = {ICSSIM — A framework for building industrial control systems security testbeds},
journal = {Computers in Industry},
volume = {148},
pages = {103906},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103906},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000568},
author = {Alireza Dehlaghi-Ghadim and Ali Balador and Mahshid Helali Moghadam and Hans Hansson and Mauro Conti},
keywords = {Cybersecurity, Industrial control system, Testbed, Network emulation, Cyberattack},
abstract = {With the advent of the smart industry, Industrial Control Systems (ICS) moved from isolated environments to connected platforms to meet Industry 4.0 targets. The inherent connectivity in these services exposes such systems to increased cybersecurity risks. To protect ICSs against cyberattacks, intrusion detection systems (IDS) empowered by machine learning are used to detect abnormal behavior of the systems. Operational ICSs are not safe environments to research IDSs due to the possibility of catastrophic risks. Therefore, realistic ICS testbeds enable researchers to analyze and validate their IDSs in a controlled environment. Although various ICS testbeds have been developed, researchers’ access to a low-cost, extendable, and customizable testbed that can accurately simulate ICSs and suits security research is still an important issue. In this paper, we present ICSSIM, a framework for building customized virtual ICS security testbeds in which various cyber threats and network attacks can be effectively and efficiently investigated. This framework contains base classes to simulate control system components and communications. Simulated components are deployable on actual hardware such as Raspberry Pis, containerized environments like Docker, and simulation environments such as GNS-3. ICSSIM also offers physical process modeling using software and hardware in the loop simulation. This framework reduces the time for developing ICS components and aims to produce extendable, versatile, reproducible, low-cost, and comprehensive ICS testbeds with realistic details and high fidelity. We demonstrate ICSSIM by creating a testbed and validating its functionality by showing how different cyberattacks can be applied.}
}
@article{MEMER2024116670,
title = {Robust numerical integration of embedded solids described in boundary representation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {419},
pages = {116670},
year = {2024},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116670},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523007934},
author = {Manuel Meßmer and Stefan Kollmannsberger and Roland Wüchner and Kai-Uwe Bletzinger},
keywords = {Embedded finite elements, Boundary representation (B-rep), Moment fitting equations, Clipping and intersection algorithms, Flawed geometries},
abstract = {Embedded and immersed methods have become essential tools in computational mechanics, as they allow discretizing arbitrarily complex geometries without the need for boundary-fitted meshes. One of their main challenges is the accurate numerical integration of cut elements. Among the various integration schemes developed for this purpose, moment fitting has proven to be a powerful technique that provides highly efficient and accurate integration rules. This publication presents a framework for the robust and efficient numerical integration of embedded solids described by oriented boundary meshes using moment fitting. The developments include an intersection algorithm that aims to drastically accelerate the computation of the necessary moments while achieving high accuracy. A closed surface parameterization of each cut domain is computed to facilitate the direct application of the divergence theorem. The algorithm is subject to a single quality criterion that guarantees the accurate evaluation of boundary integrals. At the same time, it allows to disregard classical mesh criteria, such as high aspect ratios, strongly varying angles, etc., resulting in extremely fast runtimes. In addition, an existing robust flood fill-based element classification scheme is further developed to initiate filling from arbitrary seed elements and to enable parallel execution, increasing its flexibility and efficiency. The successful application of all proposed algorithms to 4948 valid and flawed STLs from the Thingi10K database (Zhou and Jacobson, 2016) demonstrates their extraordinary robustness. In all cases, the wall-clock time scales at most linearly with the number of elements in the background mesh. We show that higher-order quadrature rules on the boundary elements enable efficient computation of the moments via the divergence theorem with near-machine precision. Finally, the presented methodologies are used to perform direct FE analyses on clean and flawed B-Rep models. All proposed algorithms are publicly available in the open-source C++ framework QuESo – Quadrature for Embedded Solids (https://github.com/manuelmessmer/QuESo), where the moment fitting equations are assembled and solved.}
}
@article{SANTAMARIAVAZQUEZ2023107357,
title = {MEDUSA©: A novel Python-based software ecosystem to accelerate brain-computer interface and cognitive neuroscience research},
journal = {Computer Methods and Programs in Biomedicine},
volume = {230},
pages = {107357},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107357},
url = {https://www.sciencedirect.com/science/article/pii/S016926072300024X},
author = {Eduardo Santamaría-Vázquez and Víctor Martínez-Cagigal and Diego Marcos-Martínez and Víctor Rodríguez-González and Sergio Pérez-Velasco and Selene Moreno-Calderón and Roberto Hornero},
keywords = {Brain-computer interfaces, Neurotechnology, Neuroscience, Electroencephalography},
abstract = {Background and objective: Neurotechnologies have great potential to transform our society in ways that are yet to be uncovered. The rate of development in this field has increased significantly in recent years, but there are still barriers that need to be overcome before bringing neurotechnologies to the general public. One of these barriers is the difficulty of performing experiments that require complex software, such as brain-computer interfaces (BCI) or cognitive neuroscience experiments. Current platforms have limitations in terms of functionality and flexibility to meet the needs of researchers, who often need to implement new experimentation settings. This work was aimed to propose a novel software ecosystem, called MEDUSA©, to overcome these limitations. Methods: We followed strict development practices to optimize MEDUSA© for research in BCI and cognitive neuroscience, making special emphasis in the modularity, flexibility and scalability of our solution. Moreover, it was implemented in Python, an open-source programming language that reduces the development cost by taking advantage from its high-level syntax and large number of community packages. Results: MEDUSA© provides a complete suite of signal processing functions, including several deep learning architectures or connectivity analysis, and ready-to-use BCI and neuroscience experiments, making it one of the most complete solutions nowadays. We also put special effort in providing tools to facilitate the development of custom experiments, which can be easily shared with the community through an app market available in our website to promote reproducibility. Conclusions: MEDUSA© is a novel software ecosystem for modern BCI and neurotechnology experimentation that provides state-of-the-art tools and encourages the participation of the community to make a difference for the progress of these fields. Visit the official website at https://www.medusabci.com/ to know more about this project.}
}
@article{VANOOSTEROM2022119,
title = {Organizing and visualizing point clouds with continuous levels of detail},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {194},
pages = {119-131},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622002647},
author = {Peter {van Oosterom} and Simon {van Oosterom} and Haicheng Liu and Rod Thompson and Martijn Meijers and Edward Verbree},
keywords = {nD point clouds, Continuous level of detail (cLoD), Space Filling Curve (SFC), Perspective view selection},
abstract = {Point clouds contain high detail and high accuracy geometry representation of the scanned Earth surface parts. To manage the huge amount of data, the point clouds are traditionally organized on location and map-scale; e.g. in an octree structure, where top-levels of the tree contain few points suitable for small scale overviews and lower levels of the tree contain more points suitable for large scale detailed views. The drawback of this solution is that it is based on discrete levels, causing visual artifacts in the form of data density shocks when creating the commonly used perspective views. This paper presents a method based on an optimized distribution of points over continuous levels, avoiding the visualization shocks. The traditional distribution ratio’s of data amounts over discrete levels of raster or vector data is considered the reference. How to convert this to point clouds with continuous levels (still benefiting from the proven advantages of the data distribution in discrete levels for efficient access at a wide range of scales)? In our solution, for each point a cLoD (continuous Level of Detail) value is computed and added as dimension to the point. A SFC (Space Filling Curve)-based nD data clustering technique can be used to organize the points, so that they can be efficiently queried. It should be noted that also other multi-dimensional indexing and clustering techniques could be applied to realize continuous levels based on the cLoD value. Besides the mathematical foundation of the approach also several implementations are described, varying from a 3D web-browser based solution to an augmented reality point cloud app in a mobile phone. The cLoD enables interactive real-time visualization using perspective views without data density shocks, while supporting continuous zoom-in/out and progressive data streaming between server and client. The described cLoD based approach is generic and supports different types of point clouds: from airborne, terrestrial, mobile and indoor laser scanning, but also from dense matching optical imagery or multi-beam echo soundings.}
}
@article{KUSUMA2018385,
title = {Analysis of Gamification Models in Education Using MDA Framework},
journal = {Procedia Computer Science},
volume = {135},
pages = {385-392},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.187},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314765},
author = {Gede Putra Kusuma and Evan Kristia Wigati and Yesun Utomo and Louis Khrisna {Putera Suryapranata}},
keywords = {Gamification in education, Gamification models, Student motivation, Student achievement},
abstract = {Gamification nowadays is being one of techniques that can increase motivation and encourage the involvement of users, particularly in education domain that requires teaching and learning activities to be more fun and interesting. This paper surveys some analysis of gamification models. MDA framework is used to identify surveyed papers by breaking them down into three categories: mechanics, dynamics and aesthetics. Findings from the survey show there are many gamification models in education domain. However, there are some very representative gamification models could be used as a method to increase motivation, achievement and engagement in learning activities. By knowing the latest gamification models in education domain stated in this paper, it could help gamification practitioners to make new strategies in learning activities to increase students’ motivation, achievement and involvement. We also suggest some gamification strategies, which combine several mechanics in such a way to create dynamics that results in all types of aesthetics outputs.}
}
@article{BUTTUSSI2020103590,
title = {A virtual reality methodology for cardiopulmonary resuscitation training with and without a physical mannequin},
journal = {Journal of Biomedical Informatics},
volume = {111},
pages = {103590},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103590},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420302197},
author = {Fabio Buttussi and Luca Chittaro and Francesca Valent},
keywords = {Virtual reality, Training, Cardiopulmonary resuscitation, Medical education, Mannequin, Experimental evaluation},
abstract = {Background
Cardiopulmonary resuscitation (CPR) is an emergency procedure that can increase survival after a cardiac arrest. Performing CPR effectively requires both procedural knowledge and manual skills. Traditional CPR training methodology includes lessons led by instructors and supervised practice on mannequins, thus requiring considerable resources.
Objective
This paper proposes a new methodology for low-cost CPR training based on virtual reality (VR) with and without the addition of a physical mannequin. Moreover, it describes an experimental evaluation of the methodology that assessed gain in manual skills during training, transfer of procedural knowledge and manual skills in a final assessment, and changes in self-efficacy with three measurements over time (pre-training, post-training, and post-assessment).
Methods
We implemented a VR application that supports the proposed methodology, and can thus be used with or without a mannequin. The experimental evaluation involved 30 participants who tried CPR in VR twice, performing two repetitions of 30 chest compressions per trial. Half participants tried the VR application with the mannequin and half without it. Final assessment required all participants to perform CPR on the mannequin without the assistance of VR. To assess self-efficacy, participants filled in a questionnaire at the three times of measurement.
Results
Mixed-design ANOVAs showed effects of repetition, effects of group, or interaction between the two variables on manual skills assessed during training. In the final assessment, participants in both groups correctly remembered most of the steps of the procedure. ANOVAs revealed differences between the two groups only in pressure-related skills (better with mannequin) and in the number of wrong steps added to the procedure (better without mannequin). Mixed-design ANOVA showed a self-efficacy increase in both groups after training, which was maintained after final assessment.
Conclusions
The proposed VR methodology for CPR training has a positive effect on procedural knowledge, manual skills, and self-efficacy, with as well as without the physical mannequin. Trials on a mannequin are required to understand the correct pressure for chest compression. This supports the adoption of the proposed VR methodology to reduce instructor and mannequin time required to teach CPR to trainees.}
}
@article{SLUPINSKA20213123,
title = {Planning an experiment in a virtual environment reality as a place of research on human behaviour using methods of neuroscience measurement – bibliometric analysis and methodological approach},
journal = {Procedia Computer Science},
volume = {192},
pages = {3123-3133},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.085},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921018226},
author = {Kamila Słupińska and Jarosław Duda and Konrad Biercewicz},
keywords = {wirtualna rzeczywistość (VR), EEG, eye traking, metodyka},
abstract = {The methods used in cognitive neuroscience allow through measurement methods to analyze the behavior of subjects. In the opinion of the authors, the application of the experiment in a virtual environment and the measurement of brain activity can obtain a detailed analysis of the information and knowledge of the subject. For this to happen, however, it is important that the entire course of the study is carried out in accordance with the assumptions, without errors that could affect the incorrect interpretation of the results obtained. Planning an experiment focuses on planning all the details involved in carrying out the study (Hicks, 1973, Goodwin, 2008). Therefore, in the course of the research, the authors sought information on the methodology of experimental research carried out in a virtual reality environment with the use of selected tools of cognitive neuroscience, including in particular the use of virtual reality (VR), electroencephalograph (EEG) and eye tracking. Valuable knowledge and information on this topic can be found, among others, in the publication of B Rey, M Alcañiz in the article "Research in neuroscience and virtual reality" [1] in which the importance of the implementation of research in virtual reality is highlighted. The authors believe that VR is an ideal tool to "generate controlled environments the can be used to observe human behavior. Different aspects can be analyzed from a neuroscience perspective, including perception, control of movement, learning, memory, and emotional aspects. " However, in order to assess the extent to which the experimental method in combination with technology allows for reliable information and knowledge management, it was necessary to look at considerations of the research methodology. Therefore, as the first stage, a bibliometric study was adopted, the analysis of the content contained in three databases such as WoS, Scopus, and Google Scholar. The analyses carried out showed a small percentage of publications presenting the methodology of implementation of an experimental study in VR treated as an environment in which research is carried out, within which the aforementioned methods of measurement from the field of neuroscience were applied. Based on the analysis, the authors, on the basis of selected materials in which the research processes have been well-grounded in methodology, as well as on their own research experience, will present the stages of design and implementation of an experimental study carried out in the virtual reality environment using the indicated methods of cognitive neuroscience. Based on a case study, they presented the activities carried out in the course of analyzed events, as well as pointed to potential errors that may occur in the course of research. A supermarket was taken as the virtual location of the study.}
}
@article{DEPAULA201839,
title = {Playing Beowulf: Bridging computational thinking, arts and literature through game-making},
journal = {International Journal of Child-Computer Interaction},
volume = {16},
pages = {39-46},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300247},
author = {Bruno Henrique {de Paula} and Andrew Burn and Richard Noss and José Armando Valente},
keywords = {Game-making, Computational thinking, Arts, Humanities},
abstract = {Preparing younger generations to engage meaningfully with digital technology is often seen as one of the goals of 21st century education. JeanetteWing’s seminal work on Computational Thinking (CT) is an important landmark for this goal: CT represents a way of thinking, a set of problem-solving skills which can be valuable when interacting with digital technologies, and with different fields of knowledge, such as Arts and Humanities. Even if this cross-areas relevance has been celebrated and acknowledged in theoretical research, there has been a lack of practical projects exploring these links between CT and non-STEM fields. This research develops these links. We present a specific case – a game produced by two 14 years-old boys – within Playing Beowulf, a collaboration with the British library’s Young Researchers programme, in which students aged 13–14 from an inner-London (UK) school have developed games based on their own readings of the Anglo-Saxon poem Beowulf during an after-school club. The game was produced using MissionMaker, a software (currently under development at UCL Knowledge Lab) that allows users to create and code their own first-person 3D games in a simple way, using pre-made 3D assets, such as rooms, props, characters and weapons and a simplified programming language manipulated through drop-down lists. We argue that MissionMaker, by simplifying the development process (low floor), can be a means to foster the building of knowledge in both STEM (CT) and Arts and Humanities, building bridges between these two areas inside and outside traditional schooling.}
}
@article{BOBOU2020e00164,
title = {Archive Archaeology in palmyra, Syria a new 3D reconstruction of the tomb of Ḥairan},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {19},
pages = {e00164},
year = {2020},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2020.e00164},
url = {https://www.sciencedirect.com/science/article/pii/S2212054820300631},
author = {Olympia Bobou and Nathalia B. Kristensen and Scott McAvoy and Rubina Raja},
keywords = {Palmyra, Cultural heritage destruction and preservation, Harald ingholt, Tomb of H, airan, 3D reconstructions, Hypogeum, Fresco},
abstract = {Since 2010, the conflict in Syria has made it almost impossible to conduct fieldwork in the country. The Palmyra Portrait Project at Aarhus University has since 2012 been compiling the now largest corpus of funerary portraits in the ancient world, outside of Rome, which were produced in the Syrian city of Palmyra in the first three centuries CE. During the project, fieldwork diaries of the Danish archaeologist Kai Harald Ingholt, who worked in Palmyra between 1924 and 1935, are also being digitized and the information in these can be used to reconstruct in-situ contexts, which no longer exist. In this paper, we, for the first time, present the sketches and information given in the diaries, along with archival photos, paintings, inscriptions, and sculptures in an interactive web-based digital model of a monumental underground tomb, the so-called tomb of Ḥairan.}
}
@article{GALVANDEBARBA2018142,
title = {Self-attribution of distorted reaching movements in immersive virtual reality},
journal = {Computers & Graphics},
volume = {76},
pages = {142-152},
year = {2018},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849318301353},
author = {Henrique {Galvan Debarba} and Ronan Boulic and Roy Salomon and Olaf Blanke and Bruno Herbelin},
keywords = {Virtual reality, Self-attribution, Evaluation},
abstract = {This study explores the extent to which individuals embodied in Virtual Reality tend to self-attribute the movements of their avatar. More specifically, we tested subjects performing goal-directed movements and distorted the mapping between user and avatar movements by decreasing or increasing the amplitude of the avatar hand movement required to reach for a target, while maintaining the apparent amplitude – visual distance – fixed. In two experiments, we asked subjects to report whether the movement that they have seen matched the movement that they have performed, or asked them to classify whether a distortion was making the task easier or harder to complete. Our results show that subjects perform poorly in detecting discrepancies when the nature of the distortion is not made explicit and that subjects are biased to self-attributing distorted movements that make the task easier. These findings, in line with previous accounts on the sense of agency, demonstrate the flexibility of avatar embodiment and open new perspectives for the design of guided interactions in Virtual Reality.}
}
@article{CHATZITOFIS2015340,
title = {HeartHealth: A Cardiovascular Disease Home-based Rehabilitation System},
journal = {Procedia Computer Science},
volume = {63},
pages = {340-347},
year = {2015},
note = {The 6th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2015)/ The 5th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2015)/ Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.352},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915024874},
author = {Anargyros Chatzitofis and David Monaghan and Edmond Mitchell and Freddie Honohan and Dimitrios Zarpalas and Noel E. O’Connor and Petros Daras},
keywords = {home-based rehabilitation platform, rehabilitation, CVD patients, CVD exercises, motion capture, serious games ;},
abstract = {The increasing pressure on medical institutions around the world requires health care professionals to be prescribing home- based exercise rehabilitation treatments to empower patients to self-monitor their rehabilitation journey. Home-based exercise rehabilitation has shown to be highly effective in treating conditions such as Cardiovascular Disease (CVD). However, adherence to home-based exercise rehabilitation remains low. Possible causes for this are that patients are not monitored, they cannot be con- fident that they are performing the exercise correctly or accurately and they receive no feedback. This paper proposes HeartHealth, a novel patient-centric gamified exercise rehabilitation platform that can help address the issue of adherence to these programmes. The key functionality is the ability to record the patient movements and compare them against the exercises that have been pre- scribed in order to return feedback to the patient and to the health care professional, as well. In order to synthesize a compact fully operational system able to work in real life scenarios, tools and services from FI-PPP projects, FIWARE 1 and FI-STAR 2, were exploited and a new FI-STAR component, Motion Evaluation Specific Enabler (SE), was designed and developed. The HeartHealth system brings together real-time cloud-based motion evaluation coupled with accurate low-cost motion capture, a personalised ex- ercise rehabilitation programme and an intuitive and fun serious game interface, designed specifically with a Cardiac Rehabilitation population in mind.}
}
@article{RIEGER2019175,
title = {Towards the definitive evaluation framework for cross-platform app development approaches},
journal = {Journal of Systems and Software},
volume = {153},
pages = {175-199},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300743},
author = {Christoph Rieger and Tim A. Majchrzak},
keywords = {Mobile app, Mobile computing, Cross-platform, Multi-platform, Development framework},
abstract = {Mobile app development is hindered by device fragmentation and vendor-specific modifications. Boundaries between devices blur with PC-tablet hybrids on the one side and wearables on the other. Future apps need to support a host of app-enabled devices with differing capabilities, along with their software ecosystems. Prior work on cross-platform app development concerned concepts and prototypes, and compared approaches that target smartphones. To aid choosing an appropriate framework and to support the scientific assessment of approaches, an up-to-date comparison framework is needed. Extending work on a holistic, weighted set of assessment criteria, we propose what could become the definitive framework for evaluating cross-platform approaches. We have based it on sound abstract concepts that allow extensions. The weighting capabilities offer customisation to avoid the proverbial comparison of apples and oranges lurking in the variety of available frameworks. Moreover, it advises on multiple development situations based on a single assessment. In this article, we motivate and describe our evaluation criteria. We then present a study that assesses several frameworks and compares them to Web Apps and native development. Our findings suggest that cross-platform development has seen much progress but the challenges are ever growing. Therefore, additional support for app developers is warranted.}
}
@article{PIENIMAKI2021100283,
title = {Finding fun in non-formal technology education},
journal = {International Journal of Child-Computer Interaction},
volume = {29},
pages = {100283},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100283},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000234},
author = {Maija Pienimäki and Marianne Kinnula and Netta Iivari},
keywords = {Non-formal education, Informal education, Technology education, After-school clubs, Out-of-school learning, Robotics, Programming, Making, Children, Teenagers, Youth, Fun, Enjoyment},
abstract = {In this exploratory study into the world of 8–17-year-old children’s non-formal technology education, two different types of technology education with varying levels of non-formality were investigated to see how participants find fun in these situations as it is apparent that if something is non-mandatory to attend to, there should be some type of enjoyment found in the process. The results of the analysis suggest that there are three main ways children and teenagers have fun in non-formal education: fun from the tasks they are doing, social fun by sharing with other attendants, and pedagogical fun that has been embedded in the learning process. Based on our findings, we offer suggestions for how to add elements of fun in the non-formal technology education, to make it more motivating and enjoyable to the participants.}
}
@article{ZDUN2004131,
title = {Supporting incremental and experimental software evolution by runtime method transformations},
journal = {Science of Computer Programming},
volume = {52},
number = {1},
pages = {131-163},
year = {2004},
note = {Special Issue on Program Transformation},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2004.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167642304000498},
author = {Uwe Zdun},
keywords = {Runtime method transformation, Software evolution, Software adaptation, Patterns},
abstract = {Transformations of object-oriented methods are a prevalent object-oriented programming technique, but in many languages they are not supported at runtime. Therefore it can be hard to apply method transformations for incremental or experimental software evolution, or other problems that require runtime software behavior adaptation. The goal of the work presented in this paper is to provide a better conceptual and technical support for runtime method transformations. A non-intrusive model for method transformations and a set of runtime method transformation primitives are presented. We also present a pattern language for implementing dynamic method abstractions and combining them with languages that do not support dynamic methods natively. As a case study we introduce a runtime transformation framework for the dynamic configuration and composition language Frag, its connection to Java, and an end user programming example.}
}
@article{GENTILE20221,
title = {The role of mental rotation in TetrisTM gameplay: An ACT-R computational cognitive model},
journal = {Cognitive Systems Research},
volume = {73},
pages = {1-11},
year = {2022},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2021.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000991},
author = {Manuel Gentile and Antonio Lieto},
keywords = {Cognitive architectures, ACT-R, Mental rotation, Tetris, Serious games design},
abstract = {The mental rotation ability is an essential spatial reasoning skill in human cognition and has proven to be an essential predictor of mathematical and STEM skills, critical and computational thinking. Despite its importance, little is known about when and how mental rotation processes are activated in games explicitly targeting spatial reasoning tasks. In particular, the relationship between spatial abilities and TetrisTM has been analysed several times in the literature. However, these analyses have shown contrasting results between the effectiveness of Tetris-based training activities to improve mental rotation skills. In this work, we studied whether, and under what conditions, such ability is used in the TetrisTM game by explicitly modelling mental rotation via an ACT-R based cognitive model controlling a virtual agent. The obtained results show meaningful insights into the activation of mental rotation during game dynamics. The study suggests the necessity to adapt game dynamics in order to force the activation of this process and, therefore, can be of inspiration to design learning activities based on TetrisTM or re-design the game itself to improve its educational effectiveness.}
}
@article{TSARKOV2021771,
title = {Toward a socially acceptable model of emotional artificial intelligence},
journal = {Procedia Computer Science},
volume = {190},
pages = {771-788},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.090},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013454},
author = {Vladimir S. Tsarkov and Vladislav A. Enikeev and Alexei V. Samsonovich},
keywords = {affective computing, emotional intelligence, eBICA, believable social agents},
abstract = {The framework of emotional Biologically Inspired Cognitive Architecture (eBICA) is used to define a cognitive model, producing believable socially emotional behavior in a social interaction paradigm in a virtual environment. The paradigm selected for this study is a virtual pet interacting with a human. Empirical results indicate that the combination of somatic factors, moral schemas and rational constraints in one model has the potential to make behavior of a virtual actor more believable, humanlike and socially acceptable. Implications concern future intelligent collaborative robots and virtual assistants.}
}
@article{VALLS20181039,
title = {Urban data and urban design: A data mining approach to architecture education},
journal = {Telematics and Informatics},
volume = {35},
number = {4},
pages = {1039-1052},
year = {2018},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2017.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0736585317303416},
author = {Francesc Valls and Ernesto Redondo and David Fonseca and Ricardo Torres-Kompen and Sergi Villagrasa and Nuria Martí},
keywords = {Data mining, Urban data, Architecture education, Informal learning},
abstract = {The configuration of urban projects using Information and Communication Technologies is an essential aspect in the education of future architects. Students must know the technologies that will facilitate their academic and professional development, as well as anticipating the needs of the citizens and the requirements of their designs. In this paper, a data mining approach was used to outline the strategic requirements for an urban design project in an architecture course using a Project-Based Learning strategy. Informal data related to an award-winning public space (Gillett Square in London, UK) was retrieved from two social networks (Flickr and Twitter), and from its official website. The analysis focused on semantic, temporal and spatial patterns, aspects generally overlooked in traditional approaches. Text-mining techniques were used to relate semantic and temporal data, focusing on seasonal and weekly (work-leisure) cycles, and the geographic patterns were extracted both from geotagged pictures and by geocoding user locations. The results showed that it is possible to obtain and extract valuable data and information in order to determine the different uses and architectural requirements of an urban space, but such data and information can be challenging to retrieve, structure, analyze and visualize. The main goal of the paper is to outline a strategy and present a visualization of the results, in a way designed to be attractive and informative for both students and professionals – even without a technical background – so the conducted analysis may be reproducible in other urban data contexts.}
}
@article{IPSITA2024100074,
title = {Authoring instructional flow in iVR learning units to promote outcome-oriented learning},
journal = {Computers & Education: X Reality},
volume = {5},
pages = {100074},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100074},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000242},
author = {Ananya Ipsita and Mayank Patel and Asim Unmesh and Karthik Ramani},
keywords = {Virtual reality training, Backward design, Welding simulator, Instruction flow, Curriculum planning, Skill learning},
abstract = {Despite the recognized efficacy of immersive Virtual Reality (iVR) in skill learning, the design of iVR-based learning units by subject matter experts (SMEs) based on target requirements is severely restricted. This is partly due to a lack of flexible ways of authoring instruction flows to arrange the learning activities in alignment with the desired learning objectives. Our research provides a workflow design enabling SMEs to author the flow of learning activities developed by the Virtual Reality (VR) developers, with an aim to enable learners achieve desired goals progressively in a virtual environment. Additionally, this outcome-oriented flow authoring utilizes a scalable learning framework that categorizes learning activities into four instructional phases: Introduction, Presentation, Practice, and Application. Such frameworks can be easily integrated into the instruction to plan a class or a series of classes to cover an entire concept or chapter. Using a welding use case, our user study evaluation with 12 experienced welders indicated positive ratings about the usefulness of such workflows for flexible planning of training scenarios. We envision adoption of such methods could facilitate greater and more efficient adoption of the iVR technologies in pedagogical settings.}
}
@article{HAMM2019103135,
title = {Enabling older adults to carry out paperless falls-risk self-assessments using guidetomeasure-3D: A mixed methods study},
journal = {Journal of Biomedical Informatics},
volume = {92},
pages = {103135},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103135},
url = {https://www.sciencedirect.com/science/article/pii/S153204641930053X},
author = {Julian Hamm and Arthur G. Money and Anita Atwal},
keywords = {Health informatics, Falls, Occupational therapy, Assistive equipment, Self-assessment, Measurement guidance, Extrinsic risk factors, 3D mobile visualisation, Technology-based systems},
abstract = {Background
The home environment falls-risk assessment process (HEFAP) is a widely used falls prevention intervention strategy which involves a clinician using paper-based measurement guidance to ensure that appropriate information and measurements are taken and recorded accurately. Despite the current use of paper-based guidance, over 30% of all assistive devices installed within the home are abandoned by patients. This is in part due to poor fit between the device, the patient, and the environment in which it is installed. Currently HEFAP is a clinician-led process, however, older adult patients are increasingly being expected to collect HEFAP measurements themselves as part of the personalisation agenda. Without appropriate patient-centred guidance, levels of device abandonment to are likely to rise to unprecedented levels. This study presents guidetomeasure-3D, a mobile 3D measurement guidance application designed to support patients in carrying out HEFAP self-assessments.
Aim
The aim of this study is to present guidetomeasure-3D, a web-enabled 3D mobile application that enables older-adult patients to carry out self-assessment measurement tasks, and to carry out a mixed-methods evaluation of its performance, and associated user perceptions of the application, compared with a 2D paper-based equivalent.
Methods
Thirty-four older adult participants took part in a mixed-methods within-subjects repeated measures study set within a living lab. A series of HEFAP self-assessment tasks were carried out according to two treatment conditions: (1) using the 3D guidetomeasure-3D application; (2) using a 2D paper-based guide. SUS questionnaires and semi-structured interviews were completed at the end of the task. A comparative statistical analysis explored performance with regards to measurement accuracy, accuracy consistency, task efficiency, and system usability. Interview transcripts were analysed using inductive and deductive thematic analysis (informed by UTAUT).
Results
The guidetomeasure-3D application outperformed the 2D paper-based guidance in terms of accuracy (smaller mean error difference in 11 out of 12 items), accuracy consistency (p < 0.05, for 6 out of 12 items), task efficiency (p = 0.003), system usability (p < 0.00625, for two out of 10 SUS items), and clarity of guidance (p < 0.0125, for three out of four items). Three high-level themes emerged from interviews: Performance Expectancy, Effort Expectancy, and Social Influence. Participants reported that guidetomeasure-3D provided improved visual quality, clarity, and more precise guidance overall. Real-time audio instruction was reported as being particularly useful, as was the use of the object rotation and zoom functions which were associated with improving user confidence particularly when carrying out more challenging tasks.
Conclusions
This study reveals that older adults using guidetomeasure-3D achieved improved levels of accuracy and efficiency along with improved satisfaction and increased levels of confidence compared with the 2D paper-based equivalent. These results are significant and promising for overcoming HEFAP equipment abandonment issue. Furthermore they constitute an important step towards overcoming challenges associated with older adult patients, the digitisation of healthcare, and realising the enablement of patient self-care and management via the innovative use of mobile technologies. Numerous opportunities for the generalisability and transferability of the findings of this research are also proposed. Future research will explore the extent to which mobile 3D visualisation technologies may be utilised to optimise the clinical utility of HEFAP when deployed by clinicians.}
}
@article{LEHIKKO2024100066,
title = {Exploring interactivity effects on learners’ sense of agency, cognitive load, and learning outcomes in immersive virtual reality: A mixed methods study},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100066},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100066},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000163},
author = {Anu Lehikko and Mikko Nykänen and Kristian Lukander and Jose Uusitalo and Heli Ruokamo},
keywords = {Immersive virtual reality, Interactivity, Sense of agency, Cognitive load, Learning Outcomes},
abstract = {This study explored the effects of IVR interactivity on learners’ sense of agency, cognitive load, and learning outcomes. The research questions were: 1. “How does interactivity influence the learners’ sense of agency?” and 2. “How does interactivity influence the learners’ cognitive load and learning outcomes?” A mixed-methods experimental design was applied. Safety training interventions, including individually performed IVR scenarios, were held for 68 participants in groups of two to four persons in two work organizations. Single- and repeated-measure questionnaires were the main source of the quantitative data. Qualitative data collection by video recordings and stimulated recall interviews was carried out on 23 persons in total. The results indicate that high interactivity enables a stronger sense of agency for the learners and yields learning benefits by supporting generative cognitive processing. Based on the results, interactivity and learner involvement may be particularly important for achieving affective training goals. Considering the sociocultural and individual factors in training design and pre-briefing the learners are also recommended.}
}
@article{EVANGELISTA20231470,
title = {Advanced visualization of ergonomic assessment data through industrial Augmented Reality},
journal = {Procedia Computer Science},
volume = {217},
pages = {1470-1478},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.346},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922024310},
author = {Alessandro Evangelista and Vito Modesto Manghisi and Sara Romano and Vito {De Giglio} and Lorenzo Cipriani and Antonio Emmanuele Uva},
keywords = {Ergonomics, Augmented Reality, RULA, Human-Centered, Kinect v2, Hololens 2},
abstract = {The industrial transition to the 4.0 paradigm defines new scenarios in which the operator plays a central role within the industrial ecosystem. Thanks to the enabling technologies of Industry 4.0, it is possible to effectively improve operators' working conditions by applying the Human-Centered approach. Nowadays, one of the main challenges is to reduce work-related musculoskeletal disorders resulting from ergonomically incorrect working conditions in order to prevent the occurrence of occupational diseases. To this end, we developed a software tool that leverages a low-cost D-RGB camera (Kinect v2) to track the human body and an Augmented Reality (AR) visualization system based on Microsoft HoloLens 2. The tool assesses postural ergonomic risk in real-time according to the Rapid Upper Limb Assessment (RULA) metric. The proposed AR application allows a three-dimensional visualization of postures, which can be observed directly superimposed on the operator's body in the real scene. This approach aims to optimize the understanding of postures by creating a link between real information (operator's body) and virtual information (virtual skeleton, RULA score, and angles) by providing a simple and immediate user interface for ergonomists.}
}
@article{HUBAL2006532,
title = {Informed consent procedures: An experimental test using a virtual character in a dialog systems training application},
journal = {Journal of Biomedical Informatics},
volume = {39},
number = {5},
pages = {532-540},
year = {2006},
note = {Dialog Systems for Health Communications},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2005.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1532046405001401},
author = {Robert C. Hubal and Ruth S. Day},
keywords = {Informed consent procedures, Intelligent agents, Discourse processes, Effectiveness evaluation, Virtual reality},
abstract = {Researchers are generally trained to administer informed consent by studying approved guidelines, but still can fail to satisfactorily answer questions from potential participants. An application using a virtual character allowed novice participants to practice administering informed consent. This character was designed to behave as a potential participant for a study and asked many of the questions research participants typically ask, such as queries about the study itself, the sponsor, timing, selection procedures, confidentiality, voluntariness, benefits and risks, and contact information. The user responded to the character’s queries as if speaking with a true potential research participant. The application was effective even after only brief usage. In a laboratory experiment, novice participants who practiced with the virtual character were later more effective in conducting informed consent interviews with a human interviewee than those who were trained only with written materials. Thus, simulated learning-by-doing improved informed consent skills. Implications for related health dialog applications are discussed.}
}
@article{BIRRELL2022103843,
title = {Urban air mobility infrastructure design: Using virtual reality to capture user experience within the world's first urban airport},
journal = {Applied Ergonomics},
volume = {105},
pages = {103843},
year = {2022},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103843},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022001661},
author = {Stewart Birrell and William Payre and Katie Zdanowicz and Paul Herriotts},
keywords = {Urban air mobility, User experience, Virtual reality, User-centred design, Human factors},
abstract = {Human factors research can play an important role in the successful design of infrastructure to support future mobility. Through engaging users and stakeholders early in the design process we can gain insights before the physical environments are built. This paper presents data from a truly novel application of Virtual Reality (VR), where user experience and wayfinding were evaluated within an emerging future transport infrastructure to support urban air mobility (UAM) – the urban airport (aka vertiports). Urban airports are located in city centres where drones or ‘flying cars’ would land and take off from. Previous quantitative studies have investigated passenger experience in traditional airports using field observation and surveys, but this paper is the first to present qualitative research on user experience in this emerging mobility infrastructure using an immersive VR environment. Twenty participants completed a series of six scenarios aimed at understanding customer ‘exciters’ and ‘pain points’ within an urban airport. Results and recommendations from this empirical research will help inform the design of all future mobility infrastructure solutions, through improving user experience before the infrastructure is physically deployed. Finally, this paper highlights the benefits of engaging users at an early stage of the design process to ensure that future transport infrastructure will be accessible, easy to navigate and a pleasure to use.}
}
@article{GOERTZEL2014158,
title = {A Software Architecture for Generally Intelligent Humanoid Robotics},
journal = {Procedia Computer Science},
volume = {41},
pages = {158-163},
year = {2014},
note = {5th Annual International Conference on Biologically Inspired Cognitive Architectures, 2014 BICA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.11.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914015440},
author = {Ben Goertzel and David Hanson and Gino Yu},
keywords = {Consciousness, Information Integration, Global Workspace, Nonlinear Dynamics},
abstract = {This paper summarizes the authors’ thinking regarding the design of a software framework for interfacing between general-intelligence-oriented software systems and complex mobile robots, including humanoid robots. The framework describes incorporates perception synthesis, action orchestration, and high level control, and is designed to effectively leverage existing relevant software frameworks such as ROS and Blender. An initial case study motivating this work is the use of the OpenCog AGI (Artificial General Intelligence) software framework to help control humanoid robots created by Hanson Robotics.}
}
@article{PINHEIRO2020102418,
title = {Mutating code annotations: An empirical evaluation on Java and C# programs},
journal = {Science of Computer Programming},
volume = {191},
pages = {102418},
year = {2020},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2020.102418},
url = {https://www.sciencedirect.com/science/article/pii/S0167642320300290},
author = {Pedro Pinheiro and José Carlos Viana and Márcio Ribeiro and Leo Fernandes and Fabiano Ferrari and Rohit Gheyi and Baldoino Fonseca},
keywords = {Mutation testing, Code annotations, Mining bugs},
abstract = {Mutation testing injects code changes to check whether tests can detect them. Mutation testing tools use mutation operators that modify program elements such as operators, names, and entire statements. Most existing mutation operators focus on imperative and object-oriented language constructs. However, many current projects use meta-programming through code annotations. In a previous work, we have proposed nine mutation operators for code annotations focused on the Java programming language. In this article, we extend our previous work by mapping the operators to the C# language. Moreover, we enlarge the empirical evaluation. In particular, we mine Java and C# projects that make heavy use of annotations to identify annotation-related faults. We analyzed 200 faults and categorized them as “misuse,” when the developer did not appear to know how to use the code annotations properly, and “wrong annotation parsing” when the developer incorrectly parsed annotation code (by using reflection, for example). Our operators mimic 95% of the 200 mined faults. In particular, three operators can mimic 82% of the faults in Java projects and 84% of the faults in C# projects. In addition, we provide an extended and improved repository hosted on GitHub with the 200 code annotation faults we analyzed. We organize the repository according to the type of errors made by the programmers while dealing with code annotations, and to the mutation operator that can mimic the faults. Last but not least, we also provide a mutation engine, based on these operators, which is publicly available and can be incorporated into existing or new mutation tools. The engine works for Java and C#. As implications for practice, our operators can help developers to improve test suites and parsers of annotated code.}
}
@article{HOU2023106066,
title = {HINNet: Inertial navigation with head-mounted sensors using a neural network},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106066},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106066},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623002506},
author = {Xinyu Hou and Jeroen H.M. Bergmann},
keywords = {Machine learning, Inertial navigation, Pedestrian Dead Reckoning, Deep neural network, Inertial measurement unit, Wearable sensors},
abstract = {Human inertial navigation systems have been developing rapidly in recent years, and it has shown great potential for applications within healthcare, smart homes, sports, and emergency services. Placing inertial measurement units on the head for localisation is relatively new. However, it provides a very interesting option, as there are several everyday head-worn items that could easily be equipped with sensors. Yet, there remains a lack of research in this area and currently no localisation solutions have been offered that allow for free head-rotations during long periods of walking. To solve this problem, we present HINNet, the first deep neural network (DNN) pedestrian inertial navigation system allowing free head movements with head-mounted inertial measurement units (IMUs), which deploys a 2-layer bi-directional LSTM. A new ’peak ratio’ feature is introduced and utilised as part of the input to the neural network. This information can be leveraged to solve the issue of differentiating between changes in movements related to the head and those that are associated with the walking pattern. A dataset with 8 subjects totalling 528 min has been collected on three different tracks for training and verification. The HINNet could effectively distinguish head rotations and changes in walking direction with a distance percentage error of 0.46%, a relative trajectory error of 3.88 m, and a absolute trajectory error of 5.98 m, which outperforms the current best head-mounted Pedestrian Dead Reckoning (PDR) method.}
}
@article{KASURINEN2017341,
title = {Usability Issues of Virtual Reality Learning Simulator in Healthcare and Cybersecurity},
journal = {Procedia Computer Science},
volume = {119},
pages = {341-349},
year = {2017},
note = {6th International Young Scientist Conference on Computational Science, YSC 2017, 01-03 November 2017, Kotka, Finland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.193},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917324031},
author = {Jussi Kasurinen},
keywords = {Virtual reality, usability, user experience, case study, cybersecurity},
abstract = {The virtualization and the digital environments are common learning platforms in several different domains, such as in flying airplanes or controlling nuclear power plants. However, virtual reality is no longer expensive special hardware; the basic installations for virtual and augmented reality can be done within household budgets and with common customer products. In this paper, we study the aspect of usability issues in a scenario, where a new virtual learning environment is built to teach correct prevention mechanics and strategies against common physical and cybersecurity threats in healthcare, namely in a hospital. Our proof-of-concept studies indicate that the concept is functional and that on hardware level components exist. The problems are in the usability and user immersion aspects, which are discussed in this paper and further studied in the proposed research setting.}
}
@article{HASENBEIN2022107282,
title = {Learning with simulated virtual classmates: Effects of social-related configurations on students’ visual attention and learning experiences in an immersive virtual reality classroom},
journal = {Computers in Human Behavior},
volume = {133},
pages = {107282},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2022.107282},
url = {https://www.sciencedirect.com/science/article/pii/S0747563222001042},
author = {Lisa Hasenbein and Philipp Stark and Ulrich Trautwein and Anna Carolina Muller Queiroz and Jeremy Bailenson and Jens-Uwe Hahn and Richard Göllner},
keywords = {Immersive virtual reality, Classroom simulation, Peer effects, Visual attention, Network analysis, Eye-tracking},
abstract = {Immersive virtual reality (IVR) provides great potential to experimentally investigate effects of peers on student learning in class and to strategically deploy virtual peer learners to improve learning. The present study examined how three social-related classroom configurations (i.e., students' position in the classroom, visualization style of virtual avatars, and virtual classmates' performance-related behavior) affect students' visual attention toward information presented in the IVR classroom using a large-scale eye-tracking data set of N = 274 sixth graders. ANOVA results showed that the IVR configurations were systematically associated with differences in learners' visual attention on classmates or the instructional content and their overall gaze distribution in the IVR classroom (Cohen's d ranging from 0.28 to 2.04 for different IVR configurations and gaze features). Gaze-based attention on classmates was negatively related to students' interest in the IVR lesson (d = 0.28); specifically, the more boys were among the observed peers, the lower students' situational self-concept (d = 0.24). In turn, gaze-based attention on the instructional content was positively related to students' performance after the IVR lesson (d = 0.26). Implications for the future use of IVR classrooms in educational research and practice are discussed.}
}
@article{LINAKER201817,
title = {Motivating the contributions: An Open Innovation perspective on what to share as Open Source Software},
journal = {Journal of Systems and Software},
volume = {135},
pages = {17-36},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302169},
author = {J. Linåker and H. Munir and K. Wnuk and C.E. Mols},
keywords = {Open innovation, Open Source Software, Software ecosystem, Contribution strategy, Product planning, Product strategy},
abstract = {Open Source Software (OSS) ecosystems have reshaped the ways how software-intensive firms develop products and deliver value to customers. However, firms still need support for strategic product planning in terms of what to develop internally and what to share as OSS. Existing models accurately capture commoditization in software business, but lack operational support to decide what contribution strategy to employ in terms of what and when to contribute. This study proposes a Contribution Acceptance Process (CAP) model from which firms can adopt contribution strategies that align with product strategies and planning. In a design science influenced case study executed at Sony Mobile, the CAP model was iteratively developed in close collaboration with the firm’s practitioners. The CAP model helps classify artifacts according to business impact and control complexity so firms may estimate and plan whether an artifact should be contributed or not. Further, an information meta-model is proposed that helps operationalize the CAP model at the organization. The CAP model provides an operational OI perspective on what firms involved in OSS ecosystems should share, by helping them motivate contributions through the creation of contribution strategies. The goal is to help maximize return on investment and sustain needed influence in OSS ecosystems.}
}
@article{GILL2024100070,
title = {Implementing Universal Design through augmented-reality game-based learning},
journal = {Computers & Education: X Reality},
volume = {4},
pages = {100070},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000205},
author = {Amarpreet Gill and Derek Irwin and Dave Towey and Yanhui Zhang and Pinzhuang Long and Linjing Sun and Wanling Yu and Yaxin Zheng},
keywords = {Augmented reality, Digital game-based learning, Microlearning, Collaborative learning, Engineering education, Educational technology},
abstract = {Technology integration in higher education (HE) provides educators with the opportunity to design stimulating learning environments, especially in design and engineering education (DEE) where it plays a unique role in nurturing creativity, problem-solving and innovation. This study investigates the integration of an augmented reality (AR) educational game in DEE, focusing on the teaching of Design for Manufacturing and Assembly (DfMA) principles. The primary aim of this research is to enhance traditional DfMA teaching and learning (T&L) practices by applying innovative T&L strategies. The resulting AR DfMA game, developed using digital game-based learning, microlearning and collaborative learning techniques, aligns with the Universal Design for Learning framework to create an inclusive learning environment that encourages participation from all students and supports several learning styles. The study tests the individual features designed for personal study opportunities to discover which elements are optimal in that environment, and then on the proposed in-class group components of the AR game, involving 68 participants from the appropriate program, though it is not currently implemented as part of a module. We utilize a mixed methods approach to examine the students’ experience and identify key design features that contribute to an inclusive educational experience. The findings highlight positive student experiences, and preferences for hands-on engagement, multimodal content, and collaborative activities. The AR DfMA game also has the potential to enhance intrinsic motivation and create an active and inclusive learning environment. Challenges and areas for improvement are also discussed.}
}
@article{LANG2019118,
title = {Mixed reality in production and logistics: Discussing the application potentials of Microsoft HoloLensTM},
journal = {Procedia Computer Science},
volume = {149},
pages = {118-129},
year = {2019},
note = {ICTE in Transportation and Logistics 2018 (ICTE 2018)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.115},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930122X},
author = {Sebastian Lang and Mohammed Saif Sheikh {Dastagir Kota} and David Weigert and Fabian Behrendt},
keywords = {Mixed Reality, Microsoft HoloLens, Production, Logistics, Manufacturing},
abstract = {In 2016, Microsoft released the mixed reality (MR) head-mounted display (HMD) HoloLensTM. As augmented reality (AR) devices, the HoloLensTM can enrich the user’s perceived environment with virtual information. However, Microsoft does not consider the HoloLensTM as AR device, but as the first MR device, since it has some additional features compared to competing AR devices. Especially to mention is the possibility to interact with virtual objects and vice versa. This paper shall provide an insight about the application potentials of MR in the field of production and logistics. For this purpose, we present the findings of a literature review about the current state of research concerning the application of Microsoft HoloLensTM in the field of production and logistics. Furthermore, we present a small HoloLensTM application, which we have developed to evaluate the capabilities of the device. Several persons tested our application and gave us feedback concerning the utility and usability of the HoloLensTM. We provide an evaluation of the user experiences at the end of the paper.}
}
@article{KRAUSEGLAU2022107007,
title = {Collaborative program comprehension via software visualization in extended reality},
journal = {Information and Software Technology},
volume = {151},
pages = {107007},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107007},
url = {https://www.sciencedirect.com/science/article/pii/S095058492200132X},
author = {Alexander Krause-Glau and Malte Hansen and Wilhelm Hasselbring},
keywords = {Program comprehension, Software visualization, City metaphor, Extended reality, Virtual reality, Augmented reality},
abstract = {Context:
In software visualization research, various approaches strive to create immersive environments by employing extended reality devices. In that context, only few research has been conducted on the effect of collaborative, i.e., multi-user, extended reality environments.
Objective:
We present our journey toward a web-based approach to enable (location-independent) collaborative program comprehension using desktop, virtual reality, and mobile augmented reality devices.
Method:
We designed and implemented three multi-user modes in our web-based live trace visualization tool ExplorViz. Users can employ desktop, mobile, and virtual reality devices to collaboratively explore software visualizations. We conducted two preliminary user studies in which subjects evaluated our VR and AR modes after solving common program comprehension tasks.
Results:
The VR and AR environments can be suitable for collaborative work in the context of program comprehension. The analyzed feedback revealed problems regarding the usability, e.g., readability of visualized entities and performance issues. Nonetheless, our approach can be seen as a blueprint for other researchers to replicate or build upon these modes and results.
Conclusions:
ExplorViz’s multi-user modes are our approach to enable heterogeneous collaborative software visualizations. The preliminary results indicate the need for more research regarding effectiveness, usability, and acceptance. Unlike related work, we approach the latter by introducing a multi-user augmented reality environment for software visualizations based on off-the-shelf mobile devices.}
}
@article{MANTZIOU2015241,
title = {Do Children in the Spectrum of Autism Interact with Real-time Emotionally Expressive Human Controlled Avatars?},
journal = {Procedia Computer Science},
volume = {67},
pages = {241-251},
year = {2015},
note = {Proceedings of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.268},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915031142},
author = {Olga Mantziou and Ioannis Vrellis and Tassos A. Mikropoulos},
keywords = {Autism Spectrum Disorder, Facial Emotion Recognition, low and high functioning autism, avatar},
abstract = {Children in Autism Spectrum Disorder (ASD) are characterized by impairments in social skills and they usually face difficulties in recognizing facial emotion expressions. Early intervention and treatment is of major concern and therefore a number of detection and teaching methods have been developed, including the use of ICT. This article presents a brief but extensive literature review on the way tutors are represented in digital environments. The results showed that there is a need for further investigation on the effectiveness of the methods used for the interaction of children on ASD with technology tools, as well as on knowledge transfer. Since there is a lack of empirical data concerning the preference of different interaction modalities by children with ASD, this article also reports on an exploratory study conducted to investigate the acceptance and preference of three different real-time modalities used in facial emotion recognition by two children with ASD (low and high functioning autism). The results indicated a discrepancy between the two children which can be mainly attributed to the differences accompanied the categorization of children with ASD in low and high functioning autism.}
}
@article{DAVID2019646,
title = {Development of Escape Room Game using VR Technology},
journal = {Procedia Computer Science},
volume = {157},
pages = {646-652},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311421},
author = {David David and  Edwin and Edward Arman and  Hikari and Natalia Chandra and Nadia Nadia},
keywords = {Virtual Reality, Presence, Prototype, Unity, Samsung Gear VR},
abstract = {Escape room is one of the media games that can improve the logic of thinking. Puzzles in the escape room traditionally have disadvantages because the type of puzzle that is made requires a lot of material. The purpose of this research is to produce a game with Escape Room as the basic theme with Virtual Reality technology. Virtual Reality technology is used to develop presence in users, attendance is about the intimacy of users with the gaming world. By using Virtual Reality, the puzzle elements that are created can be replaced regularly without the need to change the building’s skeleton. The development method used is a prototype model using Unity game machines. The research method was carried out using a questionnaire for user analysis. The application generated from this research is the Escape Room VR game that can be played on an Android smartphone that is compatible with Samsung Gear VR. The application can be used as an additional means for traditional Escape Room games.}
}
@article{SADEGHIESFAHLANI2018150,
title = {Validity of the Kinect and Myo armband in a serious game for assessing upper limb movement},
journal = {Entertainment Computing},
volume = {27},
pages = {150-156},
year = {2018},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1875952117300952},
author = {Shabnam {Sadeghi Esfahlani} and Bogdan Muresan and Alireza Sanaei and George Wilson},
keywords = {Kinect v2, Monte Carlo Tree Search (MCTS), Myo armband, FootPedal},
abstract = {A cost-effective, easily-accessible neuro-motor rehabilitation solution is proposed that can determine the range of motion and the kinematic ability of participants. A serious game comprising four-scenarios are developed in which the players control an avatar that mirrors the rotations of the upper-limb joints through multi-channel-input devices (Kinect, Myo, FootPedal). Administered functional reach tests (FRT) challenge the player to interact with a 3D-environment while standing or sitting and using the FootPedal which simulates the action of walking whilst body movement is measured concurrently. The FRT’s complexity level is adapted using a Monte Carlo Tree Search algorithm which determines a virtual object’s position based on the proved ability of the user. Twenty-three volunteers were recruited to play the game in 45-min sessions. The data show that the system has a more positive impact on players performance and is more motivating than formal therapy. The visual representation of the trajectory of the objects is shown to increase the perception of the participants voluntary/involuntary upper extremity movement, and the results show a comparable inter-session reliability (acceptable-good) over two repeated sessions. A high Pearson correlation demonstrates the validity of using Kinect and Myo devices in assessing upper-limb rehabilitation, and the timing and the clinically relevant movement data have a higher accuracy when the devices are paired.}
}
@article{FUENTESREYES202374,
title = {A 2D/3D multimodal data simulation approach with applications on urban semantic segmentation, building extraction and change detection},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {205},
pages = {74-97},
year = {2023},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2023.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S092427162300254X},
author = {Mario {Fuentes Reyes} and Yuxing Xie and Xiangtian Yuan and Pablo d’Angelo and Franz Kurz and Daniele Cerra and Jiaojiao Tian},
keywords = {3D change detection, Building extraction, Urban semantic segmentation, Synthetic datasets},
abstract = {Advances in remote sensing image processing techniques have further increased the demand for annotated datasets. However, preparing annotated multi-temporal 2D/3D multimodal data is especially challenging, both for the increased costs of the annotation step and the lack of multimodal acquisitions available on the same area. We introduce the Simulated Multimodal Aerial Remote Sensing (SMARS) dataset, a synthetic dataset aimed at the tasks of urban semantic segmentation, change detection, and building extraction, along with a description of the pipeline to generate them and the parameters required to set our rendering. Samples in the form of orthorectified photos, digital surface models and ground truth for all the tasks are provided. Unlike existing datasets, orthorectified images and digital surface models are derived from synthetic images using photogrammetry, yielding more realistic simulations of the data. The increased size of SMARS, compared to available datasets of this kind, facilitates both traditional and deep learning algorithms. Reported experiments from state-of-the-art algorithms on SMARS scenes yield satisfactory results, in line with our expectations. Both benefits of the SMARS datasets and constraints imposed by its use are discussed. Specifically, building detection on the SMARS-real Potsdam cross-domain test demonstrates the quality and the advantages of proposed synthetic data generation workflow. SMARS is published as an ISPRS benchmark dataset and can be downloaded from https://www2.isprs.org/commissions/comm1/wg8/benchmark_smars/.}
}
@article{RYDZEWSKI201722,
title = {A distributed system for conducting chess games in parallel},
journal = {Procedia Computer Science},
volume = {119},
pages = {22-29},
year = {2017},
note = {6th International Young Scientist Conference on Computational Science, YSC 2017, 01-03 November 2017, Kotka, Finland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917323669},
author = {Aleksander Rydzewski and Paweł Czarnul},
keywords = {chess engine, Universal Chess Interface protocol, Elo ranking, cloud computing, loosely coupled architecture},
abstract = {This paper proposes a distributed and scalable cloud based system designed to play chess games in parallel. Games can be played between chess engines alone or between clusters created by combined chess engines. The system has a built-in mechanism that compares engines, based on Elo ranking which finally presents the strength of each tested approach. If an approach needs more computational power, the design of the system allows it to scale. The system was designed using a loosely coupled architecture approach and the master-slave pattern. It works under Unix or MacOS operating systems. In order to split chess engine processing between every CPU in the system the Akka technology with the Scala language was used while the other part was written in Java. We tested many free chess engines connected to the system by the UCI protocol supported by the proposed system. CloudAMQP is an implementation of Advanced Message Queue Protocol and was used as a message-oriented middleware. This layer was created to split games between every available processing node connected to the system. This element also contributes to greater fault tolerance. We present results of games played between many available chess engines.}
}
@article{MATHEWS20167,
title = {A Virtual Reality Environment for Rehabilitation of Prospective Memory in Stroke Patients},
journal = {Procedia Computer Science},
volume = {96},
pages = {7-15},
year = {2016},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 20th International Conference KES-2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.081},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916318695},
author = {Moffat Mathews and Antonija Mitrovic and Stellan Ohlsson and Jay Holland and Audrey McKinley},
keywords = {virtual reality environment, rehabilitation of prospective memory, evaluation},
abstract = {Prospective Memory (PM), or remembering to perform actions in the future, is of crucial importance for everyday life. This kind of memory is often impaired in stroke survivors and can interfere with independent living. We have developed a computer-based treatment which uses visual imagery to teach participants how to remember time- and event-based prospective memory tasks better. After the treatment, participants practiced their PM skills using videos first, and later in a Virtual Reality (VR) environment. The VR environment uses Constraint-Based Modeling (CBM) to track the user actions and provide individual feedback. We report on a study with 15 stroke survivors, which shows that our treatment is highly effective.}
}
@article{KARABELNIKOVA2021414,
title = {Virtual partner dance as a paradigm for empirical study of cognitive models of emotional intelligence},
journal = {Procedia Computer Science},
volume = {190},
pages = {414-433},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.050},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012953},
author = {Yuliana Karabelnikova and Alexei V. Samsonovich},
keywords = {emotional intelligence, cognitive model, social virtual agent, affective computing, moral schema},
abstract = {This work presents a new paradigm for empirical study of cognitive models of emotional intelligence and preliminary results obtained within it. Among computational approaches to modeling human emotions, the eBICA cognitive architecture takes a special place, because it provides a framework for unification of cognitive and physiological models of emotional intelligence, dimensional and componential representations of emotions, and more. Selected paradigm involves spontaneous interactions among three dancing avatars, one or two of which can be controlled by human participants. Available behaviors include selection of a partner, changing facial expressions and dance patterns. Several types of virtual dance partners are implemented based on the eBICA model: “timid”, “ringleader”, “dancer”, “naïve”, and intermediate cases. Simulations and empirical data show that the selected approach is useful and can potentially lead to a socially attractive, emotionally intelligent behavior.}
}
@article{VANKIPURAM2011432,
title = {Toward automated workflow analysis and visualization in clinical environments},
journal = {Journal of Biomedical Informatics},
volume = {44},
number = {3},
pages = {432-440},
year = {2011},
note = {Biomedical Complexity and Error},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2010.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S1532046410000845},
author = {Mithra Vankipuram and Kanav Kahol and Trevor Cohen and Vimla L. Patel},
keywords = {Clinical workflow, Complex systems, Medical errors, Radio identification, Virtual reality, Visualization},
abstract = {Lapses in patient safety have been linked to unexpected perturbations in clinical workflow. The effectiveness of workflow analysis becomes critical to understanding the impact of these perturbations on patient outcome. The typical methods used for workflow analysis, such as ethnographic observations and interviewing, are limited in their ability to capture activities from different perspectives simultaneously. This limitation, coupled with the complexity and dynamic nature of clinical environments makes understanding the nuances of clinical workflow difficult. The methods proposed in this research aim to provide a quantitative means of capturing and analyzing workflow. The approach taken utilizes recordings of motion and location of clinical teams that are gathered using radio identification tags and observations. This data is used to model activities in critical care environments. The detected activities can then be replayed in 3D virtual reality environments for further analysis and training. Using this approach, the proposed system augments existing methods of workflow analysis, allowing for capture of workflow in complex and dynamic environments. The system was tested with a set of 15 simulated clinical activities that when combined represent workflow in trauma units. A mean recognition rate of 87.5% was obtained in automatically recognizing the activities.}
}
@article{PENTANGELO2024107512,
title = {SENEM: A software engineering-enabled educational metaverse},
journal = {Information and Software Technology},
volume = {174},
pages = {107512},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107512},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001174},
author = {Viviana Pentangelo and Dario {Di Dario} and Stefano Lambiase and Filomena Ferrucci and Carmine Gravino and Fabio Palomba},
keywords = {Metaverse engineering, Virtual learning environments, Human-centered studies, Software engineering in practice},
abstract = {Context:
The term metaverse refers to a persistent, virtual, three-dimensional environment where individuals may communicate, engage, and collaborate. One of the most multifaceted and challenging use cases of the metaverse is education, where educators and learners may require multiple technical, social, psychological, and interaction instruments to accomplish their learning objectives. While the characteristics of the metaverse might nicely fit the problem’s needs, our research points out a noticeable lack of knowledge into (1) the specific requirements that an educational metaverse should actually fulfill to let educators and learners successfully interact towards their objectives and (2) how to design an appropriate educational metaverse for both educators and learners.
Objective:
In this paper, we aim to bridge this knowledge gap by proposing SENEM, a novel software engineering-enabled educational metaverse. We first elicit a set of functional requirements that an educational metaverse should fulfill.
Method:
In this respect, we conduct a literature survey to extract the currently available knowledge on the matter discussed by the research community, and afterward, we assess and complement such knowledge through semi-structured interviews with educators and learners. Upon completing the requirements elicitation stage, we then build our prototype implementation of SENEM, a metaverse that makes available to educators and learners the features identified in the previous stage. Finally, we evaluate the tool in terms of learnability, efficiency, and satisfaction through a Rapid Iterative Testing and Evaluation research approach, leading us to the iterative refinement of our prototype.
Results:
Through our survey strategy, we extracted nine requirements that guided the tool development that the study participants positively evaluated.
Conclusion:
Our study reveals that the target audience appreciates the elicited design strategy. Our work has the potential to form a solid contribution that other researchers can use as a basis for further improvements.}
}
@article{OLAVERRIMONREAL2019880,
title = {Collaborative approach for a safe driving distance using stereoscopic image processing},
journal = {Future Generation Computer Systems},
volume = {95},
pages = {880-889},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.050},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17307124},
author = {Cristina Olaverri-Monreal and Gerd Ch. Krizek and Florian Michaeler and Rene Lorenz and Matthias Pichler},
keywords = {Cooperative systems, Tailgating, Safety distance, Image capturing},
abstract = {Disregard for the rules regarding the minimum safety distance can make the avoidance of a rear-end collision nearly impossible. In a joint effort to enhance safety and improve the decision making processes on an individual level, we contribute to the state of the art with an innovative and affordable system that identifies vehicles and provides a rear-end distance warning system capable of recognizing dangerous situations, and which can also inform other vehicles of the danger, independent of their communication capabilities or equipment. Vision sensors garner information through the stereoscopic capturing and processing of images by rear cameras to calculate the distance between the leading and following vehicles. Visual data related to the safety distance is provided to the following vehicle in real-time, relying on an asynchronous collaborative process. A detailed error analysis of the distance calculation is provided based on the measurement procedure and roadway geometry. Relying on the communication between the two vehicles, an in-vehicle system was compared to the rear-mounted distance warning system under lab-controlled conditions. Both human–machine interaction paradigms were evaluated in terms of their impact on driver response. Results showed that both systems influenced the driver in keeping a time gap of two seconds.}
}
@article{FAN2021501,
title = {Detection of scene-irrelevant head movements via eye-head coordination information},
journal = {Virtual Reality & Intelligent Hardware},
volume = {3},
number = {6},
pages = {501-514},
year = {2021},
note = {Special Issue on Locomotion Perception and Redirection},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2021.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2096579621000917},
author = {Xiaoxiong Fan and Yun Cai and Yufei Yang and Tianxing Xu and Yike Li and Songhai Zhang and Fanglue Zhang},
keywords = {Virtual reality, Human-centered computing, Human-computer interaction (HCI), Interaction paradigms, HCI design and evaluation methods, User models},
abstract = {Background
Accurate motion tracking in head-mounted displays (HMDs) has been widely used in immersive VR interaction technologies. However, tracking the head motion of users at all times is not always desirable. During a session of HMD usage, users may make scene-irrelevant head rotations, such as adjusting the head position to avoid neck pain or responding to distractions from the physical world. To the best of our knowledge, this is the first study that addresses the problem of scene-irrelevant head movements.
Methods
We trained a classifier to detect scene-irrelevant motions using temporal eyehead-coordinated information sequences. To investigate the usefulness of the detection results, we propose a technique to suspend motion tracking in HMDs where scene-irrelevant motions are detected.
Results/Conclusions
Experimental results demonstrate that the scene-relevancy of movements can be detected using eye-head coordination information, and that ignoring scene-irrelevant head motions in HMDs improves user continuity without increasing sickness or breaking immersion.}
}
@article{ESCUDEIRO2015252,
title = {Virtual Sign – A Real Time Bidirectional Translator of Portuguese Sign Language},
journal = {Procedia Computer Science},
volume = {67},
pages = {252-262},
year = {2015},
note = {Proceedings of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.269},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915031154},
author = {Paula Escudeiro and Nuno Escudeiro and Rosa Reis and Jorge Lopes and Marcelo Norberto and Ana Bela Baltasar and Maciel Barbosa and José Bidarra},
keywords = {Portuguese Sign Language, Education, Deaf, Kinect, Sensor Gloves, Translator, Machine Learning, OpenCv, Support Vector Machines, Dynamic Time Warping, Avatar.},
abstract = {Promoting equity, equal opportunities to all and social inclusion of people with disabilities is a concern of modern societies at large and a key topic in the agenda of European Higher Education. Despite all the progress, we cannot ignore the fact that the conditions provided by the society for the deaf are still far from being perfect. The communication with deaf by means of written text is not as efficient as it might seem at first. In fact, there is a very deep gap between sign language and spoken/written language. The vocabulary, the sentence construction and the grammatical rules are quite different among these two worlds. These facts bring significant difficulties in reading and understanding the meaning of text for deaf people and, on the other hand, make it quite difficult for people with no hearing disabilities to understand sign language. The deployment of tools to assist the daily communication, in schools, in public services, in museums and other, between deaf people and the rest may be a significant contribution to the social inclusion of the deaf community. The work described in this paper addresses the development of a bidirectional translator between Portuguese Sign Language and Portuguese text. The translator from sign language to text resorts to two devices, namely the Microsoft Kinect and 5DT Sensor Gloves in order to gather data about the motion and shape of the hands. The hands configurations are classified using Support Vector Machines. The classification of the movement and orientation of the hands are achieved through the use of Dynamic Time Warping algorithm. The translator exhibits a precision higher than 90%. In the other direction, the translation of Portuguese text to Portuguese Sign Language is supported by a 3D avatar which interprets the entered text and performs the corresponding animations.}
}
@article{OLIVEIRA2012274,
title = {Serious Game in Security: A Solution for Security Trainees},
journal = {Procedia Computer Science},
volume = {15},
pages = {274-282},
year = {2012},
note = {4th International Conference on Games and Virtual Worlds for Serious Applications(VS-GAMES’12)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.10.079},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912008411},
author = {Vítor Oliveira and António Coelho and Rui Guimarães and Carlos Rebelo},
keywords = {Serious Game, Security, Safety, Multiplayer, Cooperation},
abstract = {Serious games have been used with success for training field operatives in tasks where there is a danger of injury or life threatening situations. This paper presents the development of a serious game aimed at the areas of security and safety, supporting the training of specialists through supervised situational scenarios. DDThe training plans involve security against third parties, focusing on social level security at a corporate level, and also safety actions on events such as floods and fires in buildings/facilities. The game provides a 3D virtual environment of the real location/facility to be secured and a multiplayer platform to allow collaborative training and supervising.DD.}
}
@article{RIVERAFLOR2019641,
title = {Evaluation of Task Workload and Intrinsic Motivation in a Virtual Reality Simulator of Electric-Powered Wheelchairs},
journal = {Procedia Computer Science},
volume = {160},
pages = {641-646},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931734X},
author = {Hamilton Rivera-Flor and Kevin A. Hernandez-Ossa and Berthil Longo and Teodiano Bastos},
keywords = {Electric-Powered Wheelchair, Driving Training, Virtual Reality Simulator},
abstract = {For some people with severe physical disabilities, the immediate driving of an Electric-Powered Wheelchair (EPW) appears as a safety problem, which can be solved by the use of a virtual reality (VR) simulator for safe-driving learning purposes. There are several VR environment approaches in the literature applied to EPW driving training, including several tests that were performed to validate these simulators. This work evaluates the influence of different display devices on task workload and intrinsic motivation of participants, using the Simcadrom EPW Simulator developed at UFES/Brazil. Results from two qualitative tests: Intrinsic Motivation Inventory (IMI) and NASA Task Load Index (NASA-TLX) were compared for three displays (Head Mounted Display – HMD, desktop screen, and video projector). The results show that the HMD provided the highest usefulness score. On the other hand, the desktop screen reported the lowest task workload.}
}
@article{ZHAO2022101828,
title = {Comparing self-navigation and video mode in a choice experiment to measure public space preferences},
journal = {Computers, Environment and Urban Systems},
volume = {95},
pages = {101828},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2022.101828},
url = {https://www.sciencedirect.com/science/article/pii/S0198971522000722},
author = {Yuwen Zhao and Pauline E.W. {van den Berg} and Ioulia V. Ossokina and Theo A. Arentze},
keywords = {3D dynamic visualization, Stated choice experiment, Neighborhood public spaces, Virtual environment, Self-navigation},
abstract = {3D dynamic visualization technologies are increasingly used in studying residents' preferences for urban planning and design scenarios. The techniques help concentrate respondent attention and improve the measurement quality of environmental preferences. However, little is known about differences in measurement quality between different 3D dynamic visualization modes. This paper applies two modes – a self-navigation mode and a video mode - in a virtual environment-based stated choice experiment with the aim of measuring neighborhood public spaces preferences. Based on data from 276 experiment participants and applying conditional and mixed logit techniques, we find no statistically significant differences in the model fit between the two modes. Out of six public space attributes, only one shows statistically significant differences in valuation between modes, namely ‘vertical green’. Our results suggest that the choice between video and self-navigation modes can be based on other (secondary) considerations, i.e., required sample size, respondents' experiences with navigation interfaces, the specific goals of the study or application in practice, and the costs and effort needed to conduct the experiment.}
}
@article{BUREIKO2017182,
title = {Multiscale dynamic visualization of signal transduction processes with detailing of target-genes activation in three-dimensional genome structure},
journal = {Procedia Computer Science},
volume = {119},
pages = {182-189},
year = {2017},
note = {6th International Young Scientist Conference on Computational Science, YSC 2017, 01-03 November 2017, Kotka, Finland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.175},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917323852},
author = {Kseniia Bureiko and Maria Semashko and Ksenia D. Mukhina and Andrey Karsakov},
keywords = {Bioinformatics visualization, signaling pathways, spatial genome structure},
abstract = {Given the complexity of modern biological data it is essentially crucial to accord a consistent expounding. Interpreting such data into complex networks and visualizing them can reveal understanding of various processes in a cell. A consequence mapping of signal transduction processes to the spatial genome structure can benefit new insights in interaction detection in the spatial arrangement of genes. We present an approach for multiscale dynamic visualization of signal transduction processes with detailing of target-genes activation in spatial genome structure. The usage of this approach is demonstrated for the WNT signaling pathway in a human cell. We conclude with suggesting future research questions to improve our approach by considering new available data.}
}
@article{MANGHISI20221338,
title = {A Virtual Reality Approach for Assisting Sustainable Human-Centered Ergonomic Design: The ErgoVR tool},
journal = {Procedia Computer Science},
volume = {200},
pages = {1338-1346},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.335},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003441},
author = {Vito M. Manghisi and Alessandro Evangelista and Antonio E. Uva},
keywords = {Ergonomics, RULA, Virtual Reality, User-centered, Collaborative design, Kinect V2},
abstract = {Industry 4.0 is characterized by great potential for innovation impacting the operator’s role, increasingly engaged in smart activities of a decision-making nature. In such a working scenario, operators’ working conditions can be effectively improved by applying a user-centered collaborative design approach. To this end, we developed a Virtual Reality-based multiplayer tool exploiting low-cost body tracking technology to evaluate ergonomic postural risk. The tool allows evaluating both in real-time and off-line the ergonomic postural risk according to the Rapid Upper Limb Assessment metrics. By applying this approach, a twofold advantage can be achieved. On the one hand, ergonomic experts can have an immersive three-dimensional visualization of postures even in off-line observations. On the other hand, it is possible to evaluate the ergonomics of workstations in the design phase by having the operator work on virtual mock-ups of workstations, thus allowing a sustainable approach to user-centered collaborative design.}
}
@article{BIERCEWICZ20232057,
title = {VR educational game in public awareness campaign preventing the spread of COVID-19 – a pilot study},
journal = {Procedia Computer Science},
volume = {225},
pages = {2057-2066},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.196},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013546},
author = {Konrad Biercewicz and Anna Borawska and Mariusz Borawski and Jarosław Duda},
keywords = {virtual reality games, public awareness campaign, EEG, engagement},
abstract = {Within a few years, virtual reality (VR) has generated a lot of interest. With the development of this technology, many opportunities for its use have arisen. Among the most popular applications, one can also mention education. The typical VR approach in education involves applying games to increase the learner's involvement and motivation. The question is whether engaging in a virtual reality game designed for a public awareness campaign can help the participants enhance their assessment of social distancing measured in meters, potentially aiding in the reduction of COVID-19 transmission. The Sustainable Development Goals have been greatly affected by the COVID-19 pandemic, impacting almost all countries around the world. This has led to changes in political and health priorities as well as research strategies. For this purpose, a pilot study was conducted on 4 people in the game created in the Unity engine. The respondents’ reactions were examined with the use of the following EEG indices - Engagement, Arousal, Valence, and Approach-Withdrawal were used. As a result, it was obtained that the created game can improve the assessment of distance by the respondents. Therefore, we can conclude that applying virtual reality games can teach people to follow certain rules and contribute to slowing down the spread of COVID-19.}
}
@article{MARUYAMA2016250,
title = {Motion-capture-based walking simulation of digital human adapted to laser-scanned 3D as-is environments for accessibility evaluation},
journal = {Journal of Computational Design and Engineering},
volume = {3},
number = {3},
pages = {250-265},
year = {2016},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2016.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2288430015300142},
author = {Tsubasa Maruyama and Satoshi Kanai and Hiroaki Date and Mitsunori Tada},
keywords = {Digital human model, Walking simulation, Laser-scanning, Accessibility evaluation, Motion-capture, Human behavior simulation},
abstract = {Owing to our rapidly aging society, accessibility evaluation to enhance the ease and safety of access to indoor and outdoor environments for the elderly and disabled is increasing in importance. Accessibility must be assessed not only from the general standard aspect but also in terms of physical and cognitive friendliness for users of different ages, genders, and abilities. Meanwhile, human behavior simulation has been progressing in the areas of crowd behavior analysis and emergency evacuation planning. However, in human behavior simulation, environment models represent only “as-planned” situations. In addition, a pedestrian model cannot generate the detailed articulated movements of various people of different ages and genders in the simulation. Therefore, the final goal of this research was to develop a virtual accessibility evaluation by combining realistic human behavior simulation using a digital human model (DHM) with “as-is” environment models. To achieve this goal, we developed an algorithm for generating human-like DHM walking motions, adapting its strides, turning angles, and footprints to laser-scanned 3D as-is environments including slopes and stairs. The DHM motion was generated based only on a motion-capture (MoCap) data for flat walking. Our implementation constructed as-is 3D environment models from laser-scanned point clouds of real environments and enabled a DHM to walk autonomously in various environment models. The difference in joint angles between the DHM and MoCap data was evaluated. Demonstrations of our environment modeling and walking simulation in indoor and outdoor environments including corridors, slopes, and stairs are illustrated in this study.}
}
@article{JAFARI201622,
title = {PTRebeca: Modeling and analysis of distributed and asynchronous systems},
journal = {Science of Computer Programming},
volume = {128},
pages = {22-50},
year = {2016},
note = {Special issue on Automated Verification of Critical Systems (AVoCS’14)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2016.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167642316000800},
author = {Ali Jafari and Ehsan Khamespanah and Marjan Sirjani and Holger Hermanns and Matteo Cimini},
keywords = {Probabilistic Timed Automata, Timed Markov Decision Process, IMCA model checker, Probabilistic Timed Rebeca, Model checking, Performance analysis},
abstract = {Distributed systems exhibit probabilistic and non-deterministic behaviors and may have time constraints. Probabilistic Timed Rebeca (PTRebeca) is introduced as a timed and probabilistic actor-based language for modeling distributed real-time systems with asynchronous message passing. The semantics of PTRebeca is a Timed Markov Decision Process. In this paper, we provide SOS rules for PTRebeca, introduce a new tool-set and describe the corresponding mappings. The tool-set automatically generates a Markov Automaton from a PTRebeca model in the form of the input language of the Interactive Markov Chain Analyzer (IMCA). The IMCA can be used as a back-end model checker for performance analysis of PTRebeca models against expected reachability and probabilistic reachability properties. Comparing to the existing tool-set, proposed in the conference paper, we now have the ability of analyzing significantly larger models, and we also can add different rewards to the model. We show the applicability of our approach and efficiency of our tool by analyzing a Network on Chip architecture as a real-world case study.}
}
@article{HEATON2019172,
title = {Design and development of BIM models to support operations and maintenance},
journal = {Computers in Industry},
volume = {111},
pages = {172-186},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519300557},
author = {James Heaton and Ajith Kumar Parlikad and Jennifer Schooling},
keywords = {Building information modelling, Asset management, Asset information model, Asset management systems, Asset classification},
abstract = {Building Information Modelling (BIM) is one of the most significant technological advancements in recent years that has been adopted by the design and construction industry. While BIM adoption is growing, it can be witnessed that adoption is relatively weak within operational and maintenance (O&M) organisations such as Estate and Infrastructure Management, who would ultimately gain the highest value from utilising BIM. While the challenges of BIM adoption are multifaceted, there is a recurring theme of poor data integration between BIM and existing information management systems. There is a clear gap of knowledge on how to structure a BIM model that allows its efficient use in the O&M phase. Furthermore, there is a lack of claritiy on how to exchange information from a BIM model into an Asset Information Model (AIM). This paper outlines a methodology that enables extraction of BIM-related data directly from a model into a relational database for integration with existing asset management systems. The paper describes the BIM model requirements, development of the extraction platform, database architecture and framework. Furthermore, a case study is presented to demonstrate the methodology. The case study demonstrates that if the BIM model is designed from the start with consideration for the O&M requirements, it can be exploited for development into an AIM. It also shows that a structured approach to object classification within a BIM model supports the efficient exchange of data directly from the BIM model.}
}
@article{CUTUMISU200732,
title = {ScriptEase: A generative/adaptive programming paradigm for game scripting},
journal = {Science of Computer Programming},
volume = {67},
number = {1},
pages = {32-58},
year = {2007},
note = {Special Issue on Aspects of Game Programming},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2007.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167642307000536},
author = {Maria Cutumisu and Curtis Onuczko and Matthew McNaughton and Thomas Roy and Jonathan Schaeffer and Allan Schumacher and Jeff Siegel and Duane Szafron and Kevin Waugh and Mike Carbonaro and Harvey Duff and Stephanie Gillis},
keywords = {Generative pattern, Computer game, Scripting language, Adaptive programming, Game scripting, Game authoring, Game agent},
abstract = {The traditional approach to implementing interactions between a player character (PC) and objects in computer games is to write scripts in a procedural scripting language. These scripts are usually so complex that they must be written by a computer programmer rather than by the author of the game story. This interruption in the game story authoring process has two distinct disadvantages: it increases the cost of game production and it introduces a disconnect between the author’s intentions and the interactions produced from the programmer’s written scripts. We introduce a mechanism to solve these problems. We show that game authors (non-programmers) can generate the necessary scripts for implementing meaningful interactions between the PC and game objects using a three-step process. In the first step, the author uses a generative pattern (concept) to create a high-level description of a commonly occurring game scenario. In the second step, the author uses a standard set of adaptation operations to customize the high-level description to the particular circumstances of the story that is being told. In the third step, the author presses a button that automatically generates scripting code from the adapted pattern. We describe the results of three studies in which a combined total of 56 game story authors used this three-step process to construct Neverwinter Nights game stories, using a tool called ScriptEase. We believe that this generative/adaptive process is the key to future game story scripting. More generally, this article advocates the development of adaptive programming as an alternative to current constructive programming techniques, as well as the application of adaptive programming in many domains.}
}
@article{KUMAR2020100077,
title = {Use of mixed reality for surgery planning: Assessment and development workflow},
journal = {Journal of Biomedical Informatics},
volume = {112},
pages = {100077},
year = {2020},
note = {Articles initially published in Journal of Biomedical Informatics: X 5-8, 2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.yjbinx.2020.100077},
url = {https://www.sciencedirect.com/science/article/pii/S2590177X20300111},
author = {Rahul Prasanna Kumar and Egidijus Pelanis and Robin Bugge and Henrik Brun and Rafael Palomar and Davit L. Aghayan and Åsmund Avdem Fretland and Bjørn Edwin and Ole Jakob Elle},
keywords = {Mixed reality, Surgery, Planning, Segmentation},
abstract = {Meticulous preoperative planning is an important part of any surgery to achieve high levels of precision and avoid complications. Conventional medical 2D images and their corresponding three-dimensional (3D) reconstructions are the main components of an efficient planning system. However, these systems still use flat screens for visualisation of 3D information, thus losing depth information which is crucial for 3D spatial understanding. Currently, cutting-edge mixed reality systems have shown to be a worthy alternative to provide 3D information to clinicians. In this work, we describe development details of the different steps in the workflow for the clinical use of mixed reality, including results from a qualitative user evaluation and clinical use-cases in laparoscopic liver surgery and heart surgery. Our findings indicate a very high general acceptance of mixed reality devices with our applications and they were consistently rated high for device, visualisation and interaction areas in our questionnaire. Furthermore, our clinical use-cases demonstrate that the surgeons perceived the HoloLens to be useful, recommendable to other surgeons and also provided a definitive answer at a multi-disciplinary team meeting.}
}
@article{KAO2022103216,
title = {Coupled Rigid-Block Analysis: Stability-Aware Design of Complex Discrete-Element Assemblies},
journal = {Computer-Aided Design},
volume = {146},
pages = {103216},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2022.103216},
url = {https://www.sciencedirect.com/science/article/pii/S0010448522000161},
author = {Gene Ting-Chun Kao and Antonino Iannuzzo and Bernhard Thomaszewski and Stelian Coros and Tom {Van Mele} and Philippe Block},
keywords = {3D assembly, Computational fabrication, Stability-aware design, Concave shapes, Friction, Contact mechanics},
abstract = {The rigid-block equilibrium (RBE) method uses a penalty formulation to measure structural infeasibility or to guide the design of stable discrete-element assemblies from unstable geometry. However, RBE is a purely force-based formulation, and it incorrectly describes stability when complex interface geometries are involved. To overcome this issue, this paper introduces the coupled rigid-block analysis (CRA) method, a more robust approach building upon RBE’s strengths. The CRA method combines equilibrium and kinematics in a penalty formulation in a nonlinear programming problem. An extensive benchmark campaign is used to show how CRA enables accurate modelling of complex three-dimensional discrete-element assemblies formed by rigid blocks. In addition, an interactive stability-aware design process to guide user design towards structurally-sound assemblies is proposed. Finally, the potential of our method for real-world problems are demonstrated by designing complex and scaffolding-free physical models.}
}
@article{GONZALEZGONZALEZ2019103266,
title = {Serious games for rehabilitation: Gestural interaction in personalized gamified exercises through a recommender system},
journal = {Journal of Biomedical Informatics},
volume = {97},
pages = {103266},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103266},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419301856},
author = {Carina S. González-González and Pedro A. Toledo-Delgado and Vanesa Muñoz-Cruz and Pablo V. Torres-Carrion},
keywords = {Recommender systems, Exergames, Rehabilitation, Gamification, Serious games},
abstract = {One of the principal problems of rehabilitation is that therapy sessions can be boring due the repetition of exercises. Serious games, and in particular exergames in rehabilitation, can motivate, engage and increase patients’ adherence to their treatment. Also, the automatic personalization of exercises to each patient can help therapists. Thus, the main objective of this work is to build an intelligent exergame-based rehabilitation system consisting of a platform with an exergame player and a designer tool. The intelligent platform includes a recommender system which analyzes user interactions, along with the user’s history, to select new gamified exercises for the user. The main contributions of this paper focus, first, on defining a recommender system based on different difficulty levels and user skills. The recommender system offers the ability to provide the user with a personalized game mode based on his own history and preferences. The results of a triple validation with experts, users and rehabilitation center professionals reveal a positive impact on gestural interaction and rehabilitation uses. Also, different methods are presented for testing the rehabilitation recommender system.}
}
@article{MORENOLUMBRERAS2024111985,
title = {The influence of the city metaphor and its derivates in software visualization},
journal = {Journal of Systems and Software},
volume = {210},
pages = {111985},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.111985},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000281},
author = {David Moreno-Lumbreras and Jesus M. Gonzalez-Barahona and Gregorio Robles and Valerio Cosentino},
keywords = {Software visualization, City metaphor, Software comprehension, Systematic mapping study, Visualizations, State of the art, Extended reality},
abstract = {Context:
The city metaphor is widely used in software visualization to represent complex systems as buildings and structures, providing an intuitive way for developers to understand software components. Various software visualization tools have utilized this approach.
Objective:
Identify the influence of the city metaphor on software visualization research, determine its state-of-the-art status, and identify derived tools and their main characteristics.
Method:
Conduct a systematic mapping study of 406 publications that reference the first paper on the use of the city metaphor in software visualization and/or the main paper of the CodeCity tool. Analyze the 168 publications from which valuable information could be extracted, and build a complete categoric analysis.
Results:
The field has grown considerably, with an increasing number of publications since 2001, and a changing research community with evolving interconnections between groups. Researchers have developed more tools that support the city metaphor, but less than 50% of the tools were referenced in their papers. Moreover, 85% of the tools did not use extended reality environments, indicating an opportunity for further exploration.
Conclusion:
The study demonstrates the active and continually growing presence of the city metaphor in research and its impact on software visualization and its derivatives. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{MORIARTY2012200,
title = {Utilizing Depth Based Sensors and Customizable Software Frameworks for Experiential Application},
journal = {Procedia Computer Science},
volume = {12},
pages = {200-205},
year = {2012},
note = {Complex Adaptive Systems 2012},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.09.054},
url = {https://www.sciencedirect.com/science/article/pii/S187705091200645X},
author = {Brian Moriarty and Elizabeth Lennon and Frank DiCola and Kyle Buzby and Morisa Manzella and Emily Hromada},
keywords = {Kinect, Natural Gesture Interfaces, STEM education, Process Simulator, Depth Sensors, Skeletal Tracking, Artificial Intelligence},
abstract = {Using depth sensor cameras such as the Kinect and highly customizable software development frameworks in conjunction with artificial intelligence methodologies offer significant opportunities in a variety of applications, such as undergraduate science, technology, engineering, and math (STEM) education, professional or military training simulation, and individually-tailored cultural and media arts immersion. Designing a participatory educational experience where users are able to actively interface and experience given subject matter in a practical experiential manner can enhance the user's ability to learn and retain presented information. Such natural gesture user interfaces have potential for broad application in disciplines ranging from systems engineering education to process simulation. This paper will discuss progress on the development of testing environments for interactive educational methods in conjunction with artificial intelligent systems that have the ability to adjust the educational user experience based on individual user identification. This will be achieved through depth sensor skeletal tracking, allowing experience adaptation based on the nature and effectiveness of the interactive educational experience.}
}
@article{KHAN20225741,
title = {Deep Reinforcement Learning Based Unmanned Aerial Vehicle (UAV) Control Using 3D Hand Gestures},
journal = {Computers, Materials and Continua},
volume = {72},
number = {3},
pages = {5741-5759},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.024927},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822009444},
author = {Fawad Salam Khan and Mohd Norzali {Haji Mohd} and Saiful Azrin B. M. Zulkifli and Ghulam E {Mustafa Abro} and Suhail Kazi and Dur Muhammad Soomro},
keywords = {Deep reinforcement learning, UAV, 3D hand gestures, obstacle detection, polar mask},
abstract = {The evident change in the design of the autopilot system produced massive help for the aviation industry and it required frequent upgrades. Reinforcement learning delivers appropriate outcomes when considering a continuous environment where the controlling Unmanned Aerial Vehicle (UAV) required maximum accuracy. In this paper, we designed a hybrid framework, which is based on Reinforcement Learning and Deep Learning where the traditional electronic flight controller is replaced by using 3D hand gestures. The algorithm is designed to take the input from 3D hand gestures and integrate with the Deep Deterministic Policy Gradient (DDPG) to receive the best reward and take actions according to 3D hand gestures input. The UAV consist of a Jetson Nano embedded testbed, Global Positioning System (GPS) sensor module, and Intel depth camera. The collision avoidance system based on the polar mask segmentation technique detects the obstacles and decides the best path according to the designed reward function. The analysis of the results has been observed providing best accuracy and computational time using novel design framework when compared with traditional Proportional Integral Derivatives (PID) flight controller. There are six reward functions estimated for 2500, 5000, 7500, and 10000 episodes of training, which have been normalized between 0 to −4000. The best observation has been captured on 2500 episodes where the rewards are calculated for maximum value. The achieved training accuracy of polar mask segmentation for collision avoidance is 86.36%.}
}
@article{QUISHPEARMAS2015413,
title = {An Immersive 3D Virtual Learning Environment for Analyzing the Atomic Structure of MEMS-Relevant Materials},
journal = {Procedia Computer Science},
volume = {75},
pages = {413-416},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.265},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037266},
author = {Jorge A. Quishpe-Armas and Luis D. Cedeño-Viveros and Javier Meléndez-Campos and Carlos A. Suárez-Mora and Sergio Camacho-Leon},
keywords = {Immersive, Learning Environment, Virtual Reality, MEMS, NEMS},
abstract = {In this paper, an immersive three-dimensional (3D) virtual learning environment based on the Oculus head-mounted display is presented for analyzing the atomic structure of monocrystalline materials relevant to Micro-Electro-Mechanical Systems (MEMS), e.g. Silicon (Si), Chromium (Cr), Titanium (Ti) and Copper (Cu). This environment allows the real-time visualization and interactive analysis of key crystal lattice parameters, e.g. number of atoms in the lattice, atomic packing factor, linear atomic density, surface atomic density, volumetric density, etc.}
}
@article{OHMOTO2016607,
title = {A Support System to Accumulate Interpretations of Multiple Story Timelines},
journal = {Procedia Computer Science},
volume = {96},
pages = {607-616},
year = {2016},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 20th International Conference KES-2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.241},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916320518},
author = {Yoshimasa Ohmoto and Takashi Ookaki and Toyoaki Nishida},
keywords = {Knowledge archives, group learning assistance, theatrical role play, narrative technology ;},
abstract = {The story base interpretation is subjectively summarised and segmented from the first-person viewpoint. However, we often need to objectively represent an entire image by integrated knowledge. Yet, this is a difficult task. We proposed a novel approach, named the synthetic evidential study (SES), for understanding and augmenting collective thought processes through substantiated thought by interactive media. In this study, we investigated the kind of data that can be obtained through the SES sessions as interpretation archives and whether the database is useful to understand multiple story timelines. For the purpose, we designed a machine-readable interpretation data format and developed support systems to create and provide data that are easy to understand. We conducted an experiment using the simulation of the projection phase in SES sessions. From the results, we suggested that a “meta comment” which was deepened interpretation comment by the others in the interpretation archives to have been posted when it was necessary to consider other participants’ interpretation to broaden their horizons before posting the comment. In addition, the construction of networks to represent the relationships between the interpretation comments enabled us to suggest the important comments by using the degree centrality.}
}
@article{KUSUMA2021886,
title = {Enhancing Historical Learning Using Role-Playing Game on Mobile Platform},
journal = {Procedia Computer Science},
volume = {179},
pages = {886-893},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.078},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921001083},
author = {Gede Putra Kusuma and Louis Khrisna {Putera Suryapranata} and Evan Kristia Wigati and Yesun Utomo},
keywords = {Game-Based Learning, Historical Learning, Role-Playing Game, Mobile Platform, Statistical Analysis},
abstract = {Learning history means to learn what caused the world around us to become the way it is right now. For decades, historical events were recorded and taught to younger generations to learn from them. However, most students found that learning history is boring because they could not feel or understand the moment of those historical events. This paper aims to investigate how a game-based learning approach influences the achievement and motivation of historical learning through a mobile learning environment. We propose game-based historical learning using the role-playing game on a mobile platform. The implementation is based on the history of the first army general in Indonesia, General Sudirman. The game was tested to 63 Junior High School students in Jakarta. Based on the evaluation, using the game can increase student learning motivation and learning achievement.}
}
@article{SCHALBETTER2023102003,
title = {From board games to immersive urban imaginaries: Visualization fidelity's impact on stimulating discussions on urban transformation},
journal = {Computers, Environment and Urban Systems},
volume = {104},
pages = {102003},
year = {2023},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2023.102003},
url = {https://www.sciencedirect.com/science/article/pii/S0198971523000662},
author = {Laura Schalbetter and Nicolas Salliou and Ralph Sonderegger and Adrienne Grêt-Regamey},
keywords = {Serious Game, 3D point clouds, Aesthetics, Leverage point, Place},
abstract = {Given the increasing importance of public participation in urban transformation processes, serious games have emerged as a promising tool for fostering active engagement and constructive dialogue. By providing insights into opposing arguments and helping navigate complex trade-offs, serious games can support informed decision-making and promote collaborative solutions. Although digital media are increasingly used in games, applications in serious games are still in their infancy. Furthermore, previous research has primarily focused on resource use rather than on the impact of media on the debate itself. Additionally, there has been little investigation into the role of soft factors, such as aesthetics and interests, in multi-stakeholder discussions, especially as they may be influenced by the medium in immersive serious games. In this contribution, we demonstrate how a three-dimensional realistic serious game triggers different argumentation patterns compared to a board game. The serious game engages the players in a case study in Hochdorf, a Swiss suburban municipality that is stuck in urban transformation. The serious game is based on point-cloud LiDAR data that can be modified using three-dimensional hand-drawn sketches to visualize urban transformation measures. We found that aesthetical issues are discussed and emotional factors are expressed more often in the closer-to-reality game than in the board game. Immersive visualizations influence the discussion about urban transformations as the expression of visions and world views is more straightforward for stakeholders, and the patterns of argumentation appear more diverse.}
}
@article{CECIL2018128,
title = {An IoMT based cyber training framework for orthopedic surgery using Next Generation Internet technologies},
journal = {Informatics in Medicine Unlocked},
volume = {12},
pages = {128-137},
year = {2018},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352914818300285},
author = {J. Cecil and Avinash Gupta and Miguel Pirela-Cruz and Parmesh Ramanathan},
keywords = {Cyber physical systems, Internet of Things, Surgical training, Telemedicine, Virtual reality, Internet of Medical Things},
abstract = {Internet of Things based approaches and frameworks hold significant potential in changing the way in which engineering activities are accomplished. The information centric revolution underway has served as a catalyst in the design of innovative methods and practices in several engineering and other domains. In this paper, an Internet of Medical Things based framework for surgical training is discussed in the broader context of Next Generation frameworks. The design and development of this Internet of Medical Things based framework involving adoption of Global Environment for Network Innovations based networking principles is elaborated. The Virtual Reality based simulation environments incorporate haptic based interfaces which support collaborative training and interactions among expert surgeons and residents in orthopedic surgery from distributed locations. The impact of using this Internet of Medical Things based framework for medical education has also been studied; the outcomes underscore the potential of adopting such Internet of Medical Things based approaches for medical education.}
}
@article{DROZDZ20214886,
title = {Virtual Reality Training System as a comprehensive and effective method for delivering technical hands-on training in the field of Distribution System Operators},
journal = {Procedia Computer Science},
volume = {192},
pages = {4886-4899},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.267},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020068},
author = {Wojciech Drożdż},
keywords = {virtual reality system, virtual reality training, hand-on training in virtual reality, live-line work in virtual reality, commutating operation in virtual reality},
abstract = {The Paper presents the possibilities of using virtual reality techniques in interactive training of technical employees of a Distribution System Operator (DSO). It also describes the concept, assumptions and implementation method of the created Flexible Training System. Thanks to its capabilities, employees can participate in sophisticated training conducted in an innovative way eliminating the current disadvantages and limitations of classic – “real” training approaches. The System enables practice in two main categories of training used in the DSO environment: commutating operations and live-line works. Benefits and risks associated with system implementation in DSO structures are presented}
}
@article{DELAMO2022107954,
title = {Hybrid recommendations and dynamic authoring for AR knowledge capture and re-use in diagnosis applications},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {107954},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107954},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121010868},
author = {Iñigo Fernández {del Amo} and John Ahmet Erkoyuncu and Maryam Farsi and Dedy Ariansyah},
keywords = {Augmented Reality, Failure diagnosis, Authoring systems, Knowledge capture, Ontology-based reporting},
abstract = {In Industry 4.0, integrated data management is an important challenge due to heterogeneity and the lack of structure of numerous existing data sources. A relevant research gap involves human knowledge integration, especially in maintenance operations. Augmented Reality (AR) can bridge this gap, but it requires improved augmented content to enable effective and efficient knowledge capture. This paper proposes dynamic authoring and hybrid recommender methods for accurate AR-based reporting. These methods aim to provide maintainers with augmented data input formats and recommended datasets for enhancing the efficiency and effectiveness of their reporting tasks. The proposed contributions have been validated through experiments and surveys in two failure diagnosis reporting scenarios. Experimental results indicated that the proposed reporting solution can reduce reporting errors by 50% and reporting time by 20% compared to alternative recommender and AR tools. Besides, survey results suggested that testers perceived the proposed reporting solution as more effective and satisfactory for reporting tasks than alternative tools. Thus, proving that the proposed methods can improve the effectiveness and efficiency of diagnosis reporting applications. Finally, this paper proposes future works towards a framework for automatic adaptive authoring in AR knowledge transfer and capture applications for human knowledge integration in the context of Industry 4.0.}
}
@article{IVAN2017309,
title = {Help The Math Town: Adaptive Multiplayer Math-Science Games using Fuzzy Logic},
journal = {Procedia Computer Science},
volume = {116},
pages = {309-317},
year = {2017},
note = {Discovery and innovation of computer science technology in artificial intelligence era: The 2nd International Conference on Computer Science and Computational Intelligence (ICCSCI 2017)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.10.080},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917321300},
author = {Charles Ivan and Pingkan C.B. Rumondor and Michael Yoseph Ricky and Emny Harna Yossy and Widodo Budiharto},
keywords = {adaptive games, fuzzy, android, psychological aspects},
abstract = {The mobile games for education and entertainment are rapidly developed. Unfortunately the development of interesting and entertaining adaptive multiplayer game based learning for math and science that consider psychological aspects and game balancing for students on mobile platforms that have game balancing is very limited. This paper proposes an integrated model of adaptive Math and Science game based learning for elementary school students with adjustable level of difficulty based on user’s ability and additional option for “Free” and “Adventures” modes. We propose a method where level of difficulty can be adjusted based on previous score, using fuzzy two inputs in the form of percentage correct and the speed of answer to produce output. The experimental results are presented and show the adaptive games are running well on mobile devices based on Android platform and well received by elementary school students grade 3 to 6.}
}
@article{CASSOLA2014130,
title = {Online-Gym: A 3D Virtual Gymnasium Using Kinect Interaction},
journal = {Procedia Technology},
volume = {13},
pages = {130-138},
year = {2014},
note = {SLACTIONS 2013: Research conference on virtual worlds – Learning with simulations},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314000279},
author = {Fernando Cassola and Leonel Morgado and Fausto {de Carvalho} and Hugo Paredes and Benjamim Fonseca and Paulo Martins},
keywords = {Virtual Environments, Second Life, OpenSimulator, Virtual Worlds, Kinect, Motion Capture, Human Computer Interaction, Natural User Interfaces, online gymnastics, rehabilitation},
abstract = {Synchronized online gymnastics may provide new possibilities for enhancing the physical and social well-being of people with restricted mobility. We propose a prototype platform for this – Online-Gym – which allows users to interact using a Microsoft Kinect and participate in on-line gymnastics sessions. In this paper we present the Online-Gym concept and a first iteration of the platform architecture that allows interaction in OpenSimulator or Second Life virtual worlds with movement captured by a Kinect device. The exploratory work done so far provides evidence that this approach is viable and that such scenarios may be pursued.}
}
@article{ADAO2018441,
title = {A rapid prototyping tool to produce 360° video-based immersive experiences enhanced with virtual/multimedia elements},
journal = {Procedia Computer Science},
volume = {138},
pages = {441-453},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316958},
author = {Telmo Adão and Luís Pádua and Miguel Fonseca and Luís Agrellos and Joaquim J. Sousa and Luís Magalhães and Emanuel Peres},
keywords = {Virtual Reality, 360° Videos Editing, Immersive Digital Panoramas, Multimedia, 3D model, 3D text, Spatialized Sound, Prototyping},
abstract = {While the popularity of virtual reality (VR) grows in a wide range of application contexts – e.g. entertainment, training, cultural heritage and medicine –, its economic impact is expected to reach around 15bn USD, by the year of 2020. Within VR field, 360° video has been sparking the interest of development and research communities. However, editing tools supporting 360° panoramas are usually expensive and/or demand programming skills and/or advanced user knowledge. Besides, application approaches to quickly and intuitively set up such 360° video-based VR environments complemented with diverse types of parameterizable virtual assets and multimedia elements are still hard to find. Thereby, this paper aims to propose a system specification to simply and rapidly configure immersive VR environments composed of surrounding 360° video spheres that can be complemented with parameterizable multimedia contents – namely 3D models, text and spatial sound –, whose behavior can be either time-range or user-interaction dependent. Moreover, a preliminary prototype that follows a substantial part of the previously mentioned specification and implements the enhancement of 360° videos with time-range dependent virtual assets is presented. Preliminary tests evaluating usability and user satisfaction were also carried out with 30 participants, from which encouraging results were achieved.}
}
@article{TADEJA2023134,
title = {Exploring the repair process of a 3D printer using augmented reality-based guidance},
journal = {Computers & Graphics},
volume = {117},
pages = {134-144},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002546},
author = {Sławomir K. Tadeja and Luca O. {Solari Bozzi} and Kerr D.G. Samson and Sebastian W. Pattinson and Thomas Bohné},
keywords = {Augmented reality, AR, AR-guided repair, Immersive interface, 3D printing, 3D printer repair, Right to repair},
abstract = {In recent years, additive manufacturing (AM) techniques have transcended their typical rapid prototyping role and become viable methods to directly manufacture end products in a highly versatile manner. Due to its low cost and relative ease of use, fused deposition modeling (FDM) has become the most universally applied AM technology. Nonetheless, skilled operators are often still required to perform maintenance, diagnostic, and repair tasks. Such operators need to be adequately trained. Here, Augmented reality (AR) technology could be used to automate this training and help to promptly provide new operators with the necessary skills to perform specific tasks as required. However, the most effective approach to designing such AR-based assistance systems has not yet been fully explored. Consequently, we address this need by reporting on how to design such guiding systems using well-known design engineering methodologies. We then further assess the applicability of our approach through a user study with domain experts. In addition, we complete our assessment with heuristical verification of system expressiveness to reason about the influence of cognitively important components of the AR interface on the operators.}
}
@article{RIOS2013161,
title = {A Mobile Solution to Enhance Training and Execution of Troubleshooting Techniques of the Engine Air Bleed System on Boeing 737},
journal = {Procedia Computer Science},
volume = {25},
pages = {161-170},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012258},
author = {Horacio Rios and Eduardo González and Ciro Rodriguez and Hector R. Siller and Manuel Contero},
keywords = {Augmented Reality, Troubleshooting, Aircraft, Complex Assembly},
abstract = {The process of troubleshooting an aircraft engine requires highly skilled and trained personnel who must be able to respond effectively to any circumstance; therefore, new methods of training to accelerate the cognitive processes of technicians must be integrated in the industry. In this matter the Augmented Reality technology represents an innovative tool that can ensure the efficient and correct transfer of knowledge. The numbers of errors during maintenance tasks can be reduced, AR provides information that is generally not easily available during maintenance operations because, in general, the troubleshooting process for airplane engine is a highly complex task and the diagnosis of a failure is critical for the passengers’ safety. This research focuses on training and execution of tasks where an aviation technician must be familiarized with a wide variety of technical data, physical components of mechanical systems and the regulations that must be followed to release an airplane for flight, the specialist must develop a correct mind map of the system and should be able to troubleshoot if necessary. The case of study is the 737 Engine Bleed Air System that is designed to provide engine compressed air to air conditioning pack with the purpose of air pressurization during flight; engine air from the compressor is used, from the 5° and the 9° stage in a safe an economical way, knowledge of the correct function of the components will increase safety and considerably reduce cost of maintenance operations. The purpose of the investigation was to develop an ergonomic tool than improves the cognitive process of technician during training for the troubleshooting techniques of the aircraft, but it also can be used to the everyday task by capturing the know-how and helpful tips from more experienced operators. A mobile solution that functions on regular tablets was delivered to enhance the troubleshooting techniques and maintenance procedures of the Engine Air Bleed System, the software can function on two aspects for training and in situ operations. A commercial aeronautical training kit was used to validate the Fault Isolation Software; the results showed that the augmented reality technique takes 17% less time and a quality increment of 24% for this complex assembly system.}
}
@article{HO202275,
title = {Vision based crown loss estimation for individual trees with remote aerial robots},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {188},
pages = {75-88},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622000971},
author = {Boon Ho and Basaran Bahadir Kocer and Mirko Kovac},
keywords = {Aerial robots, Unmanned aerial vehicles, Crown loss estimation, Convolutional neural network, Variational autoencoder, Foliar sampling},
abstract = {With the capability of capturing high-resolution imagery data and the ease of accessing remote areas, aerial robots are becoming increasingly popular for forest health monitoring applications. For example, forestry tasks such as field surveys and foliar sampling which are generally manual and labour intensive can be automated with remotely controlled aerial robots. In this study, we propose two new online frameworks to quantify and rank the severity of individual tree crown loss. The real-time crown loss estimation (RTCLE) model localises and classifies individual trees into their respective crown loss percentage bins. Experiments are conducted to investigate if synthetically generated tree images can be used to train the RTCLE model as real images with diverse viewpoints are generally expensive to collect. Results have shown that synthetic data training helps to achieve a satisfactory baseline mean average precision (mAP) which can be further improved with just some additional real imagery data. We showed that the mAP can be increased approximately from 60% to 78% by mixing the real dataset with the generated synthetic data. For individual tree crown loss ranking, a two-step crown loss ranking (TSCLR) framework is developed to handle the inconsistently labelled crown loss data. The TSCLR framework detects individual trees before ranking them based on some relative crown loss severity measures. The tree detection model is trained with the combined dataset used in the RTCLE model training where we achieved an mAP of approximately 95% suggesting that the model generalises well to unseen datasets. The relative crown loss severity of each tree is estimated, with deep representation learning, by a probabilistic encoder from a fully trained variational autoencoder (VAE) model. The VAE is trained end-to-end to reconstruct tree images in a background agnostic way. Based on a conservative evaluation, the estimated crown loss severity from the probabilistic encoder generally showed moderate agreement with the expert’s estimation across all species of trees present in the dataset. All the software pipelines, the dataset, and the synthetic dataset generation can be found in the GitHub link.}
}
@article{ALMEIDA202351,
title = {SIT6: Indirect touch-based object manipulation for DeskVR},
journal = {Computers & Graphics},
volume = {117},
pages = {51-60},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002509},
author = {Diogo Almeida and Daniel Mendes and Rui Rodrigues},
keywords = {Virtual reality, Object manipulation, DeskVR},
abstract = {Virtual reality (VR) has the potential to significantly boost productivity in professional settings, especially those that can benefit from immersive environments that allow a better and more thorough way of visualizing information. However, the physical demands of mid-air movements make it difficult to use VR for extended periods. DeskVR offers a solution that allows users to engage in VR while seated at a desk, minimizing physical exhaustion. However, developing appropriate motion techniques for this context is challenging due to limited mobility and space constraints. This work focuses on object manipulation techniques, exploring touch-based and mid-air-based approaches to design a suitable solution for DeskVR, hypothesizing that touch-based object manipulation techniques could be as effective as mid-air object manipulation in a DeskVR scenario while less physically demanding. Thus, we propose Scaled Indirect Touch 6-DOF (SIT6), an indirect touch-based object manipulation technique incorporating scaled input mapping to address precision and out-of-reach manipulation issues. The implementation of our solution consists of a state machine with error-handling mechanisms and visual indicators to enhance interaction. User experiments were conducted to compare the SIT6 technique with a baseline mid-air approach, revealing comparable effectiveness while demanding less physical exertion. These results validated our hypothesis and established SIT6 as a viable option for object manipulation in DeskVR scenarios.}
}
@article{HUHNT2022101790,
title = {Modeling bounded and unbounded space with polyhedra: Topology and operators for manifold cell complexes},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101790},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101790},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002488},
author = {Wolfgang Huhnt and Maximilian Sternal and Peter Jan Pahl},
abstract = {This paper proposes polyhedral space partitioning as an alternative to component assembly of digital models of objects with complex linear shapes. A partition is specified with a path-connected user model, where each object is bounded by n-manifolds. Faces and cells can be non-convex, multiply-connected and unbounded. The user interacts with the user model and specifies work steps. Each work step splits one edge, face or cell of the partition, or merges two neighboring objects of equal dimension. As a consequence, only a small subset of the model objects, consisting of the user specified objects and their neighbors, are affected by a work step. The user model is automatically mapped to a core model containing methods for topological relations and navigation. The topological structure is described by bundles of twin arrows of opposite direction arranged in polygons, twin facets with normal vectors of opposite direction and dihedral facet cycles at the edges. Imaginary topological objects are introduced to define unbounded cells, faces and edges. The approach guarantees that there is no overlap or gap between any pair of neighboring objects. It supports modelling of non-convex and multiply connected bounded and unbounded objects. For verification, several example models are presented and visualized. The paper ends with conclusions and an outlook to ongoing and planned further research in this field.}
}
@article{PFOUGA201854,
title = {Leveraging 3D geometric knowledge in the product lifecycle based on industrial standards},
journal = {Journal of Computational Design and Engineering},
volume = {5},
number = {1},
pages = {54-67},
year = {2018},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2288430017300441},
author = {Alain Pfouga and Josip Stjepandić},
keywords = {3D shape representation, PDF, 3D file format, Visualization, Engineering collaboration, Data exchange},
abstract = {With their practical introduction by the 1970s, virtual product data have emerged to a primary technical source of intelligence in manufacturing. Modern organization have since then deployed and continuously improved strategies, methods and tools to feed the individual needs of their business domains, multidisciplinary teams, and supply chain, mastering the growing complexity of virtual product development. As far as product data are concerned, data exchange, 3D visualization, and communication are crucial processes for reusing manufacturing intelligence across lifecycle stages. Research and industry have developed several CAD interoperability, and visualization formats to uphold these product development strategies. Most of them, however, have not yet provided sufficient integration capabilities required for current digital transformation needs, mainly due to their lack of versatility in the multi-domains of the product lifecycle and primary focus on individual product descriptions. This paper analyses the methods and tools used in virtual product development to leverage 3D CAD data in the entire life cycle based on industrial standards. It presents a set of versatile concepts for mastering exchange, aware and unaware visualization and collaboration from single technical packages fit purposely for various domains and disciplines. It introduces a 3D master document utilizing PDF techniques, which fulfills requirements for electronic discovery and enables multi-domain collaboration and long-term data retention for the digital enterprise.}
}
@article{LUKKA2024100706,
title = {Measuring digital intervention user experience with a novel ecological momentary assessment (EMA) method, CORTO},
journal = {Internet Interventions},
volume = {35},
pages = {100706},
year = {2024},
issn = {2214-7829},
doi = {https://doi.org/10.1016/j.invent.2023.100706},
url = {https://www.sciencedirect.com/science/article/pii/S2214782923001069},
author = {Lauri Lukka and Veli-Matti Karhulahti and Vilma-Reetta Bergman and J. Matias Palva},
keywords = {Digital interventions, Ecological momentary assessment, Engagement, Evaluation methods, Formative evaluation, Interviewing, Methodology, Mental health, Mixed methods, Qualitative study, Questionnaire, Remote study, Serious games, User experience},
abstract = {Digital interventions often suffer from low usage, which may reflect insufficient attention to user experience. Moreover, the existing evaluation methods have limited applicability in the remote study of user experience of complex interventions that have expansive content and that are used over an extensive period of time. To alleviate these challenges, we describe here a novel qualitative Ecological Momentary Assessment (EMA) method: the CORTO method (Contextual, One-item, Repeated, Timely, Open-ended). We used it to gather digital intervention user experience data from Finnish adults (n = 184) who lived with interview-confirmed major depressive disorder (MDD) and took part in a randomized controlled trial (RCT) that studied the efficacy of a novel 12-week game-based digital intervention for depression. A second dataset on user experience was gathered with retrospective interviews (n = 22). We inductively coded the CORTO method and retrospective interview data, which led to four user experience categories: (1) contextual use, (2) interaction-elicited emotional experience, (3) usability, and (4) technical issues. Then, we used the created user experience categories and Template Analysis to analyze both datasets together, and reported the results qualitatively. Finally, we compared the two datasets with each other. We found that the data generated with the CORTO method offered more insights into usability and technical categories than the interview data that particularly illustrated the contextual use. The emotional valence of the interview data was more positive compared with the CORTO data. Both the CORTO and interview data detected 55 % of the micro-level categories; 20 % of micro-level categories were only detected by the CORTO data and 25 % only by the interview data. We found that the during-intervention user experience measurement with the CORTO method can provide intervention-specific insights, and thereby further the iterative user-centered intervention development. Overall, these findings highlight the impact of evaluation methods on the categories and qualities of insights acquired in intervention research.}
}
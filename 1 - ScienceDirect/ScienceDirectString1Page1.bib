@article{BRAUN201759,
title = {Curved - free-form interaction using capacitive proximity sensors},
journal = {Procedia Computer Science},
volume = {109},
pages = {59-66},
year = {2017},
note = {8th International Conference on Ambient Systems, Networks and Technologies, ANT-2017 and the 7th International Conference on Sustainable Energy Information Technology, SEIT 2017, 16-19 May 2017, Madeira, Portugal},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.295},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917309523},
author = {Andreas Braun and Sebastian Zander-Walz and Martin Majewski and Arjan Kuijper},
keywords = {Curved surfaces, capacitive sensing, gestural interaction, virtual reality},
abstract = {Abstract:
Large interactive surfaces have found increased popularity in recent years. However, with increased surface size ergonomics become more important, as interacting for extended periods may cause fatigue. Curved is a large-surface interaction device, designed to follow the natural movement of a stretched arm when performing gestures. It tracks one or two hands above the surface, using an array of capacitive proximity sensors and supports both touch and mid-air gestures. It requires specific object tracking methods and the synchronized measurement from 32 sensors. We have created an example application for users wearing a virtual reality headset while seated that may benefit from haptic feedback and ergonomically shaped surfaces. A prototype with adaptive curvature has been created that allows us to evaluate gesture recognition performance and different surface inclinations.}
}
@article{PIECZYNSKI2024107864,
title = {A fast, lightweight deep learning vision pipeline for autonomous UAV landing support with added robustness},
journal = {Engineering Applications of Artificial Intelligence},
volume = {131},
pages = {107864},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.107864},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624000228},
author = {Dominik Pieczyński and Bartosz Ptak and Marek Kraft and Mateusz Piechocki and Przemysław Aszkowski},
keywords = {Unmanned aerial vehicle, Landing support, Image processing, Deep learning, On-board processing},
abstract = {Despite massive development in aerial robotics, precise and autonomous landing in various conditions is still challenging. This process is affected by many factors, such as terrain shape, weather conditions, and the presence of obstacles. This paper describes a deep learning-accelerated image processing pipeline for accurate detection and relative pose estimation of the UAV with respect to the landing pad. Moreover, the system provides increased safety and robustness by implementing human presence detection and error estimation for both landing target detection and pose computation. Human presence and landing pad location are performed by estimating the presence probability via segmentation. This is followed by the landing pad keypoints’ location regression algorithm, which, in addition to coordinates, provides the uncertainty of presence for each defined landing pad landmark. To perform the aforementioned tasks, a set of lightweight neural network models was selected and evaluated. The resulting measurements of the system’s performance and accuracy are presented for each component individually and for the whole processing pipeline. The measurements are performed using onboard embedded UAV hardware and confirm that the method can provide accurate, low-latency feedback information for safe landing support.}
}
@article{FORTUNA2024104057,
title = {A comparative study of Augmented Reality rendering techniques for industrial assembly inspection},
journal = {Computers in Industry},
volume = {155},
pages = {104057},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104057},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523002075},
author = {Santina Fortuna and Loris Barbieri and Emanuele Marino and Fabio Bruno},
keywords = {Augmented Reality, Industrial assembly inspection, Quality inspection, AR rendering techniques, Industry 4.0},
abstract = {In the manufacturing industry, Augmented Reality (AR) has shown significant potential in enhancing operators’ capabilities while performing inspection and assembly activities. However, the augmented visualization of virtual models on physical components can present challenges and potential misunderstandings, as the visualization mode greatly influences the perception of components and the amount of information received. This study investigates the impact of rendering techniques on user performance during industrial assembly inspection tasks. In particular, a set of rendering techniques, suitable for industrial AR applications, have been selected and implemented through a dedicated AR tool. The selected AR rendering techniques have been compared, in terms of qualitative and quantitative metrics, in order to test their efficiency and effectiveness for industrial assembly inspection activities. 17 domain experts and 33 representative users have been involved in the experimental study which outcomes reveal that the rendering techniques play a significant role in assisting operators for identifying design discrepancies in industrial products. Furthermore, a correlation has been observed between the two AR rendering techniques best suited to support industrial inspection activities and the types of assembly errors detected.}
}
@article{SIGITOV2015257,
title = {Adopting a Game Engine for Large, High-Resolution Displays},
journal = {Procedia Computer Science},
volume = {75},
pages = {257-266},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.246},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915037072},
author = {Anton Sigitov and David Scherfgen and André Hinkenjann and Oliver Staadt},
keywords = {large-high-resolution displays, rapid prototyping tool, Unity, tools for education},
abstract = {The steadily decreasing prices of display technologies and computer graphics hardware contribute to the increasing popularity of multiple-display environments, like large, high-resolution displays. It is therefore necessary that educational organizations give the new generation of computer scientists an opportunity to become familiar with this kind of technology. However, there is a lack of tools that allow for getting started easily. Existing frameworks and libraries that provide support for multi-display rendering are often complex in understanding, configuration and extension. This is critical especially in educational context where the time that students have for their projects is limited and quite short. These tools are also rather known and used in research communities only, thus providing less benefit for future non-scientists. In this work we present an extension for the Unity game engine. The extension allows – with a small overhead – for implementation of applications that are apt to run on both single-display and multi-display systems. It takes care of the most common issues in the context of distributed and multi-display rendering like frame, camera and animation synchronization, thus reducing and simplifying the first steps into the topic. In conjunction with Unity, which significantly simplifies the creation of different kinds of virtual environments, the extension affords students to build mock-up virtual reality applications for large, high-resolution displays, and to implement and evaluate new interaction techniques and metaphors and visualization concepts. Unity itself, in our experience, is very popular among computer graphics students and therefore familiar to most of them. It is also often employed in projects of both research institutions and commercial organizations; so learning it will provide students with qualification in high demand.}
}
@article{CARLIN2024100639,
title = {An interactive framework to support decision-making for Digital Twin design},
journal = {Journal of Industrial Information Integration},
volume = {41},
pages = {100639},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100639},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000839},
author = {H M Carlin and P A Goodall and R I M Young and A A West},
keywords = {Digital twin, Decision-support, Design framework, Ontology},
abstract = {Producing a Digital Twin (DT) involves many inter-linking decisions. Existing research tends to describe the parts of a DT and how they work, but not the decision-making that goes into building a DT nor the consideration of alternative design options. There is therefore a need for decision support to guide developers to create DTs efficiently while meeting functional requirements such as accuracy and interoperability. This paper presents an ontology-based decision support framework to achieve this need. Firstly an analysis of the decisions required to create a predictive maintenance DT for an automotive manufacturer is performed. The analysis found that each decision point produces an output by consideration of various influencing factors, such as time constraints, computation limits and the required fidelity of the model. The network of decisions is complex, with the outcomes of earlier decisions influencing later ones. An IDEF0 diagram was found to be a useful way to represent decisions, their dependencies and their cross-linking. This knowledge was used to populate an ontology of DT components for a predictive maintenance DT. The ability of an ontology to describe concepts explicitly using standardised vocabulary ensures the integrity of the decision-making guidance. A demonstration of the functionality of the ontology-based decision support framework was made before an evaluation of the concept. The research is a fundamental component in producing decision support for DT creators so that manufacturers can realise the benefits of a connected, responsive and flexible facility.}
}
@article{SELIN2019283,
title = {Emergency exit planning and simulation environment using gamification, artificial intelligence and data analytics},
journal = {Procedia Computer Science},
volume = {156},
pages = {283-291},
year = {2019},
note = {8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311238},
author = {Jukka Selin and Mika Letonsaari and Markku Rossi},
keywords = {BIM, Gamification, Data Analytics, Simulation, AI},
abstract = {In our research, we explore methods to utilize gamified building data models and data analytics. In this paper, we present a study, where an emergency exit planning and simulation platform is implemented with a commercial center gamified data model. In the study, we compare various emergency exit location options and search the critical areas for customer evacuation. Customized user profiles are used to estimate the movement capabilities of elderly and handicapped people. The feasibility of data analytics methods for analyzing the simulation results is examined. The results show that simulations based on gamification are well-suited tools emergency exit evaluations.}
}
@article{LI2022546,
title = {Research on Key Technologies of Garbage Classification Virtual Simulation Game Development Based on unity3d Technology},
journal = {Procedia Computer Science},
volume = {208},
pages = {546-552},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.075},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922015162},
author = {Yuxuan Li and Yingying Zhu},
keywords = {Unity 3D, shadow creator SDK, refuse classification, virtual reality, game development},
abstract = {In recent years, the problems caused by garbage have become more and more serious, and many countries have begun to advocate the concept of environmental protection. China has also begun to implement the policy of waste classification, but the effect is not ideal. This is because the publicity of waste classification knowledge has not reached good expectations, and the traditional propaganda means such as static graphics and text can not give people a deep impression. Therefore, in order to better achieve the knowledge publicity of garbage classification, under the conditions of the continuous maturity of Internet technology and 5g technology, relying on virtual reality technology, combined with novel game mode and immersive game experience, through games, players' interest in learning garbage classification knowledge is increased, so as to improve the knowledge publicity efficiency of garbage classification. This paper discusses and studies a series of key technologies used in virtual simulation games, which use unity 3D as the development editor of the game engine, 3D Max as the model creation tool, shadow creator SDK as the VR technical support, and action one headset to display the game content.}
}
@article{ADENIYI20242996,
title = {Development of Two Dimension (2D) Game Engine with Finite State Machine (FSM) Based Artificial Intelligence (AI) Subsystem},
journal = {Procedia Computer Science},
volume = {235},
pages = {2996-3006},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.283},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924009621},
author = {Abidemi Emmanuel Adeniyi and Biswajit Brahma and Marion Olubunmi Adebiyi and Joseph Bamidele Awotunde and Rasheed Gbenga Jimoh and Enoch Olasinde and Anjan Bandyopadhyay},
keywords = {Artificial intelligence, 2D game engine, Finite state machine, software},
abstract = {With Al becoming more and more relevant in today’s world, this project aims to develop a 2D game engine with an Al subsystem for state-driven agents, which is rarely implemented by a lot of 2D engines out there. In this study, a 2D game engine was designed with an FSM (Finite State Machine)--based AL subsystem using state-driven game agents. The engine was implemented using the Javascript programming language and the WebGL 2.0 graphics library/API. It is targeted at web-based games/simulations. Components and subsystems include physics, audio, math, rendering, and AI (based on finite-state machines). The FSM-based AI subsystem is a solution aimed at reducing the ambiguity and performance hits associated with creating 2D game AI in the naive approach. The AI subsystem creates an interface for 2D games to be created with a common paradigm, and simulated with a great level of realism. The state machine used in this study is used to represent a variety of behaviours, such as wandering, attacking, and fleeing. The following conclusions were drawn as regards the impact of the AI approach used on rendering performance; the naive approach to implementing game AI is also compared with the FSM approach in terms of rendering (frames per second/ FPS, or frame rate). With the naive approach (vector math) being used to implement AI, there was a drop in the rendering frame rate to 50 FPS. The FSM approach didn’t affect the frame rate, which is usually at 60FPS. With a well-developed FSM (Finite State Machine), game agents can transition between states easily as a response to user input or stimulus from the game environment. The proposed method was tested by creating a prototype game with the 2D game engine. The prototype game was a straightforward side-scrolling platformer featuring a cast of non-player characters (NPCs). This approach to implementing 2D game AI goes a long way toward improving performance and mitigating ambiguity in the code.}
}
@article{HOSSEINI20242310,
title = {Immersive Interaction in Digital Factory: Metaverse in Manufacturing},
journal = {Procedia Computer Science},
volume = {232},
pages = {2310-2320},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.050},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002278},
author = {Shimasadat Hosseini and Ali Abbasi and Luis G. Magalhaes and Jaime C. Fonseca and Nuno M.C. {da Costa} and António H.J. Moreira and João Borges},
keywords = {Industry 4.0, Digital Twin, Shared Reality, Locomotion, Virtual Factory, Metaverse, Immersive Interaction, Human Motion Tracking System},
abstract = {Digital twins and virtual reality are pivotal technologies in the context of Industry 4.0, facilitating the design, simulation, optimization, and remote interaction with production systems. These technologies also present new prospects for developing immersive and hyper-realistic digital factories within the metaverse. This paper aims to enhance collaboration and communication in a 3D virtual world, focusing on shared reality and the metaverse's integration in digital factories. Our main contribution enables users to have full-body immersive interaction with virtual 3D assets and realistic locomotion in a 3D environment, fostering flexibility in collaboration and environment monitoring/control. To achieve this, we propose a systematic methodology for designing and implementing a digital twin-based factory with an IoT infrastructure. By replicating sensors and actuators in digital twins, real-time asset management synchronized with the physical factory is realized, empowering full-body interaction. To enable full-body interaction with virtual equipment, we employ a human motion tracking system. A proof-of-concept case study, developed using Unity3D game engine, Solace PubSub+ event streaming APIs, and Awinda Xsens wearable inertial sensors as the motion tracking system, validates our proposed methodology. The results of the case study demonstrate the successful integration of digital twins, virtual reality, and the metaverse, enabling real-time monitoring and control, full-body interaction, and immersive user experiences in the virtual environment.}
}
@article{NAGALINGAM2015423,
title = {User Experience of Educational Games: A Review of the Elements},
journal = {Procedia Computer Science},
volume = {72},
pages = {423-433},
year = {2015},
note = {The Third Information Systems International Conference 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.123},
url = {https://www.sciencedirect.com/science/article/pii/S187705091503584X},
author = {Vanisri Nagalingam and Roslina Ibrahim},
keywords = {User Experience, Human Computer Interaction, Educational games},
abstract = {Over the recent years, the study on User Experience (UX) have been an area of discussion among Human Computer Interaction (HCI) researchers. User Experience is a branch of HCI which focus on interaction between products and users thus in the era of growing digital games, UX plays an important part in identifying the appropriate or suitable variable in order to evaluate the UX design. This study explore about the UX elements for the purpose of evaluation and design of educational games (EG). EG have captivated most students with the idea of mixing fun with learning. A good framework of UX for educational game will help EG designer to evaluate the UX of their games in order to ensure that they have produced the effective game. Therefore, it is essential to identify the suitable elements in order to model the right UX framework for educational games.}
}
@article{ZHANG20223516,
title = {Using Simulation-software-generated Animations to Investigate Attitudes Towards Autonomous Vehicles Accidents},
journal = {Procedia Computer Science},
volume = {207},
pages = {3516-3525},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.410},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013023},
author = {Qiyuan Zhang and Christopher Wallbridge and Phillip L Morgan and Dylan M Jones},
keywords = {Autonomous driving, autonomous systems, autonomous vehicles accidents, road accidents, drivinng simulation, blame attribution, liability, trust in automation, human-robot interaction, artificial intelligence},
abstract = {Road accidents involving autonomous vehicles are inevitable and have the potential to damage the public's confidence in the technology and ultimately result in its disuse. It's important to understand how people react to such incidents and the influencing factors of blame attribution and trust restoration. Research in this field has started to grow but faces a huge methodological challenge, which is to develop high-fidelity experimental stimuli as realistic representations of accident scenarios in order to elicit valid reactions from human participants. The present paper reviews and evaluates several existing methods used in the research field before proposing an alternative method of generating animated accident sequence using driving simulation software. It is argued that this method strikes a good balance of fidelity, versatility and cost-effectiveness. We also present some preliminary evidence for the effectiveness of variable manipulation using such a methodology.}
}
@article{BHAUMIK2023488,
title = {An Intelligent Virtual Environment for Designers with Reduced Motor Abilities},
journal = {Procedia Computer Science},
volume = {218},
pages = {488-503},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000315},
author = {Rahul Bhaumik and Tarun Kumar and Unais Sait},
keywords = {Virtual Reality, design tool, intelligent interface, gaze-based input, artificial intelligence},
abstract = {Conventional CAD modelling software demands substantial utilisation of input modalities like the keyboard and mouse for creating 3-dimensional (3D) models. The dexterity measures involved in controlling input modalities could pose challenges to users with motor disabilities—including the inability to move their limbs, particularly their upper and lower arms, and fingers, due to traumatic damage or congenital problems. In order to meet these challenges, this paper proposes a virtual reality (VR)-based medium to help users with motor disabilities build simple 3D models for architectural design. The concept of operating buttons using head-gaze in the VR environment has been utilised to perform scaling—a 3D object manipulation method—to create simplified building models. Moreover, navigation in the VR space using tilting of the head has been employed with the user seated on a revolving chair, thus eliminating the need for any limbic movement. Unity game engine was used to develop two variations of the VR model with a different button layout for creating simple cuboidal volumes mimicking buildings in the virtual environment. Both variations have been tested with 32 individuals against a specific performance indicator (i.e., task completion time) and self-reported metrics, such as the perception of effort applied and degree of visual clutter, followed by retrospective participant feedback sessions. One of the VR application's variants (i.e., variant 1) produced promising results regarding overall usability and effort demand. This paper also proposes a methodological framework for an AI-based, intelligent, and adaptive VR application interface that caters to the user's abilities and pain points in real-time. In the future, this framework could be instrumental in creating a comprehensive gaze-based VR tool for 3D modelling having multiple functions to help users with motor disabilities.}
}
@article{SUNAN2023591,
title = {Feasible Technology for Augmented Reality in Fashion Retail by Implementing a Virtual Fitting Room},
journal = {Procedia Computer Science},
volume = {227},
pages = {591-598},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.562},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017295},
author = {Ronald Sumichael Sunan and Samuel Christopher and Novandy Salim and  Anderies and Andry Chowanda},
keywords = {Augmented Reality, Virtual Fitting Room, Kinect, Fashion Retail, Mobile Application},
abstract = {Augmented Reality has become a bridge from the virtual to the real world. This technology has been an implementable choice for fashion companies to provide more information and customization for their user's shopping experience. From the COVID-19 pandemic alongside technological globalization, fashion industries have to find a new way for customers to purchase products. All things considered, AR helped fashion industries keep customers attracted and satisfied. This paper is conducted in a Systematic Literature Review manner to review and compare the AR technologies used by fashion industries. Furthermore, the result would be a proposition of which Augmented Reality technology or platform option is feasible and implementable for the fashion industry. After comparing the technologies used in past papers and their technical and economic feasibility, we propose a 3-term plan. The short-term plan with zero cost and mobile platform-focused application, the medium-term plan with an upgraded version of the garment object from 2D images to 3D objects, and the long-term plan changing the platform used from mobile to a new device. We also conclude that artificial intelligence can also support and improve the augmented reality experience for customers in the fashion retail industry by implementing an AI-powered virtual assistant.}
}
@article{CHHEANG2024103879,
title = {Advanced liver surgery training in collaborative VR environments},
journal = {Computers & Graphics},
volume = {119},
pages = {103879},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000050},
author = {Vuthea Chheang and Danny Schott and Patrick Saalfeld and Lukas Vradelis and Tobias Huber and Florentine Huettl and Hauke Lang and Bernhard Preim and Christian Hansen},
keywords = {Virtual reality, Collaborative VR, Medical training, Liver surgery planning, Laparoscopic surgery training},
abstract = {Virtual surgical training systems are crucial for enabling mental preparation, supporting decision-making, and improving surgical skills. Many virtual surgical training environments focus only on training for a specific medical skill and take place in a single virtual room. However, surgical education and training include the planning of procedures as well as interventions in the operating room context. Moreover, collaboration among surgeons and other medical professionals is only applicable to a limited extent. This work presents a collaborative VR environment similar to a virtual teaching hospital to support surgical training and interprofessional collaboration in a co-located or remote environment. The environment supports photo-realistic avatars and scenarios ranging from planning to training procedures in the virtual operating room. It includes a lobby, a virtual surgical planning room with four surgical planning stations, laparoscopic liver surgery training with the integration of laparoscopic surgical instruments, and medical training scenarios for interprofessional team training in a virtual operating room. Each component was evaluated by domain experts as well as in a series of user studies, providing insights on usability, usefulness, and potential research directions. The proposed environment may serve as a foundation for future medical training simulators.}
}
@article{NUTONEN2023446,
title = {Industrial Robot Training in the Simulation Using the Machine Learning Agent},
journal = {Procedia Computer Science},
volume = {217},
pages = {446-455},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.240},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023183},
author = {Karle Nutonen and Vladimir Kuts and Tauno Otto},
keywords = {simulation, inverse kinematics, machine learning, robotics, Industry 4.0, virtual environment},
abstract = {Continuous change in manufacturing requires robotization, requiring a skilled workforce with robotic skills. It is also important for a manufacturing company to be able to transform its production process quickly. But now it is a long and complex process. The paper presents the simulation of the movement of an industrial robot in a digital environment, to which implemented the inverse kinematics functionality and machine learning model have been applied. The use of machine learning reduces the time required to develop the process and the investment in finding the path of the robot. The results obtained in the application of Bio-ik inverse kinematics and machine learning have been observed and analyzed as a simulation in the created research.}
}
@article{ALONSO2023306,
title = {Real-time rendering and physics of complex dynamic terrains modeled as CSG trees of DEMs carved with spheres},
journal = {Computers & Graphics},
volume = {114},
pages = {306-315},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323001152},
author = {Jesús Alonso and Robert Joan-Arinyo and Antoni Chica},
keywords = {Terrain modeling, CSG, Terrain erosion},
abstract = {We present a novel proposal for modeling complex dynamic terrains that offers real-time rendering, dynamic updates and physical interaction of entities simultaneously. We can capture any feature from landscapes including tunnels, overhangs and caves, and we can conduct a total destruction of the terrain. Our approach is based on a Constructive Solid Geometry tree, where a set of spheres are subtracted from a base Digital Elevation Model. Erosions on terrain are easily and efficiently carried out with a spherical sculpting tool with pixel-perfect accuracy. Real-time rendering performance is achieved by applying a one-direction CPU–GPU communication strategy and using the standard depth and stencil buffer functionalities provided by any graphics processor.}
}
@article{LATINI2024104286,
title = {Investigating the impact of greenery elements in office environments on cognitive performance, visual attention and distraction: An eye-tracking pilot-study in virtual reality},
journal = {Applied Ergonomics},
volume = {118},
pages = {104286},
year = {2024},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2024.104286},
url = {https://www.sciencedirect.com/science/article/pii/S0003687024000632},
author = {Arianna Latini and Ludovica Marcelli and Elisa {Di Giuseppe} and Marco D'Orazio},
keywords = {Eye-tracking, Virtual Reality, Greenery indoors, Cognitive tasks, Office environment},
abstract = {The human-nature connection is one of the main aspects determining supportive and comfortable office environments. In this context, the application of eye-tracking-equipped Virtual Reality (VR) devices to support an evaluation on the effect of greenery elements indoors on individuals’ efficiency and engagement is limited. A new approach to investigate visual attention, distraction, cognitive load and performance in this field is carried out via a pilot-study comparing three virtual office layouts (Indoor Green, Outdoor Green and Non-Biophilic). 63 participants completed cognitive tasks and surveys while measuring gaze behaviour. Sense of presence, immersivity and cybersickness results supported the ecological validity of VR. Visual attention was positively influenced by the proximity of users to the greenery element, while visual distraction from tasks was negatively influenced by the dimension of the greenery. In the presence of greenery elements, lower cognitive loads and more efficient information searching, resulting in improved performance, were also highlighted.}
}
@article{VANWEZEL202389,
title = {Virtual Ray Tracer 2.0},
journal = {Computers & Graphics},
volume = {111},
pages = {89-102},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323000067},
author = {Chris S. {van Wezel} and Willard A. {Verschoore de la Houssaije} and Steffen Frey and Jiří Kosinka},
keywords = {Ray tracing, Computer graphics education, Visualization},
abstract = {Building on our original Virtual Ray Tracer tool, we present Virtual Ray Tracer 2.0, an interactive and gamified application that allows students/users to view and explore the ray tracing process in real-time. The application shows a scene containing a camera casting rays which interact with objects in the scene. Users are able to modify and explore ray properties such as their animation speed, the number of rays and their visual style, as well as the material properties of the objects in the scene. The goal of the application is to help the users – students of Computer Graphics and the general public – to better understand the ray tracing process and its characteristics. This includes not only the basics of ray tracing, but also more advanced concepts such as soft shadows. To invite users to learn and explore, various explanations and scenes are provided by the application at different levels of complexity, each with a step-by-step tutorial. Several user studies showed the effectiveness of the tool in supporting the understanding and teaching of ray tracing. The educational tool is built with the cross-platform engine Unity, and we make it fully available to be extended and/or adjusted to fit the requirements of courses at other institutions, educational tutorials, or of enthusiasts from the general public.}
}
@article{CROISSANT2023100591,
title = {Theories, methodologies, and effects of affect-adaptive games: A systematic review},
journal = {Entertainment Computing},
volume = {47},
pages = {100591},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100591},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000460},
author = {Maximilian Croissant and Guy Schofield and Cade McCall},
keywords = {Adaptation, Emotion, Video games, Affective computing},
abstract = {Affect-adaptive games gained in popularity over the last years in human computer interactions studies, promising potential benefits for player experience, performance, and even health. It is however not yet clear how affective games are being evaluated, what the precise effects are, and how they are based on emotion theoretical concepts that are still not universally agreed upon. This systematic review investigated these questions by analysing relevant high-quality evaluation studies of the effect of affect-adaptive video games on various outcomes in regards to their effects, theoretical assumptions, and methodologies. Out of 3,930 papers, 26 studies were included based on preregistered inclusion and exclusion criteria. A high variance regarding theoretical assumptions and methodological approaches was observed, as well as an overall poor methodological rigour, leading to the conclusion that more work is needed in constructing better methodological standards for game evaluation studies and theoretical considerations when developing and testing affect-adaptive video games.}
}
@article{RALLIS2022513,
title = {A mobile game for enhancing Tourism and Cultural Heritage},
journal = {Procedia Computer Science},
volume = {204},
pages = {513-518},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008006},
author = {Ioannis Rallis and George Kopsiaftis and Ilias Kalisperakis and Christos Stentoumis and Dimitris Koutsomitsos and Vivian Riga},
keywords = {Tourism, Photogrammetry, Mobile Game, Cultural Heritage},
abstract = {This paper briefly describes the overall concept of ”TRAVEL TYCOON GREECE” (TTGR), a novel business simulation game which aims to simulate realistically a complete tourism experience. The latest image processing and computer graphics technologies were utilized to create accurate 3D backgrounds with different levels of detail, which were incorporated in the game engine and serve as a realistic and accurate terrain allowing the user to navigate in selected historical and touristic areas of Greece. A series of real-world scenarios representing multiple components of the tourism section were designed primarily for entertainment and marketing purposes. In order to motivate users to participate in the game or remain active, an incorporated ticketing platform allows users to win offers such as touristic products and services.}
}
@article{MORENOLUMBRERAS2023107064,
title = {CodeCity: A comparison of on-screen and virtual reality},
journal = {Information and Software Technology},
volume = {153},
pages = {107064},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107064},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001732},
author = {David Moreno-Lumbreras and Roberto Minelli and Andrea Villaverde and Jesus M. Gonzalez-Barahona and Michele Lanza},
keywords = {CodeCity, City metaphor, Software visualization, Software evolution, Reverse engineering, Virtual reality, Web, 3D},
abstract = {Context:
Over the past decades, researchers proposed numerous approaches to visualize source code. A popular one is CodeCity, an interactive 3D software visualization representing software system as cities: buildings represent classes (or files) and districts represent packages (or folders). Building dimensions represent values of software metrics, such as number of methods or lines of code. There are many implementations of CodeCity, the vast majority of them running on-screen. Recently, some implementations using virtual reality (VR) have appeared, but the usefulness of CodeCity in VR is still to be proven.
Aim:
Our comparative study aims to answer the question “Is VR well suited for CodeCity, compared to the traditional on-screen implementation?”
Methods:
We performed two experiments with our web-based implementation of CodeCity, which can be used on-screen or in immersive VR. First, we conducted a controlled experiment involving 24 participants from academia and industry. Taking advantage of the obtained feedback, we improved our approach and conducted a second controlled experiment with 26 new participants.
Results:
Our results show that people using the VR version performed the assigned tasks in much less time, while maintaining a comparable level of correctness.
Conclusion:
VR is at least equally well-suited as on-screen for visualizing CodeCity, and likely better.}
}
@article{SUNO20232283,
title = {Virtual Hydrogen, a virtual reality education tool in physics and chemistry},
journal = {Procedia Computer Science},
volume = {225},
pages = {2283-2291},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.219},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013777},
author = {Hiroya Suno and Nobuaki Ohno},
keywords = {Virtual reality, education tool, physics, chemistry},
abstract = {In efforts to explore possibilities of virtual reality (VR) as education tools, we focus on one of the most important subjects taught in undergraduate physics and chemistry classes, namely, the quantum atomic orbitals or wave functions of the hydrogen atom. Our goal is to enable students to experience attractive and enjoyable scenarios which make it more amenable to master the concepts of quantum mechanics or quantum chemistry. Along this line, we have built a virtual environment of the hydrogen atom, named “Virtual Hydrogen”, enabling to explore the atomic orbitals in 3D space. The most fundamental 21 atomic orbitals of hydrogen have been successfully visualized and will be experienced through modern VR head-mounted displays.}
}
@article{MATTHEWS2020330,
title = {Interaction design for paediatric emergency VR training},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {4},
pages = {330-344},
year = {2020},
note = {VR and experiment simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300590},
author = {Tj Matthews and Feng Tian and Tom Dolby},
keywords = {Virtual reality, Medical training, Human-Centred design, Interaction design},
abstract = {Background
Virtual reality (VR) in healthcare training has increased adoption and support, but efforts are still required to mitigate usability concerns.
Methods
This study conducted a usability study of an in-use emergency medicine VR training application, available on commercially available VR hardware and with a standard interaction design. Nine users without prior VR experience but with relevant medical expertise completed two simulation scenarios for a total of 18 recorded sessions. They completed NASA Task Load Index and System Usability Scale questionnaires after each session, and their performance was recorded for the tracking of user errors.
Results and Conclusion
s Our results showed a medium (and potentially optimal) Workload and an above average System Usability Score. There was significant improvement in several factors between users' first and second sessions, notably increased Performance evaluation. User errors with the strongest correlation to usability were not directly tied to interaction design, however, but to a limited 'possibility space'. Suggestions for closing this 'gulf of execution' were presented, including 'voice control' and 'hand-tracking', which are only feasible for this commercial product now with the availability of the Oculus Quest headset. Moreover, wider implications for VR medical training were outlined, and potential next steps towards a standardized design identified.}
}
@article{VIRTANEN2020375,
title = {Interactive dense point clouds in a game engine},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {163},
pages = {375-389},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620300691},
author = {Juho-Pekka Virtanen and Sylvie Daniel and Tuomas Turppa and Lingli Zhu and Arttu Julin and Hannu Hyyppä and Juha Hyyppä},
keywords = {Point cloud, Game engine, VR},
abstract = {With the development of 3D measurement systems, dense colored point clouds are increasingly available. However, up to now, their use in interactive applications has been restricted by the lack of support for point clouds in game engines. In addition, many of the existing applications for point clouds lack the capacity for fluent user interaction and application development. In this paper, we present the development and architecture of a game engine extension facilitating the interactive visualization of dense point clouds. The extension allows the development of game engine applications where users edit and interact with point clouds. To demonstrate the capabilities of the developed extension, a virtual reality head-mounted display is used and the rendering performance is evaluated. The result shows that the developed tools are sufficient for supporting real-time 3D visualization and interaction. Several promising use cases can be envisioned, including both the use of point clouds as 3D assets in interactive applications and leveraging the game engine point clouds in geomatics.}
}
@article{GOLUBEV2018443,
title = {A framework for a multi-agent traffic simulation using combined behavioural models},
journal = {Procedia Computer Science},
volume = {136},
pages = {443-452},
year = {2018},
note = {7th International Young Scientists Conference on Computational Science, YSC2018, 02-06 July2018, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.267},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918315722},
author = {Kirill Golubev and Aleksandr Zagarskikh and Andrey Karsakov},
keywords = {Traffic modelling, car-following models, agent-based modelling, scientific visualization},
abstract = {The task of simulation of urban processes often concerns city traffic and road dynamics. Many assumptions in an urban science may be made by analysing traffic flows and vehicle behaviour on city streets. Since road traffic is a dynamic and possibly spatially large system that depends on many external conditions, approaches like visual analysis and computational steering may be used. There are many traffic modelling frameworks presented in the field, but there are also several drawbacks most of them have: they are limited to a specific model or case; have abstracted logic which disregards some factors affecting vehicle movement; they ignore possible physical interactions which are not described in a model or use too many simplifications in vehicle interaction simulations. In this paper, a novel agent-based traffic modelling framework is presented, combining different classes of traffic models into a single vehicle agent and allowing the user to set a specific model for each supported class. The framework was made using a game engine, Unreal Engine 4, which allows users to model realistic interaction between vehicles and make realistic visualisations for purposes of visual analysis and computational steering.}
}
@article{LONGO2023100437,
title = {From “prepare for the unknown” to “train for what's coming”: A digital twin-driven and cognitive training approach for the workforce of the future in smart factories},
journal = {Journal of Industrial Information Integration},
volume = {32},
pages = {100437},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100437},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X23000109},
author = {Francesco Longo and Antonio Padovano and Fabio {De Felice} and Antonella Petrillo and Mohaiad Elbasheer},
keywords = {Smart factory, Prescriptive analytics, Knowledge management, Virtual reality, Workforce training, Fuzzy cognitive maps},
abstract = {Despite on-the-job assistance technology is getting popular in the Smart Operator 4.0 literature, non-routine work also requires highly-skilled and pre-trained workers to prevent serious errors and keep high efficiency and safety levels. The question that motivates this study is: ‘is it possible to train industrial workers for what exactly is coming instead of preparing them for a large set of scenarios - even very unlikely ones?’. This work proposes: (i) a structured On-the-Job Training strategy for non-routine tasks, called ‘training-on-the-go’ or ‘prescriptive training’, according to which a Prescriptive Analytics module schedules the training sessions not long before the actual performance, but only when and if needed; (ii) a proof-of-concept of a game-based training system where virtual scenes and context of an industrial site are faithfully recreated thanks to digital twin data and models; (iii) the use of evolutionary-based fuzzy cognitive maps (E-FCM) for the extraction of the workers’ implicit procedural knowledge and for the comparison of mental models of experienced vs. inexperienced workers to assess potential misconceptions or flaws in their decision-making process. This work contributes to the evolution of worker training paradigms and systems from the perspective of human-centric cyber-physical production systems and aims for current gaps in workforce training, i.e. poor timeliness and effectiveness, limited context and industrial information integration, and scarce focus on the experts’ implicit knowledge. An application study with a non-routine task on an offshore oil platform demonstrates how the proposed system facilitates knowledge transfer, offers situational awareness and sustains the workforce competence development process.}
}
@article{RODEN200776,
title = {Toward mobile entertainment: A paradigm for narrative-based audio only games},
journal = {Science of Computer Programming},
volume = {67},
number = {1},
pages = {76-90},
year = {2007},
note = {Special Issue on Aspects of Game Programming},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2006.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S016764230700055X},
author = {Timothy E. Roden and Ian Parberry and David Ducrest},
keywords = {Computer games, Entertainment computing, 3D audio, Interactive audio},
abstract = {The widespread use of sophisticated mobile computing devices has set the stage for a renaissance in audio only entertainment. Traditional visual games are already used widely in cellular phones and similar devices. A significant limitation is the small display size. In contrast, audio only games on suitable mobile hardware need not degrade due to the smaller form factor. This makes audio only games an attractive alternative to visual games. We describe a framework for authoring interactive narrative-based audio only games set in 3D virtual environments. Despite the novelty in audio only gaming, our approach builds on a foundation of several years of research into audio only applications for sight impaired users, augmented reality systems and human–computer interaction studies. In comparison to attempts to provide a realistic user interface, we argue a simple interface enhances both immersion and entertainment value, serendipitously making audio only games practical for mobile computing. Novel features of our system include real-time gameplay and multi-player support. We also describe our software architecture, the current implementation of which uses low-cost existing PC-based hardware and software. In addition, we describe our first game, Dragon’s Roar.}
}
@article{BERGER20152913,
title = {CFD Post-processing in Unity3D},
journal = {Procedia Computer Science},
volume = {51},
pages = {2913-2922},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.476},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915012843},
author = {Matthias Berger and Verina Cristie},
keywords = {CFD post-processing, Unity3D, urban climate, urban designs, visualization},
abstract = {In architecture and urban design, urban climate on is a strong design criterion for outdoor thermal comfort and building's energy performance. Evaluating the effect of buildings on the local climate and vice versa is done by computational fluid dynamics (CFD) methods. The results from CFD are typically visualized through post-processing software closely related to pre-processing and simulation software. The built-in functions are made for engineers and thus, it lacks user-friendliness for real-time exploration of results for architects. To bridge the gap between architect and engineer we propose visualizations based on game engine technology. This paper demonstrates the implementation of CFD to Unity3D conversion and weather data visualization.}
}
@article{ROBYNS20242366,
title = {A Digital Twin of an Off Highway Vehicle based on a Low Cost Camera},
journal = {Procedia Computer Science},
volume = {232},
pages = {2366-2375},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.055},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002321},
author = {Steven Robyns and Wouter Heerwegh and Sam Weckx},
keywords = {Digital Twin, Edge computing, Decision making, Construction 4.0},
abstract = {The operation of heavy equipment in the construction industry requires high-skilled workers: they must safely operate the equipment in an efficient manner to avoid project delays or damages, both to equipment and material as to their colleagues and themselves. Operator productivity and safety are therefore critical in the construction industry. The advancements in technologies such as digitization and automation are more and more applied in the industry, leading to more data and knowledge on the construction equipment and environment. This can be leveraged by a digital twin to provide value for the operator. In this work, we present a digital twin architecture applied to an off highway vehicle, which provides real-time feedback to the operator while operating the vehicle. Monitoring the interactions between an off highway vehicle and workers will result in a safer and more productive environment. This work focuses on detection of the environment and monitoring the state of the vehicle. Images of a low cost camera are used to extract information of the vehicle implement as well as the environment. The motion and state of the machine can be realistically visualized in real-time in Unreal, using a 3D CAD (Computer-Aided Design) geometry model, enriched with additional relevant information and color schemes. A modular pub-sub architecture enables the communication between the sensors and camera, the data processing and the visualization of the digital twin.}
}
@article{REZANAJI2022101077,
title = {Accelerating sailfish optimization applied to unconstrained optimization problems on graphical processing unit},
journal = {Engineering Science and Technology, an International Journal},
volume = {32},
pages = {101077},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621002093},
author = {Hamid {Reza Naji} and Soodeh Shadravan and Hossien {Mousa Jafarabadi} and Hossien Momeni},
keywords = {Sailfish Optimizer (SFO), Accelerated Sailfish Optimizer (ASFO) unconstrained optimization problems, Parallel processing, Shared memory, Graphic processing units, CUDA},
abstract = {The Sailfish Optimizer (SFO) is a metaheuristic algorithm inspired by a group of hunting sailfish that alternates their attacks on group of prey. The SFO algorithm takes advantage of using a simple method for providing the dynamic balance between exploration and exploitation phases, creating the swarm diversity, avoiding local optima, and guaranteeing high convergence speed. However, taking a lot of time to solve optimization problems has become a challenge for metaheuristic algorithms. Due to independence of the metaheuristics components, parallel processing is a good option to reduce the computational time and to find high quality solutions that are close to the optimum with an acceptable cost. Nowadays, combination of parallel processing and metaheuristic algorithms can provide high performance solutions to quickly solve combinatorial optimization problems. In this paper, we elaborate a novel GPU based and accelerated method of sailfish optimizer (ASFO), which improves the execution time and speedup while maintaining the results of optimization in high quality. In depth of study, we present the implementation details and performance observations of ASFO algorithm. Also, a comparative study of accelerated and sequential SFO is performed on a set of standard benchmark optimization functions and it compared with other parallel algorithms to show the speed of proposed algorithm for solving unconstrained optimization problems. The results indicate the ability of proposed approach in continuous, non-separable, non-convex and scalable optimization problems.}
}
@article{WANG2024100150,
title = {Gamifying cultural heritage: Exploring the potential of immersive virtual exhibitions},
journal = {Telematics and Informatics Reports},
volume = {15},
pages = {100150},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100150},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000367},
author = {Hanbing Wang and Ze Gao and Xiaolin Zhang and Junyan Du and Yidan Xu and Ziqi Wang},
keywords = {Cultural Heritage, Gamification, Human–computer interaction, Immersive virtual exhibition, Review},
abstract = {This paper reviews the potential of gamified cultural heritage in immersive virtual exhibitions. A systematic literature review following PRISMA guidelines identified 78 relevant papers from ACM and IEEE databases. Gamification and immersive technologies can provide interactive experiences to engage visitors and enhance their understanding of exhibits’ historical and cultural significance. Theoretical frameworks, including gamification theory, heritage interpretation theory, participatory heritage, immersive experience theory, and pedagogy, guide designing compelling experiences. Case studies like “Rome Reborn”, “Sutton House Stories”, and “Assassin’s Creed: Origins” demonstrate the efficacy of gamification in disseminating heritage. Key strategies include integrating augmented/virtual reality, multimodal data and 3D reconstruction, interactive narratives and gameplay, personalized experiences, advanced interfaces, balancing education and entertainment, and ensuring cultural sensitivity. Future work can explore AI-adaptive experiences, AR/VR integration, remote collaboration, educational game elements, and digital creativity models. Gamification and immersion provide innovative preservation and inheritance of cultural heritage. This review promotes digitalization and identifies literature gaps, supporting reflection on engagement’s past, present, and future. It aims to enable a broader appreciation of cultural heritage through technology.}
}
@article{MASOOD20229393,
title = {A novel method for adaptive terrain rendering using memory-efficient tessellation codes for virtual globes},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part B},
pages = {9393-9408},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822003469},
author = {Zafar Masood and Zheng Jiangbin and Idrees Ahmad and Muhammad Irfan and Nafees Ahmad},
keywords = {Real-time graphics, Terrain modelling, Level of detail, Hardware tessellation, Virtual globes},
abstract = {Virtual globes render large-scale real-world earth’s surface in real-time for geographic information visualization. Modern GPUs have introduced hardware tessellation features for high-performance rendering. Patch-based terrain rendering using hardware tessellation introduces cracks and swimming artifacts during navigation. Hardware tessellation-based terrain rendering methods limit the tessellation factors to power-of-two and render patches with discrete-level-of-detail (DLOD). We present a novel method for adaptive terrain rendering using tessellation codes. In the proposed method, we present novel memory-efficient bit-field-based tessellation codes to encode patch Level-of-detail (LOD). The encoding scheme enabled patch rendering with Continuous-level-of-detail (CLOD) on tessellation hardware. The view-dependent simplification algorithm simplifies and encodes the patch’s Level-of-detail (LOD) in tessellation codes. The proposed method, based on tessellation codes, tessellates patches as uniformly spaced vertices grid. A novel relocation algorithm, based on tessellation codes, relocates uniformly spaced vertices to construct an encoded simplified vertices grid to avoid boundary cracks and swimming artifacts. The tessellation codes consumed 28 kilobytes of memory for a patch size of 65. The proposed method rendered 2.5 million triangles with a frame rate of 570 frames-per second for an ultra-HD display. The proposed method achieved a high simplification and rendering rate with low computational and memory usage as compared to state-of-the-art methods.}
}
@article{STRANNEGARD2024101235,
title = {Survival games for humans and machines},
journal = {Cognitive Systems Research},
volume = {86},
pages = {101235},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101235},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000299},
author = {Claes Strannegård and Niklas Engsner and Simon Ulfsbäcker and Sebastian Andreasson and John Endler and Ann Nordgren},
keywords = {Human versus machine performance, Pure survival games, Deep reinforcement learning, Random worlds, Artificial general intelligence},
abstract = {Survival games can be described as video games where the player searches for food and treasures, while avoiding obstacles and hostile attacks. Ms.Pac-Man and Minecraft are two well-known examples. Currently there are AI models that outperform human players at Ms.Pac-Man, while AI models playing Minecraft above the human level has been a long-standing challenge. This paper concerns what we call pure survival games, which take place in previously unseen worlds containing only food, water, and obstacles. The challenge of the player is to navigate and survive in those worlds by continuously finding resources and avoiding obstacles. Arguably, animals need to master physical analogues of pure survival games in order to survive and reproduce. Here we begin to explore human and machine performance on pure survival games. We define two games called the Grid game and the Terrain game and two corresponding AI agents based on deep reinforcement learning: the Grid agent and the Terrain agent. We explore to what extent these agents can match human performance and how their performance is affected by variations in their perception, memory, and reward models. We find that (1) the Terrain agent performs above human level, while the Grid agent performs below human level; (2) the smell, touch, and interoception models contribute significantly to the performance of the Grid agent; (3) the memory model contributes significantly to the performance of the Grid agent; and (4) the performance of the Grid agent is relatively stable under three quite different reward signals, including one that rewards survival and nothing else.}
}
@article{SCHIEBER2024103907,
title = {Indoor Synthetic Data Generation: A Systematic Review},
journal = {Computer Vision and Image Understanding},
volume = {240},
pages = {103907},
year = {2024},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2023.103907},
url = {https://www.sciencedirect.com/science/article/pii/S1077314223002874},
author = {Hannah Schieber and Kubilay Can Demir and Constantin Kleinbeck and Seung Hee Yang and Daniel Roth},
keywords = {Synthetic data generation, Indoor synthetic data, Domain randomization},
abstract = {Objective:
Deep learning-based object recognition, 6D pose estimation, and semantic scene understanding require a large amount of training data to achieve generalization. Time-consuming annotation processes, privacy, and security aspects lead to a scarcity of real-world datasets. To overcome this lack of data, synthetic data generation has been proposed, including multiple facets in the area of domain randomization to extend the data distribution. The objective of this review is to identify methods applied for synthetic data generation aiming to improve 6D pose estimation, object recognition, and semantic scene understanding in indoor scenarios. We further review methods used to extend the data distribution and discuss best practices to bridge the gap between synthetic and real-world data.
Methods:
We adhered to the guidelines of the systematic PRISMA technique. Three databases, IEEE Xplore, Springer Link, and ACM, and an additional manual search were conducted. In total, we identified 241 studies and included 34 in our systematic review.
Conclusion:
In summary, synthetic data generation has been performed using crop-out methods, graphic APIs, 3D modeling or authoring tools, or game engine-based methods. To extend the data distribution, varying scene parameters, i.e., lighting conditions or textures and the use of distracting objects in the scene are promising.}
}
@article{SINGH2020103275,
title = {Combining gaze and AI planning for online human intention recognition},
journal = {Artificial Intelligence},
volume = {284},
pages = {103275},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2020.103275},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218307628},
author = {Ronal Singh and Tim Miller and Joshua Newn and Eduardo Velloso and Frank Vetere and Liz Sonenberg},
keywords = {Intention recognition, Gaze, Planning},
abstract = {Intention recognition is the process of using behavioural cues, such as deliberative actions, eye gaze, and gestures, to infer an agent's goals or future behaviour. In artificial intelligence, one approach for intention recognition is to use a model of possible behaviour to rate intentions as more likely if they are a better ‘fit’ to actions observed so far. In this paper, we draw from literature linking gaze and visual attention, and we propose a novel model of online human intention recognition that combines gaze and model-based AI planning to build probability distributions over a set of possible intentions. In human-behavioural experiments (n=40) involving a multi-player board game, we demonstrate that adding gaze-based priors to model-based intention recognition improved the accuracy of intention recognition by 22% (p<0.05), determined those intentions ≈90 seconds earlier (p<0.05), and at no additional computational cost. We also demonstrate that, when evaluated in the presence of semi-rational or deceptive gaze behaviours, the proposed model is significantly more accurate (9% improvement) (p<0.05) compared to a model-based or gaze only approaches. Our results indicate that the proposed model could be used to design novel human-agent interactions in cases when we are unsure whether a person is honest, deceitful, or semi-rational.}
}
@article{LIU2020132,
title = {Two-phase real-time rendering method for realistic water refraction},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {2},
pages = {132-141},
year = {2020},
note = {Special issue on Visual interaction and its application},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2019.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300164},
author = {Hongli Liu and Honglei Han and Guangzheng Fei},
keywords = {Real-time rendering, Refraction, Liquid rendering},
abstract = {Background
Realistic rendering has been an important goal of several interactive applications, which requires an efficient virtual simulation of many special effects that are common in the real world. However, refraction is often ignored in these applications. Rendering the refraction effect is extremely complicated and time-consuming.
Methods
In this study, a simple, efficient, and fast rendering technique of water refraction effects is proposed. This technique comprises a broad and narrow phase. In the broad phase, the water surface is considered flat. The vertices of underwater meshes are transformed based on Snell's Law. In the narrow phase, the effects of waves on the water surface are examined. Every pixel on the water surface mesh is collected by a screen-space method with an extra rendering pass. The broad phase redirects most pixels that need to be recalculated in the narrow phase to the pixels in the rendering buffer.
Results
We analyzed the performances of three different conventional methods and ours in rendering refraction effects for the same scenes. The proposed method obtains higher frame rate and physical accuracy comparing with other methods. It is used in several game scene, and realistic water refraction effects can be generated efficiently.
Conclusions
The two-phase water refraction method produces a tradeoff between efficiency and quality. It is easy to implementin modern game engines, and thus improve the quality of rendering scenes in video games or other real-time applications.}
}
@article{RONNOW202136,
title = {Fast analytical motion blur with transparency},
journal = {Computers & Graphics},
volume = {95},
pages = {36-46},
year = {2021},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2021.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0097849321000066},
author = {Mads J.L. Rønnow and Ulf Assarsson and Marco Fratarcangeli},
keywords = {Real-time rendering, Motion blur, Parallel computing},
abstract = {We introduce a practical parallel technique to achieve real-time motion blur for textured and semi-transparent triangles with high accuracy using modern commodity GPUs. In our approach, moving triangles are represented as prisms. Each prism is bounded by the initial and final position of the triangle during one animation frame and three bilinear patches on the sides. Each prism covers a number of pixels for a certain amount of time according to its trajectory on the screen. We efficiently find, store and sort the list of prisms covering each pixel including the amount of time the pixel is covered by each prism. This information, together with the color, texture, normal, and transparency of the pixel, is used to resolve its final color. We demonstrate the performance, scalability, and generality of our approach in a number of test scenarios, showing that it achieves a visual quality practically indistinguishable from the ground truth in a matter of just a few milliseconds, including rendering of textured and transparent objects. A supplementary video has been made available online.11Supplementary video available here}
}
@article{UDJAJA2018292,
title = {EKSPANPIXEL BLADSY STRANICA: Performance Efficiency Improvement of Making Front-End Website Using Computer Aided Software Engineering Tool},
journal = {Procedia Computer Science},
volume = {135},
pages = {292-301},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314662},
author = {Yogi Udjaja},
keywords = {Ekspanpixel Bladsy Stranica, Performance, Efficiency, Dynamic Canvas, Front-End, Website, Software Engineering, CASE Tool, Website Application, Time},
abstract = {The purpose of this research is to create a front-end website engine to improve the efficiency of front-end website creation called Expanpixel Bladsy Stranica (EBS). The method of making front-end website engine adopts computer aided software engineering (CASE) tool model, then to make it easier to access anywhere, it is made online (website-based), and evaluated by way of manual creation of front-end website and using EBS. After that the data obtained were analyzed using statistical formula. Results of increasing efficiency of front-end website creation performance that occurred on average by 83.60% of the overall developer.}
}
@article{FULGERI201971,
title = {Can adversarial networks hallucinate occluded people with a plausible aspect?},
journal = {Computer Vision and Image Understanding},
volume = {182},
pages = {71-80},
year = {2019},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2019.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S1077314219300438},
author = {Federico Fulgeri and Matteo Fabbri and Stefano Alletto and Simone Calderara and Rita Cucchiara},
keywords = {GAN, Attribute recognition, Occlusions},
abstract = {When you see a person in a crowd, occluded by other persons, you miss visual information that can be used to recognize, re-identify or simply classify him or her. You can imagine its appearance given your experience, nothing more. Similarly, AI solutions can try to hallucinate missing information with specific deep learning architectures, suitably trained with people with and without occlusions. The goal of this work is to generate a complete image of a person, given an occluded version in input, that should be a) without occlusion b) similar at pixel level to a completely visible people shape c) capable to conserve similar visual attributes (e.g. male/female) of the original one. For the purpose, we propose a new approach by integrating the state-of-the-art of neural network architectures, namely U-nets and GANs, as well as discriminative attribute classification nets, with an architecture specifically designed to de-occlude people shapes. The network is trained to optimize a Loss function which could take into account the aforementioned objectives. As well we propose two datasets for testing our solution: the first one, occluded RAP, created automatically by occluding real shapes of the RAP dataset created by Li et al. (2016) (which collects also attributes of the people aspect); the second is a large synthetic dataset, AiC, generated in computer graphics with data extracted from the GTA video game, that contains 3D data of occluded objects by construction. Results are impressive and outperform any other previous proposal. This result could be an initial step to many further researches to recognize people and their behavior in an open crowded world.}
}
@article{VIJAYALAKSHMI2023100011,
title = {Hybrid defense mechanism against malicious packet dropping attack for MANET using game theory},
journal = {Cyber Security and Applications},
volume = {1},
pages = {100011},
year = {2023},
issn = {2772-9184},
doi = {https://doi.org/10.1016/j.csa.2022.100011},
url = {https://www.sciencedirect.com/science/article/pii/S277291842200011X},
author = {S Vijayalakshmi and S Bose and G Logeswari and T Anitha},
keywords = {Ad hoc networks, Intrusion detection, Game theory, Game engine, Reactive game, Proactive game, Packet dropping attack, AODV},
abstract = {Ad hoc networks are a new perspective of wireless communication for versatile hosts. Security is a colossal worry for ad hoc networks, especially for those security-touchy applications. The huge highlights of ad hoc networks cause both difficulties and openings in accomplishing security objectives. One such aim is to consider the assaults from within the system by compromised nodes correspondingly as to consider harmful assaults propelled from outside the system. Designing an Intrusion Detection System (IDS) that suits the security needs and characteristics of ad hoc networks for viable and proficient performance against intrusions is one potential solution to vanquish vulnerabilities. This paper examines a genuine and hurtful attack called, “Malicious Packet Dropping Attack” in the network layer. To secure against this attack, a novel methodology utilizing game theory is proposed. The proposed system monitors the conduct of the neighbor nodes and conquers the demerits such as false positives present in traditional IDS, thereby providing secure correspondence between nodes that communicate with one another to course the traffic from source to destination. With the existence of malicious nodes, the proposed system has accomplished a 42% increase in the packet delivery ratio.}
}
@article{MIZUTANI2018308,
title = {Whole brain connectomic architecture to develop general artificial intelligence},
journal = {Procedia Computer Science},
volume = {123},
pages = {308-313},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300498},
author = {Haruo Mizutani and Michihiko Ueno and Naoya Arakawa and Hiroshi Yamakawa},
keywords = {connectome, general artificial intelligence, whole brain architecture, empirical neural circuits, efficient engineering},
abstract = {Whole Brain Connectomic Architecture (WBCA) is defined as a software architecture of the artificial intelligence (AI) computing platform which consists of empirical neural circuit information in the entire brain. It is constructed with the aim of developing a general-purpose biologically plausible AI to exert brain-like multiple cognitive functions and behaviors in a computational system. We have developed and implemented several functional machine learning modules, based on open mouse connectomic information, which correspond to specific brain regions. WBCA can accelerate efficient engineering development of the intelligent machines built on the architecture of the biological nervous system.}
}
@article{GONCALVES2023172,
title = {The role of different light settings on the perception of realism in virtual replicas in immersive Virtual Reality},
journal = {Computers & Graphics},
volume = {117},
pages = {172-182},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323002583},
author = {Guilherme Gonçalves and Miguel Melo and Pedro Monteiro and Hugo Coelho and Maximino Bessa},
keywords = {Virtual reality, Realism, Lighting, Computer graphics, User study, User perception},
abstract = {Immersive Virtual Reality (IVR) provides a platform where the real world can be replicated to a point where users can act and react in the virtual world as they would in reality. However, rendering visual stimuli is computationally heavy. Thus, optimizations must be done to take advantage of computational systems by studying our perception of reality. This study investigated parameters related to light rendering (Global Illumination, Ambient Occlusion, Screen Space Reflections (SSR) and Direct Shadows) in real-time in a virtual replica of a real place using IVR. Participants experienced both virtual and real rooms with only one flashlight and changed the quality settings of the considered parameters so that their sense of reality would be the closest to the one they felt when they experienced the real room. Participants were given a budget to drive them to prioritize what parameters, and their level of quality, are the most important for their sense of reality. Results indicated that participants considered Global Illumination the most important factor, closely followed by Direct Shadows. Ambient Occlusion and Reflections (Screen Space Reflections) were the less prioritized parameters. We conclude that in a lighting setting where only dynamic lights are used, Global Illumination and Direct Shadows should be prioritized over SSR Reflections and Ambient Occlusion when computational power is limited.}
}
@article{ZHU2020454,
title = {Development of augmented reality serious games with a vibrotactile feedback jacket},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {5},
pages = {454-470},
year = {2020},
note = {VR/AR research and commercial applications in Singapore},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300735},
author = {Lingfei Zhu and Qi Cao and Yiyu Cai},
keywords = {Augmented reality, AR serious games, Vibrotactile feedback jacket, Game scenes},
abstract = {Background
In the past few years, augmented reality (AR) has rapidly advanced and has been applied in different fields. One of the successful AR applications is the immersive and interactive serious games, which can be used for education and learning purposes.
Methods
In this project, a prototype of an AR serious game is developed and demonstrated. Gamers utilize a head-mounted device and a vibrotactile feedback jacket to explore and interact with the AR serious game. Fourteen vibration actuators are embedded in the vibrotactile feedback jacket to generate immersive AR experience. These vibration actuators are triggered in accordance with the designed game scripts. Various vibration patterns and intensity levels are synthesized in different game scenes. This article presents the details of the entire software development of the AR serious game, including game scripts, game scenes with AR effects design, signal processing flow, behavior design, and communication configuration. Graphics computations are processed using the graphics processing unit in the system.
Results/Conclusions
The performance of the AR serious game prototype is evaluated and analyzed. The computation loads and resource utilization of normal game scenes and heavy computation scenes are compared. With 14 vibration actuators placed at different body positions, various vibration patterns and intensity levels can be generated by the vibrotactile feedback jacket, providing different real-world feedback. The prototype of this AR serious game can be valuable in building large-scale AR or virtual reality educational and entertainment games. Possible future improvements of the proposed prototype are also discussed in this article.}
}
@article{BEZGODOV20152729,
title = {The Framework for Rapid Graphics Application Development: The Multi-scale Problem Visualization},
journal = {Procedia Computer Science},
volume = {51},
pages = {2729-2733},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.406},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915012144},
author = {Alexey Bezgodov and Andrey Karsakov and Aleksandr Zagarskikh and Vladislav Karbovskii},
keywords = {Visualization, Multi-scale, GIS, Virtual reality, Software development},
abstract = {Interactive real-time visualization plays a significant role in simulation research domain. Multi-scale problems are in need of high performance visualization with good quality and the same could be said about other problem domains, e.g. big data analysis, physics simulation, etc. The state of the art shows that a universal tool for solving such problem is non-existent. Modern computer graphics requires enormous efforts to implement efficient algorithms on modern GPUs and GAPIs. In the first part of our paper we introduce a framework for rapid graphics application development and its extensions for multi-scale problem visualization. In the second part of the paper we provide a prototype of multi-scale problem's solution in simulation and monitoring of high-precision agent movements starting from behavioral patterns in an airport and up to world-wide flight traffic. Finally we summarize our results and speculate about future investigations.}
}
@article{CALLARI2024104206,
title = {“Braking bad”: The influence of haptic feedback and tram driver experience on emergency braking performance},
journal = {Applied Ergonomics},
volume = {116},
pages = {104206},
year = {2024},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104206},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023002442},
author = {Tiziana C. Callari and Louise Moody and Michael Mortimer and Hans Stefan and Ben Horan and Stewart Birrell},
keywords = {Tram, Haptic feedback, Road safety, Emergency braking, Streetcar},
abstract = {Trams are experiencing a resurgence with worldwide network expansion driven by the need for sustainable and efficient cities. Trams often operate in shared or mixed-traffic environments, which raise safety concerns, particularly in hazardous situations. This paper adopts an international, mixed-methods approach, conducted through two interconnected studies in Melbourne (Australia) and Birmingham (UK). The first study involved qualitative interviews, while the second was an experimental study involving a virtual reality (VR) simulator and haptic master controller (i.e., speed lever). In tram operations, master controllers play a critical role in ensuring a smooth ride, which directly influences passenger safety and comfort. The objective was to understand how a master control system, enhanced with additional haptic feedback, could improve tram driver braking performance and perceptions in safety-critical scenarios. Interview results indicate that the use of the emergency brake is considered the final or ultimate choice by drivers, and their driving experience is a moderating factor in limiting its application. Combined with the experimental results, this paper highlights how implementing haptic feedback within a master controller can reduce the performance disparity between novice and experienced tram drivers.}
}
@article{ANTUNES2022e00237,
title = {Virtual simulations of ancient sites inhabited by autonomous characters: Lessons from the development of Easy-population},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {26},
pages = {e00237},
year = {2022},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2022.e00237},
url = {https://www.sciencedirect.com/science/article/pii/S2212054822000261},
author = {Rui Filipe Antunes and Luís Correia},
keywords = {Artificial life, Intelligent tools for digital reconstruction, Story-telling and other forms of communication, ICT technologies In support of creating new cultural experiences or digital artifacts},
abstract = {In this paper, we report the learnings from the development of Easy-Population, a software tool for generation of autonomous populations in virtual simulations and its use in the creation of a 3D recreation of a historical site. In the process, we discuss the animation of virtual historical simulations inhabited by autonomous characters. We claim the incompleteness of these simulations in representing ancient places to conclude the necessity of further research emphasizing cultural and social representation in this type of simulation. This paper contributes with new insights that help us to identify main areas for future research in this domain of knowledge. First, we start by presenting a quick overview of the field of autonomous populations in virtual simulations of ancient sites. Then, we describe the authoring tool that we have developed to simplify the process of animating such simulations with virtual populations, developed as part of the EU funded project BIHC: Bio Inspired Human Crowds. This tool was used to create a Virtual Reality experience on the simulation of Xelb, the medieval city of Silves in the South of Portugal. Finally, we present and discuss the results of two inquiries produced next to an audience of field experts on the Cultural Heritage sector and the broad public. These provide us with key insights helping us in understanding what future research in this field should take into consideration, in order to expand the use of autonomous populations in the field of cultural heritage as useful instruments in the creation of educational tools.}
}
@article{DAS2022103538,
title = {Intrinsic image decomposition using physics-based cues and CNNs},
journal = {Computer Vision and Image Understanding},
volume = {223},
pages = {103538},
year = {2022},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2022.103538},
url = {https://www.sciencedirect.com/science/article/pii/S1077314222001163},
author = {Partha Das and Sezer Karaoglu and Theo Gevers},
keywords = {Computer vision, Physics based vision, Intrinsics image decomposition, Deep learning},
abstract = {Intrinsic image decomposition is the decomposition of an image into its reflectance and shading components. The intrinsic image decomposition problem is inherently ill-posed, since there can be multiple solutions to compute the intrinsic components forming the same image. In this paper, we explore the use of physics-based priors. We also propose a new architecture that separates the learning components in a stacked manner. We explore various ways of integrating such priors into a deep learning system. Our method is trained and tested on a large synthetic garden dataset to assess its performance. It is evaluated and compared to state-of-the-art methods using two standard intrinsic datasets. Finally, the pre-trained network is tested on real world images to show the generalisation capabilities of the network.}
}
@article{THEES2020106316,
title = {Effects of augmented reality on learning and cognitive load in university physics laboratory courses},
journal = {Computers in Human Behavior},
volume = {108},
pages = {106316},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106316},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220300704},
author = {Michael Thees and Sebastian Kapp and Martin P. Strzys and Fabian Beil and Paul Lukowicz and Jochen Kuhn},
keywords = {Augmented reality, Smartglasses, Cognitive load, Split-attention effect, STEM laboratory courses, Physics laboratory courses},
abstract = {Recent studies emphasize a positive impact of learning with augmented reality (AR) systems in various instructional scenarios. Especially combining real and virtual learning components according to spatial and temporal contiguity principles is claimed to foster learning and to reduce extraneous cognitive processing. We applied these principles to a physics laboratory experiment examining heat conduction where students measure the temperature along heated metal rods via a thermal imaging camera. However, the traditional setup leads to a time delay between measuring and receiving data, and spatially separates relevant visualizations causing resource-consuming search processes. Using see-through smartglasses, traditional displays were transformed into virtual representations which were anchored to corresponding objects of the experimental setup, resulting in an integrated AR view of real-time data. Both traditional and AR-assisted workflows of data collection were investigated in a field study with undergraduate students (N=74) during a graded laboratory course. Performance and cognitive load were assessed as dependent variables. Although the AR condition did not show a learning gain in a conceptual knowledge test, they nonetheless reported a significant lower extraneous cognitive load than the traditional condition. These results contrast with recent findings on AR and integrated formats but reveal a significant impact on cognitive load research.}
}
@article{SZABO20241258,
title = {High performance GPU graphics API abstraction layer in C# for real-time graphics},
journal = {Procedia Computer Science},
volume = {235},
pages = {1258-1267},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924007956},
author = {Dávid Szabó and Dr. Zoltán Illés},
keywords = {C#, OpenGL, Vulkan, graphics, rendering, real-time},
abstract = {Real-Time rendering is the technique which allows us to have graphical applications in our everyday life, whether it is a 3D game or a tool with graphical user interface. Nowadays graphics rendering is handled by the GPU (Graphics Processing Unit) in our device. There are many layers of abstraction above the programming of GPUs through libraries and graphics engines, though the most low-level way of accessing a GPU in user-mode applications is using a Graphics API. Due to the need of high performance and low-level capabilities usually these APIs are used from C or C++, but we realized the need to utilize these APIs in higher level languages as well. In our approach we're using the .NET C# language for developing multi-platform real-time graphical applications instead of the C or C++ languages. Using the modern .NET environment, we're able to use Graphics APIs for rendering onto common .NET UI Frameworks while consuming all our previously implemented C# libraries and .NET technologies in the same application. To maintain compatibility with multiple platforms we're developing a library system allowing the use different Graphics APIs from the same C# source-code. The library system contains a Graphics API abstraction layer with multiple Graphics API implementations of this layer in C# and a C# to shader language compiler for cross-API shader development in C#. In this paper, we're proposing our considerations for implementing a library to be able to use the Vulkan and OpenGL APIs through a single C# codebase. We provide solutions for multi-platform rendering and dealing with the low-level challenges of using the two deeply different APIs, while maintaining performance capable to do real-time rendering.}
}
@article{EHRHARDT201914,
title = {Taking visual motion prediction to new heightfields},
journal = {Computer Vision and Image Understanding},
volume = {181},
pages = {14-25},
year = {2019},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2019.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1077314219300207},
author = {Sebastien Ehrhardt and Aron Monszpart and Niloy J. Mitra and Andrea Vedaldi},
abstract = {While the basic laws of Newtonian mechanics are well understood, explaining a physical scenario still requires manually modeling the problem with suitable equations and estimating the associated parameters. In order to be able to leverage the approximation capabilities of artificial intelligence techniques in such physics related contexts, researchers have handcrafted relevant states, and then used neural networks to learn the state transitions using simulation runs as training data. Unfortunately, such approaches are unsuited for modeling complex real-world scenarios, where manually authoring relevant state spaces tend to be tedious and challenging. In this work, we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data while internally modeling non-homogeneous environment and in the process enable long-term physical extrapolation. We develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates. We evaluate our setup, to extrapolate motion of rolling ball(s) on bowls of varying shape and orientation, and on arbitrary heightfields using only images as input. We report significant improvements over existing image-based methods both in terms of accuracy of predictions and complexity of scenarios; and report competitive performance with approaches that, unlike us, assume access to internal physical states.}
}
@article{MONTES2022103756,
title = {A computational model of Ostrom's Institutional Analysis and Development framework},
journal = {Artificial Intelligence},
volume = {311},
pages = {103756},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103756},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000960},
author = {Nieves Montes and Nardine Osman and Carles Sierra},
keywords = {Institutional Analysis and Development framework, Rules, Normative multiagent systems, Game theory, Logic programming},
abstract = {The Institutional Analysis and Development (IAD) framework developed by Elinor Ostrom and colleagues provides great conceptual clarity on the immensely varied topic of social interactions. In this work, we propose a computational model to examine the impact that any of the variables outlined in the IAD framework has on the resulting social interactions. Of particular interest are the rules adopted by a community of agents, as they are the variables most susceptible to change in the short term. To provide systematic descriptions of social interactions, we define the Action Situation Language (ASL) and provide a game engine capable of automatically generating formal game-theoretical models out of ASL descriptions. Then, by incorporating any agent decision-making models, the connection from a rule configuration description to the outcomes encouraged by it is complete. Overall, our model enables any community of agents to perform what-if analysis, where they can foresee and examine the impact that a set of regulations will have on the social interaction they are engaging in. Hence, they can decide whether their implementation is desirable.}
}
@article{ZAGARSKIKH20152928,
title = {Efficient Visualization of Urban Simulation Data Using Modern GPUs},
journal = {Procedia Computer Science},
volume = {51},
pages = {2928-2932},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.481},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915012892},
author = {Aleksandr Zagarskikh and Andrey Karsakov and Alexey Bezgodov},
keywords = {GIS, Visualization, City science, Virtual globe, GPU},
abstract = {Visualization of simulation results in major urban areas is a difficult task. Multi-scale processes and connectivity of the urban environment may require interactive visualization of dynamic scenes with lots of objects at different scales. To visualize these scenes it is not always possible to use standard GIS systems. Wide distribution of high-performance gaming graphics cards has led to the emergence of specialized frameworks, which are able to cope with such kinds of visualization. This paper presents a framework and special algorithms that take full advantage of the GPU to render the urban simulation data over a virtual globe. The experiments on a scalability of the framework have showed that the framework is successfully deals with the visualization of up to two million moving agents and up to eight million of fixed points of interest on top of the virtual globe without detriment to smoothness of the image.}
}
@article{KATKURI2024100361,
title = {Autonomous UAV Navigation using Deep Learning-Based Computer Vision Frameworks: A Systematic Literature Review},
journal = {Array},
pages = {100361},
year = {2024},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2024.100361},
url = {https://www.sciencedirect.com/science/article/pii/S2590005624000274},
author = {Aditya Vardhan Reddy Katkuri and Hakka Madan and Narendra Khatri and Antar Shaddad Hamed Abdul-Qawy and K. Shridhar Patnaik},
keywords = {Autonomous UAV, Deep Learning, Computer Vision, Systematic Literature Review, UAV Applications, You Only Look Once (YOLO) Framework},
abstract = {The increasing use of unmanned aerial vehicles (UAVs) in both military and civilian applications, such as infrastructure inspection, package delivery, and recreational activities, underscores the importance of enhancing their autonomous functionalities. Artificial intelligence, particularly deep learning-based computer vision (DL-based CV), plays a crucial role in this enhancement. This paper aims to provide a systematic literature review (SLR) of Scopus-indexed research studies published from 2019 to 2023, focusing on DL-based CV approaches for autonomous UAV applications. By analyzing 173 studies, we categorize the research into four domains: sensing and inspection, landing, surveillance and tracking, and search and rescue. Our review reveals a significant increase in research utilizing computer vision for UAV applications, with over 39.5% of studies employing the You Only Look Once (YOLO) framework. We discuss the key findings, including the dominant trends, challenges, and opportunities in the field, and highlight emerging technologies such as in-sensor computing. This review provides valuable insights into the current state and future directions of DL-based CV for autonomous UAVs, emphasizing its growing significance as legislative frameworks evolve to support these technologies.}
}
@article{ULLMANN2025100832,
title = {SyDRA: An approach to understand game engine architecture},
journal = {Entertainment Computing},
volume = {52},
pages = {100832},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100832},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002003},
author = {Gabriel C. Ullmann and Yann-Gaël Guéhéneuc and Fabio Petrillo and Nicolas Anquetil and Cristiano Politowski},
keywords = {Game engines, Coupling, Impact analysis, Controlled experiment},
abstract = {Game engines are tools to facilitate video game development. They provide graphics, sound, and physics simulation features, which would have to be otherwise implemented by developers. Even though essential for modern commercial video game development, game engines are complex and developers often struggle to understand their architecture, leading to maintainability and evolution issues that negatively affect video game productions. In this paper, we present the Subsystem-Dependency Recovery Approach (SyDRA), which helps game engine developers understand game engine architecture and therefore make informed game engine development choices. By applying this approach to 10 open-source game engines, we obtain architectural models that can be used to compare game engine architectures and identify and solve issues of excessive coupling and folder nesting. Through a controlled experiment, we show that the inspection of the architectural models derived from SyDRA enables developers to complete tasks related to architectural understanding and impact analysis in less time and with higher correctness than without these models.}
}
@article{CONESA2023106257,
title = {A multi-agent framework for collaborative geometric modeling in virtual environments},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106257},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106257},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623004414},
author = {J. Conesa and F.J. Mula and M. Contero and J.D. Camba},
keywords = {Agent-based system, Collaborative systems, Virtual reality},
abstract = {The use of collaborative applications in which several individuals interact to solve a problem is an important strategy both in educational settings as well as professional environments. Virtual Reality (VR) technology, in particular, has acquired a predominant role in many industries. However, as the level of immersion and realism of the VR experience increases, so does the demand for computational resources, as rendering processes become increasingly intensive and can negatively affect other communication processes that are critical in collaborative environments. In this paper, we present a software framework based on multi-agent systems that enables the separation of rendering processes from internal data management tasks and communication processes between users, which are prevalent in collaborative virtual reality applications. The results of our validation studies show that, compared to other techniques based on protocol or network enhancements, the proposed architecture can significantly improve collaborative processes in general, and virtual reality-based applications in particular.}
}
@article{FLOTYNSKI2021766,
title = {Knowledge-Based Management of Virtual Training Scenarios},
journal = {Procedia Computer Science},
volume = {192},
pages = {766-775},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.079},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015672},
author = {Jakub Flotyński and Krzysztof Walczak and Paweł Sobociński and Adam Gałązkiewicz},
keywords = {semantic web, knowledge representation, ontologies, training, virtual reality, 3D content},
abstract = {Virtual reality (VR) gains increasing attention as a method of implementing training systems in different domains, in particular, when real training is potentially dangerous for the trainees or the environment, or requires expensive equipment. The essential element of professional training is domain-specific knowledge, which can be represented using the semantic web approach. It enables reasoning as well as complex queries against the representation of training scenarios, which can be valuable for teaching purposes. However, the available methods and tools for creating VR training systems do not use semantic knowledge representation. Currently, the creation, modification, and management of training scenarios require skills in programming and computer graphics. Hence, they are unavailable to domain experts without expertise in IT. In this paper, we propose an ontology-based representation and a method of modeling VR training scenarios. In our approach, trainees’ activities, potential mistakes as well as equipment and its possible errors are represented using domain knowledge understandable to domain experts. We illustrate the approach by modeling VR training scenarios for electrical operators of high-voltage installations.}
}
@article{KARSAKOV2015730,
title = {Improving Visualization Courses in Russian Higher Education in Computational Science and High Performance Computing},
journal = {Procedia Computer Science},
volume = {66},
pages = {730-739},
year = {2015},
note = {4th International Young Scientist Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.11.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915034328},
author = {Andrey Karsakov and Anna Bilyatdinova and Alexey Bezgodov},
keywords = {Visualization course, Computational science, Higher education},
abstract = {In order to keep up with the fast-paced and widespread technologies and applications of visualization, worldwide education community is actively implementing visualization courses in curricula of undergraduate and graduate programs. A study of the state of the art in the teaching visualization in Russian higher education shows the necessity to improve the quality and breadth of knowledge of the visualization courses. In this paper we propose our approach to overcome the national and historical challenges in teaching visualization in Russian STEM higher education on the example of Computational Science and High Performance Computing double degree Master's programs in ITMO University. We offer a smooth transition to the modern relevant syllabus content by presenting two courses’ designs with same width but with various depth in knowledge that should to be studied. At the end of the paper we give some discussions about future works in development visualization courses in Russia.}
}
@article{ROMAN2024460,
title = {The Use of Augmented Reality as a University Teaching Strategy in Health Sciences Programs: A Scoping Review},
journal = {Procedia Computer Science},
volume = {238},
pages = {460-467},
year = {2024},
note = {The 15th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) / The 7th International Conference on Emerging Data and Industry 4.0 (EDI40), April 23-25, 2024, Hasselt University, Belgium},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924012845},
author = {Fabian Roman and Karina Lastre Meza and Diva Mendoza and Sonia Rodriguez Cano},
keywords = {Augmented Reality, Higher Education, Health, Education Technology, Teaching},
abstract = {The purpose of this systematic review of the literature is to identify, analyze and synthesize the findings found in the last 10 years regarding the issue of AR in academic health programs and to know the impact it has on students’ learning. At the methodological level, the databases Scopus, PubMed, Web of Science and Science Direct were consulted, inclusion and exclusion criteria were established for the selection of the most relevant articles, methodological quality and relevance were analyzed. As a result, 16 articles suggest that AR has a positive impact on health disciplines, by fostering greater interactivity, motivation, understanding of concepts, acquisition of skills, retention of knowledge and personalization of learning. The systematic review concludes that further scientific evidence is needed to determine the effectiveness of AR in learning and its impact on the development of competences and learning outcomes in current education.}
}
@article{AUNG2023200295,
title = {Development of a novel robot-assisted vocabulary learning system using pure synthetic data},
journal = {Intelligent Systems with Applications},
volume = {20},
pages = {200295},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200295},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323001205},
author = {Zaw Htet Aung and Chuenchat Songsaksuppachok and Potjanee Kanchanapiboon and Panrasee Ritthipravat},
keywords = {Synthetic data generation, Synthetic-to-real transfer, Object detection, Robot assisted language learning, Human-robot interaction},
abstract = {This study investigates the application of deep learning methods trained on synthetic data for the robust detection of vocabulary flashcards, an essential aspect of robot-assisted language learning (RALL). Despite its importance, flashcard detection has not been extensively researched in RALL systems, and it poses significant challenges due to the extensive data collection and annotation needed, especially given the large number of classes involved. These models need to generalize well to different real-world environments. To address these issues, a novel robotic platform designed for flashcard-based vocabulary learning is proposed, supported by a synthetic data generation pipeline using high dynamic range images (HDRIs) and synthetic human actors. The proposed pipeline offers an efficient data generation method and significantly enhances model generalisability to various environments. The proposed method was evaluated with five object detection models based on several challenging real-world datasets, each containing more than 200 class labels. The object detection models trained based on the proposed synthetic datasets (HDRI and HDRI+Humans) demonstrated outstanding performance, achieving median mean average precision (mAP) scores of 0.797 and 0.778. The proposed method achieves this performance without needing to be trained with real data. A comprehensive analysis comparing the proposed method with other synthetic data generation techniques is presented, and its potential for improving vocabulary flashcard detection in RALL systems is highlighted. The data and models are available at https://github.com/aimlab-mu/aimrobot.}
}
@article{NICHOLAS2023107774,
title = {Sideffect GamePlan: Development of an alcohol and other drug serious game for high school students using a systematic and iterative user-centred game development framework},
journal = {Computers in Human Behavior},
volume = {145},
pages = {107774},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107774},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223001255},
author = {Joanna Nicholas and Brennen Mills and Sara Hansen and Stephen J. Bright and Joseph Scott and Imogen Ridout and Jess Watson and Heather Boyd and Luke Brook and Luke Hopper},
keywords = {Secondary education, Alcohol and other drugs, Health education, Serious game design, Harm reduction, Games},
abstract = {Serious games have shown to be effective in improving motivation to learn, knowledge and retention, thus are being increasingly used for alcohol and other drug (AOD) education. This paper outlines the development of an online AOD serious game for in-class use by Australian secondary school teachers for students in Years 9–10. Adapted from Edwards et al. (2018), the seven-step systematic and iterative user-centred development framework included: (1) Forming an expert multidisciplinary design team, (2) Defining the problem and establishing user preferences, (3) Incorporating the evidence base, (4) Serious game design, (5) Incorporating behavioural and psychological theory, (6) Developing a logic model and investigating causal pathways, and (7) User testing. High school students (n = 8), health and physical education teachers (n = 7), and parents (n = 8) were engaged throughout different stages of the development process to inform development and provide feedback on considerations for promoting engagement, acceptability, and usability of the game amongst both students and teachers. Overall, participants rated game acceptability and usability favourably and would recommend the game for learning about AOD. Constructive feedback and suggestions for improvements from user testing sessions were implemented to form the final version of the game and module. The next step is to test Sideffect GamePlan in a simulated classroom environment before piloting in school settings.}
}
@article{ALENE2024106063,
title = {Virtual reality visualization of geophysical flows: A framework},
journal = {Environmental Modelling & Software},
volume = {177},
pages = {106063},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106063},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224001245},
author = {Gebray H. Alene and Shafaq Irshad and Adina Moraru and Ivan Depina and Oddbjørn Bruland and Andrew Perkis and Vikas Thakur},
keywords = {Geophysical flows, Framework, Natural hazards, Numerical modeling, Virtual reality, Visualization},
abstract = {This paper presents a comprehensive Virtual Reality (VR) based framework for visualizing numerical simulations of geophysical flows in a realistic and immersive manner. The framework allows integrating output data from various mesh-based Eulerian numerical models into a VR environment, enabling users to interact with and explore the terrain and geophysical flows through the VR experience. Three case studies, including a snow avalanche, quick clay landslide, and flash flood in Norway, demonstrate its versatility. The VR environment offers intuitive menus and user interactions, allowing users to read flow depth and velocity values, facilitating a direct link between numerical data and their visual representation. This framework can reshape geophysical flow hazard identification and disaster management by integrating physics-based numerical modeling results into VR Environments, thus enhancing knowledge dissemination among experts, the general public, non-expert stakeholders, and policymakers. The paper also highlights challenges and opportunities identified during the integration, guiding future developments.}
}
@article{NEKTARIOSBOLIERAKIS2023201,
title = {Training on LSA lifeboat operation using Mixed Reality},
journal = {Virtual Reality & Intelligent Hardware},
volume = {5},
number = {3},
pages = {201-212},
year = {2023},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000128},
author = {Spyridon {Nektarios Bolierakis} and Margarita Kostovasili and Lazaros Karagiannidis and Angelos Amditis},
keywords = {Augmented Reality, Mixed Reality, Maritime training, Cruise industry, Lifeboat operation, Lifeboat maintenance, H2020 research project},
abstract = {Background
This work aims to provide an overview of the Mixed Reality (MR) technology's use in maritime industry for training purposes. Current training procedures cover a broad range of procedural operations for Life-Saving Appliances (LSA) lifeboats; however, several gaps and limitations have been identified related to the practical training that can be addressed through the use of MR. Augmented, Virtual and Mixed Reality applications are already used in various fields in maritime industry, but their full potential have not been yet exploited. SafePASS project aims to exploit MR advantages in the maritime training by introducing a relevant application focusing on use and maintenance of LSA lifeboats.
Methods
An MR Training application is proposed supporting the training of crew members in equipment usage and operation, as well as in maintenance activities and procedures. The application consists of the training tool that trains crew members on handling lifeboats, the training evaluation tool that allows trainers to assess the performance of trainees, and the maintenance tool that supports crew members to perform maintenance activities and procedures on lifeboats. For each tool, an indicative session and scenario workflow are implemented, along with the main supported interactions of the trainee with the equipment.
Results
The application has been tested and validated both in lab environment and using a real LSA lifeboat, resulting to improved experience for the users that provided feedback and recommendations for further development. The application has also been demonstrated onboard a cruise ship, showcasing the supported functionalities to relevant stakeholders that recognized the added value of the application and suggested potential future exploitation areas.
Conclusions
The MR Training application has been evaluated as very promising in providing a user-friendly training environment that can support crew members in LSA lifeboat operation and maintenance, while it is still subject to improvement and further expansion.}
}
@article{BOONBRAHM201512,
title = {Realistic Simulation in Virtual Fitting Room Using Physical Properties of Fabrics},
journal = {Procedia Computer Science},
volume = {75},
pages = {12-16},
year = {2015},
note = {2015 International Conference Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036509},
author = {Poonpong Boonbrahm and Charlee Kaewrat and Salin Boonbrahm},
keywords = {Virtual fitting room, 3D simulation, Physical propoerties of fabrics},
abstract = {Virtual Dressing Room (VDR) is the popular topics during the past 10 years and it seemed to have the bright future when Microsoft's Kinect tracking system appeared in the market. The VDR is not only helping the online customer making decision on buying apparels but also helping customers in department store to save time in waiting for the fitting room. In facts, there are few parts of VDR that make virtual fitting more realistic and tracking the movement of the body is only one of them. The other parts are dealing with the clothing and how they interact with the body even when the customer moves around. The virtual fitting rooms available in the market today and on the website used 2D dresses putting in front of the body. These dresses did not fit on the body and did not interact with the body except they were appeared on the front part of the body. In order to make the simulation process of the virtual fitting room more realistic, 3D virtual dress will be used along with the physical interaction of the fabrics and the environment. In this research, we have to define the shape of the 3D dress and the physical properties of the fabrics. Since the dress did not come as one piece of fabrics but composed of many pieces with different textures and physical properties, for example, some parts is thicker than others or made with different materials, so all these issues have to be taken into account. In this research, the interaction occurred between fabric and the environment were calculated and applied. This covered all of the interactions occurred on the fabrics such as stretching, bending, damping, accelerating, colliding and gravity pulling. The simulation results can tell the difference among customers wearing jean, satin, silk or cotton. In this research, Unity 3D game engine was used for simulation process along with Maya for model creation and Microsoft's Kinect2 was used for tracking the dress and the body.}
}
@article{FERREIRA20242521,
title = {Digital twinning for smart restoration of classic cars},
journal = {Procedia Computer Science},
volume = {232},
pages = {2521-2530},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.070},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002473},
author = {Frederico Ferreira and Vasco Amaral and Fernando {Brito e Abreu}},
keywords = {digital twin, classic cars, bodywork restoration, digital transformation, industry 4.0, IoT},
abstract = {Classic cars hold substantial value in the automotive industry, and the restoration process plays a pivotal role in increasing their worth. Ensuring the certification of these restoration processes is vital, as is keeping owners well-informed about the ongoing procedures taking place in their appreciated vehicles. By monitoring and controlling these restoration activities, the management of classic car workshops can effectively optimize their operations, empower owners with pertinent information, and preserve the authentic nature of these vintage automobiles. This study aims to develop a digital twin system for a classic car bodywork restoration shop workshop. The latter integrates multiple sensor technologies, including location tracking, humidity and temperature sensing, accelerometer monitoring, and smart plugs, to facilitate the identification of ongoing activities of classic car bodywork restoration process instances. By leveraging these sensors, the digital twin system may simulate and control workshop operations more effectively. We perform a systematic rapid review of related work and, based on state-of-the-art practices, we identify existing architectures and software applications used for creating digital twin systems. Then, we propose the architecture for our digital twin system, detailing its functionalities. We aim at contributing to advancing digital twin technology in classic car bodywork restoration, enhancing its authenticity, and fostering improved management practices, and overall experience for classic car owners.}
}
@article{WANG2024200115,
title = {A pose generation model for animated characters based on DCNN and PFNN},
journal = {Systems and Soft Computing},
volume = {6},
pages = {200115},
year = {2024},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2024.200115},
url = {https://www.sciencedirect.com/science/article/pii/S2772941924000449},
author = {Boli Wang},
keywords = {DCNN, ResNet, 3D pose estimation, Animation, Pose generation, Phase function neural network},
abstract = {In the current field of animation and gaming, the action collection cost for 3D animated character generation is high, and the accuracy of action recognition is poor. Therefore, to reduce the cost of generating 3D animated characters and improve the similarity between animated characters and real humans, a 3D action recognition and animated character generation model based on ResNet and phase function neural network is proposed. The experiment outcomes denote that the raised model begins to converge at 50 iterations, with a minimum loss value of 0.13. The convergence speed and loss value are better than other models. In human pose classification, the raised algorithm has the highest accuracy of 99.46 % and an average accuracy of 99.13 %. The highest classification precision and average precision are 97.79 % and 97.33 %, respectively. In terms of human pose orientation classification, the average accuracy and precision of the raised algorithm are 98.09 % and 97.41 %, respectively, which are also higher than other models. In addition, the mean per joint position error of the proposed algorithm is the highest at 80.1 mm and the lowest at 79.3 mm, respectively. The average recognition time for each image is only 46.8 ms, which is lower than other algorithms. In addition, the average update times of the algorithm and the Unreal Engine are 39.28 ms and 27.52 ms, respectively, and both run at different frame rates. The above results indicate that the proposed 3D human pose recognition and animated character generation model based on ResNet and phase function neural network can not only improve the accuracy of pose recognition, but also improve recognition speed, effectively reducing the cost of 3D animated character generation. The animation character generation method includes data collection and the application after data collection, which shows the various roles that deep learning technology can play in the field of computer graphics animation, and also provides excellent solutions for other computer graphics problems.}
}
@article{KIRYUKHIN2020192,
title = {Development of a virtual analogue of uranium-graphite subcritical assembly and visualization of the neutron flux distribution in virtual reality},
journal = {Procedia Computer Science},
volume = {169},
pages = {192-197},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.135},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302581},
author = {Pavel Kiryukhin and Alexander Shcherbakov and Vladislav Romanenko and Pavel Pugachev and Dmitriy Khomyakov and Georgy Tikhomirov and Egor Zadeba},
keywords = {Virtual reality, Nuclear reactor physics, Simulation, Education},
abstract = {The article describes the new software product developed at MEPhI. It represents a virtual reality simulation of an experiment on a subcritical uranium-graphite assembly. This practical work plays an important role in the training of young specialists studying the physics of nuclear reactors. However not all students have access to real experimental facilities, this fact makes it necessary to complement real experiment with simulation in virtual reality that allows to accurately reproduce the actions that the student performs during the real practical work. This approach let to increase the efficiency of the educational process and even expand the capabilities of real experimental assembly by visualizing physical processes during its operation.}
}
@article{POGLITSCH2024103942,
title = {XR technologies to enhance the emotional skills of people with autism spectrum disorder: A systematic review},
journal = {Computers & Graphics},
volume = {121},
pages = {103942},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103942},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000773},
author = {Christian Poglitsch and Saeed Safikhani and Erin List and Johanna Pirker},
keywords = {Extended reality (XR), Virtual reality (VR), Augmented reality (AR), Autism spectrum disorder, Emotional skills, Emotion recognition},
abstract = {In this paper, we present a systematic review of the applications of (1) Extended Reality (XR), (2) Augmented Reality (AR), and (3) Virtual Reality (VR) technologies to enhance emotion recognition and emotion expression in people with Autism Spectrum Disorder (ASD). ASD can affect various abilities, and poses challenges to the recognition of emotions in others, which is often referred to as “social blindness”. Treating this condition typically requires intensive one-on-one or small-group therapy sessions, which can be costly and limited in terms of availability. With the growing number of diagnoses of ASD, concerns have risen regarding a potential “lost generation” that may face difficulties in fulfilling its potential. Through this comprehensive review, we aim to provide an overview of innovative approaches that use XR technologies to improve the learning experience of individuals with ASD.}
}
@article{NISHIKAWA20224055,
title = {Analysis of cerebral blood flow state during a mental rotation task to assess spatial perception ability},
journal = {Procedia Computer Science},
volume = {207},
pages = {4055-4064},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.468},
url = {https://www.sciencedirect.com/science/article/pii/S187705092201362X},
author = {Reina Nishikawa and Hirokazu Miura and Hirokazu Takib},
keywords = {Near-infrared spectroscopy, mental rotation, spatial perception ability},
abstract = {Spatial recognition is the ability to quickly and accurately grasp the position, posture, direction, size, shape, spacing, speed, etc. of objects in three-dimensional space, and is said to be established through the cooperation of multiple sensory organs such as vision and hearing, and controlled by the right brain. Currently, paper tests such as MRT and MCT are the most common methods for measuring spatial recognition ability. Although measurement methods and learning systems using AR and VR have also been proposed, objective evaluation indices have yet to be established. Therefore, it is considered necessary to measure and analyze the state of brain activity during spatial recognition in order to objectively evaluate spatial recognition. Group comparisons based on task performance showed a significant increase in oxygenated hemoglobin in the higher group compared to the lower group in both score and clear time. In addition, a comparison between the right and left brains showed that the right brain tended to have a higher percentage of increased oxygenated hemoglobin, which is consistent with the view that spatial recognition ability is significantly right-brained. These results suggest that the mental rotation task is closely related to prefrontal regions and that changes in cerebral blood flow in the right brain are reflected in spatial recognition ability.}
}
@article{FUKUDA2019179,
title = {An indoor thermal environment design system for renovation using augmented reality},
journal = {Journal of Computational Design and Engineering},
volume = {6},
number = {2},
pages = {179-188},
year = {2019},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S228843001830068X},
author = {Tomohiro Fukuda and Kazuki Yokoi and Nobuyoshi Yabuki and Ali Motamedi},
keywords = {Environmental design, Indoor thermal environment, Intuitive visualization, Interactive environment, Computational Fluid Dynamics (CFD), Augmented Reality (AR)},
abstract = {The renovation projects of buildings and living spaces, which aim to improve the thermal environment, are gaining importance because of energy saving effects and occupants' health considerations. However, the indoor thermal design is not usually performed in a very efficient manner by stakeholders, due to the limitations of a sequential waterfall design process model, and due to the difficulty in comprehending the CFD simulation results for stakeholders. On the other hand, indoor greenery has been introduced to buildings as a method for adjusting the thermal condition. Creating a VR environment, which can realistically and intuitively visualize a thermal simulation model is very time consuming and the resulting VR environment created by 3D computer graphics objects is disconnected from the reality and does not allow design stakeholders to experience the feelings of the real world. Therefore, the objective of this research is to develop a new AR-based methodology for intuitively visualizing indoor thermal environment for building renovation projects. In our proposed system, easy-to-comprehend visualization of CFD results augment the real scenes to provide users with information about thermal effects of their renovation design alternatives interactively. Case studies to assess the effect of indoor greenery alternatives on the thermal environment are performed. In conclusion, integrating CFD and AR provides users with a more natural feeling of the future thermal environment. The proposed method was evaluated feasible and effective.}
}
@article{BUROVA2022103663,
title = {Asynchronous industrial collaboration: How virtual reality and virtual tools aid the process of maintenance method development and documentation creation},
journal = {Computers in Industry},
volume = {140},
pages = {103663},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103663},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000604},
author = {Alisa Burova and John Mäkelä and Hanna Heinonen and Paulina Becerril Palma and Jaakko Hakulinen and Viveka Opas and Sanni Siltanen and Roope Raisamo and Markku Turunen},
keywords = {Virtual reality, Industrial collaboration, Industrial maintenance, Technical documentation, Virtual tools, Asynchronous collaboration, Collaborative virtual reality, Maintenance method development},
abstract = {In the light of Industry 4.0, the field of Industrial Maintenance faces a large digital transformation, adopting Extended Reality (XR) technologies to aid industrial operations. For the manufacturing corporations that provide maintenance services, the efficiency of industrial maintenance plays a crucial role in the competitiveness and is tightly related to the technical documentation supporting maintenance. However, the process of documentation creation faces several challenges due to lack of access to the physical equipment and difficulties in remote communication between globally distributed departments. To address these shortcomings, this research investigates the utilization of Virtual Reality (VR) to facilitate asynchronous collaboration of globally dispersed departments involved in the pipeline of maintenance method and documentation creation. The presented proof-of-concept (the COVE-VR platform) has been developed as an academia-industry collaboration and evaluated iteratively with subject matter experts. The proposed VR platform consists of two virtual environments and eight virtual tools, which allow interaction with virtual prototypes (3D CAD models) and means of digital content creation. Our findings show the high relevance of the developed solution for the needs of industrial departments and the ability to support asynchronous collaboration among them. This article delivers qualitative findings on the value of VR technology and presents guidelines on how to develop virtual tools for digital content creation within VR, adaptable to other industrial contexts. We suggest providing embedded guidance and design consistency to ensure smooth interactions with virtual tools and further discuss the importance of proper positioning, the transparency of operations and the information property of generated content.}
}
@article{JOHRI2024100213,
title = {Crafting the techno-functional blocks for Metaverse - A review and research agenda},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {1},
pages = {100213},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000028},
author = {Amar Johri and Anu Sayal and Chaithra N and Janhvi Jha and Navya Aggarwal and Darshan Pawar and Veethika Gupta and Ashulekha Gupta},
keywords = {Artificial intelligence, Augmented reality, Blockchain, Metaverse, Virtual reality},
abstract = {The "Metaverse," a term popularized by Neal Stephenson's novel Snow Crash, has been discussed in the science fiction community for decades, but technological advancements have only recently made it a reality. The Metaverse is an all-encompassing, interconnected virtual environment where users can freely communicate with one another and digital content. This article examines how various technologies, primarily Virtual Reality (VR) and Augmented Reality (AR), have contributed to the development of the Metaverse (AR). These innovations have revolutionized the way we interact with digital media by enabling us to have genuine, realistic experiences. In addition, we examine the Metaverse technologies that make it possible to construct a fully realized, functional virtual world. Among these are recent advances in artificial intelligence (AI), cryptocurrencies, spatial and peripheral computing, and other fields. Our research investigates the advantages and disadvantages of these technologies, as well as how they may influence the future of the Metaverse. Furthermore, the article explores the darker aspects of the Metaverse, particularly the emergence of the "dark verse," which underscores the potential for organized illicit activities within the Internet due to insufficient oversight and governance of the Metaverse.}
}
@article{GONCALVES20231333,
title = {Cognitive Rehabilitation: A Comparison Model of a Digital Environment based on Serious Games and the Traditional Methods},
journal = {Procedia Computer Science},
volume = {219},
pages = {1333-1340},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.418},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923004271},
author = {Helena Isabel Marques Gonçalves and Firmino Oliveira {da Silva}},
keywords = {Serious Games, Health, Neuro-Cognitive Rehabilitation, Traditional Methods},
abstract = {Technological innovation contributes to a personalised and integrated approach in education, journalism, communication, management, marketing, and above all, in healthcare. Facing the increasing number of new patients and the shortage of health specialists, the emergence of innovative technologies has brought the fields of health and technology closer together. This research aims to study digital environments in the context of neurocognitive rehabilitation, for the care of patients with deficits in cognitive functions, supported by Serious Games (SG). Typically, neurological patients suffer from cognitive deficits in executive, visuospatial, attention and/or memory functions. In this context, SG prefigure an appropriated tool that combines healthcare and rehabilitation, which allows the connection of healthcare specialists with a platform, as the connection of patients with life and disease companions with potential for collaboration and recording the evolution of the rehabilitation process. Aiming to identify the characteristics and requirements of SG in exposing neurological patients to a safe technological environment that simulates rehabilitation activities supported by SG (developed and oriented to their specificities), but also to make a comparison with traditional methods (TM). A model of comparison between the two paradigms was elaborated, which allowed the collection of requirements and characteristics for future developments. In conclusion, SG are not strictly better than traditional treatment (TT) methods in this context, but the elaborated comparison tends to point out that SG are sovereign to TM in improving training and producing quality data (in safety) for analysis, which allows the total rehabilitation process to be better conducted.}
}
@article{JARRETT2021100502,
title = {Exploring and interrogating astrophysical data in virtual reality},
journal = {Astronomy and Computing},
volume = {37},
pages = {100502},
year = {2021},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2021.100502},
url = {https://www.sciencedirect.com/science/article/pii/S2213133721000561},
author = {T.H. Jarrett and A. Comrie and L. Marchetti and A. Sivitilli and S. Macfarlane and F. Vitello and U. Becciani and A.R. Taylor and J.M. {van der Hulst} and P. Serra and N. Katz and M.E. Cluver},
keywords = {Virtual reality, Data visualisation, Radio astrophysics, 3D catalogues, Volumetric rendering},
abstract = {Scientists across all disciplines increasingly rely on machine learning algorithms to analyse and sort datasets of ever increasing volume and complexity. Although trends and outliers are easily extracted, careful and close inspection will still be necessary to explore and disentangle detailed behaviour, as well as identify systematics and false positives. We must therefore incorporate new technologies to facilitate scientific analysis and exploration. Astrophysical data is inherently multi-parameter, with the spatial-kinematic dimensions at the core of observations and simulations. The arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as well as the availability of versatile development tools for video games, has enabled scientists to deploy such technology to effectively interrogate and interact with complex data. In this paper we present development and results from custom-built interactive VR tools, called the iDaVIE suite, that are informed and driven by research on galaxy evolution, cosmic large-scale structure, galaxy–galaxy interactions, and gas/kinematics of nearby galaxies in survey and targeted observations. In the new era of Big Data ushered in by major facilities such as the SKA and LSST that render past analysis and refinement methods highly constrained, we believe that a paradigm shift to new software, technology and methods that exploit the power of visual perception, will play an increasingly important role in bridging the gap between statistical metrics and new discovery. We have released a beta version of the iDaVIE software system that is free and open to the community.}
}
@article{HUANG2023104019,
title = {BIM-supported drone path planning for building exterior surface inspection},
journal = {Computers in Industry},
volume = {153},
pages = {104019},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104019},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001690},
author = {Xiongwei Huang and Yongping Liu and Lizhen Huang and Sverre Stikbakke and Erling Onstein},
keywords = {Building information modeling, Drone, Coverage path planning, Virtual simulation, Building inspection},
abstract = {Digitalization in the architectural, engineering, and construction (AEC) industry highlights the interdisciplinary collaboration between complex systems to provide fast and efficient services. This paper incorporates Building Information Modeling (BIM) and drone, and generates feasible paths for exterior building surface inspections. A systematic approach was proposed, focusing on the overall automatic procedure from the BIM model to the actual flight of the drone in a real-world building environment. The proposed method comprises five main steps: BIM to point cloud represented surface model generation, viewpoint determination, path planning, virtual simulation, and actual flight. In particular, the basic function could generate an inspection path for the whole building that guarantees high coverage rates, obstacle avoidance, and collision-free operation. The advanced function provides four types of building-specific decomposition strategies capable of overcoming the complex building structure shape, enabling reasonable inspection ranges, and enhancing computation efficiency. To simulate a drone’s inspection procedure and evaluate its performance in various contexts, an integrated platform based on a game engine and Airsim was developed. In addition, a case study was conducted to demonstrate the effectiveness of the method. The results demonstrate that the proposed BIM-enabled path planning can assist drones in conducting building inspection tasks with considerable applicability and flexibility.}
}
@article{ROCA2024107412,
title = {Co-evolving scenarios and simulated players to locate bugs that arise from the interaction of software models of video games},
journal = {Information and Software Technology},
volume = {169},
pages = {107412},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107412},
url = {https://www.sciencedirect.com/science/article/pii/S095058492400017X},
author = {Isis Roca and Óscar Pastor and Carlos Cetina and Lorena Arcega},
keywords = {Bug localization, Model interaction, Game software engineering, Search-based software engineering, Model-driven engineering},
abstract = {Context:
Game Software Engineering (GSE) is a field that focuses on developing and maintaining the software part of video games. A key component of video game development is the utilization of game engines, with many engines using software models to capture various aspects of the game.
Objective:
A challenge that GSE faces is the localization of bugs, mainly when working with large and intricated software models. Additionally, the interaction between software models (i.e. bosses, enemies, or environmental elements) during gameplay is often a significant source of bugs. In response to this challenge, we propose a co-evolution approach for bug localization in the software models of video games, called CoEBA.
Methods:
The CoEBA approach leverages Search-Based Software Engineering (SBSE) techniques to locate bugs in software models while considering their interactions. We conducted an evaluation in which we applied our approach to a commercial video game, Kromaia. We compared our approach with a state-of-the-art baseline approach that relied on the bug localization approach used by Kromaia’s developers and a random search used as a sanity check.
Results:
Our co-evolution approach outperforms the baseline approach in precision, recall, and F-measure. In addition, to provide evidence of the significance of our results, we conducted a statistical analysis. that shows significant differences in precision and recall values.
Conclusion:
The proposed approach, CoEBA, which considers the interaction between software models, can identify and locate bugs that other bug localization approaches may have overlooked.}
}
@article{SIMONOV2019404,
title = {Applying Behavior characteristics to decision-making process to create believable game AI},
journal = {Procedia Computer Science},
volume = {156},
pages = {404-413},
year = {2019},
note = {8th International Young Scientists Conference on Computational Science, YSC2019, 24-28 June 2019, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.222},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931141X},
author = {Andrey Simonov and Aleksandr Zagarskikh and Victor Fedorov},
keywords = {Game artificial intelligence, Utility theory, Decision-making systems, Believable behavior},
abstract = {With the development of artificial intelligence in computer games the problem of creating characters with believable and diverse behavior to inhabit in-game worlds becomes more and more actual. A big number of required characters and high standards of a modern game artificial intelligence makes the problem even more complex. In this paper we propose a utility-based decision-making model which gives the possibility to generate characters with believable behavior. The believability of such characters comes from their decision-making process that takes into account not only assessment of game environment, but also their personal characteristics and social status. Designed model was used to generate AI driven characters for a development of player’s opponents with personality traits for a computer card strategy. The model was used to simulate human flows on main railroad hub of 2014 Winter Olympics in order to reveal areas where pedestrian flow should be controlled.}
}
@article{HERY2024920,
title = {The Development and Prototyping of Game Modeling At A Family Entertainment Center, Utilizing Web-Based Arduino Technology For Calculating CAGR},
journal = {Procedia Computer Science},
volume = {234},
pages = {920-927},
year = {2024},
note = {Seventh Information Systems International Conference (ISICO 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.080},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924004381},
author = { Hery and Jonathan Budiman and Andree E. Widjaja and Calandra A. Haryani and Riswan E. Tarigan},
keywords = {Arduino, Development, Family entertainment center, Prototyping, Web based},
abstract = {The family entertainment center serves as the primary entertainment intermediary, catering especially to young people and adults through various games, such as virtual reality and the latest entertainment revolution of 3D technology. In order to sustain growth, family entertainment centers must regularly invest in new machines. The process of financing these investments must be precise to streamline company costs and ensure visitor satisfaction. The leadership's decision-making factors for investments can be effectively supported through technology. Therefore, the objective of this research is to create a prototype modeling system that simplifies the ongoing installation process and integrates Compound Annual Growth Rate (CAGR) figures through Arduino, displaying them on a website. The end result of this research is a game prototype model for a family center that effectively assists managers in decision-making, particularly in evaluating machine ratings based on visitor transactions and revenue for each machine using the CAGR metric.}
}
@article{TONG201596,
title = {Rapid Deployment and Evaluation of Mobile Serious Games: A Cognitive Assessment Case Study},
journal = {Procedia Computer Science},
volume = {69},
pages = {96-103},
year = {2015},
note = {The 7th International Conference on Advances in Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915031749},
author = {Tiffany Tong and Victor Guana and Andrea Jovanovic and Fiona Tran and Golnaz Mozafari and Mark Chignell and Eleni Stroulia},
keywords = {cognitive assessments, human computer interaction, human factors, serious games, usability},
abstract = {Serious games are proposed as a more efficient and enjoyable way to carry out cognitive assessment. We compare prediction of cognitive ability with a purpose-built serious game and with a similar game built using a game engine. In an experiment conducted with 28 participants, performance on the two games is assessed relative to three cognitive abilities, using two different tablet sizes and two different input methods. The results for the game-engine variant were similar to the purpose-built game, where both games significantly predicted performance on the three cognitive abilities, and were sensitive to the effects of age. Performance on both games was not significantly affected by tablet size or input method. These results support earlier findings that serious games can provide valid cognitive assessment, and they show that game engines can be used to develop serious games for cognitive assessment, cost effectively and without loss of predictive validity.}
}
@article{HOLM2024106159,
title = {Virtual forests for decision support and stakeholder communication},
journal = {Environmental Modelling & Software},
volume = {180},
pages = {106159},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106159},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224002202},
author = {Stefan Holm and Janine Schweier},
keywords = {3D visualization, Digital twin, Forest growth simulator, Forest planning, Game engine, Unity},
abstract = {Challenges in forest management are increasing due to climate change and its associated risks. Considering the needs and demands of various stakeholders leads to more complex decision-making. The increasing amount and quality of available geographic, forest and individual tree data, the combination of this data, and the use of forest growth simulators make it possible to support forest managers in this decision-making process. Our aim was to develop a strong visualization instrument that can be used in both forest planning and stakeholder communication. We present a solution based on a game engine, where data from multiple sources (terrain data, satellite imagery, tree data) is combined into a virtual environment. The user can move freely inside this virtual forest, look at the forest from arbitrary perspectives, and observe its development over the years under different management scenarios. We demonstrate the usefulness of this approach with a study region in Switzerland.}
}
@article{ZHAO2020354,
title = {Virtual simulation experiment of the design and manufacture of a beer bottle-defect detection system},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {4},
pages = {354-367},
year = {2020},
note = {VR and experiment simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300553},
author = {Yuxiang Zhao and Xiaowei An and Nongliang Sun},
keywords = {Virtual simulation experiment, Beer bottle defect detection, Image processing, Training tool},
abstract = {Background
Machine learning-based beer bottle-defect detection is a complex technology that runs automatically; however, it consumes considerable memory, is expensive, and poses a certain danger when training novice operators. Moreover, some topics are difficult to learn from experimental lectures, such as digital image processing and computer vision. However, virtual simulation experiments have been widely used to good effect within education. A virtual simulation of the design and manufacture of a beer bottle-defect detection system will not only help the students to increase their image-processing knowledge, but also improve their ability to solve complex engineering problems and design complex systems.
Methods
The hardware models for the experiment (camera, light source, conveyor belt, power supply, manipulator, and computer) were built using the 3DS MAX modeling and animation software. The Unreal Engine 4 (UE4) game engine was utilized to build a virtual design room, design the interactive operations, and simulate the system operation.
Results
The results showed that the virtual-simulation system received much better experimental feedback, which facilitated the design and manufacture of a beer bottle-defect detection system. The specialized functions of the functional modules in the detection system, including a basic experimental operation menu, power switch, image shooting, image processing, and manipulator grasping, allowed students (or virtual designers) to easily build a detection system by retrieving basic models from the model library, and creating the beer-bottle transportation, image shooting, image processing, defect detection, and defective-product removal. The virtual simulation experiment was completed with image processing as the main body.
Conclusions
By mainly focusing on bottle mouthdefect detection, the detection system dedicates more attention to the user and the task. With more detailed tasks available, the virtual system will eventually yield much better results as a training tool for imageprocessing education. In addition, a novel visual perception-thinking pedagogical framework enables better comprehension than the traditional lecture-tutorial style.}
}
@article{CASASNOVAS2024107919,
title = {Experimental evaluation of interactive Edge/Cloud Virtual Reality gaming over Wi-Fi using unity render streaming},
journal = {Computer Communications},
volume = {226-227},
pages = {107919},
year = {2024},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2024.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0140366424002585},
author = {Miguel Casasnovas and Costas Michaelides and Marc Carrascosa-Zamacois and Boris Bellalta},
keywords = {Virtual Reality, Wi-Fi, Cloud gaming, Edge computing, Unity, WebRTC},
abstract = {Virtual Reality (VR) streaming enables end-users to seamlessly immerse themselves in interactive virtual environments using even low-end devices. However, the quality of the VR experience heavily relies on Wireless Fidelity (Wi-Fi) performance, since it serves as the last hop in the network chain. Our study delves into the intricate interplay between Wi-Fi and VR traffic, drawing upon empirical data and leveraging a Wi-Fi simulator. In this work, we further evaluate Wi-Fi’s suitability for VR streaming in terms of the Quality of Service (QoS) it provides. In particular, we employ Unity Render Streaming to remotely stream real-time VR gaming content over Wi-Fi 6 using Web Real-Time Communication (WebRTC), considering a server physically located at the network’s edge, near the end user. Our findings demonstrate the system’s sustained network performance, showcasing minimal round-trip time (RTT) and jitter at 60 and 90 frames per second (fps). In addition, we uncover the characteristics and patterns of the generated traffic streams, unveiling a distinctive video transmission approach inherent to WebRTC-based services: the systematic packetization of video frames (VFs) and their transmission in discrete batches at regular intervals, regardless of the targeted frame rate. This interval-based transmission strategy maintains consistent video packet delays across video frame rates but leads to increased Wi-Fi airtime consumption. Our results demonstrate that shortening the interval between batches is advantageous, as it enhances Wi-Fi efficiency and reduces delays in delivering complete frames.}
}
@article{SENESI2024831,
title = {User-Centred Product Design with Photorealistic Virtual Prototypes: A Case Study on Process Optimisation for Aesthetic Quality Enhancement},
journal = {Procedia Computer Science},
volume = {232},
pages = {831-838},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924000838},
author = {Paolo Senesi and Marco Mandolini and Barbara Lonzi and Riccardo Rosati},
keywords = {Generative Model, Texture Mapping, 3D Modelling, Virtual Prototyping},
abstract = {In Industry 4.0, companies must focus on human-centred design for a competitive edge. The 4USER project aims to establish a user-centred design method using an interactive Photorealistic Virtual Prototype (PVP) based on Extended Reality (XR) technology, objectifying customer requirements into technical specifications. The PVP overcomes limitations associated with traditional physical prototypes, serving as a quality benchmark for final products. The research focuses on a case study involving the development of sports rifles, emphasising the importance of aesthetic quality. The proposed semi-automatic process in Blender enables the generation of low-poly PVPs, incorporating hyper-realistic textures and high-frequency details. In particular, the overall process is composed of the following steps: i) wooden texture generation via the Wasserstein Generative Adversarial Network (WGAN); ii) model creation based on a low poly “Shrinkwrap Cage”; iii) integration of generated textures into the UV-mapped model. This approach accelerates the product development cycle, reduces costs, and facilitates efficient quality control.}
}
@article{DALEY2024100529,
title = {Wearables to detect independent variables, objective task performance, and metacognitive states},
journal = {Machine Learning with Applications},
volume = {15},
pages = {100529},
year = {2024},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2024.100529},
url = {https://www.sciencedirect.com/science/article/pii/S2666827024000057},
author = {Matthew S. Daley and Jeffrey B. Bolkhovsky and Rachel Markwald and Timothy Dunn},
keywords = {Wearable technologies, Predictive modeling, Metacognition, Supervised learning, Support vector machines},
abstract = {Wearable biometric tracking devices are becoming increasingly common, providing users with physiological metrics such as heart rate variability (HRV) and skin conductance. We hypothesize that these metrics can be used as inputs for machine learning models to detect independent variables, such as target prevalence or hours awake, objective task performance, and metacognitive states. Over the course of 1–25 h awake, 40 participants completed four sessions of a simulated mine hunting task while non-invasive wearables collected physiological and behavioral data. The collected data were used to generate multiple machine learning models to detect the independent variables of the experiment (e.g., time awake and target prevalence), objective task performance, or metacognitive states. The strongest generated model was the time awake detection model (area under the curve = 0.92). All other models performed much closer to chance (area under the curve = 0.57–0.66), suggesting the model architecture used in this paper can detect time awake but falls short in other domains.}
}
@article{MOHAMMADLANGARI2024102724,
title = {Improving the performance of RRT path planning of excavators by embedding heuristic rules},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102724},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102724},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003720},
author = {Seied {Mohammad Langari} and Faridaddin Vahdatikhaki and Amin Hammad},
keywords = {Earthwork, Excavator, Path Planning, Rapidly Exploring Random Tree},
abstract = {Improving the safety and productivity of earthwork operations is of paramount importance, especially in congested sites where collisions are more probable. Real-time Location Systems and Automated Machine Guidance and Control are expected to improve both the safety and productivity of earthwork operations by providing excavator operators with a higher level of support regarding the path planning of excavators based on site conditions. However, in spite of the large number of studies related to automated path planning of excavators using well-established algorithms from robotics, such as Rapidly-exploring Random Trees and Probabilistic Roadmaps, these studies do not fully consider the engineering constraints of the equipment and do not result in smooth and optimal paths that can be applied in practice. This paper aims to develop a more practical algorithm for the path planning of excavators by embedding heuristic rules and engineering constraints specific to excavators. The proposed method is implemented and tested in a game engine environment. The efficiency of the proposed method in generating a collision-free path, which is expected to result in improved productivity, is validated both quantitatively and visually. The comparative results with other recent and modified versions of the RRT algorithm show that the proposed algorithm is able to find a more realistic path in a shorter time.}
}
@article{ESFAHLANI201942,
title = {Mixed reality and remote sensing application of unmanned aerial vehicle in fire and smoke detection},
journal = {Journal of Industrial Information Integration},
volume = {15},
pages = {42-49},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X18300773},
author = {Shabnam Sadeghi Esfahlani},
keywords = {Fire detection, Autonomous flight, Crazyflie 2.0, Monocular camera, Computer vision},
abstract = {This paper proposes the development of a system incorporating inertial measurement unit (IMU), a consumer-grade digital camera and a fire detection algorithm simultaneously with a nano Unmanned Aerial Vehicle (UAV) for inspection purposes. The video streams are collected through the monocular camera and navigation relied on the state-of-the-art indoor/outdoor Simultaneous Localisation and Mapping (SLAM) system. It implements the robotic operating system (ROS) and computer vision algorithm to provide a robust, accurate and unique inter-frame motion estimation. The collected onboard data are communicated to the ground station and used the SLAM system to generate a map of the environment. A robust and efficient re-localization was performed to recover from tracking failure, motion blur, and frame lost in the data received. The fire detection algorithm was deployed based on the color, movement attributes, temporal variation of fire intensity and its accumulation around a point. The cumulative time derivative matrix was utilized to analyze the frame-by-frame changes and to detect areas with high-frequency luminance flicker (random characteristic). Color, surface coarseness, boundary roughness, and skewness features were perceived as the quadrotor flew autonomously within the clutter and congested area. Mixed Reality system was adopted to visualize and test the proposed system in a physical environment, and the virtual simulation was conducted through the Unity game engine. The results showed that the UAV could successfully detect fire and flame, autonomously fly towards and hover around it, communicate with the ground station and simultaneously generate a map of the environment. There was a slight error between the real and virtual UAV calibration due to the ground truth data and the correlation complexity of tracking real and virtual camera coordinate frames.}
}
@article{LI20221516,
title = {Non-photorealistic Visualization of 3D City Models using Visual Variables in Virtual Reality Environments},
journal = {Procedia Computer Science},
volume = {214},
pages = {1516-1521},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.338},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020506},
author = {Bingchan Li and Zhangsong Luo and Bo Mao},
keywords = {visual variable, 3D city model, non-photorealistic visualization, virtual reality},
abstract = {Visual variables are important factors for visualization in 2D maps, and the study of non-photorealistic visualization of 3D city models in virtual environments using visual variables is also necessary to represent the attributes of city such as energy consumption. This study proposed a set of visual variable mapping model based on 3D geographical environment and conducted user cognitive test. The goal of this research is to verify the applicability of visual variables in 3D space and explore the impact of disturbing visual variables on user cognition. The results show that visualization of compound visual variables in 3D space is more effective than single variable mapping, and it makes up for the shortcomings of single mapping. It is effective to study the visualization effect in three-dimensional geographical environment using virtual reality technology. The results from this case study provide guidance for 3D user-centric visualization technology and make contribute to the development of smart city.}
}
@article{SIMONOV2018453,
title = {Multi-agent crowd simulation on large areas with utility-based behavior models: Sochi Olympic Park Station use case},
journal = {Procedia Computer Science},
volume = {136},
pages = {453-462},
year = {2018},
note = {7th International Young Scientists Conference on Computational Science, YSC2018, 02-06 July2018, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.266},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918315710},
author = {Andrey Simonov and Aleksandr Lebin and Bogdan Shcherbak and Aleksandr Zagarskikh and Andrey Karsakov},
keywords = {Artificial intelligence, Crowd Simulation, Multi-agent modeling},
abstract = {Visualization of human flows and crowd behavior is a complex problem of multi-agent modeling. It can be applied to a various set of problems, from emergency case planning to city life visualization. In this paper, we propose a system to build composite behavior structures for models with a big number of agents. It is designed with combining game development technologies of creating artificial intelligence for ambient characters and traditional multi-agent modeling methods. The system was applied to simulate and to visualize human flows on Sochi Olympic Park station during 2014 Winter Olympics and nowadays in order to reveal areas where pedestrian flow should be controlled.}
}
@article{PEUHKURINEN2011645,
title = {Using RDF Data as Basis for 3D Window Management in Mobile Devices},
journal = {Procedia Computer Science},
volume = {5},
pages = {645-652},
year = {2011},
note = {The 2nd International Conference on Ambient Systems, Networks and Technologies (ANT-2011) / The 8th International Conference on Mobile Web Information Systems (MobiWIS 2011)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.07.084},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911004091},
author = {Antti Peuhkurinen and Tommi Mikkonen and Mikko Terho},
keywords = {Window Manager, Game Engine, 3D, RDF},
abstract = {Computer users are commonly familiar with the WIMP (Windows, Icons, Menus and Pointing device) paradigm that has been the dominant design for decades. Despite its common use, the WIMP paradigm has a fundamental problem: it clutters the precious screen space with a plethora of open windows. The cluttering becomes an even bigger problem when using mobile devices that have smaller screens than conventional computers. Furthermore, accuracy issues commonly arise with touch screens. In this paper, we are introducing a 3D window manager for mobile devices which aims at solving the above issues by providing simple yet powerful interaction mechanisms and graphics as well as the use of only RDF data instead of application specific content. For improved user experience the user interface has been designed with playful use in mind, and this gives the system unique look and feel.}
}
@article{CIRULIS2015199,
title = {Virtualization of Digitalized Cultural Heritage and Use Case Scenario Modeling for Sustainability Promotion of National Identity},
journal = {Procedia Computer Science},
volume = {77},
pages = {199-206},
year = {2015},
note = {ICTE in regional Development 2015 Valmiera, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.384},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915038946},
author = {Arnis Cirulis and Lucio Tommaso De Paolis and Mikheil Tutberidze},
keywords = {Visualization, Cultural heritage, Gamification, sustainability},
abstract = {Nowadays, in the digitalization era, it is becoming ever more pertinent to have reasonable use of digital content. This consideration should also apply to cultural heritage, not only conservation. In the context of globalization, smart actions for content use should be carried out to provide sustainability of national identities. In the last decades, new technologies have been developed and used for digital conservation in the form of three-dimensional computer models with varying size, starting from very small museum exhibits and ending with the largest cathedrals and castles. The benefits computer models are undeniable. By prototyping and reverse engineering, significant exhibits are developed providing possibilities not only to see, but also to hold in one's hands and interact thereby allowing a better understanding of historical events and their meaning. Three-dimensional visualization provides virtual tours in different places and in different times. Unfortunately, development of such content is very expensive. Furthermore, technologies for a successful immersion level are in the development phase. This applies to criteria of content quality and availability and functionality of interaction. It is also vital to understand what elements to virtualize and how it lines up with the provision of national identity. Competition and tourism promotion conditions are undoubtedly a significant driving force for technological development, but, from an identity sustainability point of view, it is important that they go hand in hand. The aim of this research is to develop a baseline design for a set of technologies and the use of virtual and augmented reality to find recommendations for the sustainability of the national identity of countries via the prism of cultural heritage. Thereby providing planned and global technological solutions that are not aimed towards individual museums and separate objects of cultural heritage but focused on the overall region in the specialisation of ancient sites.}
}
@article{YIANNAKOULIAS2024102142,
title = {Parameterizing agent-based models using an online game},
journal = {Computers, Environment and Urban Systems},
volume = {112},
pages = {102142},
year = {2024},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2024.102142},
url = {https://www.sciencedirect.com/science/article/pii/S0198971524000711},
author = {Niko Yiannakoulias and Michel Grignon and Tara Marshall},
keywords = {Research gaming, Agent-based models, Online data collection, Route choice},
abstract = {Agent-based models (ABMs) of human systems are often parameterized using real-world data. For some ABMs this is not possible because the reality upon which the models are based does not exist or is not generalizable from one setting to another. In this paper we implement an online decision game to parameterize an agent-based model of pedestrian and cyclist route choice decisions in a neighbourhood. Our conceptual framework is to use an experimental game to log decision-making behaviour, summarize this behaviour into a decision model, and then transfer this model to an ABM. The product of this framework is an ABM with agents informed by human decision making made within the game, rather than the real world. The results of our analysis suggest that the decision model is consistent with some general theory about decision making, but the ABM illustrates some unique and contextually specific patterns of flow. ABMs parameterized with game data may be useful for forecasting the effects of change on urban transportation infrastructure.}
}
@article{IKENO2021101380,
title = {An enhanced 3D model and generative adversarial network for automated generation of horizontal building mask images and cloudless aerial photographs},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101380},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101380},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001336},
author = {Kazunosuke Ikeno and Tomohiro Fukuda and Nobuyoshi Yabuki},
keywords = {Deep learning, Generative adversarial network, Semantic segmentation, Mask image, Training data, Urban planning and design},
abstract = {Information extracted from aerial photographs is widely used in the fields of urban planning and design. An effective method for detecting buildings in aerial photographs is to use deep learning to understand the current state of a target region. However, the building mask images used to train the deep learning model must be manually generated in many cases. To overcome this challenge, a method has been proposed for automatically generating mask images by using textured three-dimensional (3D) virtual models with aerial photographs. Some aerial photographs include clouds, which degrade image quality. These clouds can be removed by using a generative adversarial network (GAN), which leads to improvements in training quality. Therefore, the objective of this research was to propose a method for automatically generating building mask images by using 3D virtual models with textured aerial photographs. In this study, using GAN to remove clouds in aerial photographs improved training quality. A model trained on datasets generated by the proposed method was able to detect buildings in aerial photographs with IoU = 0.651.}
}
@article{ALCE201435,
title = {Feasibility Study of Ubiquitous Interaction Concepts},
journal = {Procedia Computer Science},
volume = {39},
pages = {35-42},
year = {2014},
note = {The 6th international conference on Intelligent Human Computer Interaction, IHCI 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914014252},
author = {Günter Alce and Lars Thern and Klas Hermodsson and Mattias Wallergård},
keywords = {Natural User Interfaces, Virtual and Augmented Reality, Ubiquitous computing},
abstract = {There are all sorts of consumer electronics in a home environment. Using “apps” to interact with each device is neither feasible nor practical in an ubicomp future. Prototyping and evaluating interaction concepts for this future is a challenge. This paper proposes four concepts for device discovery and device interaction implemented in a virtual environment. The interaction concepts were compared in a controlled experiment for evaluation and comparison. Some statistically significant differences and subjective preferences could be observed in the quantitative and qualitative data respectively. Overall, the results indicate that the proposed interaction concepts were found natural and easy to use.}
}
@article{KIDO2021101281,
title = {Assessing future landscapes using enhanced mixed reality with semantic segmentation by deep learning},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101281},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101281},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000367},
author = {Daiki Kido and Tomohiro Fukuda and Nobuyoshi Yabuki},
keywords = {Mixed reality, Dynamic occlusion handling, Landscape index estimation, Landscape design, Video communication, Deep learning},
abstract = {Architecture, engineering, and construction projects need to be promoted in harmony with the natural environment and with the aim of preserving people’s living environment. At the planning and design stage, decision-makers and stakeholders share and assess landscape images during and after construction in order to avoid as much uncertainty as possible when performing environmental impact assessment. Given the lack of a standard visualization method for future landscapes that do not yet exist, mixed reality (MR), which overlays virtual content onto a real scene, has attracted attention in the field of landscape design. One challenge in MR is occlusion, which occurs when virtual objects obscure physical objects that should be rendered in the foreground. In MR-based landscape visualization, the distance between the MR camera and real objects located in front of the virtual objects might vary and might be large, causing difficulty for existing occlusion handling methods. In the process of landscape design, an evidence-based approach has also become important. Landscape index estimation using semantic segmentation by deep learning, which can recognize the surrounding environment, has been actively studied for landscape assessment. In this study, semantic segmentation by deep learning was integrated into an MR system to enable dynamic occlusion handling and landscape index estimation for both existing and designed landscape assessment. This system can be operated on a mobile device with video communication over the internet by connecting to real-time semantic segmentation on a high-performance personal computer. The applicability of the developed system is demonstrated through accuracy verification and case studies.}
}
@article{REINALDO2021773,
title = {Prototyping "Color in Life" EduGame for Dichromatic Color Blind Awareness},
journal = {Procedia Computer Science},
volume = {179},
pages = {773-780},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921000995},
author = {Ivan Reinaldo and Nadia Sarah Pulungan and Herru Darmadi},
keywords = {color blind, educational game, gaming experience, mobile game},
abstract = {The purpose of this research was to escalate players’ knowledge on color blindness by designing an educational video game which design was oriented to dichromatism color blind. The topic selection was based on the lack of players’ deeper knowledge on color blindness. The graphic and gameplay selection on this research was adjusted to the chosen color blind category. Research methods were conducted by analysis, development, and evaluation. Analysis was done by questionnaire. Development was done by game design document, UML, storyboard, and was implemented using Unity Game Engine. Evaluation on 35 players, which are 32 with normal eyes and 3 with color blindness, was done by two approaches, which are t-test and questionnaire. The result of t-test was t(34) = -7.704, p < 0.05 and Enjoyment score on CEGE is 0.763 for normal eyes and 0.651 for colorblind. To conclude, there was an improvement on knowledge from the video game and the design was enjoyable.}
}
@article{TAI2021274,
title = {Augmented reality-based visual-haptic modeling for thoracoscopic surgery training systems},
journal = {Virtual Reality & Intelligent Hardware},
volume = {3},
number = {4},
pages = {274-286},
year = {2021},
note = {Special Issue on Virtual reality and Augmented Reality in Medical Simulation},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579621000528},
author = {Yonghang Tai and Junsheng Shi and Junjun Pan and Aimin Hao and Victor Chang},
keywords = {Augmented reality, VATS, Surgery training, XPBD},
abstract = {Background
Compared with traditional thoracotomy, video-assisted thoracoscopic surgery (VATS) has less minor trauma, faster recovery, higher patient compliance, but higher requirements for surgeons. Virtual surgery training simulation systems are important and have been widely used in Europe and America. Augmented reality (AR) in surgical training simulation systems significantly improve the training effect of virtual surgical training, although AR technology is still in its initial stage. Mixed reality has gained increased attention in technology-driven modern medicine but has yet to be used in everyday practice.
Methods
This study proposed an immersive AR lobectomy within a thoracoscope surgery training system, using visual and haptic modeling to study the potential benefits of this critical technology. The content included immersive AR visual rendering, based on the cluster-based extended position-based dynamics algorithm of soft tissue physical modeling. Furthermore, we designed an AR haptic rendering systems, whose model architecture consisted of multi-touch interaction points, including kinesthetic and pressure-sensitive points. Finally, based on the above theoretical research, we developed an AR interactive VATS surgical training platform.
Results
Twenty-four volunteers were recruited from the First People's Hospital of Yunnan Province to evaluate the VATS training system. Face, content, and construct validation methods were used to assess the tactile sense, visual sense, scene authenticity, and simulator performance.
Conclusions
The results of our construction validation demonstrate that the simulator is useful in improving novice and surgical skills that can be retained after a certain period of time. The video-assisted thoracoscopic system based on AR developed in this study is effective and can be used as a training device to assist in the development of thoracoscopic skills for novices.}
}
@article{UNTERGUGGENBERGER2023155,
title = {Vulkan all the way: Transitioning to a modern low-level graphics API in academia},
journal = {Computers & Graphics},
volume = {111},
pages = {155-165},
year = {2023},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2023.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0097849323000249},
author = {Johannes Unterguggenberger and Bernhard Kerbl and Michael Wimmer},
keywords = {Real-time rendering, GPU, Graphics API, Teaching, Vulkan, Programming framework},
abstract = {For over two decades, the OpenGL API provided users with the means for implementing versatile, feature-rich, and portable real-time graphics applications. Consequently, it has been widely adopted by practitioners and educators alike and is deeply ingrained in many curricula that teach real-time graphics for higher education. Over the years, the architecture of graphics processing units (GPUs) incrementally diverged from OpenGL’s conceptual design. The more recently introduced Vulkan API provides a more modern, fine-grained approach for interfacing with the GPU, which allows a high level of controllability and, thereby, deep insights into the inner workings of modern GPUs. This property makes the Vulkan API especially well suitable for teaching graphics programming in university education, where fundamental knowledge shall be conveyed. Hence, it stands to reason that educators who have their students’ best interests at heart should provide them with corresponding lecture material. However, Vulkan is notoriously verbose and rather challenging for first-time users, thus transitioning to this new API bears a considerable risk of failing to achieve expected teaching goals. In this paper, we document our experiences after teaching Vulkan in both introductory and advanced graphics courses side-by-side with conventional OpenGL. A collection of surveys enables us to draw conclusions about perceived workload, difficulty, and students’ acceptance of either approach. In doing so, we identify suitable conditions and recommendations for teaching Vulkan to both undergraduate and graduate students.}
}
@article{HU202120,
title = {Asyncflow: A visual programming tool for game artificial intelligence},
journal = {Visual Informatics},
volume = {5},
number = {4},
pages = {20-25},
year = {2021},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X21000498},
author = {Zhipeng Hu and Changjie Fan and Qiwei Zheng and Wei Wu and Bai Liu},
keywords = {Visual programming, Flowchart, Game Artificial Intelligence},
abstract = {Visual programming tools are widely applied in the game industry to assist game designers in developing game artificial intelligence (game AI) and gameplay. However, testing multiple game engines is a time-consuming operation, which degrades development efficiency. To provide an asynchronous platform for game designers, this paper introduces Asyncflow, an open-source visual programming solution. It consists of a flowchart maker for game logic explanation and a runtime framework integrating an asynchronous mechanism based on an event-driven architecture. Asyncflow supports multiple programming languages and can be easily embedded in various game engines to run flowcharts created by game designers.}
}
@article{PONSEN200759,
title = {Knowledge acquisition for adaptive game AI},
journal = {Science of Computer Programming},
volume = {67},
number = {1},
pages = {59-75},
year = {2007},
note = {Special Issue on Aspects of Game Programming},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2007.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167642307000548},
author = {Marc Ponsen and Pieter Spronck and Héctor Muñoz-Avila and David W. Aha},
keywords = {Computer games, Artificial intelligence, Real-time strategy, Reinforcement learning, Dynamic scripting, Evolutionary algorithm, Knowledge acquisition},
abstract = {Game artificial intelligence (AI) controls the decision-making process of computer-controlled opponents in computer games. Adaptive game AI (i.e., game AI that can automatically adapt the behaviour of the computer players to changes in the environment) can increase the entertainment value of computer games. Successful adaptive game AI is invariably based on the game’s domain knowledge. We show that an offline evolutionary algorithm can learn important domain knowledge in the form of game tactics (i.e., a sequence of game actions) for dynamic scripting, an offline algorithm inspired by reinforcement learning approaches that we use to create adaptive game AI. We compare the performance of dynamic scripting under three conditions for defeating non-adaptive opponents in a real-time strategy game. In the first condition, we manually encode its tactics. In the second condition, we manually translate the tactics learned by the evolutionary algorithm, and use them for dynamic scripting. In the third condition, this translation is automated. We found that dynamic scripting performs best under the third condition, and both of the latter conditions outperform manual tactic encoding. We discuss the implications of these results, and the performance of dynamic scripting for adaptive game AI from the perspective of machine learning research and commercial game development.}
}
@article{IMBERT2013364,
title = {Adding Physical Properties to 3D Models in Augmented Reality for Realistic Interactions Experiments},
journal = {Procedia Computer Science},
volume = {25},
pages = {364-369},
year = {2013},
note = {2013 International Conference on Virtual and Augmented Reality in Education},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.11.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913012490},
author = {Nicolas Imbert and Frederic Vignat and Charlee Kaewrat and Poonpong Boonbrahm},
keywords = {Augmented Reality, Physical Properties, Realistic Interaction},
abstract = {Augmented Reality is the combination of virtual objects (created by computer i.e. video, texts or 3D computer models) overlay on top of real world image. Applications of Augmented Reality can be ranged from advertising, edutainment, education, engineering, medicine to industrial manufacturing. In basic applications, like in advertisement or games, users only see the actions or interact with part of the screen designed for initiate some actions. In order to make users have realistic experiences, the interaction amongst virtual objects in Augmented Reality must be restricted to the law of Physics. Virtual objects can have their own dimensions, volumes or weights. When interaction between virtual objects occurred, the collision for example, they should not penetrate each other. The objects will react to each other by the law of Physics. With this concept, all kinds of experiments can be tested or practiced without spending a lot of fortunes with the real setup ranging from simple science experiment, medical training or even assembly process of equipment. In this research, Unity 3D game engine is used on Vuforia platform. Unity is a fully integrated development engine for creating games and other interactive 3D content and Vuforia platform make it possible to write a single native application that runs on almost all smartphones and tablets. To test the concept, 8 pieces of virtual 3D puzzle modules were created using 8 markers. Each virtual module was assigned with physical properties such dimensions, shapes and positions. When assemble the puzzle, each piece of the marker must be able to move around so that the virtual modules can fit to each other. By lifting and rotating the markers, the virtual module will snap with the other proper virtual part, forming virtual 3D puzzle. The virtual module will not penetrate each other because they have their own territory due to their dimensions. With this experiment, users will have a realistic feeling on assembling the virtual model. The concept can be implemented for experiments that are dangerous or expensive to setup. Experiments related to interaction between objects such as physics and chemistry experiments, engineering and medical training are the main targets for using this kind of technology.}
}
@article{PLAVSIC2024100,
title = {VR-based digital twin for remote monitoring of mining equipment: Architecture and a case study},
journal = {Virtual Reality & Intelligent Hardware},
volume = {6},
number = {2},
pages = {100-112},
year = {2024},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2023.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096579623000852},
author = {Jovana Plavšić and Ilija Mišković},
keywords = {Virtual reality, Digital twin, Condition monitoring, Mining equipment},
abstract = {Background
Traditional methods for monitoring mining equipment rely primarily on visual inspections, which are time-consuming, inefficient, and hazardous. This article introduces a novel approach to monitoring mission-critical systems and services in the mining industry by integrating virtual reality (VR) and digital twin (DT) technologies. VR-based DTs enable remote equipment monitoring, advanced analysis of machine health, enhanced visualization, and improved decision making.
Methods
This article presents an architecture for VR-based DT development, including the developmental stages, activities, and stakeholders involved. A case study on the condition monitoring of a conveyor belt using real-time synthetic vibration sensor data was conducted using the proposed methodology. The study demonstrated the application of the methodology in remote monitoring and identified the need for further development for implementation in active mining operations. The article also discusses interdisciplinarity, choice of tools, computational resources, time and cost, human involvement, user acceptance, frequency of inspection, multiuser environment, potential risks, and applications beyond the mining industry.
Results
The findings of this study provide a foundation for future research in the domain of VR-based DTs for remote equipment monitoring and a novel application area for VR in mining.}
}